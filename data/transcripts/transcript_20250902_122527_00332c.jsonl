{"ts": "00:00", "speaker": "I", "text": "Können Sie zu Beginn bitte kurz schildern, wie Sie aktuell in das Orion Edge Gateway Projekt eingebunden sind?"}
{"ts": "01:45", "speaker": "E", "text": "Ja, gerne. Ich bin als Senior DevOps Engineer direkt in der Build-Phase von P-ORI eingebunden. Mein Fokus liegt auf der technischen Umsetzung des API Gateways, inklusive Rate Limiting, Auth-Integration und CI/CD-Pipelines. Ich arbeite eng mit dem Security-Team zusammen, um die SLA-ORI-02 Vorgaben einzuhalten, insbesondere was Latenz und Verfügbarkeit betrifft."}
{"ts": "05:10", "speaker": "I", "text": "Welche Hauptaufgaben übernehmen Sie aktuell, bezogen auf diese Build-Phase?"}
{"ts": "06:50", "speaker": "E", "text": "Derzeit implementiere ich Infrastructure-as-Code Templates für die Gateway-Cluster in zwei Regionen, konfiguriere das mTLS Setup und baue die Blue/Green Deployment-Pipeline nach Runbook RB-GW-011. Außerdem definiere ich die Observability-Metriken, um unser p95 Latenz-Ziel zu überwachen."}
{"ts": "12:30", "speaker": "I", "text": "Können Sie etwas näher auf die eingesetzten IaC-Tools eingehen und warum Sie diese gewählt haben?"}
{"ts": "14:20", "speaker": "E", "text": "Wir nutzen primär Terraform, weil wir damit Multi-Cloud-Templates konsistent umsetzen können. Für die Kubernetes-Teile greifen wir zusätzlich auf Helm Charts zurück. Die Entscheidung basiert auf unserer Standardisierung aus dem Poseidon Networking Projekt, damit wir die gleichen Module für Netzwerkrichtlinien wiederverwenden können."}
{"ts": "20:05", "speaker": "I", "text": "Wie genau haben Sie denn die Blue/Green Deployment Pipeline implementiert?"}
{"ts": "22:15", "speaker": "E", "text": "Wir haben in GitLab CI mehrere Stages, die Blue- und Green-Cluster parallel provisionieren. Über ein Feature-Flag im Config-Repo wird dann der Traffic-Switch gesteuert. Das Ganze ist in RB-GW-011 dokumentiert, inklusive Rollback-Schritten und Health-Checks, die via Prometheus abgefragt werden."}
{"ts": "30:40", "speaker": "I", "text": "Kommen wir zum Rate Limiting: Wie wird das in der Architektur technisch realisiert?"}
{"ts": "32:25", "speaker": "E", "text": "Wir setzen ein Token-Bucket-Algorithmus-Plugin im Gateway ein, das in Redis die Token Counts pro API-Key speichert. Der Vorteil ist, dass wir so horizontal skalieren können ohne State im Gateway zu halten. Die Konfiguration ist versioniert und Teil der IaC-Pipeline."}
{"ts": "40:15", "speaker": "I", "text": "Wie sieht es mit der Authentifizierungsintegration aus?"}
{"ts": "42:05", "speaker": "E", "text": "Hier greifen wir auf das Aegis IAM zu. Das Gateway validiert bei jeder Anfrage das JWT gegen den Public Key des IAM. Zusätzlich erzwingen wir mTLS für Service-to-Service Communication. Wir mussten dafür eine eigene Cert-Rotation-Logik implementieren, weil die Standardlösung nicht mit unserem Blue/Green-Modell harmonierte."}
{"ts": "50:45", "speaker": "I", "text": "Apropos mTLS: Können Sie den Debug-Prozess für den MTLS Handshake Bug, Ticket GW-4821, erläutern?"}
{"ts": "53:10", "speaker": "E", "text": "Ja, das war knifflig. Wir hatten sporadische Handshake-Fehler zwischen Gateway und internen Services. Im Debugging haben wir mit OpenSSL und Jaeger-Tracing den Verbindungsaufbau analysiert. Es stellte sich heraus, dass ein veraltetes Zwischenzertifikat im Green-Cluster lag, weil der Rotationsjob dort nicht lief. Wir haben daraufhin einen zusätzlichen Check in die Pipeline eingebaut, um Cert-Stände vor Traffic-Switch zu prüfen."}
{"ts": "60:35", "speaker": "I", "text": "Wie ist aktuell die Schnittstelle zwischen Orion Edge Gateway und Aegis IAM gestaltet und welche Abhängigkeiten bestehen?"}
{"ts": "63:00", "speaker": "E", "text": "Die Schnittstelle läuft über eine REST-basierte Introspection-API. Änderungen im IAM, wie neue Claims-Struktur, müssen wir im Gateway-Mapping berücksichtigen. Wir haben nach Lessons Learned aus Helios Datalake die API-Versionierung strikt eingeführt, um Deployments entkoppeln zu können. Das verhindert, dass eine Änderung im IAM sofort das Gateway lahmlegt."}
{"ts": "90:00", "speaker": "I", "text": "Sie hatten vorhin die Schnittstellen zum Aegis IAM erwähnt. Mich würde jetzt interessieren: Wie genau beeinflusst eine Änderung im IAM die Deployment-Pipeline des Gateways?"}
{"ts": "90:08", "speaker": "E", "text": "Also, wenn im IAM ein neues Auth-Policy-Format eingeführt wird, müssen wir im Gateway die Policy-Parser innerhalb des Auth-Adapters anpassen. Das hat direkte Folgen auf die CI Tests – Stage 'integration-auth' schlägt dann eventuell fehl, wenn wir den Parser nicht synchron updaten."}
{"ts": "90:28", "speaker": "I", "text": "Verstehe. Gibt es für solche synchronen Updates einen definierten Prozess oder ist das eher ad hoc?"}
{"ts": "90:34", "speaker": "E", "text": "Wir haben da tatsächlich einen Runbook-Eintrag, RB-IAM-DEP-07, der eine Sequenz beschreibt: erst Merge Request im IAM, dann Trigger eines Pre-Deployment Jobs im Gateway, der Dummy-Tokens gegen die neue Policy validiert."}
{"ts": "90:55", "speaker": "I", "text": "Und wie testen Sie in diesem Pre-Deployment Job, dass auch das Rate Limiting noch konsistent funktioniert?"}
{"ts": "91:01", "speaker": "E", "text": "Wir haben Synthetic Load Tests, die parallel Auth-Handshake und Request Burst fahren. Das Script 'synthetic_burst_auth.py' simuliert 500 RPS mit validen und invaliden Tokens. So sehen wir, ob der Token-Parsing-Overhead uns über p95 >120ms bringt."}
{"ts": "91:21", "speaker": "I", "text": "Spannend. Inwiefern fließen Lessons Learned aus dem Helios Datalake in diese Tests ein?"}
{"ts": "91:27", "speaker": "E", "text": "Im Helios Projekt hatten wir das Problem, dass Streams mit hohen Latenzen erst spät auffielen. Daher haben wir im Gateway die Observability so erweitert, dass wir Latenzverteilungen per mTLS-Tagging aufschlüsseln. Das haben wir direkt aus Helios' 'latency buckets' übernommen."}
{"ts": "91:48", "speaker": "I", "text": "Das heißt, Sie können Latenzprobleme differenziert nach Auth-Mechanismus analysieren?"}
{"ts": "91:54", "speaker": "E", "text": "Genau. Wir trennen mTLS, JWT-only und API-Key Requests. So können wir erkennen, ob z.B. eine neue mTLS-Handshake-Implementierung wie bei GW-4821 wieder Probleme macht."}
{"ts": "92:10", "speaker": "I", "text": "GW-4821 war der Handshake Bug, oder? Wie sind Sie da beim Debugging vorgegangen?"}
{"ts": "92:16", "speaker": "E", "text": "Wir haben zuerst mit OpenSSL-Trace den TLS-Flow mitgeschnitten. Dann im Gateway-Log nach 'handshake_complete' Events gesucht. Es stellte sich heraus, dass beim Reconnect der Session Cache nicht invalidiert wurde – gefixt mit Patch in commit GWB-221a."}
{"ts": "92:36", "speaker": "I", "text": "Kommen wir zu Cross-Projekt Abhängigkeiten: Gab es auch Einflüsse aus Poseidon Networking, die hier relevant waren?"}
{"ts": "92:42", "speaker": "E", "text": "Ja, Poseidon hat uns eine Library für adaptive Connection Pool Sizes geliefert, die wir im Gateway übernommen haben. Dadurch stabilisieren wir Latenzen bei schwankender Netzlast."}
{"ts": "93:00", "speaker": "I", "text": "Wie dokumentieren Sie solche Übernahmen?"}
{"ts": "93:05", "speaker": "E", "text": "Wir schreiben ein RFC-Dokument – z.B. RFC-GW-ADPCON-01 – mit Motivation, Implementierungsdetails und Risikoeinschätzung. Das wird im Confluence unter 'Cross Project Integrations' abgelegt."}
{"ts": "102:00", "speaker": "I", "text": "Wir sind jetzt in der Endphase des Gesprächs. Mich interessiert, welche Risiken Sie aktuell in der Build-Phase des Orion Edge Gateway sehen, vielleicht auch welche, die nicht explizit in unseren Risikoregistersheets stehen."}
{"ts": "102:10", "speaker": "E", "text": "Ein wesentliches Risiko ist tatsächlich die noch nicht ausreichend getestete Interoperabilität zwischen unserem Rate Limiting Modul und der geplanten mTLS-Erweiterung. Laut unserem internen Testticket GW-5078 gab es in Staging sporadische Abbrüche, wenn gleichzeitig hohe Request-Raten und mTLS Handshakes auftraten."}
{"ts": "102:25", "speaker": "I", "text": "Das klingt nach einem klassischen Performance-vs-Security-Trade-off. Wie haben Sie das in Ihren Entscheidungen berücksichtigt?"}
{"ts": "102:34", "speaker": "E", "text": "Ja, genau. Wir haben das in RFC-OG-014 festgehalten. Dort haben wir den Trade-off beschrieben: entweder striktes mTLS mit längeren Handshake-Zeiten, oder ein optimiertes Setup mit vorausgeladenen Session Keys. Wir haben uns für Letzteres entschieden, weil die p95-Latenz laut SLA-ORI-02 unter 120 ms bleiben muss."}
{"ts": "102:50", "speaker": "I", "text": "Wie wird so etwas dokumentiert, damit spätere Teams verstehen, warum diese Entscheidung fiel?"}
{"ts": "102:57", "speaker": "E", "text": "Neben dem RFC selbst pflegen wir im Projektwiki eine Decision Log Tabelle. Da verlinken wir auf die Messdaten, z. B. aus der Observability-Integration mit Nimbus Metrics, und auf die Runbooks, etwa RB-SEC-077 für das Session Key Preloading."}
{"ts": "103:15", "speaker": "I", "text": "Gab es Entscheidungen, die Sie heute rückblickend anders treffen würden?"}
{"ts": "103:20", "speaker": "E", "text": "Vielleicht die frühe Wahl des JSON-basierten Inter-Services-Protokolls. Im Kontext mit dem Aegis IAM wäre ein binäres Format effizienter gewesen. Wir haben erst durch die Helios-Datalake-Latenzanalysen gemerkt, dass unser Parser in Lastspitzen ein Bottleneck ist."}
{"ts": "103:38", "speaker": "I", "text": "Wie würden Sie so einen Wechsel jetzt angehen, ohne das Build zu verzögern?"}
{"ts": "103:45", "speaker": "E", "text": "Ich würde ein Feature Flag einführen und den binären Codec zunächst nur für nicht-kritische Endpunkte aktivieren. Dann parallel im Staging mit synthetischem Traffic, wie im Performance-Runbook RB-PERF-022 beschrieben, validieren."}
{"ts": "104:00", "speaker": "I", "text": "Und wie binden Sie Lessons Learned aus Poseidon Networking konkret ein, wenn es um solche Änderungen geht?"}
{"ts": "104:08", "speaker": "E", "text": "Bei Poseidon haben wir damals Routing-Änderungen ohne ausreichendes Canary Testing ausgerollt – das wollen wir vermeiden. Deshalb nutzen wir jetzt immer Blue/Green nach RB-GW-011 und observieren beide Umgebungen parallel, bevor wir Umschalten."}
{"ts": "104:25", "speaker": "I", "text": "Sehen Sie noch weitere Risiken im Kontext der Cross-Projekt-Abhängigkeiten?"}
{"ts": "104:32", "speaker": "E", "text": "Ja, Änderungen im Aegis IAM Schema könnten Auth-Flows brechen. Das Monitoring-Alert NR-AL-319 aus der letzten Woche zeigte uns, dass ein Minor Release im IAM den Token-Endpoint unbemerkt für 12 Sekunden unresponsive machte."}
{"ts": "104:48", "speaker": "I", "text": "Wie mitigieren Sie so etwas?"}
{"ts": "104:55", "speaker": "E", "text": "Wir haben jetzt ein Pre-Deploy Hook im Gateway-Pipeline-Skript, das gegen die IAM-Staging-API Smoke-Tests ausführt. Falls POL-SEC-001-Checks oder Liveness-Tests fehlschlagen, wird das Deployment blockiert und ein Incident wie INC-OG-221 automatisch erstellt."}
{"ts": "118:00", "speaker": "I", "text": "Sie hatten eben die Risiken und Trade-offs angesprochen. Mich würde interessieren, wie Sie konkret die offenen Punkte aus RFC-OG-014 weiterverfolgen?"}
{"ts": "118:07", "speaker": "E", "text": "Wir haben dafür ein internes Tracking in unserem Confluence-Bereich 'OG-RFC-Board'. Jeder offene Punkt aus RFC-OG-014 hat ein eigenes Sub-Ticket im Jira-Board OG-BUILD, z.B. OG-BUILD-221 für die TLS Cipher Suite Auswahl. Die werden wöchentlich im Architektur-Review abgeglichen."}
{"ts": "118:18", "speaker": "I", "text": "Und wie stellen Sie sicher, dass diese Punkte nicht untergehen, wenn parallel andere Projekte wie Nimbus neue Anforderungen stellen?"}
{"ts": "118:25", "speaker": "E", "text": "Wir haben einen Dependency-Check im CI/CD, der Änderungen an externen Schnittstellen wie Nimbus Observability API erkennt. Ein Merge-Blocker in der Pipeline verweist dann auf ein Review nach Runbook RB-DEP-042, bevor Deployments passieren."}
{"ts": "118:39", "speaker": "I", "text": "Können Sie ein Beispiel geben, wann dieser Merge-Blocker zuletzt ausgelöst hat?"}
{"ts": "118:44", "speaker": "E", "text": "Ja, vor drei Wochen bei OG-BUILD-237. Nimbus hatte die Payload-Struktur für Latenzmetriken geändert. Unser Parser im Gateway hat das Schema nicht mehr validiert, der Blocker griff, und wir haben den Parser innerhalb eines Tages angepasst."}
{"ts": "118:57", "speaker": "I", "text": "Das klingt nach einer gut eingespielten Pipeline. Wie binden Sie dabei Lessons Learned aus Helios und Poseidon konkret ein?"}
{"ts": "119:03", "speaker": "E", "text": "Aus Helios haben wir mitgenommen, dass Schema-Änderungen immer versioniert und rückwärtskompatibel sein sollten. Aus Poseidon kam die Erkenntnis, dass Networking-Policies früh in der Testumgebung simuliert werden müssen. Beides ist jetzt fester Bestandteil unseres Stage-Gate im Build."}
{"ts": "119:18", "speaker": "I", "text": "Sie erwähnten RB-SEC-077 im Kontext Security. Wie fließt das hier mit ein?"}
{"ts": "119:24", "speaker": "E", "text": "RB-SEC-077 beschreibt den Prozess für Security Regression Tests. Wir triggern diesen Job vor jedem Blue/Green Switch, speziell um mTLS, Auth-Flows und Rate Limiting Policies erneut zu prüfen."}
{"ts": "119:36", "speaker": "I", "text": "Gab es in letzter Zeit Findings aus diesen Regression Tests?"}
{"ts": "119:40", "speaker": "E", "text": "Ein kleineres Finding: Bei OG-BUILD-241 wurde ein neues Auth-Plugin integriert, das versehentlich zu breite Berechtigungen hatte. RB-SEC-077-Testfall 'LeastPrivilege-AuthZ-Check' hat das gefunden, und wir haben den Scope im Plugin sofort angepasst."}
{"ts": "119:56", "speaker": "I", "text": "Wie dokumentieren Sie solche Incidents für die Zukunft?"}
{"ts": "120:01", "speaker": "E", "text": "Wir legen pro Incident einen Eintrag im 'OG-Lessons' Wiki an, verlinken das zu den entsprechenden Tickets und ergänzen die betroffenen Runbook-Abschnitte. So war es auch bei diesem Auth-Plugin; RB-SEC-077 hat jetzt einen zusätzlichen Pre-Check Step."}
{"ts": "120:14", "speaker": "I", "text": "Wenn Sie nach vorne blicken: wo sehen Sie den größten Hebel für Risikominimierung im Rest der Build-Phase?"}
{"ts": "120:20", "speaker": "E", "text": "Der größte Hebel liegt aktuell im automatisierten Contract Testing zwischen Gateway und Aegis IAM. Damit können wir Breaking Changes früh erkennen und verhindern, dass sie in die Produktions-Lanes gelangen."}
{"ts": "126:00", "speaker": "I", "text": "Lassen Sie uns beim Thema Observability noch etwas tiefer gehen – wie genau setzen Sie das Cross-Projekt Monitoring vom Nimbus-Stack im Orion Edge Gateway um?"}
{"ts": "126:20", "speaker": "E", "text": "Wir haben den Nimbus-Collector als Sidecar in den Gateway-Pods integriert. Die Konfiguration basiert auf RB-MON-042, angepasst für die Orion-spezifischen Labels und p95 Monitoring. Dadurch können wir die Metriken ohne zusätzliche Latenz an die zentrale TSDB pushen."}
{"ts": "126:48", "speaker": "I", "text": "Gab es bei der Integration Komplikationen mit den bestehenden Helios-Datalake Pipelines?"}
{"ts": "127:05", "speaker": "E", "text": "Ja, die Multi-Hop-Korrelation war tricky. Wir mussten den Trace-Header-Standard aus Helios übernehmen, damit die Requests im Datalake korrekt korreliert werden. Ohne das hätten wir nur isolierte Gateway-Traces gehabt, die nicht mit den Backend-Events verknüpft sind."}
{"ts": "127:32", "speaker": "I", "text": "Und wie wirkt sich das auf Ihre Fähigkeit aus, die SLA-ORI-02 zu erfüllen?"}
{"ts": "127:49", "speaker": "E", "text": "Direkt positiv. Wenn wir die End-to-End-Traces sehen, können wir schneller Engpässe identifizieren. In einem Fall (Ticket GW-4975) konnten wir so einen falsch konfigurierten Rate-Limiter in einem Upstream-Service finden und die Latenz um 35 ms senken."}
{"ts": "128:15", "speaker": "I", "text": "Zum Thema Rate Limiting: setzen Sie auf statische oder adaptive Policies?"}
{"ts": "128:30", "speaker": "E", "text": "Eine Kombination. Statisch für kritische APIs, adaptive basierend auf Load-Metriken. Implementiert via Envoy-Filters und konfiguriert durch IaC-Module in Terraform, was im RB-GW-011 dokumentiert ist."}
{"ts": "128:56", "speaker": "I", "text": "Sie hatten vorhin Runbook RB-SEC-077 erwähnt. Können Sie erläutern, wie das beim Debugging des MTLS Handshake Bugs geholfen hat?"}
{"ts": "129:15", "speaker": "E", "text": "Klar, RB-SEC-077 enthält eine Sequenzdiagramm-Analyse für den TLS-Handshake. Wir haben damit den Schritt identifiziert, bei dem der Client falsche Cipher Suites anbietet. Mit einem Hotfix (HF-GW-4821) konnten wir den Cipher-Set auf beiden Seiten synchronisieren."}
{"ts": "129:42", "speaker": "I", "text": "Wie dokumentieren Sie solche Hotfixes, um spätere Regressionen zu vermeiden?"}
{"ts": "130:00", "speaker": "E", "text": "Wir erfassen sie in der RFC-Datenbank, selbst wenn es nur temporäre Fixes sind. Für GW-4821 gibt es eine Nachverfolgung in RFC-OG-021, inklusive Lessons Learned und einem Verweis auf RB-SEC-077."}
{"ts": "130:26", "speaker": "I", "text": "Gab es einen Trade-off, den Sie in diesem Kontext bewusst eingegangen sind?"}
{"ts": "130:42", "speaker": "E", "text": "Ja, wir haben kurzfristig ein älteres Cipher Suite Set erlaubt, um Verfügbarkeit zu sichern. Das war gegen POL-SEC-001, aber das Riskboard hat es für 48 h freigegeben, bis das Upstream-IAM gepatcht war."}
{"ts": "131:08", "speaker": "I", "text": "Welche Risiken sehen Sie, falls solche Ausnahmen häufiger werden?"}
{"ts": "131:24", "speaker": "E", "text": "Die Gefahr ist, dass temporäre Ausnahmen sich einschleifen und die Security-Posture schwächen. Deshalb haben wir im Risk Log RSK-OG-009 festgehalten, dass jede Ausnahme ein zwingendes Sunset-Datum und eine explizite Remediation-Story im Backlog haben muss."}
{"ts": "144:00", "speaker": "I", "text": "Sie hatten vorhin die Build-Phase Risiken angesprochen. Können Sie das bitte noch einmal konkretisieren in Bezug auf den geplanten Go-Live-Termin?"}
{"ts": "144:05", "speaker": "E", "text": "Ja, klar. Wir haben aktuell zwei Haupt-Risiken: Erstens die Verzögerung bei der mTLS-Zertifikatsrotation, die laut RB-SEC-077 eigentlich automatisiert laufen sollte. Zweitens die Abhängigkeit von Aegis IAM, wo ein API-Änderungsticket (IAM-CHG-309) noch nicht durch das Staging ist."}
{"ts": "144:12", "speaker": "I", "text": "Wie wirkt sich denn das zweite Risiko auf die CI/CD-Pipeline aus?"}
{"ts": "144:18", "speaker": "E", "text": "Das ist tricky: Die Blue/Green Deployments gemäß RB-GW-011 erwarten, dass das Auth-Modul im Gateway kompatibel bleibt. Wenn Aegis IAM das Payload-Format ändert, schlägt der Green Slot Health Check fehl und wir müssen manuell umschalten."}
{"ts": "144:26", "speaker": "I", "text": "Sie beziehen sich auf den Health Check im Stage-Layer?"}
{"ts": "144:31", "speaker": "E", "text": "Genau, Stage-Layer 3 in unserer Pipeline. Dort laufen die synthetischen Auth-Tests, die wir auch aus dem Projekt Nimbus übernommen haben. Lessons Learned von Helios haben uns gelehrt, dass wir hier frühzeitig Fail-Fast implementieren müssen."}
{"ts": "144:39", "speaker": "I", "text": "Das ist der Multi-Hop-Aspekt zwischen Orion, Aegis und Nimbus?"}
{"ts": "144:44", "speaker": "E", "text": "Richtig. Orion Edge Gateway hängt am Aegis IAM für Auth und zieht sich Observability-Module aus Nimbus, z. B. für Latenztracking. Wenn einer dieser Stränge langsamer reagiert, sieht man es sofort im p95-Monitoring."}
{"ts": "144:52", "speaker": "I", "text": "Und wie mitigieren Sie das im Moment?"}
{"ts": "144:57", "speaker": "E", "text": "Wir haben in RFC-OG-014 den Trade-off dokumentiert: temporär eine Retry-Logik im Gateway einzubauen, obwohl das gegen POL-SEC-001 Minimalprivilegien spricht, weil wir Tokens länger im Speicher halten. Das war eine bewusste Risikoabwägung."}
{"ts": "145:05", "speaker": "I", "text": "Das klingt nach einem Balanceakt zwischen Sicherheit und Verfügbarkeit."}
{"ts": "145:09", "speaker": "E", "text": "Ja, absolut. Wir haben das Risiko im Risk Register RSK-ORI-22 erfasst, mit der Maßgabe, dass wir nach dem Release sofort auf einen stateless Auth Flow zurückgehen."}
{"ts": "145:16", "speaker": "I", "text": "Wie stellen Sie sicher, dass solche temporären Lösungen nicht permanent bleiben?"}
{"ts": "145:21", "speaker": "E", "text": "Wir knüpfen sie an Runbook-Tasks mit klaren Exit-Kriterien. In RB-DEP-014 ist z. B. festgehalten, dass die Retry-Logik entfernt werden muss, sobald IAM-CHG-309 im Prod erfolgreich läuft."}
{"ts": "145:29", "speaker": "I", "text": "Gibt es weitere Lessons Learned, die Sie hier anwenden konnten?"}
{"ts": "145:34", "speaker": "E", "text": "Ja, von Poseidon Networking haben wir das Canary-Release-Pattern übernommen, um schrittweise IAM-Änderungen auszurollen. Dadurch sehen wir potenzielle Auth-Probleme bei nur 5 % des Traffics und können reagieren, bevor der Großteil betroffen ist."}
{"ts": "146:00", "speaker": "I", "text": "Könnten Sie bitte noch einmal etwas detaillierter auf die Abhängigkeiten zwischen Orion Edge Gateway und dem Aegis IAM eingehen? Mich interessiert vor allem, wie Sie das in der Build-Phase managen."}
{"ts": "146:05", "speaker": "E", "text": "Ja, klar. Also, wir haben eine dedizierte Schnittstelle über gRPC, die stark versioniert ist. Änderungen im Aegis IAM, z.B. an den Claims-Strukturen, triggern bei uns automatisch ein Build in der CI-Pipeline. Das passiert über den Job `gw-iam-sync` in unserer Jenkins-Instanz. Wir prüfen dann gegen die Stubs aus dem IAM-Projekt, bevor wir das Gateway neu deployen."}
{"ts": "146:16", "speaker": "I", "text": "Und wie wird sichergestellt, dass ein Breaking Change im IAM nicht unbemerkt bleibt?"}
{"ts": "146:21", "speaker": "E", "text": "Dafür haben wir in RB-GW-021 einen Contract-Test definiert, der bei jeder Nacht-Build-Phase läuft. Zusätzlich gibt es ein Alerting via Slack-Webhook, wenn `gw-iam-sync` fehlschlägt. Das reduziert die Zeit, bis wir reagieren, auf unter 15 Minuten im Schnitt."}
{"ts": "146:34", "speaker": "I", "text": "Interessant. Gab es denn schon konkrete Vorfälle?"}
{"ts": "146:38", "speaker": "E", "text": "Ja, Ticket GW-4910 im März: Das IAM-Team hat neue Scopes eingeführt, ohne alte zu deprecaten. Unser Gateway hat daraufhin Autorisierungsfehler für bestimmte API-Calls geliefert. Wir haben innerhalb von 2 Stunden einen Hotfix über Blue/Green Deployment ausgerollt."}
{"ts": "146:51", "speaker": "I", "text": "Das klingt nach einer engen Verzahnung. Wie binden Sie dabei die Lessons Learned aus Helios Datalake ein?"}
{"ts": "146:56", "speaker": "E", "text": "Aus Helios haben wir v.a. das Konzept der Canary Releases übernommen, um Breaking Changes schrittweise zu erkennen. Im Gateway testen wir neue IAM-Versionen zunächst auf einem isolierten Edge-Knoten, der nur mit Staging-Daten arbeitet."}
{"ts": "147:08", "speaker": "I", "text": "Und Poseidon Networking? Sie hatten vorhin schon kurz erwähnt, dass dortige Erfahrungen eingeflossen sind."}
{"ts": "147:13", "speaker": "E", "text": "Genau, aus Poseidon haben wir die mTLS-Handshake-Debug-Methoden übernommen, die in RB-SEC-077 dokumentiert sind. Das hilft uns, IAM- und Gateway-Kommunikation verschlüsselt und debug-fähig zu halten, ohne die Least-Privilege-Policy zu verletzen."}
{"ts": "147:25", "speaker": "I", "text": "Wie dokumentieren Sie solche Cross-Projekt-Abhängigkeiten, damit auch neue Teammitglieder schnell einsteigen können?"}
{"ts": "147:30", "speaker": "E", "text": "Wir nutzen dafür ein Confluence-Space, in dem ein Architekturdiagramm die Verbindung zwischen Orion, Aegis und externen Services zeigt. Jede Verbindung hat einen Link zum relevanten Runbook, z.B. RB-GW-011 für Blue/Green oder RB-SEC-077 für mTLS."}
{"ts": "147:43", "speaker": "I", "text": "Jetzt in Hinblick auf Risiken: Sehen Sie derzeit kritische Punkte, falls das IAM-Team kurzfristig ein Update pusht?"}
{"ts": "147:48", "speaker": "E", "text": "Ja, die größte Gefahr ist, dass ein Schema-Change im JWT-Token-Format die Authentifizierung lahmlegt. Wir haben dafür in RFC-OG-014 festgehalten, dass alle Änderungen mindestens zwei Wochen vorher angekündigt werden müssen – aber das ist eine Policy, keine technische Barriere."}
{"ts": "148:01", "speaker": "I", "text": "Würde es Sinn machen, hier eine Art Feature-Flag-System einzubauen, um bei Problemen schnell umschalten zu können?"}
{"ts": "148:06", "speaker": "E", "text": "Das evaluieren wir gerade. In Ticket GW-5002 prüfen wir, ob wir die IAM-Integration über ein konfigurierbares Auth-Modul abstrahieren können. Trade-off laut RFC-OG-015: Mehr Flexibilität vs. potenziell höhere Latenz um 5–8 ms p95, was unser SLA-ORI-02 knapp tangieren könnte."}
{"ts": "148:00", "speaker": "I", "text": "Können Sie bitte noch genauer auf die Auswirkungen der Entscheidung aus RFC-OG-014 eingehen, insbesondere im Hinblick auf die Rate-Limiting-Strategie?"}
{"ts": "148:05", "speaker": "E", "text": "Ja, klar. Wir haben uns damals entschieden, die Redis-basierte Token-Bucket-Implementierung beizubehalten, obwohl ein Teil des Teams für eine In-Memory-Variante plädiert hat. Das war ein Trade-off zwischen Latenz und Konsistenz über mehrere Gateway-Instanzen hinweg."}
{"ts": "148:15", "speaker": "E", "text": "Die Entscheidung fiel aus, weil wir die SLA-ORI-02 Einhaltung priorisiert haben – also maximal 0,1% Fehlerquote bei Auth Requests – und dafür Konsistenz wichtiger war als die minimale Latenzverbesserung."}
{"ts": "148:24", "speaker": "I", "text": "Das heißt, Sie haben bewusst etwas mehr Latenz in Kauf genommen, um Ausreißer zu verhindern?"}
{"ts": "148:28", "speaker": "E", "text": "Genau. Unsere p95 Latenz liegt aktuell bei 112ms, also noch unter dem Zielwert. Mit In-Memory hätten wir vielleicht 105ms erreicht, aber die Verteilung wäre uneinheitlicher gewesen."}
{"ts": "148:38", "speaker": "I", "text": "Gab es dazu auch ein Runbook oder eine Dokumentation, die diese Entscheidung für spätere Phasen festhält?"}
{"ts": "148:43", "speaker": "E", "text": "Ja, wir haben das im Runbook RB-GW-RateLim-03 dokumentiert, inklusive Failover-Szenarien und der Lessons Learned aus dem Poseidon Networking Projekt, wo wir genau den gegenteiligen Weg gegangen sind und Probleme mit inkonsistenten Limits hatten."}
{"ts": "148:55", "speaker": "I", "text": "Interessant. Wie haben Sie die Teams überzeugt, die eine andere Meinung hatten?"}
{"ts": "149:00", "speaker": "E", "text": "Wir haben ein Proof-of-Concept unter Last gefahren. Die Ergebnisse – besonders bei 80% Lastspitzen – haben gezeigt, dass verteilte Limits stabiler sind. Diese Metriken aus unserem Testcluster (Ticket GW-Bench-772) haben letztlich überzeugt."}
{"ts": "149:12", "speaker": "I", "text": "Gab es Risiken, die Sie bei dieser Entscheidung besonders im Auge hatten?"}
{"ts": "149:16", "speaker": "E", "text": "Ja, das größte Risiko war, dass der Redis-Cluster selbst zum Bottleneck wird. Deswegen haben wir in RB-SEC-077 nicht nur Security-Aspekte, sondern auch ein Failover-Konzept mit Read-Replicas dokumentiert."}
{"ts": "149:27", "speaker": "I", "text": "Wie haben Sie das Failover getestet?"}
{"ts": "149:31", "speaker": "E", "text": "Mit einem Chaos-Engineering-Ansatz: Wir haben gezielt einzelne Redis-Knoten terminiert, während Lasttests liefen. Unser Alerting (basierend auf Nimbus Observability) hat in <5 Sekunden reagiert, und die Requests wurden nahtlos auf die Replica umgeleitet."}
{"ts": "149:45", "speaker": "I", "text": "Und diese Vorgehensweise haben Sie auch für andere Subsysteme übernommen?"}
{"ts": "149:49", "speaker": "E", "text": "Ja, insbesondere für die mTLS-Handshake-Komponente aus dem Auth-Flow. Die Lessons Learned aus Helios Datalake – also dass Failover nicht nur Datenlese-, sondern auch Auth-Pfade betreffen muss – haben wir hier direkt angewandt."}
{"ts": "149:56", "speaker": "I", "text": "Vielen Dank, das zeigt sehr gut, wie Sie systemübergreifend denken und Risiken mit belastbaren Daten adressieren."}
{"ts": "149:36", "speaker": "I", "text": "Lassen Sie uns nochmal auf die Risiken eingehen, die Sie jetzt kurz vor dem Abschluss der Build-Phase sehen. Welche sind aus Ihrer Sicht am kritischsten?"}
{"ts": "149:42", "speaker": "E", "text": "Also, das größte aktuelle Risiko ist die enge Kopplung zwischen dem Rate-Limiting-Modul und der Auth-Integration. Wenn wir in Aegis IAM einen Breaking Change haben, könnte das Gateway Requests fälschlicherweise drosseln. Das steht auch so im Risk Register RR-ORI-07."}
{"ts": "149:58", "speaker": "I", "text": "Wie dokumentieren Sie solche Abhängigkeiten und wie fließt das in Ihre Deployment-Planung ein?"}
{"ts": "150:03", "speaker": "E", "text": "Wir führen dazu eine Dependency-Matrix im Confluence-Space ORI-DEP, gekoppelt mit den Runbooks RB-GW-Deployment-03 für Blue/Green. Jede kritische Schnittstelle hat einen Pre-Deploy Check, der auch die IAM-API-Version prüft."}
{"ts": "150:17", "speaker": "I", "text": "Gab es in letzter Zeit einen Fall, wo diese Checks Sie vor einem Vorfall bewahrt haben?"}
{"ts": "150:21", "speaker": "E", "text": "Ja, vor drei Wochen: Ticket GW-4932. Aegis IAM hatte einen Patch, der den Token-Refresh-Endpunkt leicht verändert hat. Unser Pre-Deploy hat das erkannt, wir haben den Deploy um 48 Stunden verschoben und die Rate-Limit-Config angepasst."}
{"ts": "150:36", "speaker": "I", "text": "Interessant. Und wie kommunizieren Sie solche Verzögerungen projektübergreifend?"}
{"ts": "150:40", "speaker": "E", "text": "Wir nutzen den Orion-Projektkanal im internen Chat, plus einen wöchentlichen Sync mit den Leads von Helios und Poseidon. Lessons Learned daraus landen in RFC-Logs, z.B. RFC-OG-015, damit spätere Builds darauf aufbauen."}
{"ts": "150:54", "speaker": "I", "text": "Sie erwähnten ja vorhin die Lessons Learned aus Helios/Poseidon. Hat das Ihr Risikomanagement beeinflusst?"}
{"ts": "150:58", "speaker": "E", "text": "Definitiv. Bei Helios hatten wir damals keinen klaren Fallback-Plan. Jetzt ist im Runbook RB-GW-Rollback-02 genau beschrieben, wie wir bei Auth-Fehlern binnen 15 Minuten auf die vorherige Version zurückschalten."}
{"ts": "151:12", "speaker": "I", "text": "Wie stellen Sie sicher, dass diese Runbooks aktuell bleiben?"}
{"ts": "151:16", "speaker": "E", "text": "Wir haben einen vierteljährlichen Runbook-Review im Sprint-Plan, gekoppelt mit den SLO-Reviews. Änderungen aus Tickets wie GW-4821 oder GW-4975 fließen direkt ein, es gibt einen Merge-Request-Prozess mit Peer-Review."}
{"ts": "151:30", "speaker": "I", "text": "Und bei diesen Reviews, wie gehen Sie mit Trade-offs um, wenn zum Beispiel Performance gegen Sicherheit steht?"}
{"ts": "151:34", "speaker": "E", "text": "Das dokumentieren wir explizit in der Decision-Section der RFCs. Beispiel RFC-OG-014: Wir haben damals entschieden, das mTLS-Handshake-Timeout um 50ms zu erhöhen, um False Negatives zu vermeiden. Performanceverlust war minimal, Sicherheitsgewinn groß."}
{"ts": "151:50", "speaker": "I", "text": "Gab es auch Entscheidungen, die Sie rückblickend bereuen?"}
{"ts": "151:54", "speaker": "E", "text": "Vielleicht die frühe Wahl des proprietären Rate-Limiting-Adapters. Hätten wir damals gleich die Open-Source-Variante genommen, wäre die Integration mit Aegis IAM einfacher gewesen. Wir überlegen, das im nächsten Refactor zu ändern."}
{"ts": "152:06", "speaker": "I", "text": "Sie hatten vorhin RFC-OG-014 erwähnt. Können Sie bitte genauer erläutern, wie das in der Build-Phase jetzt noch Einfluss auf Ihre Arbeit hat?"}
{"ts": "152:11", "speaker": "E", "text": "Ja, klar. Der RFC-OG-014 hat im Prinzip die Entscheidung festgeschrieben, dass wir für das Orion Edge Gateway ein hybrides Auth-Modell fahren – also statische API Keys für Legacy-Services und OIDC für neue Clients. Das beeinflusst die Build-Phase, weil wir in den IaC-Templates für beide Pfade unterschiedliche Module pflegen müssen. Unser Terraform-Modul 'gw_auth' hat deshalb Conditional Blocks, die über Variablen aus der CI-Pipeline gesetzt werden."}
{"ts": "152:23", "speaker": "I", "text": "Und wie verifizieren Sie, dass diese Conditional Blocks korrekt greifen?"}
{"ts": "152:28", "speaker": "E", "text": "Wir haben dafür einen Test-Stage in der Pipeline, der zwei Deployments in einer Isolationsumgebung fährt – eines mit UseCase=Legacy, eines mit UseCase=OIDC. Die Checks laufen über ein Script aus RB-GW-011, Step 4, das mTLS-Handshake und Auth-Flow simuliert. Bei Abweichungen wird ein Blocker-Flag gesetzt, was wiederum laut unserer SLA-ORI-02 innerhalb von 30 Minuten bearbeitet werden muss."}
{"ts": "152:41", "speaker": "I", "text": "Gab es da in letzter Zeit Auffälligkeiten?"}
{"ts": "152:46", "speaker": "E", "text": "Ja, Ticket GW-5127. Da hat ein falsch gesetzter Default-Wert im Helm-Chart dazu geführt, dass der OIDC-Flow immer aktiv war, selbst wenn Legacy konfiguriert war. Wir haben das durch einen Hotfix im Chart und eine erweiterte Unit-Test-Reihe im Jenkins-Job behoben."}
{"ts": "152:58", "speaker": "I", "text": "Wie spielt hier Ihr Monitoring-Setup eine Rolle?"}
{"ts": "153:03", "speaker": "E", "text": "Unser Monitoring greift über Prometheus-Exporter direkt auf die Gateway-Instanzen zu. Wir messen p95 Latenz, Error Rate und Auth-Failure-Rate. Für das Problem in GW-5127 haben wir z. B. gesehen, dass die Auth-Failure-Rate im Legacy-Segment sprunghaft von 0,2 % auf 8 % gestiegen ist – das war der Trigger für die tiefergehende Analyse."}
{"ts": "153:15", "speaker": "I", "text": "Wie binden Sie Observability-Tools aus Nimbus ein, um solche Anomalien früh zu erkennen?"}
{"ts": "153:20", "speaker": "E", "text": "Wir haben die Trace-IDs aus Nimbus übernommen, damit wir Cross-System-Requests verfolgen können. Das heißt, wenn ein Request vom Gateway ins Aegis IAM geht, wird der Trace übergeben und in Jaeger visualisiert. Durch die Kombination mit den Metriken aus Orion sehen wir dann nicht nur die Latenz, sondern auch, ob z. B. ein spezifischer IAM-Node ein Bottleneck erzeugt."}
{"ts": "153:34", "speaker": "I", "text": "Gab es Lessons Learned aus Helios Datalake, die Sie hier angewendet haben?"}
{"ts": "153:39", "speaker": "E", "text": "Ja, definitiv. In Helios hatten wir das Problem, dass wir Alerts ohne Kontext hatten. Jetzt im Orion-Projekt versehen wir jeden Alert mit einem Runbook-Link, wie in RB-OPS-042 beschrieben, und Tagging für betroffene Subsysteme. Das reduziert die Mean Time to Resolution um rund 25 %."}
{"ts": "153:51", "speaker": "I", "text": "Wie dokumentieren Sie solche Optimierungen?"}
{"ts": "153:56", "speaker": "E", "text": "Wir pflegen ein zentrales Confluence-Board \"Gateway Ops Improvements\", wo jede Änderung mit Ticketnummer, Runbook-Referenz und Impact versehen wird. Für das Alert-Tagging war das ORI-IMP-009. Zusätzlich wird im Monthly Ops Review geprüft, ob das SLA-ORI-02 dadurch besser erfüllt wird."}
{"ts": "154:08", "speaker": "I", "text": "Gibt es aktuelle Risiken aus Ihrer Sicht, die wir adressieren sollten?"}
{"ts": "154:13", "speaker": "E", "text": "Ein Risiko ist die Abhängigkeit vom Aegis IAM-Release-Zyklus. Wenn die ein Breaking Change im OIDC-Endpunkt machen, müssen wir sofort reagieren. Deshalb haben wir im RFC-OG-014 festgehalten, dass ein Canary-Deployment gegen deren Staging-API Pflicht ist, bevor wir in Produktion gehen. Das minimiert Ausfälle, ist aber natürlich zusätzlicher Aufwand in der Build-Phase."}
{"ts": "153:36", "speaker": "I", "text": "Sie hatten vorhin erwähnt, dass im RFC-OG-014 späte Änderungen eingeflossen sind. Können Sie erläutern, wie sich das konkret auf die Build-Phase des Orion Edge Gateway auswirkt?"}
{"ts": "153:41", "speaker": "E", "text": "Ja, diese Änderungen haben z. B. die Art und Weise beeinflusst, wie wir das Auth-Modul im API Gateway laden. In der Build-Phase mussten wir die IaC-Templates in Terraform anpassen, damit die mTLS-Handshake-Parameter aus den neuen Secrets-Providern gezogen werden. Das war ein direkter Impact aus RFC-OG-014, der wiederum aus Lessons Learned aus Poseidon Networking hervorging."}
{"ts": "153:48", "speaker": "I", "text": "Gab es dabei Abhängigkeiten zu Aegis IAM, die Sie berücksichtigen mussten?"}
{"ts": "153:52", "speaker": "E", "text": "Ja, eindeutig. Die Änderung in der Zertifikatskette erforderte zeitgleich ein Update im Aegis IAM, Ticket IAM-3274. Wir mussten im Gateway-Deployment-Job in der GitLab-CI Pipeline eine Stage einfügen, die prüft, ob die entsprechende Version des IAM-Services verfügbar ist, bevor der Rollout erfolgen kann."}
{"ts": "153:59", "speaker": "I", "text": "Wie haben Sie das technisch in der Pipeline umgesetzt?"}
{"ts": "154:03", "speaker": "E", "text": "Wir haben im RB-GW-011 Runbook einen neuen Schritt 'Pre-Deploy IAM Sync' ergänzt. Dort wird per API-Call gegen den IAM Health Endpoint geprüft, ob die mTLS-Konfiguration aktiv ist. Falls nicht, wird der Deploy-Job mit Status 'manual' angehalten. Das verhindert, dass wir ein Gateway mit inkompatibler Auth-Konfig ausrollen."}
{"ts": "154:10", "speaker": "I", "text": "Interessant. Und hat das Auswirkungen auf die Einhaltung der SLA-ORI-02?"}
{"ts": "154:14", "speaker": "E", "text": "Ja, minimal. Die SLA-ORI-02 definiert eine maximale Downtime von 120 Sekunden pro Deployment-Fenster. Durch die zusätzliche Prüfung verlängert sich das Deployment um etwa 15 Sekunden. Wir haben das in einem Change Advisory Board Meeting dokumentiert und als akzeptabel bewertet, weil es die Sicherheit deutlich erhöht."}
{"ts": "154:21", "speaker": "I", "text": "Gab es spezielle Risiken, die Sie bei dieser Entscheidung im Blick hatten?"}
{"ts": "154:25", "speaker": "E", "text": "Das Hauptrisiko war, dass Deployments hängen bleiben, wenn das IAM nicht rechtzeitig aktualisiert wird. Um dem zu begegnen, gibt es in RB-SEC-077 einen Fallback-Pfad: Wir können temporär auf einen Standard-mTLS-Truststore zurückfallen, bis der IAM-Patch da ist. Das ist allerdings nur als Notfallplan gedacht."}
{"ts": "154:33", "speaker": "I", "text": "Wie stellen Sie sicher, dass dieser Fallback nicht missbraucht wird?"}
{"ts": "154:37", "speaker": "E", "text": "Über Audit-Logs und einen 4-Augen-Review. Jeder Fallback-Einsatz muss in Ticket GW-OPS protokolliert werden, mit Begründung und Genehmigung durch den Duty Manager. Wir haben das aus Erfahrungen im Helios Datalake übernommen, wo ähnliche Notfallmechanismen schnell zur Routine wurden, was wir hier strikt verhindern wollen."}
{"ts": "154:44", "speaker": "I", "text": "Hat die Anpassung auch Einfluss auf das Monitoring und die p95 Latenz-Metrik?"}
{"ts": "154:48", "speaker": "E", "text": "Indirekt ja. Wenn der Pre-Deploy-Check eine Verzögerung erzeugt, beeinflusst das nicht die Live-Latenz. Aber wir haben im Observability-Tool aus Nimbus ein Custom-Dashboard gebaut, das die Dauer von Deployments trackt, um Muster zu erkennen. So können wir frühzeitig eingreifen, falls die Durchlaufzeit zu stark ansteigt."}
{"ts": "154:55", "speaker": "I", "text": "Würden Sie rückblickend etwas an dieser Entscheidung ändern?"}
{"ts": "154:59", "speaker": "E", "text": "Nein, im Gegenteil. Die Integration des Checks hat uns bereits vor einem potenziellen Ausfall bewahrt, als das IAM-Update IAM-3291 sich um zwei Stunden verzögert hat. Ohne die Blockade im Pipeline-Job hätten wir eine fehlerhafte mTLS-Konfiguration im Gateway gehabt. Das wäre ein klarer SLA-Verstoß gewesen."}
{"ts": "155:06", "speaker": "I", "text": "Wir hatten eben die späten Architekturentscheidungen gestreift. Mich würde interessieren: Welche konkreten Risiken sehen Sie jetzt noch in der Build-Phase des Orion Edge Gateway?"}
{"ts": "155:15", "speaker": "E", "text": "Aktuell sehe ich zwei Hauptrisiken: Zum einen die Verzögerung bei der finalen mTLS-Implementierung, die in Ticket GW-4950 dokumentiert ist, und zum anderen eine potenzielle Performance-Degradation bei hoher Last, die wir im Stresstest ST-ORI-22 erkannt haben. Das Risiko ist, dass wir das p95 Latenz-Ziel aus SLA-ORI-02 reißen könnten."}
{"ts": "155:33", "speaker": "I", "text": "Wie dokumentieren Sie diese Risiken und die Abwägungen, falls Sie Workarounds einsetzen müssen?"}
{"ts": "155:42", "speaker": "E", "text": "Wir halten das im Risk Log des Projekts fest und verlinken zu den jeweiligen RFCs, z.B. RFC-OG-021 für den vorgeschlagenen Rate Limiter Workaround. Außerdem nutzen wir ein Feld 'Trade-off Notes' in Confluence, in dem wir etwaige SLA-Abweichungen und technische Schulden dokumentieren."}
{"ts": "155:58", "speaker": "I", "text": "Gab es Entscheidungen, die Sie im Rückblick anders treffen würden?"}
{"ts": "156:05", "speaker": "E", "text": "Ja, rückblickend hätten wir die Authentifizierungsintegration enger mit dem Aegis IAM Team verzahnen sollen. Die asynchrone Koordination hat zu Verzögerungen geführt. In RFC-OG-014, den wir eben schon gestreift haben, habe ich das als 'Lesson Learned' vermerkt."}
{"ts": "156:21", "speaker": "I", "text": "Apropos Aegis IAM – wie beeinflussen Änderungen dort aktuell Ihre Gateway-Deployments?"}
{"ts": "156:28", "speaker": "E", "text": "Änderungen im IAM, wie neue Claim-Strukturen im JWT, erfordern Anpassungen in unserem Auth-Plugin. Wir haben dafür einen eigenen Runbook-Abschnitt RB-GW-IAM-03, der beschreibt, wie man bei inkompatiblen Claims das Gateway im Blue/Green-Modus neu deployt, um Downtime zu vermeiden."}
{"ts": "156:46", "speaker": "I", "text": "Wie binden Sie dabei Monitoring ein, um solche Änderungen früh zu erkennen?"}
{"ts": "156:53", "speaker": "E", "text": "Wir haben Canary Endpoints, die kontinuierlich gegen das IAM validieren. Sobald ein Claim-Parsing-Error im Log auftaucht, triggert unser Observability-Tool eine Warnung nach POL-SEC-001 und erstellt automatisch ein Ticket im JIRA-Board."}
{"ts": "157:08", "speaker": "I", "text": "Sie hatten vorhin ST-ORI-22 erwähnt. Können Sie den Ablauf dieses Stresstests kurz schildern?"}
{"ts": "157:15", "speaker": "E", "text": "Der Stresstest simulierte 50k gleichzeitige Verbindungen mit variabler Payloadgröße. Wir haben dabei die p95-Latenz, Error-Rate und CPU-Auslastung gemessen. Die Tests liefen automatisiert via unserem CI/CD-Job 'perf-check-gw', der in RB-GW-011 beschrieben ist."}
{"ts": "157:32", "speaker": "I", "text": "Und welche Trade-offs mussten Sie bei der Behebung der gefundenen Engpässe machen?"}
{"ts": "157:39", "speaker": "E", "text": "Wir haben temporär die maximale Concurrent Connection Limit im Envoy-Proxy erhöht, was zwar Lastspitzen besser abfängt, aber mehr Memory verbraucht. Dieser Trade-off ist in RFC-OG-023 dokumentiert, zusammen mit einem Plan zur Optimierung des Upstream-Poolings."}
{"ts": "157:55", "speaker": "I", "text": "Wie gehen Sie damit um, dass solche Anpassungen eventuell die Security-Policies tangieren?"}
{"ts": "158:02", "speaker": "E", "text": "Wir prüfen jede Änderung gegen RB-SEC-077, um sicherzustellen, dass keine Policy-Verletzungen entstehen. Bei der Connection-Limit-Erhöhung haben wir etwa mTLS-Session-Reuse komplett getestet, um sicherzustellen, dass keine unverschlüsselten Verbindungen durchrutschen."}
{"ts": "160:06", "speaker": "I", "text": "Lassen Sie uns direkt an Ihre Ausführungen zu RFC-OG-014 anschließen. Wie genau haben diese späten Architekturentscheidungen Ihre Arbeit an der Pipeline beeinflusst?"}
{"ts": "160:12", "speaker": "E", "text": "Das war tatsächlich ein signifikanter Eingriff – wir mussten die Stage für Canary Deployments in YAML neu definieren, um die geänderte Auth-Integration aus RB-SEC-077 sauber zu unterstützen. Das bedeutete auch, dass wir die Blue/Green-Logik aus RB-GW-011 refaktorn mussten."}
{"ts": "160:20", "speaker": "I", "text": "Gab es dabei Herausforderungen im Zusammenspiel mit den Observability-Tools aus Nimbus?"}
{"ts": "160:26", "speaker": "E", "text": "Ja, das war der Punkt, an dem die Multi-Hop-Abhängigkeit spürbar wurde – wir mussten Metriken aus Nimbus' Prometheus-Cluster in unsere Gateway-Dashboards übernehmen, um p95-Latenzwerte <120ms gemäß SLA-ORI-02 in Echtzeit zu validieren."}
{"ts": "160:34", "speaker": "I", "text": "Wie haben Sie das technisch umgesetzt?"}
{"ts": "160:39", "speaker": "E", "text": "Über einen dedizierten Metrics-Collector, der via gRPC die Daten aus Nimbus abholt und in unser internes TSDB-Format konvertiert. Runbook MON-ORI-005 beschreibt diesen Prozess Schritt für Schritt."}
{"ts": "160:46", "speaker": "I", "text": "Können Sie noch etwas zu den Schnittstellen mit Aegis IAM erläutern, gerade in Bezug auf mTLS?"}
{"ts": "160:53", "speaker": "E", "text": "Klar, das Gateway validiert bei jedem Request das Client-Zertifikat gegen die Aegis-CA. Der GW-4821-Bug entstand, weil beim Renew der Zertifikate ein Feld im SAN nicht konsistent gesetzt wurde. Wir haben das mit einem Hotfix im Zertifikatsparser gelöst."}
{"ts": "161:01", "speaker": "I", "text": "Wie haben Sie den Fix getestet?"}
{"ts": "161:06", "speaker": "E", "text": "Mit einer Kombination aus Unit- und Integrationstests. Wir haben in einer isolierten Staging-Umgebung mit simulierten Aegis-IAM-Endpunkten gearbeitet, siehe Testplan TST-GW-091."}
{"ts": "161:14", "speaker": "I", "text": "Gab es bei der Umsetzung von POL-SEC-001 noch andere Risiken, die Sie identifiziert haben?"}
{"ts": "161:20", "speaker": "E", "text": "Ja, insbesondere beim Least-Privilege-Prinzip für Service Accounts. In der Build-Phase besteht das Risiko, dass zu weit gefasste Rollen in Terraform-Modulen landen. Wir haben das durch ein Review-Gate in der CI-Pipeline abgesichert."}
{"ts": "161:28", "speaker": "I", "text": "Welche Lessons Learned aus Helios oder Poseidon haben Sie hier konkret angewandt?"}
{"ts": "161:34", "speaker": "E", "text": "Aus Helios haben wir die Erkenntnis übernommen, dass eindeutige Schnittstellendefinitionen zwischen Daten- und Transportebene früh erstellt werden müssen, um spätere Brüche zu vermeiden. Aus Poseidon kam die Praxis, Netzwerkpolicies frühzeitig in Pre-Prod zu validieren."}
{"ts": "161:42", "speaker": "I", "text": "Rückblickend – gab es Entscheidungen, die Sie anders getroffen hätten?"}
{"ts": "161:48", "speaker": "E", "text": "Ja, ich hätte die Entscheidung, das Rate Limiting zunächst komplett clientseitig zu simulieren, wahrscheinlich vermieden. Das führte zu einer Lücke in der Lasttest-Abdeckung, die wir erst nach einem Incident (INC-ORI-204) erkannt haben."}
{"ts": "161:36", "speaker": "I", "text": "Wir sind ja nun schon tief in die Build-Phase eingestiegen. Mich würde interessieren: welche Risiken sehen Sie aktuell beim Orion Edge Gateway, gerade im Hinblick auf die API-Rate-Limiting-Logik?"}
{"ts": "161:42", "speaker": "E", "text": "Das größte Risiko ist aktuell, dass unsere Redis-basierten Token Buckets unter Last in bestimmten Clustern nicht schnell genug replizieren. Das hat sich in Testlauf TST-GW-902 gezeigt, und könnte bei SLA-ORI-02 zu Verstößen führen, wenn nicht optimiert."}
{"ts": "161:50", "speaker": "I", "text": "Wie dokumentieren Sie diese Erkenntnis und die möglichen Gegenmaßnahmen?"}
{"ts": "161:54", "speaker": "E", "text": "Wir erstellen dafür ein RFC, in diesem Fall RFC-OG-019, und verlinken direkt die Metriken aus unserem Grafana-Dashboard sowie die Runbooks RB-GW-011 und RB-SEC-077, damit bei Deployment- oder Security-Anpassungen klar ist, welche Stellschrauben existieren."}
{"ts": "162:02", "speaker": "I", "text": "Sie haben vorhin Helios und Poseidon erwähnt – gab es dort ähnliche Probleme?"}
{"ts": "162:06", "speaker": "E", "text": "Ja, bei Helios hatten wir eine ähnliche Bottleneck-Situation mit Kafka-Consumer-Lags, und bei Poseidon waren es Latenzspitzen durch VPN-Tunnel. In beiden Fällen haben wir gelernt, dass man horizontale Skalierung frühzeitig mit Chaos-Tests kombinieren sollte."}
{"ts": "162:15", "speaker": "I", "text": "Wie übertragen Sie diese Lessons Learned jetzt in Orion?"}
{"ts": "162:19", "speaker": "E", "text": "Wir haben vor dem Go-Live einen Lasttestplan in JIRA-Ticket GW-5123 angelegt, der gezielt Bucket-Replication und mTLS-Handshake parallel stresst. Außerdem setze ich Canary Deployments ein, um im Blue/Green-Kontext RB-GW-011 Abweichungen schnell zu isolieren."}
{"ts": "162:28", "speaker": "I", "text": "Und wie stellen Sie sicher, dass Sicherheitsrichtlinien wie POL-SEC-001 bei diesen Tests nicht verletzt werden?"}
{"ts": "162:32", "speaker": "E", "text": "Wir haben in die Testpipelines einen Policy-Check-Step integriert, der die Konfiguration gegen POL-SEC-001 validiert. Das ist eine YAML-basierte Definition, die im IaC-Repo versioniert wird, damit wir keine Abweichungen im Least-Privilege-Ansatz riskieren."}
{"ts": "162:40", "speaker": "I", "text": "Haben Sie dafür ein spezielles Tool genutzt?"}
{"ts": "162:43", "speaker": "E", "text": "Ja, wir verwenden ein internes Linter-Tool namens SecLint-GW, das wir aus Lessons Learned von Poseidon abgeleitet haben. Es läuft in der Jenkins-Pipeline, siehe Runbook RB-SEC-077 Abschnitt 4."}
{"ts": "162:51", "speaker": "I", "text": "Könnten Änderungen im Aegis IAM hier noch unvorhergesehene Probleme erzeugen?"}
{"ts": "162:55", "speaker": "E", "text": "Definitiv. Wenn das Aegis IAM z.B. sein Tokenformat ändert, müssten wir in der Auth-Middleware des Gateways Anpassungen vornehmen. Wir haben dafür in RFC-OG-014 eine Kompatibilitätsschicht spezifiziert, um Downtime zu vermeiden."}
{"ts": "163:03", "speaker": "I", "text": "Letzte Frage: gab es Entscheidungen in der Architektur, die Sie im Rückblick anders treffen würden?"}
{"ts": "163:08", "speaker": "E", "text": "Ja, ich hätte die mTLS-Implementierung nicht in einem späten Sprint angegangen, sondern früher parallelisiert. Das hätte den Debug-Aufwand, wie bei Bug GW-4821 beschrieben, deutlich reduziert. Im Nachhinein zeigt das, wie wichtig frühes Prototyping ist."}
{"ts": "162:12", "speaker": "I", "text": "Sie hatten vorhin bereits die RFC-OG-014 erwähnt. Können Sie jetzt im Detail erklären, wie diese Entscheidung konkret mit den Security-Richtlinien aus RB-SEC-077 verzahnt wurde?"}
{"ts": "162:27", "speaker": "E", "text": "Ja, also in RFC-OG-014 ging es um die finale Auswahl des Service Mesh Patterns für das Orion Edge Gateway. Wir haben dort bewusst ein mTLS-First-Design gewählt, um die in RB-SEC-077 festgelegten Anforderungen an Verschlüsselung in Transit und Service Isolation zu erfüllen. Das heißt, schon auf Pod-Ebene im Kubernetes-Cluster wird ein Zertifikats-Rolling nach Runbook RB-SEC-077-04 durchgeführt."}
{"ts": "162:52", "speaker": "I", "text": "Wie haben Sie diese Änderung in die CI/CD-Pipeline integriert, ohne die SLA-ORI-02 zu gefährden?"}
{"ts": "163:06", "speaker": "E", "text": "Wir haben in der Pipeline einen separaten Stage für Security Regression Tests eingeführt. Dieser Stage zieht die mTLS-Handshake-Simulation aus dem Helios-Testframework, das wir angepasst haben. So konnten wir sicherstellen, dass die Latenz unter den in SLA-ORI-02 definierten 120 ms p95 bleibt. Ticket GW-5123 dokumentiert den initialen Rollout."}
{"ts": "163:32", "speaker": "I", "text": "Gab es beim Rollout dieser mTLS-First-Architektur Probleme, z. B. Interoperabilität mit Aegis IAM?"}
{"ts": "163:46", "speaker": "E", "text": "Ja, der JWT-Assertion-Flow vom Aegis IAM musste leicht angepasst werden. Wir mussten den Token Exchange vor der mTLS-Session verlagern, weil sonst der Handshake in 15 % der Fälle in Timeout lief. Das haben wir im Runbook RB-GW-021 als 'Handshake PreAuth Adjustment' festgehalten."}
{"ts": "164:12", "speaker": "I", "text": "Interessant. Haben Sie dafür auch Lessons Learned aus Poseidon Networking einfließen lassen?"}
{"ts": "164:25", "speaker": "E", "text": "Genau, aus Poseidon kannten wir schon das Problem mit zu langen TLS Negotiations bei gleichzeitigen DNS-Lookups. Deshalb haben wir im Orion Edge Gateway ein DNS-Prefetching in der Init-Phase des Sidecars implementiert, wodurch die Gesamtdauer des Handshakes um ca. 18 ms reduziert wurde."}
{"ts": "164:49", "speaker": "I", "text": "Wie wurde diese Optimierung dokumentiert und abgesegnet?"}
{"ts": "165:01", "speaker": "E", "text": "Wir haben das als Minor Change in RFC-OG-018 dokumentiert, mit Verweis auf die Messdaten aus unserem Observability-Stack (Prometheus + Loki). Der Architekturbeirat hat in Sitzung AB-2024-05-14 zugestimmt, weil keine negativen Seiteneffekte auftraten."}
{"ts": "165:26", "speaker": "I", "text": "Gab es in diesem Kontext Trade-offs, die Sie bewusst eingegangen sind?"}
{"ts": "165:39", "speaker": "E", "text": "Ja, wir haben uns entschieden, den Cipher-Suite-Katalog leicht zu verkleinern, um Handshake-Zeiten zu optimieren. Dadurch entfallen einige exotische, aber sichere Algorithmen. Das Risiko wurde in Risk Log RL-ORI-09 aufgenommen mit dem Hinweis, dass diese Suiten bei Bedarf schnell reaktiviert werden können."}
{"ts": "166:02", "speaker": "I", "text": "Könnten Sie ein Beispiel für eine Situation geben, in der dieser Trade-off problematisch werden könnte?"}
{"ts": "166:15", "speaker": "E", "text": "Falls ein Partnerdienst aus einem zukünftigen Projekt, etwa Helios v3, zwingend auf eine der entfernten Cipher Suites angewiesen ist, müssten wir kurzfristig patchen. Das würde dann einen Out-of-Band-Deploy erfordern, was die Change-Freeze-Policy verletzen könnte."}
{"ts": "166:38", "speaker": "I", "text": "Wie bereiten Sie sich organisatorisch auf so ein Szenario vor?"}
{"ts": "166:51", "speaker": "E", "text": "Wir halten in Runbook RB-GW-031 einen Notfallpfad bereit, der die Reaktivierung innerhalb von 2 Stunden vorsieht, inklusive automatisierter Pipeline-Tests. Außerdem haben wir mit dem Release-Management abgestimmt, dass in solchen Fällen eine Ausnahme von der Freeze-Policy mit Genehmigung durch den CTO möglich ist."}
{"ts": "172:32", "speaker": "I", "text": "Lassen Sie uns noch einmal zu den Risiken in der aktuellen Build-Phase kommen – was sehen Sie momentan als größte Gefahr für SLA-ORI-02?"}
{"ts": "172:47", "speaker": "E", "text": "Also, die größte Gefahr ist momentan, dass die Rate-Limiting-Implementierung unter Lastspitzen nicht linear skaliert. Wir haben in Testlauf T-ORI-219 festgestellt, dass bei 5x Traffic das p95 Latenz-Ziel knapp überschritten wird."}
{"ts": "173:05", "speaker": "I", "text": "Wie haben Sie diese Erkenntnis dokumentiert?"}
{"ts": "173:14", "speaker": "E", "text": "Im Incident-Log GW-INC-096 und ergänzend im Performance-Runbook RB-GW-041. Dort haben wir auch den Workaround beschrieben: temporäre Erhöhung des Redis-Cluster-Speichers und Anpassen des Token-Bucket-Parameters."}
{"ts": "173:36", "speaker": "I", "text": "Gab es dazu eine formale RFC oder war das eher ad hoc?"}
{"ts": "173:44", "speaker": "E", "text": "Es war zunächst ad hoc, dann haben wir es in RFC-OG-022 formalisiert, um spätere Blue/Green-Deployments nicht zu gefährden. Das wurde mit RB-DEP-005 verknüpft."}
{"ts": "174:02", "speaker": "I", "text": "Interessant. Welche Trade-offs mussten Sie hier eingehen?"}
{"ts": "174:11", "speaker": "E", "text": "Wir haben bewusst eine leicht höhere Speicherlast in Kauf genommen, um Latenzspitzen abzufangen. Der Trade-off war, dass wir dadurch kurzfristig von POL-SEC-001 abweichen mussten, weil zusätzliche Caches nicht vollständig verschlüsselt waren."}
{"ts": "174:33", "speaker": "I", "text": "Wie haben Sie das Risiko der Abweichung mitigiert?"}
{"ts": "174:42", "speaker": "E", "text": "Durch eine begrenzte TTL der unverschlüsselten Caches und einen manuellen Purge-Prozess, dokumentiert in RB-SEC-080. Außerdem haben wir einen temporären Audit-Alert eingerichtet, um Zugriffsmuster zu überwachen."}
{"ts": "175:04", "speaker": "I", "text": "Gab es Parallelen zu Problemen in Helios oder Poseidon?"}
{"ts": "175:13", "speaker": "E", "text": "Ja, in Helios hatten wir ähnliche Lastspitzen bei Bulk-Ingestion. Dort half uns ein doppelstufiges Queue-System. Wir haben daraus gelernt, beim Orion Edge Gateway gleich zwei Stufen für das Rate Limiting vorzusehen – eine auf Gateway-Ebene und eine tiefer im Service Mesh."}
{"ts": "175:36", "speaker": "I", "text": "Wenn Sie zurückblicken – würden Sie eine Entscheidung anders treffen?"}
{"ts": "175:44", "speaker": "E", "text": "Ja, ich hätte den Multi-Tenant-Aspekt früher in die Architekturplanung einbezogen. In RFC-OG-014 ist er nur als spätere Option beschrieben. Dadurch mussten wir jetzt, in der Build-Phase, einige Auth-Integrationen nachträglich umbauen."}
{"ts": "176:06", "speaker": "I", "text": "Welche Auswirkungen hatte das auf die Pipeline?"}
{"ts": "176:14", "speaker": "E", "text": "Wir mussten RB-GW-011 für Blue/Green um zusätzliche Tenant-Konfigurationen erweitern und die Secrets-Verteilung anpassen, was die Deployments um ca. 15% verlängert hat."}
{"ts": "181:32", "speaker": "I", "text": "Lassen Sie uns jetzt noch einmal auf die Risiken in der aktuellen Build-Phase eingehen. Was sehen Sie konkret als kritisch an, gerade in Bezug auf den Orion Edge Gateway Scope?"}
{"ts": "181:39", "speaker": "E", "text": "Ich sehe vor allem die Abhängigkeit vom Aegis IAM als kritisch – wenn dort API-Änderungen ohne Vorankündigung kommen, können unsere Auth-Flows brechen. Wir hatten das Szenario im Ticket GW-4932 dokumentiert, inklusive temporärer Whitelist-Kompromisse."}
{"ts": "181:47", "speaker": "I", "text": "Wie dokumentieren Sie solche Trade-offs? Nutzen Sie ein bestimmtes Template oder Verfahren?"}
{"ts": "181:54", "speaker": "E", "text": "Ja, wir nutzen dafür das RFC-Template aus Confluence, Version 3.2. Entscheidungen wie dieser Whitelist-Bypass wurden in RFC-OG-022 festgehalten, mit Verweis auf das Runbook RB-GW-017 für den Rollback-Prozess."}
{"ts": "182:01", "speaker": "I", "text": "Gab es Entscheidungen, die Sie rückblickend anders treffen würden?"}
{"ts": "182:08", "speaker": "E", "text": "Definitiv. Beim Rate Limiting haben wir initial nur auf Header-basierte Limits gesetzt. Rückblickend hätte ich direkt die Redis-basierte zentrale Counter-Lösung implementiert, wie in Helios schon bewährt. Das hätte uns den Hotfix GW-5007 erspart."}
{"ts": "182:16", "speaker": "I", "text": "Sie erwähnen Helios – können Sie die Verbindung noch einmal skizzieren, wie Sie dortige Erfahrungen hier anwenden?"}
{"ts": "182:23", "speaker": "E", "text": "In Helios hatten wir eine ähnliche Latenzproblematik bei parallelen Requests. Wir haben dort ein Shard-basiertes Counter-Design etabliert. Für Orion habe ich in RFC-OG-019 diese Architektur übernommen und RB-GW-014 für die Implementierung angepasst."}
{"ts": "182:31", "speaker": "I", "text": "Wie gehen Sie mit solchen Cross-Projekt Lessons Learned prozessual um?"}
{"ts": "182:37", "speaker": "E", "text": "Wir pflegen ein internes Wiki \"Patterns & Pitfalls\". Dort werden Learnings aus Helios, Poseidon und Nimbus in einer Tabelle festgehalten, inkl. Metriken vor/nach Änderung. Diese fließen in die Architektur-Reviews ein."}
{"ts": "182:45", "speaker": "I", "text": "Kommen wir noch einmal auf das Thema Observability zurück. Wie stellen Sie sicher, dass das p95 Latenz-Ziel aus SLA-ORI-02 eingehalten wird?"}
{"ts": "182:52", "speaker": "E", "text": "Wir haben einen Prometheus-Export für Gateway-Metriken, speziell p50, p95 und p99 Latenzen. Ein AlertManager-RuleSet schlägt ab 110 ms p95 an. Die Dashboards sind in Grafana hinterlegt, mit Drill-Down auf Endpoint-Ebene."}
{"ts": "182:59", "speaker": "I", "text": "Und wie vermeiden Sie dabei Alert Fatigue?"}
{"ts": "183:05", "speaker": "E", "text": "Wir haben die Alert-Noise-Reduktion aus Nimbus übernommen: dedizierte Quiet-Hours und ein Rate-Limit für identische Alerts. Im Runbook RB-GW-021 ist dokumentiert, wie man Alerts korreliert, bevor man Incident Channels öffnet."}
{"ts": "183:12", "speaker": "I", "text": "Abschließend: Gibt es aus Ihrer Sicht noch offene Risiken, die nicht im Risiko-Register stehen?"}
{"ts": "183:18", "speaker": "E", "text": "Ein Punkt ist die noch fehlende Chaos-Testing-Pipeline. Ohne diese haben wir keine Sicherheit, wie sich das Gateway bei partiellen Netzwerkausfällen verhält. Ich plane dazu einen RFC-OG-025, basierend auf den Erfahrungen aus Poseidon-Netzwerkstörungen."}
{"ts": "182:28", "speaker": "I", "text": "Lassen Sie uns noch einmal gezielt auf die Risiken in der aktuellen Build-Phase eingehen. Welche sehen Sie als die kritischsten, Stand heute?"}
{"ts": "182:34", "speaker": "E", "text": "Aus meiner Sicht sind es vor allem zwei: Erstens die Latenzsteigerung bei hoher Last, speziell wenn Aegis IAM-Tokenvalidierungen in Serie laufen. Zweitens das Risiko, dass unsere mTLS-Handshake-Optimierung aus GW-4821 bei bestimmten Client-Bibliotheken nicht greift."}
{"ts": "182:46", "speaker": "I", "text": "Und wie dokumentieren Sie diese Risiken intern, damit sie auch in künftigen Phasen berücksichtigt werden?"}
{"ts": "182:51", "speaker": "E", "text": "Wir pflegen dazu einen Abschnitt im Runbook RB-GW-RISK-03, wo wir jeweils die Ticket-ID, die betroffenen Module und mögliche Workarounds eintragen. Außerdem schreiben wir für größere Themen RFCs, wie z.B. RFC-OG-019 zu den Latenztrends."}
{"ts": "183:03", "speaker": "I", "text": "Gab es kürzlich eine Entscheidung, bei der Sie im Nachhinein sagen würden, dass ein anderer Weg besser gewesen wäre?"}
{"ts": "183:08", "speaker": "E", "text": "Ja, wir haben anfangs das Rate Limiting auf Redis-Basis zentralisiert, was bei Ausfall von zwei Shards zu SLA-Verletzungen führte. Rückblickend hätten wir eher das dezentrale Token-Bucket-Modell aus Poseidon übernehmen sollen."}
{"ts": "183:20", "speaker": "I", "text": "Wie haben Sie die Entscheidung damals begründet?"}
{"ts": "183:24", "speaker": "E", "text": "Damals war die Annahme, dass zentrale Steuerung uns bessere Kontrolle gibt. Die Lessons Learned aus Helios und Poseidon – die wir erst später tiefer analysiert haben – zeigten jedoch, dass Resilienz wichtiger ist als zentrale Metriksammlung."}
{"ts": "183:37", "speaker": "I", "text": "Gibt es Maßnahmen, um diese Trade-offs künftig frühzeitiger zu evaluieren?"}
{"ts": "183:41", "speaker": "E", "text": "Ja, wir haben im Build-Gremium ein verpflichtendes Cross-Projekt-Review eingeführt, bei dem mindestens ein Architekt aus einem anderen Projekt wie Nimbus oder Helios die RFCs prüft. Das ist jetzt auch in POL-QA-005 verankert."}
{"ts": "183:54", "speaker": "I", "text": "Sie sprachen vorhin von GW-4821. Können Sie kurz umreißen, wie Sie den Bug damals debuggt haben?"}
{"ts": "184:00", "speaker": "E", "text": "Sicher, wir haben zunächst die TLS-Debug-Logs im Gateway aktiviert und parallel mit einem OpenSSL-Client reproduziert. Dann haben wir im Runbook RB-SEC-077 die Handshake-Sequenzen mit den Zertifikatsketten verglichen und festgestellt, dass ein Intermediate-CA-Update fehlte."}
{"ts": "184:14", "speaker": "I", "text": "Wie haben Sie das im Deployment dann abgesichert?"}
{"ts": "184:18", "speaker": "E", "text": "Wir haben einen Canary-Release-Fahrplan erstellt, der in RB-GW-DEP-02 dokumentiert ist, und die mTLS-Handshake-Änderung zunächst nur für 5% des Traffics aktiviert. Erst nach 48 Stunden ohne Errors laut SLA-ORI-02 haben wir auf 100% erhöht."}
{"ts": "184:32", "speaker": "I", "text": "Abschließend: Gibt es einen Bereich, in dem Sie bewusst ein Risiko in Kauf nehmen, um schneller voranzukommen?"}
{"ts": "184:38", "speaker": "E", "text": "Ja, bei der Observability haben wir aktuell nur p95-Latenz und Error-Rate in der Pipeline-Metrik, p99 folgt erst später. Das ist ein kalkuliertes Risiko, weil wir die Build-Phase termingerecht abschließen wollen, dokumentiert im Risk-Log-Eintrag RL-OG-07."}
{"ts": "186:28", "speaker": "I", "text": "Sie hatten vorhin RB-SEC-077 erwähnt – können Sie noch einmal erläutern, wie Sie diesen Security-Runbook-Abschnitt konkret im Orion Edge Gateway umgesetzt haben?"}
{"ts": "186:33", "speaker": "E", "text": "Ja, klar. RB-SEC-077 beschreibt ja die Standardprozedur für das Härtungsprofil von API-Gateways. Wir haben die mTLS-Parametrisierung dort 1:1 übernommen und im Ansible-Playbook so integriert, dass bei jedem Blue/Green-Switch die Zertifikate neu geladen werden. Das minimiert Downtime und erfüllt gleichzeitig die Vorgaben aus SLA-ORI-02."}
{"ts": "186:41", "speaker": "I", "text": "Gab es beim Einbinden der mTLS-Konfiguration in die CI/CD-Pipeline besondere Herausforderungen?"}
{"ts": "186:45", "speaker": "E", "text": "Ja, beim ersten Versuch haben wir im Jenkins-Stage 'deploy-green' einen Fehler im Handshake gehabt, der nur bei Lasttests auftrat. Das war in Ticket GW-5129 dokumentiert. Wir mussten die Cipher Suites anpassen und einen Wait-Step einfügen, um Race Conditions zu vermeiden."}
{"ts": "186:54", "speaker": "I", "text": "Wie haben Sie diesen Fix validiert?"}
{"ts": "186:57", "speaker": "E", "text": "Wir haben einen dedizierten mTLS-Smoke-Test aus Runbook RB-GW-015 ausgeführt und zusätzlich mit dem Lastprofil aus Helios Datalake die Verbindung simuliert. Stable handshake rates > 99,9 % über 72h waren der Nachweis."}
{"ts": "187:06", "speaker": "I", "text": "Stichwort Lessons Learned: Was aus Helios oder Poseidon war hier besonders wertvoll?"}
{"ts": "187:10", "speaker": "E", "text": "Aus Helios haben wir das Prinzip übernommen, TLS-Session-Resumption gezielt zu deaktivieren, um Debugging zu erleichtern. Aus Poseidon kam die Erfahrung, dass wir Rate Limiting nicht nur auf IP-Basis, sondern auch pro API-Key tracken sollten – das hat uns vor einem Engpassbewusstsein in Stage 3 bewahrt."}
{"ts": "187:19", "speaker": "I", "text": "Wie beeinflussen diese Erkenntnisse aktuell Ihre Arbeit an der Error-Budget-Überwachung?"}
{"ts": "187:23", "speaker": "E", "text": "Indem wir jetzt im Observability-Stack (basierend auf Nimbus-Exports) separate Panels für API-Key-basierte Latenzen haben. So sehen wir, wenn ein einzelner Partner sein p95-Limit von 120 ms reißt, ohne dass das Gesamtsystem darunter leidet."}
{"ts": "187:31", "speaker": "I", "text": "Gab es in letzter Zeit eine Entscheidung, bei der Sie bewusst einen Trade-off eingegangen sind?"}
{"ts": "187:35", "speaker": "E", "text": "Ja, im RFC-OG-022 haben wir dokumentiert, dass wir zugunsten schnellerer Deployments auf eine vollständige Canary-Phase verzichten. Das Risiko eines unentdeckten Fehlers wurde akzeptiert, weil wir mit dem Rollback-Skript aus RB-GW-020 in <5 min zurückgehen können."}
{"ts": "187:44", "speaker": "I", "text": "Wie sichern Sie diese Entscheidung gegen zukünftige Änderungen ab?"}
{"ts": "187:47", "speaker": "E", "text": "Wir haben in den Deployment-Policies (POL-DEP-004) klar vermerkt, dass der Canary-Schritt optional ist, solange das Rollback unter 5 min bleibt und Error-Budget-Verbrauch < 2 % im Monat liegt. Das wird monatlich im Gateway-Review geprüft."}
{"ts": "187:56", "speaker": "I", "text": "Können Sie ein Beispiel geben, wo diese Policy schon gegriffen hat?"}
{"ts": "188:00", "speaker": "E", "text": "Letzte Woche beim Patch für das Auth-Modul (Ticket GW-5302). Wir haben direkt Blue→Green geschaltet, ein Memory Leak entdeckt und innerhalb von 3 min zurückgerollt. Dank der klaren Policy war das kein Diskussionspunkt, sondern ein Routinefall."}
{"ts": "194:08", "speaker": "I", "text": "Sie hatten vorhin schon angedeutet, dass Sie die Rate Limiting Komponente in der API Gateway Architektur mit spezifischen Patterns aus Helios kombiniert haben. Können Sie das bitte im Detail erklären?"}
{"ts": "194:20", "speaker": "E", "text": "Ja, klar. Wir haben im Orion Edge Gateway die Token Bucket Implementierung aus RFC-OG-014 genommen, aber um die Burst-Kapazität stabil zu halten, haben wir die Queue-Drain-Logik von Helios Datalake adaptiert. Dadurch können wir unter Lastspitzen die p95 Latenz unter 120ms halten, wie in SLA-ORI-02 gefordert."}
{"ts": "194:45", "speaker": "I", "text": "Spannend. Und wie haben Sie das in der Blue/Green-Pipeline (RB-GW-011) abgebildet?"}
{"ts": "194:54", "speaker": "E", "text": "In der Pipeline haben wir einen Canary-Step eingebaut, der die Rate-Limit-Regeln gegen eine Staging-Umgebung mit synthetischem Traffic testet. Das ist ein Pre-Deployment-Check, der via IaC-Template aus RB-GW-011 provisioniert wird."}
{"ts": "195:15", "speaker": "I", "text": "Welche Observability-Tools haben Sie dabei eingesetzt?"}
{"ts": "195:21", "speaker": "E", "text": "Primär Prometheus für Metrics und Loki für Logs. Zusätzlich haben wir das Trace-Modul aus Nimbus angebunden, um End-to-End-Latenzen zu messen und MTLS-Handshake-Dauer zu visualisieren."}
{"ts": "195:40", "speaker": "I", "text": "Wie sah der Debug-Prozess für den MTLS Bug (GW-4821) konkret aus?"}
{"ts": "195:49", "speaker": "E", "text": "Wir haben in der Staging-Umgebung mit aktiviertem mTLS die Handshake-Pakete via Wireshark analysiert. Dabei fiel auf, dass der Gateway-Node in einer bestimmten Region ein veraltetes Zertifikat aus RB-SEC-077 nutzte. Durch ein automatisiertes Cert-Rollout-Skript konnten wir den Fehler in weniger als 2 Stunden beheben."}
{"ts": "196:15", "speaker": "I", "text": "Gab es Abhängigkeiten zu Aegis IAM, die diesen Bug beeinflusst haben?"}
{"ts": "196:22", "speaker": "E", "text": "Ja, die mTLS-Handshake-Parameter waren abhängig von den Policy-Einstellungen im Aegis IAM. Eine Änderung im Cipher-Set dort hatte unbeabsichtigt den Gateway-Handshake gebrochen. Das wurde erst über Cross-Projekt-Alerts sichtbar."}
{"ts": "196:45", "speaker": "I", "text": "Wie dokumentieren Sie solche Cross-Projekt-Risiken?"}
{"ts": "196:53", "speaker": "E", "text": "Wir pflegen dafür im Confluence den Abschnitt \"Cross-System Impact\" pro RFC. Für diesen Fall haben wir in RFC-OG-014 nachträglich einen Verweis auf den Aegis-Change-Log ergänzt und eine Runbook-Referenz für mTLS-Fehlerbehebung erstellt."}
{"ts": "197:15", "speaker": "I", "text": "Schauen wir auf die Build-Phase allgemein: wo sehen Sie aktuell die größten Risiken?"}
{"ts": "197:23", "speaker": "E", "text": "Eines der größten Risiken ist die Koordination der Rate-Limiting-Updates mit den IAM-Policy-Deployments. Wenn wir das nicht synchronisieren, kann es zu temporären Auth-Denials kommen. Wir haben im Risk-Register RSK-ORI-07 dokumentiert, dass jede Policy-Änderung mindestens 24h vor Rollout abgestimmt wird."}
{"ts": "197:50", "speaker": "I", "text": "Gab es Trade-offs, die Sie rückblickend anders treffen würden?"}
{"ts": "197:58", "speaker": "E", "text": "Ja, beim ersten Entwurf der Auth-Integration hätten wir stärker auf Least Privilege achten sollen. Wir haben anfangs zu breite Scopes vergeben, um schneller zu testen. Das führte zu einer längeren Audit-Phase. Im Nachhinein wäre eine strengere Umsetzung von POL-SEC-001 von Beginn an effizienter gewesen."}
{"ts": "202:08", "speaker": "I", "text": "Sie hatten vorhin das Rate Limiting angesprochen – könnten Sie noch einmal genauer erläutern, wie Sie die Parameter im Gateway konfiguriert haben, um SLA-ORI-02 einzuhalten?"}
{"ts": "202:20", "speaker": "E", "text": "Ja, klar. Wir haben im API Gateway die Burst- und Sustained-Rate in den Envoy-Configs so gesetzt, dass wir bei 95% Auslastung noch unter 120 ms p95 Latenz bleiben. Das war erst nach mehreren Loadtests mit JMeter und Gatling möglich, weil die initialen Limits zu konservativ waren und zu 429-Fehlern führten."}
{"ts": "202:46", "speaker": "I", "text": "Gab es dabei Abhängigkeiten zu anderen Systemen, die Sie berücksichtigen mussten?"}
{"ts": "202:55", "speaker": "E", "text": "Ja, wir mussten das Aegis IAM berücksichtigen, weil bei Auth-Token-Validierungen zusätzliche Latenz entsteht. Deshalb haben wir in RB-GW-015 definiert, dass Token-Caching auf 60 Sekunden gesetzt wird, um Roundtrips zu Aegis zu minimieren."}
{"ts": "203:16", "speaker": "I", "text": "Interessant. Und wie beeinflusst das Blue/Green Deployment-Konzept aus RB-GW-011 diese Konfigurationen?"}
{"ts": "203:27", "speaker": "E", "text": "Im Prinzip müssen wir bei jedem Green-Slot sicherstellen, dass die Rate Limits identisch zur Blue-Umgebung sind, um inkonsistente Client-Erfahrungen zu vermeiden. Deshalb gibt es in der Pipeline einen automatischen Config-Diff-Check, der vor dem Umschalten läuft."}
{"ts": "203:50", "speaker": "I", "text": "Wie sieht es beim mTLS Handshake Bug GW-4821 aus – gibt es da noch offene Punkte?"}
{"ts": "204:02", "speaker": "E", "text": "Nein, der ist seit Build 1.4.2 behoben. Die Ursache war ein fehlerhafter Cipher-Suite-String in der Envoy YAML. Wir haben dazu einen Patch im Runbook RB-SEC-080 dokumentiert und zusätzlich ein Preflight-Handshake-Testskript in die CI eingebaut."}
{"ts": "204:24", "speaker": "I", "text": "Wie monitoren Sie aktuell die Auswirkungen dieser Fixes auf die Servicequalität?"}
{"ts": "204:34", "speaker": "E", "text": "Wir nutzen Prometheus-Metriken für Handshake-Dauer und Fehlerquote, visualisiert in Grafana-Dashboards. Zusätzlich gibt es Alertmanager-Regeln, die bei >0,5% Fehlerquote ein Ticket in unserem Incident-Board erzeugen."}
{"ts": "204:56", "speaker": "I", "text": "Gibt es aus Helios Datalake oder Poseidon Networking noch weitere Lessons Learned, die Sie hier einsetzen?"}
{"ts": "205:08", "speaker": "E", "text": "Ja, aus Helios haben wir gelernt, dass zentrale Config-Management-Services wie Consul ein Single Point of Failure sein können. Deshalb replizieren wir Gateway-Konfigurationen lokal. Aus Poseidon stammt die Praxis, NetFlow-Daten für Anomalieerkennung heranzuziehen."}
{"ts": "205:32", "speaker": "I", "text": "Welche Risiken sehen Sie jetzt noch in der Build-Phase?"}
{"ts": "205:43", "speaker": "E", "text": "Ein Risiko ist, dass wir durch häufige API-Schema-Änderungen im Upstream-Backend ständig Anpassungen im Gateway vornehmen müssen. Das kann zu regressions führen, wenn Tests nicht vollständig sind. Dokumentiert ist das im RFC-OG-019 unter 'Schema Volatility'."}
{"ts": "206:08", "speaker": "I", "text": "Und wie gehen Sie bei solchen Trade-offs vor – gibt es ein festes Format?"}
{"ts": "206:18", "speaker": "E", "text": "Ja, wir nutzen ein Decision-Log-Template aus Confluence, in dem wir Problemstellung, Alternativen, Bewertung und Entscheidung festhalten. Für kritische Punkte wie Schema-Handling haben wir zusätzlich einen Review-Kreis mit Architekt:innen aus anderen Projekten."}
{"ts": "210:08", "speaker": "I", "text": "Wir waren zuletzt bei den Lessons Learned aus Helios und Poseidon. Mich würde noch interessieren, wie Sie diese konkret in die Orion Edge Gateway Pipeline eingebracht haben."}
{"ts": "210:36", "speaker": "E", "text": "Ja, also wir haben aus Helios die Erfahrung mit sehr granularen Health-Checks übernommen, um Deployment-Fehler früh zu erkennen. In Poseidon haben wir gelernt, wie wichtig isolierte Test-Namespaces im Kubernetes-Cluster sind. Diese beiden Punkte haben wir in RB-GW-011 ergänzt, sodass die Blue/Green Deployments jetzt vor dem Switch einen Namespace-Health-Report generieren."}
{"ts": "211:05", "speaker": "I", "text": "Gab es dabei technische Herausforderungen, etwa beim Einbinden in den bestehenden CI/CD-Flow?"}
{"ts": "211:22", "speaker": "E", "text": "Definitiv. Unsere GitLab-CI-Jobs mussten wir um einen Conditional Stage erweitern, der den Health-Report erst erzeugt, wenn die Integrationstests aus Stage 'int-tests' grün sind. Das war tricky, weil wir gleichzeitig die SLA-ORI-02 Latenzvorgaben einhalten mussten."}
{"ts": "211:54", "speaker": "I", "text": "Sie haben SLA-ORI-02 erwähnt – wie tracken Sie die entsprechenden Metriken im Build?"}
{"ts": "212:12", "speaker": "E", "text": "Wir nutzen Prometheus mit einem Custom Exporter, der API-Gateway Response-Zeiten im Millisekundenbereich aufnimmt. Das p95 Latenzziel von <120ms wird in einem Grafana-Dashboard visualisiert. Alerts laufen über Alertmanager in unseren Slack-Channel #gw-alerts."}
{"ts": "212:45", "speaker": "I", "text": "Und wie verhindern Sie Alert Fatigue in diesem Setup?"}
{"ts": "213:00", "speaker": "E", "text": "Wir haben ein dediziertes Runbook RB-MON-042 erstellt, das Threshold-Anpassungen dokumentiert. Außerdem werden Alerts automatisch dedupliziert, und wir bündeln ähnliche Events, bevor sie das On-Call-Team erreichen."}
{"ts": "213:28", "speaker": "I", "text": "Lassen Sie uns nochmal auf die Integration mit Aegis IAM zurückkommen. Welche Abhängigkeiten sind dort kritisch?"}
{"ts": "213:44", "speaker": "E", "text": "Die Token-Validierung hängt stark von der Aegis-JWT-Signaturrotation ab. Wenn sich dort der Key wechselt, müssen wir sofort das Gateway neu mit den JWKS-Endpunkten synchronisieren. Dazu gibt es das Runbook RB-IAM-009."}
{"ts": "214:12", "speaker": "I", "text": "Gab es schon Situationen, in denen diese Synchronisation nicht rechtzeitig erfolgte?"}
{"ts": "214:26", "speaker": "E", "text": "Ja, Ticket GW-5078 dokumentiert so einen Fall. Der JWKS-Fetch-Job hing wegen eines DNS-Timeouts. Wir haben daraus gelernt, einen Fallback auf den zuletzt bekannten Key einzubauen, um Ausfälle zu vermeiden."}
{"ts": "214:56", "speaker": "I", "text": "Zum Abschluss: Welche Risiken sehen Sie jetzt noch in der Build-Phase, und welche Trade-offs mussten Sie akzeptieren?"}
{"ts": "215:14", "speaker": "E", "text": "Ein Risiko ist die noch nicht vollständig getestete Rate-Limiting-Logik unter Extrem-Last. Wir haben uns entschieden, vorerst auf einen statischen Burst-Parameter zu setzen, obwohl dynamische Anpassung aus Performance-Sicht besser wäre. Das ist in RFC-OG-019 als Trade-off dokumentiert."}
{"ts": "215:44", "speaker": "I", "text": "Würden Sie diese Entscheidung im Nachhinein ändern?"}
{"ts": "216:08", "speaker": "E", "text": "Vermutlich ja, sobald wir mehr Testdaten aus der Staging-Umgebung haben. Aber aktuell ist die statische Variante einfacher zu überwachen und entspricht den Vorgaben aus POL-SEC-001, weil weniger dynamische Eingriffe notwendig sind."}
{"ts": "219:28", "speaker": "I", "text": "Wir hatten vorhin kurz über die Lessons Learned aus Poseidon Networking gesprochen. Können Sie mir ein aktuelles Beispiel nennen, wo Sie diese im Orion Gateway angewendet haben?"}
{"ts": "219:43", "speaker": "E", "text": "Ja, klar. In Poseidon hatten wir massive Probleme mit L3 Load Balancing bei hohen Verbindungszahlen. Für Orion habe ich dieses Wissen genutzt, um im Vorfeld eine adaptive Routing-Policy zu definieren, die in RB-GW-019 dokumentiert ist. Das verhindert, dass wir bei Burst-Traffic in denselben Flaschenhals laufen."}
{"ts": "220:06", "speaker": "I", "text": "Spannend. Wie wird diese Policy in der CI/CD-Pipeline berücksichtigt?"}
{"ts": "220:18", "speaker": "E", "text": "Wir haben im Pipeline-Skript einen Pre-Deploy-Check eingebaut, der die aktuellen Routing-Configmaps gegen die Policy validiert. Wenn ein Verstoß entdeckt wird, schlägt der Deploy-Job fehl und verweist auf Runbook RB-NET-044."}
{"ts": "220:39", "speaker": "I", "text": "Und wie testen Sie das unter Lastbedingungen?"}
{"ts": "220:49", "speaker": "E", "text": "Wir nutzen dafür eine Kombination aus k6-Skripten und unserem internen Tool 'BurstSim'. Die Tests laufen in einer Staging-Umgebung mit Traffic-Mirroring, ähnliches Setup wie in Helios. Ergebnisse werden mit SLA-ORI-02 korreliert."}
{"ts": "221:12", "speaker": "I", "text": "Sie erwähnten SLA-ORI-02 – wie stellen Sie sicher, dass das p95 Latenz-Ziel weiterhin eingehalten wird, wenn wir neue Auth Features hinzufügen?"}
{"ts": "221:26", "speaker": "E", "text": "Wir haben ein Observability-Dashboard, das Metriken wie handshake_duration_ms und auth_token_validation_ms getrennt erfasst. Bei jedem Merge prüft ein Canary-Release diese Werte gegen den Grenzwert von 120ms. Falls überschritten, wird automatisch ein JIRA Ticket im Projekt ORI-PERF erstellt."}
