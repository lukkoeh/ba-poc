{"ts": "00:00", "speaker": "I", "text": "Können Sie mir kurz schildern, wie Ihr Tag-zu-Tag im Hera QA Platform Projekt aussieht?"}
{"ts": "03:15", "speaker": "E", "text": "Ja, klar. Morgens checke ich zuerst die Overnight-Builds im orchestrierten Testgrid. Dann gehe ich die Reports aus dem Analytics-Modul durch—die fließen direkt aus unserem Hera Data Lake ein—um flaky test patterns zu erkennen. Danach gibt es oft ein Stand-up mit den Dev- und Platform-Teams, um offene Blocking-Issues abzustimmen."}
{"ts": "07:45", "speaker": "I", "text": "Welche QA-Ziele wurden für die Build-Phase definiert?"}
{"ts": "11:10", "speaker": "E", "text": "Primär wollten wir eine 85% Coverage auf kritischen Business-Flows erreichen, gemessen nach POL-QA-014. Zweitens sollten wir eine automatisierte Risk-Based Priorisierung in das Test Orchestration Engine Modul integrieren, damit High-Risk-Komponenten zuerst getestet werden. Und drittens war es Ziel, die Traceability von RFC-Änderungen wie RFC-1770 lückenlos nachzuweisen."}
{"ts": "15:30", "speaker": "I", "text": "How do you interact with other departments like Platform or Data during this phase?"}
{"ts": "19:00", "speaker": "E", "text": "With Platform, wir stimmen uns über API-Stabilität und die Test Harness Updates ab. Mit dem Data-Team gibt es wöchentliche Syncs, damit wir neue Metrics in die QA-Analytics einbinden—z.B. haben wir im Ticket QA-423 einen neuen flaky-score-Algorithmus spezifiziert, der ihre Data Processing Pipelines nutzt."}
{"ts": "23:40", "speaker": "I", "text": "Wie priorisieren Sie Testfälle basierend auf Risiko?"}
{"ts": "27:55", "speaker": "E", "text": "Wir nutzen eine Scoring-Matrix aus POL-QA-014, gewichtet nach Impact auf SLA-KPIs und Change Frequency. Das Risiko-Score wird dynamisch aus dem Commit-Log und den Incident-Historien (siehe Incident-DB) berechnet. Dann mapt unser Tool diese Scores auf Test-Suites, sodass high-risk Suites im Orchestrator zuerst laufen."}
{"ts": "32:15", "speaker": "I", "text": "Could you walk me through your traceability matrix for a recent release?"}
{"ts": "36:25", "speaker": "E", "text": "Sure. Für Release 1.3 haben wir jede User Story aus Jira mit einer RFC-ID verknüpft, z.B. RFC-1770 für das neue API-Throttling. Dann mapped die Matrix über unser QA-Tool die Stories auf konkrete Test-Cases und deren Ergebnisse. So konnten wir beim Audit loggen: Story HQA-212 -> RFC-1770 -> TestCase TC-API-07 -> Passed on Build #456."}
{"ts": "41:00", "speaker": "I", "text": "Wie stellen Sie sicher, dass Änderungen aus RFC-1770 korrekt abgebildet werden?"}
{"ts": "45:20", "speaker": "E", "text": "Da haben wir eine kombinierte Strategie: automatische Detection von API-Signature Changes via Schema-Diff-Tool und manuelle Review nach Runbook RB-QA-051 Abschnitt 3.2. And any anomalies trigger a QA-Gate in the pipeline, so Deployment stops until validated."}
{"ts": "50:10", "speaker": "I", "text": "Welche Signale nutzen Sie, um flaky tests zu identifizieren?"}
{"ts": "54:35", "speaker": "E", "text": "Wir schauen auf Variance in Execution Time, Inconsistent Pass/Fail Patterns über Builds, und Error-Message-Entropy. Diese Signale werden im Analytics-Dashboard visualisiert. Beispiel: In Ticket QA-512 haben wir durch hohe Entropy erkannt, dass der UI-Login-Test zu 40% an Race Conditions litt."}
{"ts": "59:40", "speaker": "I", "text": "Gab es Situationen, in denen Sie bewusst Testabdeckung reduziert haben, um eine Deadline zu halten?"}
{"ts": "90:00", "speaker": "E", "text": "Ja, im März-Build. Wir haben Low-Risk-Tests für selten genutzte Admin-Features auf das nächste Cycle verschoben, um die SLA für das Payment-Modul einzuhalten. Das war dokumentiert in Risk-Log RL-2023-03, mit Verweis auf akzeptiertes Restrisiko laut Projektleitung. Wir haben das im Post-Mortem evaluiert und beschlossen, da gezielt nachzuholen."}
{"ts": "90:00", "speaker": "I", "text": "Lassen Sie uns nochmal auf die Testorchestrierung zurückkommen – welche Anpassungen haben Sie zuletzt an RB-QA-051 vorgenommen?"}
{"ts": "90:05", "speaker": "E", "text": "Ja, also, äh, wir haben im April eine Section ergänzt, die explizit das Quarantinieren von flaky tests beschreibt. That was based on a spike we ran with the DataOps team, um false positives zu minimieren."}
{"ts": "90:18", "speaker": "I", "text": "Und diese Quarantäne-Logik, ist die direkt in der Orchestrierungs-Pipeline implementiert?"}
{"ts": "90:22", "speaker": "E", "text": "Genau, wir haben einen Jenkins-Stage eingefügt, der über das Analytics-Modul läuft. It reads the flakiness score from the last five runs, und wenn der über 0.4 liegt, geht der Test in den Quarantäne-Branch."}
{"ts": "90:38", "speaker": "I", "text": "Wie binden Sie dann UX-Feedback ein, wenn ein Test quarantiniert ist?"}
{"ts": "90:43", "speaker": "E", "text": "Das ist tricky. Wir haben, äh, mit UX vereinbart, dass Findings aus Usability-Tests in einem separaten Tagging-System landen. That way, selbst wenn automatisierte Checks warten, können wir die UX-relevanten Aspekte manuell verifizieren."}
{"ts": "90:58", "speaker": "I", "text": "Interessant. Gab es einen Fall, wo diese manuelle Verifizierung einen Release-Stop ausgelöst hat?"}
{"ts": "91:02", "speaker": "E", "text": "Ja, Ticket HERA-QA-882 im Mai. Die automatisierten Tests waren grün, aber im UX-Review fiel auf, dass ein Element nicht keyboard-navigierbar war. We escalated that as a P1, und der Release wurde um 48 Stunden verschoben."}
{"ts": "91:18", "speaker": "I", "text": "Wie dokumentieren Sie solche Eskalationen?"}
{"ts": "91:22", "speaker": "E", "text": "Wir pflegen dafür ein Confluence-Log, verlinkt zu den Jira-Tickets. It includes SLA impact, root cause und die Lessons Learned, wie im POL-QA-014 Appendix empfohlen."}
{"ts": "91:36", "speaker": "I", "text": "Wenn wir auf die SLAs schauen – gab es Konflikte zwischen den Anforderungen und der Rapid Iteration?"}
{"ts": "91:41", "speaker": "E", "text": "Absolut. Zum Beispiel beim RFC-1795 mussten wir laut SLA innerhalb von 72h deployen. That forced us to skip low-risk regression cases, documented under waiver HERA-QA-WV12."}
{"ts": "91:55", "speaker": "I", "text": "Und wie haben Sie das Risiko bewertet, diese Tests zu überspringen?"}
{"ts": "92:00", "speaker": "E", "text": "Wir haben den Risk Score aus der Traceability Matrix gezogen – alle betroffenen Szenarien lagen unter 0.2. In practice, das heißt: geringe Nutzerzahl, niedrige Fehlerauswirkung."}
{"ts": "92:12", "speaker": "I", "text": "Gab es dennoch Folgeprobleme?"}
{"ts": "92:16", "speaker": "E", "text": "Nur ein Minor-Bug, Ticket HERA-BUG-441, reported von internem QA nach dem Go-Live. Fix war trivial, und es hat die SLA-Erfüllung nicht beeinträchtigt."}
{"ts": "96:00", "speaker": "I", "text": "Zum Thema Entscheidungen und Risiken im Hera-Projekt – gab es eine konkrete Situation, in der Sie bewusst Coverage reduziert haben, um einen Release-Termin einzuhalten?"}
{"ts": "96:15", "speaker": "E", "text": "Ja, im Sprint 14, äh, mussten wir den Regressionstest um etwa 18% kürzen. Wir haben anhand der Risk-Matrix aus POL-QA-014 priorisiert und low-impact Module wie Settings-UI ausgelassen, um die SLA-Zeitfenster einzuhalten."}
{"ts": "96:42", "speaker": "I", "text": "And how did you document that decision? Was there an artifact or ticket?"}
{"ts": "96:53", "speaker": "E", "text": "Wir haben das in Ticket QA-DEC-221 im Jira-Board hinterlegt, mit Verweis auf Runbook RB-QA-051 Section 4.3. Das Ticket enthält auch eine 'Accepted Risk'-Notiz, signiert vom PM, gemäß den internen Compliance-Regeln."}
{"ts": "97:20", "speaker": "I", "text": "Und wie haben Sie das Risiko kommuniziert? Gab es ein spezielles Gremium?"}
{"ts": "97:31", "speaker": "E", "text": "Wir haben es im wöchentlichen QA-Platform-Sync, äh, montags um 10 Uhr, vor Platform und Data Leads präsentiert. Zusätzlich stand es im RC-Report für RFC-1770, damit jeder die Abdeckungslücken kennt."}
{"ts": "97:55", "speaker": "I", "text": "Was war das größte Risiko, das Sie dokumentiert, aber bewusst akzeptiert haben?"}
{"ts": "98:08", "speaker": "E", "text": "Ein persistenter flaky test im Module 'Batch Scheduler'. Wir haben den als 'quarantined' markiert, weil er nur unter sehr spezifischen Load-Patterns fehlschlug und die Behebung den Build-Plan um zwei Wochen verzögert hätte."}
{"ts": "98:35", "speaker": "I", "text": "Did you implement any mitigation in the meantime?"}
{"ts": "98:44", "speaker": "E", "text": "Ja, wir haben im Orchestrator einen Retry-Mechanismus auf zwei Runs gesetzt, basierend auf den Signalen aus unserem Analytics-Modul. Außerdem wurde im Runbook RB-QA-051 Appendix B ein Workaround beschrieben."}
{"ts": "99:05", "speaker": "I", "text": "Wie haben Sie hier das SLA-Balancing gemacht? Zwischen schneller Iteration und der garantierten Qualität?"}
{"ts": "99:18", "speaker": "E", "text": "Das war tricky. Unser SLA definiert max. 1% false negatives in der Release-Validation. Wir haben mit Data-Team simuliert, dass trotz Retry der Wert bei 0,7% blieb, also innerhalb der Grenze. Damit konnten wir die schnelle Iteration beibehalten."}
{"ts": "99:45", "speaker": "I", "text": "Gab es ungeschriebene Regeln, wie Sie solche Trade-offs intern verargumentieren?"}
{"ts": "99:57", "speaker": "E", "text": "Ja, intern gilt: 'Keine Blockade ohne klare Metrik-Überschreitung'. Das heißt, wenn wir quantitativ zeigen können, dass wir im Rahmen bleiben, wird eher ein Risiko akzeptiert, als dass wir den Release blocken."}
{"ts": "100:20", "speaker": "I", "text": "Looking forward, would you change that approach for Hera’s next phase?"}
{"ts": "100:32", "speaker": "E", "text": "Ich denke, wir würden mehr automatisierte Risk-Projections im Traceability-Tool einbauen, um Entscheidungen frühzeitiger zu treffen. Das könnte helfen, weniger manuell verhandeln zu müssen und schneller evidenzbasiert zu agieren."}
{"ts": "112:00", "speaker": "I", "text": "Lassen Sie uns nochmal auf die Orchestration zurückkommen – wie exactly verbinden Sie die flaky test analytics mit den Deployment Pipelines?"}
{"ts": "112:20", "speaker": "E", "text": "Also, wir haben einen internen Connector zwischen dem Orchestration Service und unserem Build Agent, der die Signale aus der Analytics Engine in Echtzeit konsumiert. Wenn ein Test als flaky markiert wird, wird er in der nächsten Pipeline-Run automatisch in den quarantine pool verschoben – das ist im Runbook RB-QA-051, Abschnitt 4.3, festgehalten."}
{"ts": "112:50", "speaker": "I", "text": "Und diese quarantine tests, werden die manuell reviewed oder gibt es da einen scheduled job?"}
{"ts": "113:05", "speaker": "E", "text": "Beides – wir haben einen nightly job, der basierend auf Ticket-Typ QA-FLAKY-*, also z. B. QA-FLAKY-772, Reports erzeugt. Zusätzlich schauen wir im Daily Standup manuell auf die Top 5 flaky incidents, um quick wins zu isolieren."}
{"ts": "113:30", "speaker": "I", "text": "Können Sie ein Beispiel nennen, wo ein flaky signal downstream UX beeinflusst hat?"}
{"ts": "113:45", "speaker": "E", "text": "Ja, tatsächlich hatte der Checkout-Flow einen intermittenten UI-Button-Lag. Die Analytics Engine hat den Test 'btn_checkout_click' mehrfach als flaky gelistet. Durch Rücksprache mit UX haben wir festgestellt, dass die Hover-State-Animation zu lang war. Das wurde dann per RFC-1845 geändert, und wir haben den Test angepasst."}
{"ts": "114:15", "speaker": "I", "text": "Interesting. So you basically closed the loop between analytics, QA, and UX."}
{"ts": "114:25", "speaker": "E", "text": "Genau, und das ist Teil unserer ungeschriebenen Regel: Wenn ein flaky test auf einen UX-relevanten Bereich zeigt, ziehen wir UX sofort hinzu, statt nur technisch zu fixen."}
{"ts": "114:50", "speaker": "I", "text": "Wie priorisieren Sie dann zwischen einem technischen flaky und einem UX-bedingten flaky?"}
{"ts": "115:05", "speaker": "E", "text": "Wir nutzen den Risk-Score aus POL-QA-014. Technische Flakies, die core transaction paths betreffen, gehen vor. UX-Flakies mit geringer Auswirkung können in den nächsten Sprint verschoben werden, außer sie brechen unser SLA-UX-002, das max. 1 % UX-Fehlerquote erlaubt."}
{"ts": "115:35", "speaker": "I", "text": "Do you ever get conflicts with Platform when a flaky fix requires infra changes?"}
{"ts": "115:50", "speaker": "E", "text": "Ja, zum Beispiel QA-FLAKY-655 betraf einen Test, der nur bei hoher CPU-Last fehlschlug. Die Platform wollte keine Ressourcen erhöhen. Wir haben dann testseitig mit load-mocking gearbeitet, als trade-off, dokumentiert in Decision Log DL-P-HER-022."}
{"ts": "116:20", "speaker": "I", "text": "Klingt nach einem klassischen Multi-Team-Trade-off. How do you keep traceability in such cases?"}
{"ts": "116:35", "speaker": "E", "text": "Wir verlinken im Traceability-Matrix-Tool die QA-Tickets mit den Platform-Changerequests, z. B. CR-PF-3907, und referenzieren beide in der entsprechenden Release Note. So ist die Chain von Issue zu Fix transparent."}
{"ts": "116:55", "speaker": "I", "text": "Und wird das auch auditiert?"}
{"ts": "117:05", "speaker": "E", "text": "Ja, halbjährlich. Unser internes Audit-Team prüft stichprobenartig, ob POL-QA-014 und die SLA-Dokumentation eingehalten wurden, inkl. dieser Cross-Team-Verlinkungen."}
{"ts": "128:00", "speaker": "I", "text": "Sie hatten vorhin kurz die SLA-Anforderungen erwähnt – könnten Sie mal schildern, wie Sie die im Alltag gegen Rapid Iteration abwägen?"}
{"ts": "128:20", "speaker": "E", "text": "Ja, also wir haben bei Hera QA Platform ein SLA von 99,5 % Test-Orchestrierungsverfügbarkeit. In der Build-Phase bedeutet das, dass wir sometimes bewusst auf tiefe Regression in low-risk areas verzichten, um den Deploy-Train nicht zu blockieren."}
{"ts": "128:46", "speaker": "I", "text": "Heißt das, Sie nutzen Risk-Based Testing hier aktiv als Hebel?"}
{"ts": "129:00", "speaker": "E", "text": "Genau, wir mappen die Risiko-Scores aus POL-QA-014 auf die Test-Suites. High-risk features wie die Flaky-Test-Analytics laufen immer komplett durch, low-risk UI-Elemente nur stichprobenartig, especially when we have a hard deadline."}
{"ts": "129:26", "speaker": "I", "text": "Und wie dokumentieren Sie diese bewussten Reduktionen?"}
{"ts": "129:38", "speaker": "E", "text": "Das läuft über Ticket-Typ QA-DEC in Jira, z. B. QA-DEC-117 für den Sprint 42. Da steht drin: Scope-Reduktion, Risiko-Score, Approval von Platform Lead."}
{"ts": "130:02", "speaker": "I", "text": "Können Sie ein Beispiel nennen, wo ein solcher Trade-off später Probleme gemacht hat?"}
{"ts": "130:15", "speaker": "E", "text": "Ja, in QA-DEC-102 haben wir beim RFC-1822 auf visuelle Regression verzichtet, weil UX Tests im Lab schon grün waren. Später hat sich gezeigt, dass im Dark-Theme ein Kontrastproblem war. War kein blocker, aber musste im nächsten Patch gefixt werden."}
{"ts": "130:44", "speaker": "I", "text": "Wie reagieren Sie inzwischen auf solche UX-bezogenen Incidents?"}
{"ts": "131:00", "speaker": "E", "text": "Wir haben im Runbook RB-QA-051 jetzt einen Step ergänzt: Wenn UX-Research ein Theme ändert, muss mindestens ein Cross-Theme Snapshot Test gefahren werden, regardless of risk score."}
{"ts": "131:28", "speaker": "I", "text": "Interessant. Gibt es ungeschriebene Regeln, wie Sie mit den UX-Teams diese Anpassungen kommunizieren?"}
{"ts": "131:42", "speaker": "E", "text": "Ja, wir machen keine formellen Change Requests für Kleinigkeiten, sondern haben ein wöchentliches 'Design & QA Sync'. Dort bringen UX-Leads und wir quick wins ein, ohne den offiziellen RFC-Prozess zu belasten."}
{"ts": "132:10", "speaker": "I", "text": "Switching gears – wie fließen denn Analytics-Insights aus den flaky Tests zurück in Ihre Testpläne?"}
{"ts": "132:22", "speaker": "E", "text": "Wir haben einen nightly job, der die Flake-Raten aus dem Orchestrator-Log in unser QA-Dashboard zieht. Bei >15 % Flake-Rate wird ein QA-IMPR Ticket erstellt, und der betroffene Test geht in Quarantäne, until fixed by feature or infra teams."}
{"ts": "132:50", "speaker": "I", "text": "Und wie priorisieren Sie unter diesen QA-IMPR Tickets?"}
{"ts": "133:05", "speaker": "E", "text": "Die Priorisierung hängt von der betroffenen Komponente ab. Core Orchestration gets P1, peripheral analytics views P3. Das ist in POL-QA-014 Annex B so definiert, und wir halten uns da ziemlich strikt dran."}
{"ts": "144:00", "speaker": "I", "text": "Zum Thema SLA — könnten Sie ein bisschen genauer ausführen, wie Sie bei der Hera QA Platform die 200 ms response time target in den Tests abbilden?"}
{"ts": "144:05", "speaker": "E", "text": "Ja, sicher. Wir haben die SLA-Metriken direkt in unsere orchestrierten Test-Suites integriert, mit Threshold-Checks in den Jenkins Pipelines. Also, if a service call exceeds 180 ms, it triggers a warning stage; über 200 ms wird der Build als 'unstable' markiert."}
{"ts": "144:12", "speaker": "I", "text": "Und wie reagieren Sie dann praktisch auf so einen 'unstable' Build?"}
{"ts": "144:18", "speaker": "E", "text": "Da greifen wir auf RB-QA-051, Abschnitt 4.2 zurück — dort ist ein Sofort-Analyse-Flow beschrieben. Usually, we first check the synthetic monitoring logs und dann, wenn's reproduzierbar ist, erstellen wir ein Ticket im Hera-QA Jira Board, meistens mit Label 'SLA-Breach'."}
{"ts": "144:27", "speaker": "I", "text": "Sie hatten vorhin erwähnt, dass UX-Befunde manchmal Prioritäten verschieben. Gab es mal einen Fall, wo ein SLA-Test dafür zurückgestellt wurde?"}
{"ts": "144:34", "speaker": "E", "text": "Ja, im Sprint 38. Wir hatten einen UX-Finding aus einer Remote-Session, der zeigte, dass die Filter-UI extrem verwirrend war. We decided to reassign two QA engineers from SLA-load testing to exploratory usability testing — das war ein bewusster Trade-off, dokumentiert in Decision Log DL-HER-038."}
{"ts": "144:44", "speaker": "I", "text": "Interessant. Wie haben Sie das Risiko dabei bewertet?"}
{"ts": "144:50", "speaker": "E", "text": "Nach POL-QA-014 haben wir den Risk Score für die betroffenen APIs geprüft — low usage, no external clients — deswegen akzeptabel. Wir haben das Risk Acceptance Form RAF-22-038 ausgefüllt und vom Product Owner signieren lassen."}
{"ts": "144:59", "speaker": "I", "text": "How do these acceptance forms integrate with your traceability matrix?"}
{"ts": "145:05", "speaker": "E", "text": "Sie sind als Artefakt-Typ 'RiskAcceptance' in den Matrix-Tool-HER integriert. That way, wenn jemand in der Zukunft eine Lücke sieht, sieht er auch die Begründung und kann die Entscheidung revisiten."}
{"ts": "145:12", "speaker": "I", "text": "Gab es schon mal einen Fall, wo Sie so eine Entscheidung rückgängig gemacht haben?"}
{"ts": "145:18", "speaker": "E", "text": "Einmal, ja. Drei Monate später gab’s eine neue Kundenintegration, suddenly die low-usage API wurde kritisch. We reopened the RAF, haben neue Tests geschrieben und das SLA-Profil aktualisiert."}
{"ts": "145:27", "speaker": "I", "text": "Das heißt, die Orchestrierung musste auch angepasst werden?"}
{"ts": "145:32", "speaker": "E", "text": "Genau, wir haben im Orchestration-Config File 'her_orch.yaml' die Priorität hochgesetzt. Außerdem ein neues Analytics-Signal hinzugefügt: latency_p95. That way, flaky behaviour in high-load conditions wurde schneller erkannt."}
{"ts": "145:41", "speaker": "I", "text": "Zum Abschluss: gibt es ungeschriebene Regeln, wie Sie mit neuen SLA-Anforderungen umgehen?"}
{"ts": "145:45", "speaker": "E", "text": "Ja, wir sagen intern: 'erst messen, dann alarmieren'. Also, wir setzen neue thresholds zunächst in passive mode, observe für zwei Sprints. Only when wir genug Baseline-Daten haben, schalten wir das als hard gate."}
{"ts": "145:35", "speaker": "I", "text": "Bevor wir tiefer in die aktuellen Risiken gehen – können Sie mir kurz schildern, wie sich die internen QA-Guidelines seit Beginn der Build-Phase verändert haben?"}
{"ts": "145:39", "speaker": "E", "text": "Ja, also äh, am Anfang waren die Vorgaben noch ziemlich generisch, viel aus POL-QA-014 übernommen. Inzwischen haben wir spezifische Hera-Anhänge, z. B. Annex-HER-02, der genau beschreibt, wie wir die Orchestrierungs-Module mit den Analytics-Hooks verbinden."}
{"ts": "145:46", "speaker": "I", "text": "And that annex, does it explicitly mention how to handle test retries for flaky detection?"}
{"ts": "145:50", "speaker": "E", "text": "Yes, es gibt einen Abschnitt, der die Retry-Strategie definiert – max zwei automatische Wiederholungen, danach markiert das System den Test als 'flaky candidate' und schiebt ihn in die Analysepipeline."}
{"ts": "145:56", "speaker": "I", "text": "Können Sie ein Beispiel aus einem echten Ticket nennen, wo diese Regel relevant war?"}
{"ts": "146:00", "speaker": "E", "text": "Klar, Ticket QA-HER-584 zeigt das gut: Wir hatten eine API-Response-Delay in Stage, der Test schlug zweimal fehl, beim dritten Lauf lief er durch. Analytics hat dann das Delay mit der Infrastruktur-Latenz korreliert."}
{"ts": "146:07", "speaker": "I", "text": "Interesting – und wie fließt so eine Erkenntnis zurück ins Team?"}
{"ts": "146:11", "speaker": "E", "text": "Wir haben einen wöchentlichen Flaky-Review-Call mit DevOps und Data. Dort verknüpfen wir die Analytics-Reports mit der Traceability-Matrix, sodass klar ist, welche User Stories indirekt betroffen sein könnten."}
{"ts": "146:18", "speaker": "I", "text": "Speaking of traceability, do you integrate UX findings into that matrix?"}
{"ts": "146:22", "speaker": "E", "text": "Teilweise – z. B. wenn UX-Research eine Anpassung am Workflow empfiehlt, dann tragen wir das als Non-Functional Requirement in die Matrix ein mit einem Verweis auf das UX-Board-ID."}
{"ts": "146:28", "speaker": "I", "text": "Gab es denn Fälle, wo UX-Änderungen direkt zu einem Risiko im Testplan geführt haben?"}
{"ts": "146:32", "speaker": "E", "text": "Ja, etwa bei RFC-1770. Da wurde ein Button-Placement geändert, was in automatisierten UI-Tests zu massiven false negatives geführt hat. Wir mussten kurzfristig Testabdeckung reduzieren, um die Deadline für Sprint 14 zu halten."}
{"ts": "146:40", "speaker": "I", "text": "And that was a conscious trade-off, documented somewhere?"}
{"ts": "146:44", "speaker": "E", "text": "Genau, das steht im Risk Log RL-HER-09, mit Verweis auf SLA-Sektion 4.2, wo wir festgehalten haben, dass eine temporäre Abdeckungslücke tolerierbar ist, solange sie innerhalb von 2 Releases geschlossen wird."}
{"ts": "146:51", "speaker": "I", "text": "Wie haben Sie sichergestellt, dass dieser Gap tatsächlich geschlossen wird?"}
{"ts": "146:55", "speaker": "E", "text": "Wir haben im Runbook RB-QA-051 einen Reminder-Check eingebaut, der vor jedem Release einen diff der offenen Gaps gegen den Risk Log zieht. So konnten wir im Release 16 bestätigen, dass der RFC-1770-Gap geschlossen war."}
{"ts": "147:35", "speaker": "I", "text": "Könnten Sie mir ein Beispiel geben, bei dem ein UX-Research-Ergebnis direkt in Ihre Teststrategie eingeflossen ist?"}
{"ts": "147:39", "speaker": "E", "text": "Ja, klar. Wir hatten im Februar eine UX-Session, bei der festgestellt wurde, dass das Dashboard der Hera QA Platform bei hoher Testlast die Übersicht verliert. In der Folge haben wir einen neuen Usability-Testfall in die Build-Phase-Suite aufgenommen, der simuliert, wie ein QA-Lead 200+ Testläufe überwacht. Dieser Test orientiert sich an den UX-Wireframes aus Ticket UXF-234 und fließt auch in unsere Regression ein."}
{"ts": "147:47", "speaker": "I", "text": "So that meant you had to adjust existing cases or create new ones entirely?"}
{"ts": "147:51", "speaker": "E", "text": "Mostly create new. Einige bestehende Performance-Tests haben wir leicht angepasst, um die Scroll- und Filter-Interaktionen mit zu messen. Aber der Hauptteil war neu, und wir haben das mit der Orchestrierungs-Engine so verknüpft, dass die UI-Checks parallel zu den API-Loadtests laufen."}
{"ts": "147:59", "speaker": "I", "text": "Interessant. Gibt es ungeschriebene Regeln für die Zusammenarbeit mit dem UX-Team?"}
{"ts": "148:03", "speaker": "E", "text": "Ja, eine wichtige ist: Änderungen an den UI-Elementen werden mindestens zwei Sprints vorher kommuniziert. Das steht nicht in einem offiziellen Dokument, aber es ist eine Art Gentleman's Agreement, um zu vermeiden, dass wir in der Build-Phase plötzlich Tests brechen."}
{"ts": "148:10", "speaker": "I", "text": "Switching gears — how do you feed analytics from flaky test detection back into planning?"}
{"ts": "148:15", "speaker": "E", "text": "Wir nehmen die Signals aus dem Analytics-Modul, z.B. den Flake-Score > 0.3 aus Runbook RB-QA-051, und markieren diese Tests in der Traceability-Matrix mit einer gelben Flagge. In der Sprintplanung priorisieren wir deren Stabilisierung, oft in Abstimmung mit dem Platform-Team, wenn es Infrastrukturthemen sind."}
{"ts": "148:24", "speaker": "I", "text": "Gab es zuletzt ein Beispiel, wo ein Infrastrukturproblem die Flakiness verursacht hat?"}
{"ts": "148:28", "speaker": "E", "text": "Ja, im Build 1.7 hatten wir im Modul TestExec-Node sporadische Timeout-Fehler. Das ging auf eine falsch konfigurierte Container-Limit-Einstellung zurück. Nach Korrektur gemäß RFC-1821 ist die Flake-Rate von 0.42 auf 0.09 gefallen."}
{"ts": "148:37", "speaker": "I", "text": "Und wie beeinflusst so ein Fix Ihre SLA-Ziele?"}
{"ts": "148:41", "speaker": "E", "text": "Direkt positiv. Unser internes SLA für Testdurchlaufzeit ist 95% < 12 Minuten. Mit weniger Flakes brauchen wir weniger Re-Runs, also bleiben wir unter der Schwelle. Das ist besonders kritisch, weil Build-Phase-Deployments laut SLA-Dokument SLA-QA-002 innerhalb einer Stunde auf Staging sein müssen."}
{"ts": "148:50", "speaker": "I", "text": "Gab es auch Momente, wo Sie Testabdeckung bewusst reduziert haben für eine Deadline?"}
{"ts": "148:54", "speaker": "E", "text": "Ja, Release 1.6. Wir haben Low-Risk-UI-Tests (Risiko < 0.1 nach POL-QA-014) temporär ausgelassen, um die Deadline für eine Kunden-Demo einzuhalten. Das habe ich in der Risk-Acceptance-List RAL-2023-11 dokumentiert und im Review mit Product abgesegnet."}
{"ts": "149:03", "speaker": "I", "text": "Und welche Risiken haben Sie damals bewusst in Kauf genommen?"}
{"ts": "149:07", "speaker": "E", "text": "Das Hauptrisiko war, dass kleinere UI-Inkonsistenzen unbemerkt bleiben. Wir haben das akzeptiert, weil die Demo-Funktionalität primär API-basiert war. Im Post-Mortem (PM-Note PMN-1.6-UI) haben wir das dokumentiert und festgelegt, dass solche Abweichungen innerhalb von 48h nach Go-Live gefixt werden müssen."}
{"ts": "149:35", "speaker": "I", "text": "Zum Thema flaky tests noch einmal — welche Signale aus dem Analytics-Modul waren für Sie zuletzt besonders hilfreich?"}
{"ts": "149:40", "speaker": "E", "text": "Also, wir haben in den letzten zwei Sprints vor allem die Signalgruppe 'intermittent API timeout' im Hera-Analytics-Dashboard beobachtet. That pattern popped up in the nightly orchestration runs, und wir haben das mit den Logs aus RB-QA-051 Step 7 korreliert, um falsche Positivmeldungen zu reduzieren."}
{"ts": "149:52", "speaker": "I", "text": "Und wie fließen diese Erkenntnisse konkret zurück in Ihre Testplanung?"}
{"ts": "149:56", "speaker": "E", "text": "Wir haben eine Heuristik eingeführt: wenn ein Test drei Mal in Folge mit identischem Stacktrace flaky ist, dann markieren wir ihn im Orchestration-Plan als 'low priority' bis ein Fix-Ticket — wie z.B. QA-4821 — implementiert ist. This reduces noise in the risk-based prioritization."}
{"ts": "150:08", "speaker": "I", "text": "Gab es dabei Anpassungen an den Runbook-Schritten?"}
{"ts": "150:12", "speaker": "E", "text": "Ja, Step 5 des RB-QA-051, der eigentlich nur die Standard-Retries setzt, wurde erweitert um eine Condition, die das UX-Tag berücksichtigt. That way, wenn UX-critical tests flaken, they are escalated despite low risk tags."}
{"ts": "150:23", "speaker": "I", "text": "Interessant, also ein Crossover zwischen UX und Orchestration."}
{"ts": "150:27", "speaker": "E", "text": "Genau, das war so ein klassischer multi-hop Zusammenhang: wir mussten die UX-Prioritäten aus dem Research-Backlog mit den technischen Metriken aus Analytics verheiraten. Without that, some usability regressions wären uns bis nach dem Release entgangen."}
{"ts": "150:39", "speaker": "I", "text": "Wie kam es zu dieser Entscheidung, war das ein formelles Change Request?"}
{"ts": "150:43", "speaker": "E", "text": "Nein, eher ein Soft-Change. Wir haben das in einer QA-Guild-Session diskutiert und dann als 'Runbook Note' in Confluence dokumentiert. It bypassed the full RFC process because es als operative Feinjustierung galt."}
{"ts": "150:54", "speaker": "I", "text": "Gab es Risiken bei diesem Vorgehen?"}
{"ts": "150:58", "speaker": "E", "text": "Ja, das größte Risiko war, dass wir durch das manuelle Eskalieren das SLA-Ratio für 'resolved flakies' unter 95% drücken. Wir haben das explizit im Risk Log unter ID R-HER-092 vermerkt und als akzeptiert gekennzeichnet, weil der Benefit für die UX-Kritikalität höher bewertet wurde."}
{"ts": "151:10", "speaker": "I", "text": "Wie haben Sie das SLA trotzdem im Blick behalten?"}
{"ts": "151:14", "speaker": "E", "text": "Wir haben wöchentliche SLA-Checks eingeführt, small script in Python that parses the orchestration output und die offenen Flaky-Tickets zählt. So konnten wir sicherstellen, dass wir nicht dauerhaft unter die 95% fallen."}
{"ts": "151:25", "speaker": "I", "text": "Wurde diese Maßnahme schon in einem Release Post-Mortem evaluiert?"}
{"ts": "151:29", "speaker": "E", "text": "Ja, im Post-Mortem zu Release 0.9-Build15 haben wir die Metriken verglichen: UX-critical Pass Rate verbesserte sich um 12%, SLA sank temporär auf 93,8%, war aber zum nächsten Sprint wieder bei 96%. The trade-off war clearly worth it for us."}
{"ts": "152:05", "speaker": "I", "text": "Bevor wir tiefer in die aktuellen Deployments gehen, könnten Sie noch mal erklären, wie genau Sie die Schnittstelle zwischen QA Orchestration und Data Analytics im Hera-Projekt gestalten?"}
{"ts": "152:14", "speaker": "E", "text": "Ja, klar. Wir haben im Build-Phase-Setup einen wöchentlichen sync mit dem Data-Team, wo wir anomaly patterns aus den Test Runs besprechen. About 60% of the meeting is about flaky signal patterns in the orchestrator logs, the rest about predictive risk scoring from their models."}
{"ts": "152:28", "speaker": "I", "text": "Das heißt, Sie verknüpfen quasi die Orchestrator-Metriken direkt mit den Risikopriorisierungen aus POL-QA-014?"}
{"ts": "152:34", "speaker": "E", "text": "Genau, und das ist ein Punkt, der uns durch die Traceability-Matrix hilft: Wir sehen, welche RFIs aus RFC-1770 z.B. mehrfach flaky waren und können die in der nächsten Testwelle höher raten. Without that cross-link, we would miss systemic issues."}
{"ts": "152:47", "speaker": "I", "text": "Interessant. Gibt es da ein aktuelles Beispiel?"}
{"ts": "152:51", "speaker": "E", "text": "Letzte Woche hatten wir im Ticket QA-6721 einen API-Endpoint, der in 3 von 10 orchestrated runs gefailed ist. Das Data-Team hat das mit einem Memory-Leak korreliert, das aus einer UX-getriggerten Komponente kam – eine Search Suggestion, die in der Build-Phase neu war."}
{"ts": "153:08", "speaker": "I", "text": "Ah, damit schlagen Sie die Brücke zur UX-Kollaboration. Können Sie beschreiben, wie so ein UX-Fund in die Teststrategie zurückfließt?"}
{"ts": "153:15", "speaker": "E", "text": "Ja, wir haben eine ungeschriebene Regel: wenn ein UX-Research-Prototyp Performance-Kosten hat, markieren wir das im Orchestration-Runbook RB-QA-051 mit einem ⚠️-Tag. That tag triggers extra load-tests in the next nightly build."}
{"ts": "153:29", "speaker": "I", "text": "Und diese Tags sind nur intern sichtbar oder auch im UX-Backlog?"}
{"ts": "153:33", "speaker": "E", "text": "Nur intern im QA-Orchestrator, aber wir exportieren wöchentlich eine vereinfachte Liste für UX, damit sie Prioritäten anpassen können. It keeps the loop tight without overloading them with technical noise."}
{"ts": "153:46", "speaker": "I", "text": "Gab es denn Situationen, wo diese enge Kopplung auch Nachteile hatte?"}
{"ts": "153:52", "speaker": "E", "text": "Ja, einmal im Sprint 14 haben wir extra UX-Tests gefahren, obwohl das SLA-Deadline-Fenster für den Build schon eng war. We ended up reducing coverage on a low-risk module to keep the delivery date — documented under Risk Log R-882."}
{"ts": "154:08", "speaker": "I", "text": "Wie haben Sie das Risiko damals bewertet?"}
{"ts": "154:12", "speaker": "E", "text": "Wir haben POL-QA-014 angewendet, Impact war 'low' auf einen nicht-kritischen Batch-Processor. Combined with SLA-T-3 tolerance, it was acceptable. Wir haben aber eine Notiz ins Runbook geschrieben, falls Folgeprobleme auftreten."}
{"ts": "154:26", "speaker": "I", "text": "Das klingt nach einer bewussten Risikoakzeptanz. Haben Sie dafür noch andere Beispiele, vielleicht mit Analytics-Bezug?"}
{"ts": "154:33", "speaker": "E", "text": "Ja, in QA-Analytic-Report #57 haben wir einen Flaky-Test im Payment-Mock festgestellt, der nur 0,5% der Runs betraf. We accepted the flakiness for this phase to avoid blocking the orchestration pipeline, noting it in RFC-Closure 1770-C."}
{"ts": "153:35", "speaker": "I", "text": "Noch einmal zur Orchestrierung, können Sie mir ein Beispiel geben, wie sich eine Anpassung in den Analytics-Dashboards direkt auf den Testplan ausgewirkt hat?"}
{"ts": "153:39", "speaker": "E", "text": "Ja, im letzten Sprint hat das Analytics-Modul plötzlich eine Spike in den 'retry counts' für Module M4 und M7 angezeigt. Das hat uns veranlasst, laut Runbook RB-QA-051 Schritt 6 zu frühzeitigem Re‑Priorisieren überzugehen."}
{"ts": "153:45", "speaker": "I", "text": "So you basically pivoted mid‑sprint? That seems disruptive."}
{"ts": "153:49", "speaker": "E", "text": "Genau, aber wir haben gelernt, dass ein früher Pivot weniger disruptive ist als das Ignorieren solcher Signale. Im POL-QA-014 Appendix C steht sogar, dass high‑risk Module sofortige Aufmerksamkeit erfordern."}
{"ts": "153:55", "speaker": "I", "text": "Und wie haben Sie die Cross‑Team Kommunikation gehandhabt?"}
{"ts": "153:59", "speaker": "E", "text": "Wir haben in unserem internen Tool 'Connex' ein Alert-Flag gesetzt, das automatisch den Platform‑ und Data‑Channel benachrichtigt. Innerhalb von 20 Minuten hatten wir ein gemeinsames Stand‑up."}
{"ts": "154:05", "speaker": "I", "text": "Did UX get looped in as well?"}
{"ts": "154:09", "speaker": "E", "text": "Ja, weil M7 direkt den Reporting‑Screen beeinflusst. Unser UX-Lead hat sofort einen Quick-Check der User Journeys gemacht, um zu sehen, ob wir test cases im Bereich Accessibility erweitern müssen."}
{"ts": "154:15", "speaker": "I", "text": "Interessant, und gab es da eine ungeschriebene Regel, wie UX und QA interagieren?"}
{"ts": "154:19", "speaker": "E", "text": "Die ungeschriebene Regel lautet: 'Kein UI‑Change ohne QA‑Sicht'. Das heißt, selbst kleinste Farb‑ oder Layoutänderungen werden in unserem Traceability‑Sheet vermerkt."}
{"ts": "154:25", "speaker": "I", "text": "Could you link that with the RFC process?"}
{"ts": "154:29", "speaker": "E", "text": "Ja, jede RFC, wie z. B. RFC‑1822 für den neuen Filter, wird im Sheet mit Test‑IDs verknüpft. So stellen wir sicher, dass Änderungen im Code, im UX‑Design und in der QA‑Abdeckung synchron laufen."}
{"ts": "154:35", "speaker": "I", "text": "Gab es daraus schon mal einen Engpass?"}
{"ts": "154:39", "speaker": "E", "text": "Klar, bei RFC‑1761 mussten wir 14 zusätzliche Testfälle schreiben, was die Build‑Phase um zwei Tage verlängert hat. Aber dadurch konnten wir zwei kritische Defects (DEF‑908 und DEF‑912) vor dem Release finden."}
{"ts": "154:45", "speaker": "I", "text": "That’s a good catch. Did that influence your SLA adherence?"}
{"ts": "154:49", "speaker": "E", "text": "Minimal, unser SLA für die QA‑Durchlaufzeit wurde um 3 % überschritten, was durch ein akzeptiertes Risiko‑Ticket RT‑QA‑077 dokumentiert ist. Der Benefit in Defect‑Vermeidung hat das klar gerechtfertigt."}
{"ts": "155:05", "speaker": "I", "text": "Bevor wir in die Details der letzten Release gehen — können Sie mir schildern, wie Sie in der Build-Phase jetzt konkret mit den Data Services interagieren? Ich denke da an das Hera Analytics Modul."}
{"ts": "155:12", "speaker": "E", "text": "Ja, klar. Also, in der Build-Phase habe ich täglich ein 15‑Minuten Slot mit dem Data Services Lead. Wir synchronisieren die Event-Schema-Änderungen, die ja auch im Orchestrator Debug-Modus landen. That’s critical because analytics misalignment will break our flaky test detection pipeline."}
{"ts": "155:25", "speaker": "I", "text": "Und die Änderungen fließen dann direkt in Ihre Teststrategien ein?"}
{"ts": "155:29", "speaker": "E", "text": "Genau, wir haben im Runbook RB-QA-051 einen Abschnitt 'Data Dependency Hooks'. Dort steht, wie wir neue Felder in den Event Payloads sofort in die Testdatengenerierung aufnehmen. Otherwise, the coverage matrix in our risk model would get skewed."}
{"ts": "155:42", "speaker": "I", "text": "Interessant. Speaking of coverage — wie wirkt sich das auf Ihre Priorisierung aus?"}
{"ts": "155:47", "speaker": "E", "text": "Nun, das ist der Multi-Hop-Teil: Wir mappen die neuen Datenfelder auf kritische User Journeys, die wiederum im UX-Lab getestet werden. Wenn UX eine Änderung am Flow anstößt, geben wir diese Info zurück an Data, damit die Analytics-Checks nicht ins Leere laufen."}
{"ts": "155:59", "speaker": "I", "text": "Das heißt, UX, Data und QA hängen in dieser Phase eng zusammen."}
{"ts": "156:03", "speaker": "E", "text": "Ja, und manchmal müssen wir Kompromisse machen. For example, in Ticket QA-775 haben wir bewusst den Analytics-Drilldown für zwei Low-Risk Szenarien deaktiviert, um die Deadline für den Demo-Sprint zu halten."}
{"ts": "156:16", "speaker": "I", "text": "War das nicht riskant im Hinblick auf Ihre SLAs?"}
{"ts": "156:20", "speaker": "E", "text": "Es war ein kalkuliertes Risiko. SLA-QA-02 erlaubt uns in der Build-Phase bis zu 5% Abdeckungslücke für Non-Critical Features. Wir haben das im Release-Note dokumentiert und im Risk Log RL-HER-09 verlinkt."}
{"ts": "156:33", "speaker": "I", "text": "Und wie reagieren Stakeholder darauf?"}
{"ts": "156:37", "speaker": "E", "text": "Solange wir transparent sind und die Trade-offs mit Zahlen belegen — etwa aus dem Flaky-Test-Dashboard — akzeptieren sie das. Transparency is key, sonst verlieren wir Vertrauen."}
{"ts": "156:46", "speaker": "I", "text": "Gab es schon mal den Fall, dass ein solcher Trade-off später Probleme gemacht hat?"}
{"ts": "156:51", "speaker": "E", "text": "Ja, bei RFC-1821. Wir hatten damals ein Analytics-Event nicht in die Traceability Matrix aufgenommen, weil es als Low-Risk eingestuft wurde. Drei Sprints später stellte sich heraus, dass genau dieses Event für ein kritisches UX-Signal nötig war."}
{"ts": "157:05", "speaker": "I", "text": "Wie haben Sie das behoben?"}
{"ts": "157:09", "speaker": "E", "text": "Wir haben sofort einen Hotfix-Branch erstellt, Runbook RB-QA-051 um einen Check erweitert und in POL-QA-014 die Klassifizierungskriterien nachgeschärft. Even if it was late in build phase, the corrective action restored both coverage and trust."}
{"ts": "156:37", "speaker": "I", "text": "Lassen Sie uns noch etwas tiefer gehen — when you had to connect the orchestration signals from the flaky test analytics to the risk categories in POL-QA-014, how did you actually link them in practice?"}
{"ts": "156:43", "speaker": "E", "text": "Das war so'n bisschen knifflig, weil wir zwei verschiedene Datenpipelines hatten. Wir haben erst im Analytics-Cluster die flaky signals aggregiert, dann über ein kleines Python-Skript gemappt auf die Risikokategorien aus POL-QA-014, und diese Mapping-Tabelle haben wir dann wieder in die Traceability-Matrix importiert."}
{"ts": "156:57", "speaker": "I", "text": "And did that mapping feed directly into your test scheduling logic on the Hera QA Platform?"}
{"ts": "157:01", "speaker": "E", "text": "Ja, indirectly. Wir haben im Scheduler-Modul einen Hook, der die Mapping-Tabelle als JSON einliest. Basierend auf dem Risikolevel wird dann die Testreihenfolge angepasst — high risk zuerst, low risk später in den nightly builds."}
{"ts": "157:12", "speaker": "I", "text": "Können Sie ein Beispiel geben, wo diese Priorisierung einen Release gerettet hat?"}
{"ts": "157:16", "speaker": "E", "text": "Klar, bei Release 1.8.4 hatten wir einen Payment-Flow-Test, der plötzlich flaky wurde. Das Mapping hat den als high risk markiert, der Scheduler hat ihn ganz nach vorne gezogen, wir fanden einen Datenbank-Lock-Bug rechtzeitig — sonst wäre der in Produktion gegangen."}
{"ts": "157:29", "speaker": "I", "text": "In Bezug auf RFC-1770, how did you ensure that change traces didn't break when the orchestration logic changed?"}
{"ts": "157:34", "speaker": "E", "text": "Wir haben im Runbook RB-QA-051 eine Section ergänzt, die nach jeder Änderung am Orchestrator einen Traceability-Check vorsieht. Das heißt, wir triggern automatisch ein diff-Tool, das die RFC-Bezüge prüft, und im Zweifel ein Ticket im QA-Backlog anlegt, z.B. QA-5921."}
{"ts": "157:48", "speaker": "I", "text": "Gab es da schon mal false positives?"}
{"ts": "157:51", "speaker": "E", "text": "Oh ja, besonders wenn die RFC-IDs in Kommentaren geändert wurden, ohne dass die Funktionalität betroffen war. Wir haben dann 'ne kleine Regex-Whitelist in das diff-Tool eingebaut, um die Noise zu reduzieren."}
{"ts": "158:02", "speaker": "I", "text": "Switching gears — wie lief die Zusammenarbeit mit dem UX-Team, als sie neue Usability-Kriterien für den Testreport vorgeschlagen haben?"}
{"ts": "158:08", "speaker": "E", "text": "Das war spannend: die wollten, dass Reports weniger technisch und mehr visuell werden. Wir haben dafür einen separaten UX-Review-Step in den QA-Pipeline-Jobs eingeführt, bevor Reports finalisiert und im Confluence hochgeladen werden."}
{"ts": "158:19", "speaker": "I", "text": "Und gab es ungeschriebene Regeln in dieser Zusammenarbeit?"}
{"ts": "158:22", "speaker": "E", "text": "Ja, stillschweigend gilt: UX darf Änderungen am Layout vorschlagen, aber nicht an den Metriken selbst schrauben. Das ist so 'ne etiquette, die wir etabliert haben, um die Validität der QA-Daten zu schützen."}
{"ts": "158:33", "speaker": "I", "text": "Last question — when you had to meet the SLA of 95% critical test coverage but only had 2 days before release, what trade-off did you make?"}
{"ts": "158:39", "speaker": "E", "text": "Wir haben documented in Risk Log RL-P-HER-07, dass wir zwei low-risk Szenarien ausließen — beide aus einem selten genutzten Admin-Panel. Das war bewusst akzeptiert, mit Approval aus dem Change Advisory Board, weil wir sonst die Major Release Deadline verpasst hätten."}
{"ts": "158:37", "speaker": "I", "text": "Lassen Sie uns noch mal auf die Build-Phase zurückkommen – gibt es gerade Schnittstellen, bei denen QA besonders stark involviert ist?"}
{"ts": "158:43", "speaker": "E", "text": "Ja, aktuell vor allem mit dem Data-Team. Wir haben im Hera QA Platform Projekt mehrere Pipelines, die neue Testdaten generieren, und QA muss sicherstellen, dass diese Daten konsistent mit unseren Referenz-Schemas aus RB-QA-051 sind."}
{"ts": "158:56", "speaker": "I", "text": "Do you feed back issues from those pipelines directly into their backlog, or via a shared dashboard?"}
{"ts": "159:02", "speaker": "E", "text": "Meistens direkt über unser gemeinsames Ticket-System, z.B. TCK-HER-482. Aber, äh, wenn es sich um flaky behaviour handelt, loggen wir das zusätzlich im QA Analytics Dashboard, damit das Data-Team die Muster erkennt."}
{"ts": "159:15", "speaker": "I", "text": "Und wie verknüpfen Sie das mit Risk-Based Testing, also praktisch über mehrere Subsysteme hinweg?"}
{"ts": "159:22", "speaker": "E", "text": "Genau da wird es spannend: Wir haben in der Traceability-Matrix eine Spalte 'Subsystem Impact'. Wenn der Data-Pipeline-Fehler etwa einen Bereich mit hoher UX-Relevanz betrifft, priorisieren wir ihn trotz niedriger technischer Severity höher – das ist quasi eine Multi-Hop-Entscheidung zwischen Data, UX und QA."}
{"ts": "159:39", "speaker": "I", "text": "Sounds like a complex mapping. Do you automate any of that?"}
{"ts": "159:44", "speaker": "E", "text": "Teilweise. Wir haben ein Script im Build-Server, das die Impact-Kategorien aus dem letzten Release-Plan zieht und mit den aktuellen Defects korreliert. Das ersetzt aber nicht den manuellen Review, gerade wenn UX-Aspekte involviert sind."}
{"ts": "159:58", "speaker": "I", "text": "Gibt es Stellen im Runbook, die Sie dafür angepasst haben?"}
{"ts": "160:03", "speaker": "E", "text": "Ja, in RB-QA-051 haben wir Schritt 4.3 erweitert: Neben der technischen Risikobewertung ist jetzt auch eine UX-Impact-Checkliste enthalten, die von den QA-Analyst:innen abgeprüft wird."}
{"ts": "160:15", "speaker": "I", "text": "Let’s pivot to decisions under pressure – hatten Sie zuletzt einen Fall, wo Sie Coverage reduzieren mussten?"}
{"ts": "160:21", "speaker": "E", "text": "Ja, Release 0.9.7. Wir mussten die End-to-End-Tests für zwei Low-Risk-Module streichen, um die SLA von 48h Turnaround für einen kritischen Patch (siehe SLA-QA-03) einzuhalten. Das war im Risikolog HER-RISK-112 dokumentiert."}
{"ts": "160:35", "speaker": "I", "text": "Wie sind Sie mit dem Risiko umgegangen, das dann offen blieb?"}
{"ts": "160:40", "speaker": "E", "text": "Wir haben Monitoring-Hooks in die betroffenen Module implementiert, um im Live-Betrieb sofortige Alerts zu erhalten. Außerdem haben wir mit dem Product Owner vereinbart, dass diese Tests im nächsten Sprint nachgezogen werden."}
{"ts": "160:52", "speaker": "I", "text": "And did you get any pushback from stakeholders?"}
{"ts": "160:56", "speaker": "E", "text": "Minimal. Weil wir transparent im Weekly QA-Report, inkl. Verweis auf HER-RISK-112 und die Monitoring-Maßnahmen, kommuniziert haben, war das für alle nachvollziehbar – trotz der, äh, ungeschriebenen Regel, dass wir ungern Abdeckung opfern."}
{"ts": "160:37", "speaker": "I", "text": "Bevor wir zum Abschluss kommen, wollte ich noch verstehen—äh—wie Sie im Build-Phase-Kontext mit den Data Engineers konkret zusammenarbeiten, wenn ein Testdatenproblem auftaucht?"}
{"ts": "160:43", "speaker": "E", "text": "Also, meistens starte ich mit einem Quick-Sync via unserem internen Kanal, und dann nutzen wir das Datenprofiling-Tool aus dem Hera Orchestration Stack. We align on the schema changes first, dann aktualisiere ich die Testdaten-Sets im Sandbox-Cluster, bevor wir überhaupt einen Build durchjagen."}
{"ts": "160:55", "speaker": "I", "text": "Und gibt es dazu einen formalen Schritt in einem Runbook?"}
{"ts": "161:00", "speaker": "E", "text": "Ja, das ist im RB-QA-051 unter Abschnitt 4.2 dokumentiert, dort steht 'Data Schema Validation before Orchestration Trigger'. It’s a gate we cannot skip, selbst wenn die Deadline drückt."}
{"ts": "161:12", "speaker": "I", "text": "Sie hatten vorhin flaky tests erwähnt, wie verbinden Sie denn die Analytics Signale mit der Release-Pipeline?"}
{"ts": "161:18", "speaker": "E", "text": "Wir haben ein kleines Microservice-Modul, das wir intern 'FlakeWatch' nennen. It ingests failure patterns aus den letzten zehn Runs und annotiert dann die entsprechenden Test IDs in der Orchestration Queue. So kann der Scheduler die Priorität senken oder sie in einen quarantined bucket verschieben."}
{"ts": "161:34", "speaker": "I", "text": "Interessant. Und wie wirkt sich das auf die UX-getriebenen Tests aus?"}
{"ts": "161:39", "speaker": "E", "text": "Naja, UX-Scenarios haben bei uns meistens eine hohe Business-Priorität. Wenn FlakeWatch dort Alarm schlägt, dann setzen wir sofort einen Hotfix-Sprint an, weil usability regressions under SLA T-12 days sind kritisch für P-HER."}
{"ts": "161:52", "speaker": "I", "text": "Gibt es dafür ein Beispiel aus den letzten Wochen?"}
{"ts": "161:57", "speaker": "E", "text": "Ja, Ticket QA-7782. Da hat ein A/B-Test-Flow für das Onboarding intermittierend versagt. We traced it back to a race condition im neuen Tooltip-Rendering, was wir innerhalb von 48h gefixt haben, um das SLA zu halten."}
{"ts": "162:11", "speaker": "I", "text": "Wie entscheiden Sie in solchen Fällen, ob Sie einen Release blocken?"}
{"ts": "162:16", "speaker": "E", "text": "Das hängt von der Severity ab, die wir im Risk Register hinterlegt haben. For UX-critical paths, severity 1 always blocks. Alles andere wird anhand der Impact-Matrix in POL-QA-014 bewertet, und dann mit dem Product Owner abgestimmt."}
{"ts": "162:29", "speaker": "I", "text": "Und wenn der Product Owner drängt, trotzdem zu releasen?"}
{"ts": "162:34", "speaker": "E", "text": "Dann dokumentiere ich das als 'Accepted Risk' im Confluence QA Log, verweise auf das zugehörige Runbook und markiere es mit der Risk-ID, z.B. R-UX-042. That way, wir haben später eine saubere Audit-Spur."}
{"ts": "162:47", "speaker": "I", "text": "Letzte Frage: Welche ungeschriebenen Regeln helfen Ihnen, diese Balance zwischen Qualität und Time-to-Market zu finden?"}
{"ts": "162:53", "speaker": "E", "text": "Eine ist: 'Never surprise the PO'. Wenn wir früh warnen, gibt es Spielraum. Und zweitens: 'Test the riskiest thing first', auch wenn es nicht im Sprint-Plan steht. Those heuristics sind nicht offiziell, aber sie halten uns oft aus der Schusslinie."}
{"ts": "162:13", "speaker": "I", "text": "Bevor wir auf die konkrete Releaseplanung eingehen, können Sie mir kurz sagen, ob es seit unserem letzten Gespräch Änderungen an der Test-Orchestrierung gegeben hat?"}
{"ts": "162:18", "speaker": "E", "text": "Ja, wir haben den Scheduler-Teil im Hera QA Orchestrator angepasst. Vorher hatten wir ein statisches Zeitfenster, now we switched to a load-based trigger. Das bedeutet, wenn die Build-Pipeline über 80% Auslastung hat, verschieben wir low-priority tests automatisch."}
{"ts": "162:26", "speaker": "I", "text": "Interessant. Hat das Auswirkungen auf die Risk-Priorisierung aus POL-QA-014 gehabt?"}
{"ts": "162:31", "speaker": "E", "text": "Indirekt, ja. Weil wir diese low-priority cases oft aus der Kategorie 'UI regression low impact' nehmen, müssen wir die Traceability-Matrix entsprechend updaten. Wir markieren in der Matrix einen temporären Defer-Status, damit bei RFC-Änderungen klar ist, dass es einen Gap gab."}
{"ts": "162:40", "speaker": "I", "text": "Und wie kommunizieren Sie solche Gaps an das UX-Team?"}
{"ts": "162:45", "speaker": "E", "text": "Wir haben einen Slack-Webhook, der aus dem Runbook RB-QA-051 Abschnitt 4.3 angestoßen wird. Dort steht: 'If UI regression low impact tests are deferred, push notification to #ux-qa-bridge'. So bekommen die UX-Designer gleich den Hinweis und können ggf. manuell checken."}
{"ts": "162:54", "speaker": "I", "text": "That actually sounds like a neat feedback loop. Gab es schon mal den Fall, dass UX dadurch einen kritischen Bug entdeckt hat?"}
{"ts": "162:59", "speaker": "E", "text": "Ja, im Ticket QA-HER-882. UX hat bemerkt, dass nach einer kleinen Änderung aus RFC-1782 ein Button-Label abgeschnitten war. Das wäre in unserer automatisierten Suite nicht aufgefallen, weil wir den entsprechenden Test deferred hatten."}
{"ts": "163:08", "speaker": "I", "text": "Verstehe. Kommen wir auf die Analytics zurück: how do you integrate flaky test data into these deferral decisions?"}
{"ts": "163:13", "speaker": "E", "text": "Wir nutzen ein Signal aus dem FlakyTest-Analyzer-Modul. Wenn ein Test in den letzten 5 Runs mehr als 40% non-deterministic results zeigte, wird er automatisch ins 'monitor-only' Segment verschoben. Das beeinflusst dann auch, ob er bei Lastspitzen deferred wird."}
{"ts": "163:22", "speaker": "I", "text": "Gibt es da nicht die Gefahr, dass kritische Tests versehentlich zu oft im Monitor-Modus laufen?"}
{"ts": "163:27", "speaker": "E", "text": "Doch, deshalb haben wir in POL-QA-014 einen Override-Mechanismus. High-risk category tests, even if flaky, müssen bei bestimmten Release-Gates zwingend laufen. Das wird im Gate-Config YAML festgelegt."}
{"ts": "163:36", "speaker": "I", "text": "Wie stimmen Sie diese Gate-Configs mit den SLA-Vorgaben ab?"}
{"ts": "163:41", "speaker": "E", "text": "Wir haben eine SLA-Mapping-Tabelle im Confluence, die jedem Gate-Level eine maximale Allowable-Deferral-Rate zuordnet. For example, Gate-1 darf max 5% deferral bei high-risk haben, Gate-3 bis 20%."}
{"ts": "163:49", "speaker": "I", "text": "Das klingt nach einem komplexen Balanceakt. Gibt es ungeschriebene Regeln, wie Sie bei Zielkonflikten vorgehen?"}
{"ts": "163:54", "speaker": "E", "text": "Ja, inoffiziell gilt: Wenn ein Ticket als 'customer visible' markiert ist, darf es nicht durch Last-bedingtes Deferral verzögert werden. Auch wenn das formal nicht in POL-QA-014 steht, ist es ein Konsens im Team, um Reputationsrisiken zu vermeiden."}
{"ts": "163:49", "speaker": "I", "text": "Sie hatten vorhin kurz erwähnt, dass Sie bei der letzten Iteration im Hera-QA Build explizit den Regression-Umfang angepasst haben. Können Sie beschreiben, wie das im Abgleich mit den Build-SLAs passiert ist?"}
{"ts": "163:56", "speaker": "E", "text": "Ja, genau, wir mussten im Release 0.9.4 einen Teil der Low-Risk-Testfälle schieben. Im SLA-Dokument SLA-QA-2023 steht klar, dass P1- und P2-Defects unter 2% bleiben müssen. Also haben wir unter POL-QA-014 eine Risikomatrix angewandt und nur die P3/P4 Kombis temporär depriorisiert."}
{"ts": "164:09", "speaker": "I", "text": "Und wie haben Sie das Team dazu gebracht, diese Priorisierung mitzutragen?"}
{"ts": "164:13", "speaker": "E", "text": "Honestly, transparency war da key. Wir haben im Daily den Trace aus der Matrix gezeigt, plus einen Auszug aus Ticket QA-7751, wo wir die erwarteten Impacts notiert hatten. Außerdem hat der Platform Lead unsere Entscheidung im Retro unterstützt."}
{"ts": "164:26", "speaker": "I", "text": "Gab es dabei Abhängigkeiten zu den Analytics-Feeds, die flaky Tests markieren?"}
{"ts": "164:30", "speaker": "E", "text": "Ja, wir haben die Insights aus RB-QA-051 Schritt 7 genutzt, um zu sehen, welche Tests ohnehin instabil waren. Das war ein Multi-Hop: Analytics liefert das Signal, das fließt in die Risikomatrix, und beeinflusst direkt die Regression-Planung."}
{"ts": "164:45", "speaker": "I", "text": "Das heißt, Sie haben technisch gesehen drei Subsysteme miteinander verknüpft?"}
{"ts": "164:49", "speaker": "E", "text": "Genau, Hera-Orchestration API, das Analytics-Modul und unser Test-Mgmt-Tool Testrix. We built a small Python bridge script, welches die flaky scores aus Analytics zieht und automatisch in Testrix-Tagging schreibt."}
{"ts": "165:02", "speaker": "I", "text": "Können Sie ein Beispiel geben, wie eine UX-Anforderung hier noch reingespielt hat?"}
{"ts": "165:06", "speaker": "E", "text": "Klar, im UX-Research Report UX-HER-12 gab es die Empfehlung, die Ladezeiten im Dashboard zu verkürzen. Daraus resultierte RFC-1770-B, und wir mussten in der Regression gezielt Performance-Tests priorisieren. That meant shifting some functional cases to the next sprint."}
{"ts": "165:20", "speaker": "I", "text": "Wie wurde das in der Traceability Matrix erfasst?"}
{"ts": "165:24", "speaker": "E", "text": "Wir haben im Traceability Sheet v4.3 eine eigene Spalte 'UX Impact' eingeführt. Dort steht z.B. RFC-1770-B mapped auf TestCase IDs PERF-21 bis PERF-29, mit Querverweis auf das UX-Doc."}
{"ts": "165:36", "speaker": "I", "text": "Gab es bei diesen Verschiebungen ein Risiko, das Sie bewusst akzeptiert haben?"}
{"ts": "165:40", "speaker": "E", "text": "Ja, wir haben im Risk Log RL-HER-058 dokumentiert, dass ein P3-Defect in der Export-Funktion ungetestet blieb. The impact war low und der Fix war schon in Dev-Branch, deshalb haben wir es für das Release akzeptiert."}
{"ts": "165:53", "speaker": "I", "text": "Wie stellen Sie sicher, dass solche akzeptierten Risiken im Nachgang nicht vergessen werden?"}
{"ts": "165:57", "speaker": "E", "text": "Wir haben eine Review-Routine im Runbook RB-QA-074 Abschnitt 5.4 – dort gehen wir quartalsweise alle accepted risks durch. Außerdem gibt’s einen Jira-Filter 'Accepted Risk' der automatisch im Sprint Planning angezeigt wird."}
{"ts": "165:49", "speaker": "I", "text": "Sie hatten vorhin erwähnt, dass Sie manchmal gezielt Coverage reduzieren, um einen Build rechtzeitig rauszubringen. Könnten Sie ein konkretes Beispiel aus der Hera QA Platform nennen, vielleicht mit Referenz auf ein Ticket?"}
{"ts": "165:56", "speaker": "E", "text": "Ja, äh, im Sprint vor drei Wochen, also Sprint 34-B, haben wir im Ticket QA-9452 documented, dass wir die Low-Risk-UI-Regression-Suite für das Admin-Panel übersprungen haben. Hintergrund: laut POL-QA-014 war das Risiko unter 2%, und wir mussten die RFC-1822 Änderungen im Backend dringend deployen, um SLA-Breach zu vermeiden."}
{"ts": "166:12", "speaker": "I", "text": "So you used the internal risk score to justify that omission. How did you document the acceptance of that risk?"}
{"ts": "166:18", "speaker": "E", "text": "Wir haben das im Risk Log RL-HER-07 eingetragen, mit Verweis auf die Traceability Matrix v5.3. Da steht explizit: 'Admin-Panel UI regression deferred; mitigation plan: test in next maintenance window.' Wir verlinken auch auf Runbook RB-QA-051 Abschnitt 4.2, der beschreibt, wie man deferred tests priorisiert."}
{"ts": "166:34", "speaker": "I", "text": "Gab es an der Stelle Diskussionen mit dem UX-Team, weil UI-Tests verschoben wurden?"}
{"ts": "166:40", "speaker": "E", "text": "Ja, und zwar in unserem wöchentlichen UX-QA-Sync. Die UX-Researcherin hat gefragt, ob wir wenigstens Hotspot-Checks auf den geänderten Buttons machen können. Wir haben dann ad hoc ein mini Exploratory Testing durchgeführt, documented im ET-Log 34-B-UX."}
{"ts": "166:55", "speaker": "I", "text": "Interesting. Switching gears — when flaky tests cluster around certain microservices, how do you decide whether to quarantine or refactor them?"}
{"ts": "167:02", "speaker": "E", "text": "Das hängt ab von der Analytics-Confidence, die wir aus dem Hera Test Orchestration Modul bekommen. Wenn die Flaky-Rate > 0.3 und korreliert mit Deploy Windows, dann quarantine. Wenn sie >0.5 und unabhängig vom Deploy, dann refactor, wie es in RB-QA-051 Annex B definiert ist."}
{"ts": "167:18", "speaker": "I", "text": "And how quickly can you loop that decision back into the orchestration pipeline?"}
{"ts": "167:23", "speaker": "E", "text": "Wir haben einen webhook in Hera Orchestrator, der Trigger setzt, sobald wir in Jira einen Test als 'Quarantine' labeln. Innerhalb von 15 Minuten wird der Test aus der main pipeline entfernt und in den nightly-run verschoben."}
{"ts": "167:36", "speaker": "I", "text": "Gab es schon mal den Fall, dass diese Automatik einen Test zu früh entfernt hat?"}
{"ts": "167:41", "speaker": "E", "text": "Ja, Ticket QA-9520. Da war die Flaky-Rate durch einen externen API-Ausfall hoch. Wir haben daraus gelernt, und jetzt prüft ein human gate in dem Prozess, ob externe Dependencies betroffen sind, bevor wir quarantinen."}
{"ts": "167:55", "speaker": "I", "text": "Letzte Frage zum Thema Trade-offs: How did you balance the SLA 99.5% uptime requirement with running extended load tests during build phase?"}
{"ts": "168:02", "speaker": "E", "text": "Wir haben die Load Tests auf die Blue-Deployment-Umgebung verschoben, die hinter einem Feature-Flag liegt. So konnten wir heavy traffic simulieren ohne das Live-System zu gefährden. Der einzige Trade-off: Wir testen nicht exakt die Produktions-Latenzen, aber halten das SLA-Risiko minimal."}
{"ts": "168:17", "speaker": "I", "text": "Wurde das in den SLA-Reports auch so transparent gemacht?"}
{"ts": "168:22", "speaker": "E", "text": "Ja, im SLA-Monitoring-Report MR-2024-05 steht im Anhang, dass Performance-Tests auf Blue liefen. Unser Compliance-Officer hat das abgenickt, weil das Runbook RB-QA-099 diesen Weg explizit erlaubt, wenn wir uns in der Build-Phase von Projekten wie Hera befinden."}
{"ts": "167:49", "speaker": "I", "text": "Lassen Sie uns noch einmal tiefer in den Fall von RFC-1825 eintauchen – wie wurde diese Änderung in Ihrer Traceability-Matrix konkret erfasst?"}
{"ts": "167:53", "speaker": "E", "text": "Ja, also in Sheet TCM-HER-05 haben wir die RFC-1825 Änderung unter Modul *Data Ingest* verlinkt und dann im selben Row die relevanten Risk IDs aus POL-QA-014 zugeordnet. That way, when the test orchestration pipeline picks up the nightly build, it automatically knows which high-risk cases to run first."}
{"ts": "167:59", "speaker": "I", "text": "Interesting, und das hat dann auch direkte Auswirkungen auf die Analytics-Module?"}
{"ts": "168:03", "speaker": "E", "text": "Genau, die Hera Analytics Engine zieht sich die Execution Order und kann daraus ableiten, ob flaky patterns in den first-priority cases auftreten. Wenn das so ist, wird im Dashboard ein gelber Marker gesetzt, und wir haben im Runbook RB-QA-051 jetzt einen Schritt 4a hinzugefügt, um diese Marker täglich zu reviewen."}
{"ts": "168:10", "speaker": "I", "text": "Wie schnell fließt so ein Finding zurück ins Testplan-Update?"}
{"ts": "168:14", "speaker": "E", "text": "Within 24h. Wir nutzen dafür ein kleines Script, das ein JIRA-Ticket vom Typ QA-Review erstellt, z. B. QA-7721 letzte Woche, und das triggert ein Sync-Meeting mit dem Platform-Team."}
{"ts": "168:20", "speaker": "I", "text": "Das klingt nach einer engen Schleife. Gab es mal den Fall, dass UX-Feedback diese Pipeline unterbrochen hat?"}
{"ts": "168:25", "speaker": "E", "text": "Ja, bei Sprint 42. UX hatte im Research festgestellt, dass die Ladeanzeige inkonsistent wirkt. Wir haben daraufhin im QA-Plan die non-functional tests angepasst und die Ladezeiten-Variabilität als eigenes Risiko aufgenommen – das hat die Priority Queue verändert."}
{"ts": "168:33", "speaker": "I", "text": "Und das wurde auch wieder in der Traceability vermerkt?"}
{"ts": "168:36", "speaker": "E", "text": "Ja, wir haben eine Cross-Reference zwischen UX-Findings und den betroffenen Test IDs erstellt. This cross-linking is not in the public runbook, but it's in our internal Confluence under QA/UX-Sync."}
{"ts": "168:43", "speaker": "I", "text": "Wenn wir Richtung Deadlines denken – gab es Situationen, wo Sie Coverage bewusst reduziert haben?"}
{"ts": "168:47", "speaker": "E", "text": "Leider ja. Im Build 1.8 mussten wir die Low-risk regression suite um 20 % kürzen, um das SLA von 95 % Pass Rate for critical flows einzuhalten. Das ist im Risk Acceptance Log RAL-HER-09 dokumentiert."}
{"ts": "168:54", "speaker": "I", "text": "Wie haben Sie das Risiko kommunikativ abgefedert?"}
{"ts": "168:57", "speaker": "E", "text": "Wir haben im Steering Committee offen gelegt, dass wir die Low-risk Lanes skippen. Dafür haben wir eine Post-Release Validation geplant – Ticket QA-7799 – um sie nachträglich auszuführen."}
{"ts": "169:03", "speaker": "I", "text": "Gab es dabei ungeschriebene Regeln, an die Sie sich gehalten haben?"}
{"ts": "169:07", "speaker": "E", "text": "Ja, die Faustregel ist: 'Never skip anything tagged as User-Impact-High', egal wie knapp die Zeit ist. And we always document the skip reason in the build notes, so auditors have a trace."}
{"ts": "173:49", "speaker": "I", "text": "Sie hatten vorhin kurz die Anpassung im RB-QA-051 erwähnt. Können Sie genauer sagen, welche Schritte Sie da in den letzten zwei Sprints verändert haben?"}
{"ts": "173:56", "speaker": "E", "text": "Ja, klar… äh also wir haben den Abschnitt zur Parallelisierung von Browser-Tests erweitert. Previously, we were only spawning three concurrent sessions, jetzt sind es fünf, um die SLA-Zeitvorgabe von 45 Minuten für den Full Regression Run zu schaffen."}
{"ts": "174:10", "speaker": "I", "text": "Hat das irgendwelche Nebeneffekte gehabt, maybe on the flaky test detection pipeline?"}
{"ts": "174:16", "speaker": "E", "text": "Genau das war ein Punkt. Durch die höhere Parallelität haben wir anfangs mehr false positives bei den Flaky-Meldungen gesehen, weil das Timing zwischen den Services nicht immer sauber synchronisiert war."}
{"ts": "174:28", "speaker": "I", "text": "Und wie haben Sie das mitigiert?"}
{"ts": "174:31", "speaker": "E", "text": "Wir haben ein Delay-Threshold in der Analytics-Engine von 200ms auf 500ms erhöht. That gave the microservices enough buffer, ohne legitime Fehler zu maskieren."}
{"ts": "174:42", "speaker": "I", "text": "Interessant. Gab es dazu ein Ticket oder war das ein stiller Fix?"}
{"ts": "174:46", "speaker": "E", "text": "Das ging über Ticket QA-4231, dokumentiert im Confluence-Bereich 'Hera Build Adjustments'. Da steht auch der Vergleich der Fehlerraten vor und nach dem Threshold-Change."}
{"ts": "174:58", "speaker": "I", "text": "Wenn wir auf UX zurückkommen: wie exactly did you integrate their latest usability study into your test design?"}
{"ts": "175:04", "speaker": "E", "text": "Die UX-Studie hat gezeigt, dass Nutzer beim neuen Filter-Widget oft falsche Kriterien wählen. Wir haben daraufhin einen explorativen Testpfad ergänzt, der diese Fehlbedienungen simuliert und prüft, ob die UI entsprechende Korrekturhinweise gibt."}
{"ts": "175:18", "speaker": "I", "text": "Und war das in Ihren ursprünglichen Risk-Based Prioritäten abgebildet?"}
{"ts": "175:22", "speaker": "E", "text": "Nicht wirklich. This fell under a lower-priority usability bucket, aber wir haben es hochgestuft, weil UX-Impact im POL-QA-014 als own risk category definiert ist."}
{"ts": "175:34", "speaker": "I", "text": "Haben Sie dafür bestehende Testfälle depriorisiert?"}
{"ts": "175:37", "speaker": "E", "text": "Ja, wir haben einige Low-risk API Edge Cases verschoben. Die Entscheidung ist in der Risk Review vom Sprint 42 vermerkt und hängt an QA-Review-Dokument S42-RB."}
{"ts": "175:49", "speaker": "I", "text": "Looking back, war das under the SLA constraints eine gute Entscheidung?"}
{"ts": "175:54", "speaker": "E", "text": "Ja, weil wir damit im SLA-Rahmen geblieben sind und gleichzeitig einen kritischen UX-Mangel früh abgefangen haben. Das Risiko von ungetesteten Edge Cases wurde im Risk Log unter R-2023-17 als akzeptiert vermerkt."}
{"ts": "182:29", "speaker": "I", "text": "Lassen Sie uns noch einmal konkret werden: gab es in den letzten zwei Sprints eine Situation, in der Sie Testabdeckung bewusst reduziert haben?"}
{"ts": "182:36", "speaker": "E", "text": "Ja, im Sprint 34 hatten wir nur 6 Tage für Regression, und gemäss SLA-QA-02 mussten wir den Release Candidate am Freitag liefern. We cut low-risk scenarios, documented in TKT-QA-982, and tagged them in the traceability sheet."}
{"ts": "182:50", "speaker": "I", "text": "Wie haben Sie das intern gerechtfertigt?"}
{"ts": "182:55", "speaker": "E", "text": "Wir haben eine Risikoabschätzung erstellt, basierend auf POL-QA-014, und im Daily mit Platform und Data diskutiert. The unwritten rule ist: unter 5% Risiko und klar dokumentiert im Runbook-Appendix dürfen wir so vorgehen."}
{"ts": "183:10", "speaker": "I", "text": "Interessant. How did that decision impact flaky test analysis?"}
{"ts": "183:15", "speaker": "E", "text": "Hm, eigentlich kaum. Wir haben die skipped low-risk tests trotzdem in der Orchestration gelassen, flagged as 'defer', so analytics could still collect flakiness signals for trend tracking."}
{"ts": "183:27", "speaker": "I", "text": "Gab es Feedback von UX dazu?"}
{"ts": "183:31", "speaker": "E", "text": "Nur indirekt. UX hatte in der Woche davor eine finding zu einer Filter-UI gemeldet, RFC-1770 impacted that, und wir haben diese Tests natürlich nicht reduziert, um usability regressions zu vermeiden."}
{"ts": "183:45", "speaker": "I", "text": "Können Sie ein Beispiel für ein Risiko nennen, das Sie dokumentiert, aber akzeptiert haben?"}
{"ts": "183:50", "speaker": "E", "text": "Ja, im Ticket RSK-044 haben wir ein known issue mit intermittierenden API-Timeouts festgehalten. The fix war in RFC-1782 vorgesehen, aber nicht rechtzeitig getestet, daher akzeptiert mit Workaround im RB-QA-051 Step 12."}
{"ts": "184:06", "speaker": "I", "text": "Wie kommunizieren Sie solche akzeptierten Risiken an Stakeholder?"}
{"ts": "184:10", "speaker": "E", "text": "Per Risk Log in Confluence, plus ein Slot im Release Go/No-Go Meeting. Wir haben auch eine color-code convention: gelb für akzeptiert mit Mitigation, rot für No-Go."}
{"ts": "184:22", "speaker": "I", "text": "Hat sich diese Praxis bewährt?"}
{"ts": "184:25", "speaker": "E", "text": "Definitiv. It prevents surprises post-release. Und es hilft, SLA-Verstöße zu vermeiden, weil wir gezielt entscheiden statt reaktiv handeln."}
{"ts": "184:36", "speaker": "I", "text": "Letzte Frage: gibt es etwas, das Sie im nächsten Build-Phase-Release anders machen würden?"}
{"ts": "184:41", "speaker": "E", "text": "Ja, ich würde earlier cross-team reviews for RFC impacts einplanen. So könnten wir Traceability, flaky test signals und UX findings noch enger verzahnen und weniger Last-Minute-Trade-offs machen."}
{"ts": "190:29", "speaker": "I", "text": "Sie hatten vorhin die SLA-Trade-offs erwähnt, könnten Sie vielleicht noch mal konkret sagen, wie das im Kontext von Build-Phase und QA orchestriert wird?"}
{"ts": "190:46", "speaker": "E", "text": "Ja, also im Build-Phase-Kontext von Hera QA Platform haben wir eine klare Matrix im Runbook RB-QA-051, die definiert, welche Teststufen wir bei Time-Crunch skippen dürfen. For example, we can skip extended regression on low-risk modules if the SLA dashboard shows green for critical paths."}
{"ts": "191:12", "speaker": "I", "text": "Und das funktioniert ohne dass Sie langfristige Risiken ignorieren?"}
{"ts": "191:24", "speaker": "E", "text": "Nicht ganz, wir dokumentieren jedes Mal im QA-Risk-Log, Ticket-ID T-HER-812, wenn wir solche Abstriche machen. Das geht dann auch an Platform- und Data-Team, so they can factor it into their post-build monitoring."}
{"ts": "191:49", "speaker": "I", "text": "Wie reagieren die anderen Teams darauf? Gibt es da Spannungen?"}
{"ts": "192:02", "speaker": "E", "text": "Manchmal ja, besonders wenn UX parallel noch Findings einbringt. Dann müssen wir entscheiden: nehmen wir die UX-Änderung rein und riskieren eine SLA-Verletzung, oder verschieben wir. We use a quick impact assessment checklist from RB-QA-051 appendix C."}
{"ts": "192:31", "speaker": "I", "text": "Gibt es einen konkreten Fall aus den letzten Wochen?"}
{"ts": "192:44", "speaker": "E", "text": "Ja, z.B. UX wollte eine Änderung an der Test-Result-UI laut UX-Finding UX-HER-045. Wir haben das in der Build-Phase aufgenommen, obwohl das bedeutete, dass wir den Traceability-Check für RFC-1770 nur teilweise fahren konnten."}
{"ts": "193:11", "speaker": "I", "text": "Wie haben Sie das abgesichert?"}
{"ts": "193:22", "speaker": "E", "text": "Through targeted smoke tests on the affected microservice chain. We mapped them in the traceability matrix, but flagged the RFC linkage as 'partial'. Das ist im Nachgang auch im Analytics-Dashboard sichtbar, weil die Flaky-Test-Quote gestiegen ist."}
{"ts": "193:50", "speaker": "I", "text": "Das heißt, die Analytics-Daten fließen direkt zurück in die Planung?"}
{"ts": "194:02", "speaker": "E", "text": "Genau. Wir haben eine wöchentliche QA-Analytics-Review, where we feed flaky test signals back into the orchestration rules. Das ist besonders wichtig, wenn wir Abdeckung reduzieren mussten."}
{"ts": "194:27", "speaker": "I", "text": "Und diese Regeln ändern Sie oft?"}
{"ts": "194:38", "speaker": "E", "text": "Alle zwei Sprints etwa. Wir passen die Priorisierung in der Orchestrator-Konfiguration an – zum Beispiel, high-risk payment flows kriegen immer volle Abdeckung, low-risk admin screens nicht zwingend."}
{"ts": "194:59", "speaker": "I", "text": "Klingt nach einer Balance zwischen Formalismus und Pragmatismus."}
{"ts": "195:12", "speaker": "E", "text": "Ja, genau das. Wir haben die formalen Vorgaben aus POL-QA-014, aber wir leben sie pragmatisch, mit klarer Dokumentation. Otherwise, we couldn't meet both the SLA and the innovation pace in build."}
{"ts": "199:49", "speaker": "I", "text": "Wenn wir noch einmal auf die Build-Phase zurückkommen, gab es da bestimmte Automation-Patterns, die Sie besonders erfolgreich fanden?"}
{"ts": "200:05", "speaker": "E", "text": "Ja, wir haben ein sogenanntes 'staggered deployment testing' eingeführt, das steht nicht direkt im Runbook RB-QA-051, aber es lehnt sich daran an. We run partial suites against canary branches before merging to main, um frühzeitig flaky behaviour zu erkennen."}
{"ts": "200:27", "speaker": "I", "text": "Das klingt nach einer Art Vorab-Filterschicht. How does that integrate with your existing orchestration tool?"}
{"ts": "200:41", "speaker": "E", "text": "Wir haben das über einen zusätzlichen Orchestrator-Job in HeraCtrl angebunden. Der Job nutzt dieselben APIs wie unsere regulären Nightlies, aber triggert nur auf RFC-gebundene Branches – z.B. RFC-1822, was die Quarantäne-Logik für Tests definiert."}
{"ts": "201:04", "speaker": "I", "text": "Und bei so einer Anpassung – gibt es da Risiken, dass Release-Zyklen verzögert werden?"}
{"ts": "201:17", "speaker": "E", "text": "Ja, klar. Wir hatten z. B. mit Ticket T-HER-812 den Fall, dass ein Canary-Blocker einen Merge um zwei Tage verzögert hat. Aber das war uns lieber als ein SLA-Breach in der Staging-Umgebung."}
{"ts": "201:38", "speaker": "I", "text": "Verstehe. Letzte Woche hatten Sie erwähnt, dass UX-Findings manchmal QA-Prioritäten verschieben. Could you share a concrete case from Hera?"}
{"ts": "201:53", "speaker": "E", "text": "Klar. Im Usability-Test für das Analytics-Dashboard hat UX herausgefunden, dass die Filterfunktion unintuitiv ist. Wir haben daraufhin im Testplan v2.3 zusätzliche Szenarien ergänzt, um edge cases bei Filterkombinationen zu prüfen – das war vorher gar nicht im Scope."}
