{"ts": "00:00", "speaker": "I", "text": "Können Sie bitte kurz den aktuellen Stand des Titan DR Projekts schildern?"}
{"ts": "03:15", "speaker": "E", "text": "Ja, gern. Wir sind derzeit mitten in der Drill-Phase, also im geplanten Ausfall-Szenario, um unsere Disaster-Recovery-Prozesse zu testen. Das heißt, wir haben letzte Woche den Failover von Region Süd auf Region Nord simuliert, basierend auf dem Runbook RB-DR-001. Laut Plan sollten wir innerhalb von 45 Minuten die Kernsysteme wieder online haben – der Testlauf lag bei 42 Minuten, also im Ziel."}
{"ts": "06:20", "speaker": "I", "text": "Welche Ihrer Verantwortlichkeiten sind für diesen Drill besonders kritisch?"}
{"ts": "09:05", "speaker": "E", "text": "Als Cloud Architect bin ich zuständig für die definierte Multi-Region-Architektur, die Netzwerkpfade und die Replikationsstrategie. Kritisch ist hierbei, dass die Datenströme zwischen den Regionen verschlüsselt und konsistent repliziert werden. Außerdem muss ich sicherstellen, dass die Teamleiter im Betrieb die Runbooks korrekt interpretieren."}
{"ts": "12:40", "speaker": "I", "text": "Wie stellen Sie sicher, dass die Projektziele mit den Unternehmenswerten übereinstimmen?"}
{"ts": "15:55", "speaker": "E", "text": "Wir orientieren uns an den Werten von Novereon Systems, insbesondere Verfügbarkeit und Kundentransparenz. In jeder Architekturentscheidung prüfen wir, ob die Maßnahmen nicht nur technisch sinnvoll, sondern auch nachhaltig und für Kunden nachvollziehbar sind. Das bedeutet zum Beispiel, dass wir Ausfallzeiten proaktiv kommunizieren und umweltfreundlichere Rechenzentrums-Optionen prüfen."}
{"ts": "19:10", "speaker": "I", "text": "Welche Multi-Region-Strategie haben Sie gewählt und warum?"}
{"ts": "23:25", "speaker": "E", "text": "Wir fahren eine aktive-passive Strategie. Region Süd ist primär aktiv, Region Nord steht als Cold Standby bereit. Das reduziert Kosten und komplexe Konfliktfälle bei gleichzeitiger Sicherstellung der SLA-Vorgaben. Wir haben in der Architektur bewusst BLAST_RADIUS minimiert, indem wir kritische Datenbanken in separate Fault Domains gekapselt haben."}
{"ts": "27:40", "speaker": "I", "text": "Wie ist das Runbook RB-DR-001 in den Architekturentscheidungen verankert?"}
{"ts": "31:05", "speaker": "E", "text": "Das Runbook definiert Schritt für Schritt, wie Failover durchzuführen ist – von DNS-Umschaltungen über Storage-Mounts bis zur Validierung von Microservices. Unsere Architektur erlaubt, dass diese Schritte automatisiert werden können, was wir über unser internes Orchestrierungs-Tool \"Helix\" abbilden."}
{"ts": "34:50", "speaker": "I", "text": "Können Sie ein Beispiel geben, wie Sie den BLAST_RADIUS weiter begrenzen?"}
{"ts": "38:15", "speaker": "E", "text": "Wir segmentieren z.B. unsere Messaging-Queues, sodass ein Ausfall in einer Region nur einen Teil der Mandanten betrifft. Außerdem setzen wir auf regionale Feature-Flags, um fehlerhafte Deployments gezielt zu isolieren."}
{"ts": "42:20", "speaker": "I", "text": "Wie messen Sie, ob die RTO und RPO Ziele erreicht werden?"}
{"ts": "46:05", "speaker": "E", "text": "Wir haben in unserem Monitoring klare Metriken: RTO wird über Zeitstempel aus Helix Logs gemessen – Startpunkt ist der Incident-Trigger, Endpunkt die erste erfolgreiche Antwort eines Kernservices. RPO tracken wir über die Differenz zwischen letztem replizierten Datensatz und Zeitpunkt des Ausfalls, gemessen mit dem Tool 'ChronosReplica'."}
{"ts": "51:50", "speaker": "I", "text": "Welche Observability-Tools sind eingebunden, um Failover-Ereignisse zu tracken?"}
{"ts": "54:45", "speaker": "E", "text": "Wir nutzen hauptsächlich die Nimbus Observability Suite, die direkt mit Helix integriert ist. Darüber hinaus laufen synthetische Tests über Poseidon Networking, damit wir auch die Latenzveränderungen in Echtzeit sehen können."}
{"ts": "90:00", "speaker": "I", "text": "Könnten Sie bitte im Detail erläutern, welche konkreten Maßnahmen Sie implementiert haben, um das BLAST_RADIUS im Multi-Region-Design zu begrenzen?"}
{"ts": "90:08", "speaker": "E", "text": "Ja, gern. Wir haben in der Architektur eine strikte Segmentierung zwischen der primären und der sekundären Region eingeführt. Das bedeutet, dass bestimmte kritische Microservices, wie der AuthN/AuthZ Service, nur asynchron repliziert werden und keine direkten Cross-Region Calls zulassen, um einen möglichen Ausfall nicht kaskadieren zu lassen."}
{"ts": "90:25", "speaker": "E", "text": "Zusätzlich setzen wir auf isolierte IAM Policies pro Region und nutzen laut Runbook RB-DR-001 Kapitel 4.3 den sogenannten 'Region Fence', der in Terraform Modulen implementiert ist."}
{"ts": "90:43", "speaker": "I", "text": "Verstehe. Und wie messen Sie dann, ob die Recovery Time Objectives im Drill tatsächlich eingehalten werden?"}
{"ts": "90:50", "speaker": "E", "text": "Wir haben in Nimbus Observability sogenannte 'Failover Timers' konfiguriert, die bei Auslösen des Runbooks automatisch starten. Diese Timer messen die Zeit bis zur vollständigen Service-Wiederaufnahme und schicken die Daten in unser SLA-Dashboard."}
{"ts": "91:05", "speaker": "E", "text": "Die Sollwerte für RTO liegen bei 15 Minuten, RPO bei maximal 5 Minuten Datenverlust. Wir vergleichen die Messwerte nach jedem Drill und loggen Abweichungen im Ticket-System, z. B. in DR-ANOM-2025-17."}
{"ts": "91:22", "speaker": "I", "text": "Gab es in den letzten Monaten Fälle, in denen diese Werte nicht erreicht wurden?"}
{"ts": "91:28", "speaker": "E", "text": "Ja, im November-Drill hatten wir bei einer Region-zu-Region Datenbank-Synchronisation einen RPO von 7 Minuten. Grund war eine fehlerhafte Konfiguration im Poseidon Networking Peering, die die Latenz stark erhöht hat."}
{"ts": "91:45", "speaker": "E", "text": "Wir haben daraufhin ein RFC erstellt (RFC-DR-2024-11) und die Peering-Konfiguration so angepasst, dass die Bandbreitenreservierung dynamisch skaliert."}
{"ts": "92:00", "speaker": "I", "text": "Wie genau interagiert denn Poseidon Networking mit Ihrer DR-Strategie?"}
{"ts": "92:06", "speaker": "E", "text": "Poseidon liefert uns das Multi-VPC Routing und die Cross-Region Tunnel. Für Titan DR ist das kritisch, weil wir im Failover-Fall sowohl Applikations- als auch Datenströme über diese Tunnel leiten. Eine suboptimale Konfiguration dort wirkt sich direkt auf RTO und RPO aus."}
{"ts": "92:23", "speaker": "E", "text": "Nimbus Observability überwacht wiederum die Poseidon Tunnel Health; das ist die Multi-Hop-Verknüpfung, die wir in der Architektur bewusst so vorgesehen haben, um Probleme schneller zu erkennen."}
{"ts": "92:38", "speaker": "I", "text": "Gab es organisatorische Hürden bei dieser Integration?"}
{"ts": "92:44", "speaker": "E", "text": "Ja, anfangs gab es keine einheitliche Metrikdefinition zwischen den Projekten. Poseidon hat Latenz in Millisekunden gemessen, Nimbus hat aber nur Prozentwerte für Paketverlust angezeigt. Wir mussten ein gemeinsames Schema definieren, was wir im Cross-Project Runbook CP-RB-002 dokumentiert haben."}
{"ts": "93:02", "speaker": "I", "text": "Interessant. Welche weiteren technischen Risiken sehen Sie aktuell im Setup?"}
{"ts": "93:08", "speaker": "E", "text": "Ein Risiko ist, dass wir aktuell nur zwei aktive Regionen plus eine Cold-Standby-Region haben. Bei Ausfall einer aktiven Region und gleichzeitigen Problemen im Cold-Standby würde unser SLA gefährdet. Das haben wir im Risk Log DR-RISK-2025-03 vermerkt und evaluieren gerade eine dritte aktive Region."}
{"ts": "96:00", "speaker": "I", "text": "Kommen wir nun zu den SLA- und SLO-Anforderungen. Wie stellen Sie im Drill fest, dass RTO und RPO tatsächlich erreicht werden?"}
{"ts": "96:09", "speaker": "E", "text": "Wir messen das mit einem kombinierten Ansatz: automatisierte Zeitstempel in den Failover-Skripten gemäß Runbook RB-DR-001 und manueller Validierung über unser DR-Board. RTO liegt laut SLA bei 45 Minuten, und im letzten Test lagen wir bei 39,7 Minuten."}
{"ts": "96:22", "speaker": "I", "text": "Und wie erfassen Sie den RPO im Testbetrieb?"}
{"ts": "96:28", "speaker": "E", "text": "RPO wird über das Snapshot-Delta im Storage-Layer gemessen. Die Poseidon-Netzwerk-Latenzwerte fließen hier indirekt ein, weil sie die Replikationsfrequenz beeinflussen. Nimbus Observability aggregiert diese Statistiken in den DR-Metriken-Channel."}
{"ts": "96:46", "speaker": "I", "text": "Gab es in der Drill-Phase Abweichungen von den SLA-Standards?"}
{"ts": "96:51", "speaker": "E", "text": "Ja, im Ticket DRINC-2025-17 mussten wir dokumentieren, dass ein Failback-Vorgang 12 Minuten länger dauerte als vereinbart. Ursache war ein fehlerhafter Routing-Eintrag im Secondary-Region BGP, der aus Poseidon übernommen wurde."}
{"ts": "97:07", "speaker": "I", "text": "Welche Observability-Tools neben Nimbus setzen Sie noch ein, um Failover zu tracken?"}
{"ts": "97:13", "speaker": "E", "text": "Wir nutzen intern 'Helios Trace' für tiefe Transaktions-Analysen und 'LogSentinel' für Audit Trails. Nimbus fungiert als zentrales Dashboard, zieht aber die Rohdaten aus diesen Tools."}
{"ts": "97:26", "speaker": "I", "text": "Lassen Sie uns den Blick auf Risiken richten: Was sehen Sie aktuell als größtes Risiko im DR-Setup?"}
{"ts": "97:32", "speaker": "E", "text": "Das größte Risiko ist derzeit die Abhängigkeit von drei kritischen Inter-Region-Links. Fällt einer aus, können wir durch Route-Rebalancing reagieren, aber ein simultaner Ausfall von zweien würde den BLAST_RADIUS vergrößern und die RTO in Frage stellen."}
{"ts": "97:49", "speaker": "I", "text": "Gab es Situationen, in denen Sie bewusst ein SLA-Risiko eingegangen sind, um Kosten zu sparen?"}
{"ts": "97:55", "speaker": "E", "text": "Ja, bei der Storage-Replikation in die dritte Region haben wir das Intervall temporär von 5 auf 15 Minuten erhöht, um Bandbreitenkosten zu senken. Das war in RFC-DR-2025-03 dokumentiert und mit dem Management abgestimmt."}
{"ts": "98:08", "speaker": "I", "text": "Welche Lessons Learned aus dem letzten GameDay, TEST-DR-2025-Q1, haben Sie übernommen?"}
{"ts": "98:15", "speaker": "E", "text": "Wir haben gelernt, dass die Failback-Prozesse stärker parallelisiert werden müssen. Außerdem haben wir einen Pre-Failover-Check ins Runbook aufgenommen, um DNS-Propagation-Delays zu erkennen, bevor wir umschalten."}
{"ts": "98:29", "speaker": "I", "text": "Wie haben Sie diese Änderungen organisatorisch verankert?"}
{"ts": "98:35", "speaker": "E", "text": "Über ein Update des Runbooks RB-DR-001 v2.3 und eine verpflichtende Schulung für alle DR-OnCall-Ingenieure. Die Änderungen wurden in unserem Confluence-Space 'Titan DR Ops' veröffentlicht und im Change Advisory Board (CAB) abgenommen."}
{"ts": "104:00", "speaker": "I", "text": "Können Sie jetzt etwas genauer darauf eingehen, wie Sie derzeit prüfen, ob die im Runbook RB-DR-001 definierten RTO- und RPO-Ziele eingehalten werden?"}
{"ts": "104:07", "speaker": "E", "text": "Ja, klar. Wir nutzen dafür eine Kombination aus automatisierten Drill-Skripten und dem Nimbus Observability Dashboard. Die Drill-Skripte messen die Zeit vom Auslösen des Failovers bis zur vollständigen Wiederherstellung der Services, und Nimbus liefert uns über die Metrik-Kanäle `nimbus.drill.latency` und `nimbus.drill.data_loss` die Kennzahlen, die wir mit den Sollwerten im RB-DR-001 abgleichen."}
{"ts": "104:25", "speaker": "I", "text": "Gab es in den letzten Drills Abweichungen von diesen Standards?"}
{"ts": "104:29", "speaker": "E", "text": "Ja, im Drill vom Februar hatten wir beim Ticket DR-INC-458 eine RTO-Überschreitung von 2 Minuten. Ursache war eine unerwartete Latenz im Poseidon Networking Layer zwischen Region EU-Central und US-East."}
{"ts": "104:44", "speaker": "I", "text": "Wie sind Sie mit dieser Abweichung umgegangen?"}
{"ts": "104:48", "speaker": "E", "text": "Wir haben zunächst eine temporäre Anpassung der Failover-Prioritäten vorgenommen, um kritische Services zuerst zu verschieben, und dann in Absprache mit dem Poseidon-Team einen Patch im Routing-Modul bereitgestellt. Der Patch ist seit Release 2.3.4 aktiv und hat die Latenz um ca. 15% reduziert."}
{"ts": "105:06", "speaker": "I", "text": "Welche Observability-Tools neben Nimbus sind sonst noch eingebunden?"}
{"ts": "105:10", "speaker": "E", "text": "Wir setzen ergänzend auf LogScope für die Echtzeit-Logaggregation und Alerting. LogScope ist direkt an unser Incident-Response-Playbook IR-DR-002 gekoppelt, sodass bei einem Failover-Event automatisch ein Incident-Kanal im internen Chat erstellt wird."}
{"ts": "105:25", "speaker": "I", "text": "Kommen wir zu den Risiken: Welche größten Risiken sehen Sie im aktuellen DR-Setup?"}
{"ts": "105:30", "speaker": "E", "text": "Das größte Risiko ist aktuell die Abhängigkeit von drei gleichzeitig aktiven Regionen. Wenn eine Region länger als 30 Minuten ausfällt und eine zweite kurz darauf Performance-Degradation zeigt, kommen wir in die Nähe unserer SLA-Grenzen."}
{"ts": "105:45", "speaker": "I", "text": "Gab es schon Situationen, in denen Sie bewusst SLA-Risiken in Kauf genommen haben, um Kosten zu sparen?"}
{"ts": "105:50", "speaker": "E", "text": "Ja, im Q4 2024 haben wir für weniger kritische Microservices von aktiver Replikation auf asynchrone Snapshot-Replikation umgestellt. Das hat uns monatlich rund 8% Cloudkosten erspart, aber wir haben RPO von 15 auf 60 Sekunden erhöht, was in den SLAs als akzeptable Ausnahme dokumentiert ist."}
{"ts": "106:09", "speaker": "I", "text": "Welche Lessons Learned aus dem letzten GameDay TEST-DR-2025-Q1 haben Sie konkret umgesetzt?"}
{"ts": "106:14", "speaker": "E", "text": "Wir haben gelernt, dass unsere Cross-Team-Kommunikation noch zu sehr von manuellen Eskalationspfaden abhängt. Deshalb haben wir im Runbook RB-DR-001 ein neues Kapitel für automatisierte Eskalation basierend auf Nimbus Alerts eingefügt und die Poseidon-Netzwerkdiagramme direkt integriert, damit alle Teams sofort die Topologie sehen."}
{"ts": "106:36", "speaker": "I", "text": "Klingt nach einer Verbesserung. Gibt es weitere bevorstehende Änderungen?"}
{"ts": "106:40", "speaker": "E", "text": "Ja, wir planen für Q3 ein Upgrade auf den neuen Poseidon Edge Router, der laut internen Benchmarks eine um 22% geringere Failover-Latenz bietet. Das wird helfen, die SLA-Puffer zu vergrößern und Risiken zu minimieren."}
{"ts": "112:00", "speaker": "I", "text": "Sie hatten vorhin erwähnt, dass Sie beim letzten Drill bewusst eine geringere Redundanz in einer Region akzeptiert haben. Können Sie das bitte noch etwas genauer erläutern?"}
{"ts": "112:15", "speaker": "E", "text": "Ja, das war in Region EU-Central, wo wir laut Ticket DR-RISK-042 vorübergehend nur eine AZ aktiv hatten. Wir haben das nach Runbook RB-DR-001, Abschnitt 'Graceful Degradation', so abgestimmt, um Kosten für den Testzeitraum zu sparen, wohlwissend, dass das RTO dort um ca. 15 Minuten steigen könnte."}
{"ts": "112:44", "speaker": "I", "text": "Gab es dazu eine formale Risikoanalyse oder lief das eher adhoc?"}
{"ts": "112:55", "speaker": "E", "text": "Wir haben eine schnelle Risikoabschätzung anhand der internen Matrix im Confluence gemacht, basierend auf den Lessons Learned aus TEST-DR-2024-Q4. Adhoc war es nicht, aber die Genehmigung kam per E-Mail durch den DR Governance Lead, ohne einen formellen RFC-Prozess."}
{"ts": "113:20", "speaker": "I", "text": "Und wie wurde das Monitoring so angepasst, dass Sie diese höhere Verwundbarkeit im Blick behalten konnten?"}
{"ts": "113:33", "speaker": "E", "text": "Wir haben in Nimbus Observability ein temporäres Dashboard 'EU-Central HighRisk' erstellt, mit speziellen Alerts aus Poseidon Networking für Latenz und Package Loss, damit wir im Drill sofort reagieren konnten."}
{"ts": "113:55", "speaker": "I", "text": "Gab es während des Drills tatsächlich ein Ereignis, das diesen Bereich betroffen hat?"}
{"ts": "114:07", "speaker": "E", "text": "Ja, wir hatten einen simulierten Fiber Cut zwischen zwei Core-Routern der AZ. Das wurde um 14:32 UTC von Nimbus erfasst, Alarm-ID NO-AL-778, und wir konnten innerhalb von 6 Minuten auf die Backup-Route schwenken."}
{"ts": "114:28", "speaker": "I", "text": "Wie hat sich das auf die SLA-Metriken ausgewirkt?"}
{"ts": "114:39", "speaker": "E", "text": "Gemäß SLA-Report DR-SLA-2025-Q1 lagen wir bei einem RTO von 21 Minuten für EU-Central. Das ist über unserem Ziel von 15, aber im Drill akzeptabel. RPO blieb unter 30 Sekunden dank asynchroner Replikation."}
{"ts": "115:02", "speaker": "I", "text": "Wurden daraus Maßnahmen abgeleitet für den nächsten Drill?"}
{"ts": "115:14", "speaker": "E", "text": "Ja, wir planen in DR-RFC-119 die Einführung einer dritten minimalen AZ in EU-Central, nur für kritische Services, um den Blast Radius zu verringern, auch wenn die Kosten leicht steigen."}
{"ts": "115:34", "speaker": "I", "text": "Das heißt, Sie nehmen höhere Betriebskosten in Kauf, um die SLA-Einhaltung zu sichern?"}
{"ts": "115:45", "speaker": "E", "text": "Genau. Die Abwägung hat gezeigt, dass die Opportunitätskosten eines SLA-Bruchs bei Kundenprojekten, die auf Titan DR setzen, deutlich höher wären als die zusätzlichen Infrastrukturkosten."}
{"ts": "116:05", "speaker": "I", "text": "Würden Sie sagen, dass diese Erkenntnis auf alle Regionen übertragbar ist?"}
{"ts": "116:20", "speaker": "E", "text": "Nicht eins zu eins. In Regionen mit geringerer Last wie AP-South könnten wir weiter mit reduzierten Ressourcen arbeiten. Aber für EU-Central und US-East, die im Poseidon-Netzwerk Knotenpunkte sind, ist die zusätzliche Resilienz strategisch notwendig."}
{"ts": "120:00", "speaker": "I", "text": "Könnten Sie bitte noch etwas näher auf die Lessons Learned aus dem letzten Drill eingehen, speziell im Hinblick auf die Interaktion mit Poseidon Networking?"}
{"ts": "120:35", "speaker": "E", "text": "Ja, klar. Wir haben festgestellt, dass unser Cross-Region Routing in Poseidon nicht optimal auf die Failover-Latenz abgestimmt war. Das war im Ticket DR-NET-342 dokumentiert. Nach der Analyse haben wir in Absprache mit dem Poseidon-Team die BGP-Failover-Timer von 60s auf 20s reduziert, um die RTO im Rahmen des SLA zu halten."}
{"ts": "121:10", "speaker": "I", "text": "Gab es dabei unerwartete Nebeneffekte, die Sie berücksichtigen mussten?"}
{"ts": "121:42", "speaker": "E", "text": "Ja, die Reduzierung der Timer hat zunächst zu instabilen Routen bei kleineren Netzwerkflaps geführt. Wir haben deshalb laut Runbook RB-DR-004 einen zusätzlichen Health-Check-Mechanismus implementiert, um false positives zu minimieren."}
{"ts": "122:20", "speaker": "I", "text": "Sie hatten vorhin von BLAST_RADIUS gesprochen. Können Sie ein konkretes Beispiel aus dem Drill nennen?"}
{"ts": "122:55", "speaker": "E", "text": "Im Drill haben wir den BLAST_RADIUS begrenzt, indem wir den Storage-Failover nur für die betroffene Region ausgelöst haben, nicht global. Das ist in RB-DR-001 Abschnitt 3.2 verankert. Dadurch konnten wir die Auswirkungen auf ungeplante Regionen auf unter 5% der Services reduzieren."}
{"ts": "123:30", "speaker": "I", "text": "Wie fließen diese Erkenntnisse jetzt in die nächste Phase des Projekts ein?"}
{"ts": "124:00", "speaker": "E", "text": "Wir haben im RFC DR-ARCH-2025-07 festgehalten, dass alle künftigen Änderungen an Netzwerk- und Storage-Failover-Mechanismen gemeinsam mit den Teams aus Poseidon und Nimbus abgestimmt werden müssen. Das ist eine direkte Folge der Drill-Ergebnisse."}
{"ts": "124:40", "speaker": "I", "text": "Wie stellen Sie sicher, dass diese Änderungen auch unter Live-Bedingungen überprüft werden?"}
{"ts": "125:12", "speaker": "E", "text": "Wir planen vierteljährliche Mini-Drills, die unter Produktionsbedingungen, aber mit begrenztem Scope laufen. Dabei verwenden wir Observability-Metriken aus Nimbus, um die Reaktionszeit und Stabilität zu messen."}
{"ts": "125:50", "speaker": "I", "text": "Gab es aus Management-Sicht Widerstand gegen diese häufigeren Tests?"}
{"ts": "126:20", "speaker": "E", "text": "Ja, primär wegen befürchteter Produktivitätsverluste. Wir haben das Risiko aber mit einer Kosten-Nutzen-Analyse aus Ticket DR-BIZ-118 entkräftet, die zeigte, dass proaktiv entdeckte Schwachstellen langfristig günstiger sind als reale Ausfälle."}
{"ts": "126:58", "speaker": "I", "text": "Wenn Sie auf die nächsten 12 Monate blicken – welches Risiko halten Sie für das kritischste?"}
{"ts": "127:28", "speaker": "E", "text": "Das größte Risiko sehe ich in der Abhängigkeit von einer einzigen Cloud-Region für unseren Identity-Provider. Das ist nicht DR-redundant und wurde in Risk-Log-Eintrag RL-DR-2025-09 als 'High' eingestuft."}
{"ts": "128:05", "speaker": "I", "text": "Und gibt es bereits Pläne, dieses Risiko zu mitigieren?"}
{"ts": "128:40", "speaker": "E", "text": "Ja, wir evaluieren gerade eine Multi-Region-IDP-Lösung, die in Q3 als Pilot laufen soll. Das wird auch in RB-DR-006 neu dokumentiert werden, um klare Failover-Pfade zu definieren."}
{"ts": "136:00", "speaker": "I", "text": "Könnten Sie bitte noch einmal konkret erläutern, wie Sie im Drill die Failover-Sequenz anstoßen, gerade in Bezug auf die Vorgaben aus RB-DR-001?"}
{"ts": "136:15", "speaker": "E", "text": "Ja, klar. In RB-DR-001 ist im Abschnitt 4.2 genau beschrieben, dass wir zuerst die Read-Only Umschaltung der Primärdatenbank durchführen, dann das DNS-Failover initiieren. Im Drill habe ich das über unser internes Orchestrator-Script 'drill_trigger_v3' gemacht, das sowohl die Poseidon Networking APIs anspricht als auch Nimbus Hooks für das Monitoring aktiviert."}
{"ts": "136:38", "speaker": "I", "text": "Und wie stellen Sie sicher, dass keine Race Conditions zwischen DNS-Umschaltung und Storage Replikation auftreten?"}
{"ts": "136:53", "speaker": "E", "text": "Wir haben eine eingebaute Wartezeit von 90 Sekunden, die im Runbook als Mandatory Delay markiert ist. Außerdem prüfen wir via Nimbus Event-Streams, ob der Storage-Cluster in Region B den Status 'sync_complete' erreicht hat, bevor der DNS-Eintrag geändert wird."}
{"ts": "137:14", "speaker": "I", "text": "Gab es im Drill Abweichungen von dieser Reihenfolge?"}
{"ts": "137:25", "speaker": "E", "text": "Einmal ja, im Drill-Abschnitt Ticket DR-INC-2025-044, wo das Sync-Signal verspätet kam. Wir haben das Protokoll angepasst, um in solchen Fällen automatisch einen Alert an das On-Call-Team zu schicken."}
{"ts": "137:46", "speaker": "I", "text": "Interessant. Wie schnell konnte das On-Call-Team reagieren?"}
{"ts": "138:00", "speaker": "E", "text": "In diesem Fall innerhalb von 3 Minuten. Unsere SLA für Incident Response im DR-Kontext liegt bei 5 Minuten, also waren wir innerhalb der Vorgaben."}
{"ts": "138:15", "speaker": "I", "text": "Gab es dabei Schnittstellenprobleme mit Poseidon Networking?"}
{"ts": "138:28", "speaker": "E", "text": "Minimal. Poseidon hatte in Region C einen kurzen Service Hiccup, wodurch ein Health-Check nicht sofort zurückkam. Das war aber durch den Fallback-Pfad im Failover-Script abgesichert."}
{"ts": "138:48", "speaker": "I", "text": "Können Sie dazu eine Lesson Learned formulieren?"}
{"ts": "139:02", "speaker": "E", "text": "Ja, wir haben gelernt, dass wir Health-Checks immer über mindestens zwei unabhängige Pfade fahren sollten. Ein Pfad über Poseidon, einer direkt über den Service-Endpunkt, um Abhängigkeiten zu reduzieren."}
{"ts": "139:20", "speaker": "I", "text": "Wie dokumentieren Sie solche Änderungen?"}
{"ts": "139:33", "speaker": "E", "text": "Wir erstellen ein RFC im internen System, z.B. RFC-DR-2025-09, und verlinken alle relevanten Tickets und Runbook-Updates. So bleibt die DR-Architektur aktuell und auditierbar."}
{"ts": "139:52", "speaker": "I", "text": "Sehen Sie hier irgendein Risiko, das wir noch adressieren sollten?"}
{"ts": "140:00", "speaker": "E", "text": "Das größte Risiko ist aktuell, dass bei gleichzeitigen Region-Ausfällen unsere RPO von 5 Minuten nicht haltbar ist. Dafür müssten wir entweder die Replikationsfrequenz erhöhen, was höhere Kosten hat, oder akzeptieren, dass wir in solchen Extremszenarien Daten verlieren könnten."}
{"ts": "144:00", "speaker": "I", "text": "Wir hatten vorhin schon kurz über die Lessons Learned aus dem GameDay gesprochen. Können Sie jetzt konkret erläutern, wie Sie diese in RB-DR-001 Version 3.2 eingearbeitet haben?"}
{"ts": "144:05", "speaker": "E", "text": "Ja, klar. In der neuen Runbook-Version haben wir zum Beispiel die Failback-Sequenz um zwei zusätzliche Verification-Steps erweitert – inspiriert durch einen Ausfall im Nimbus Alerting Modul während TEST-DR-2025-Q1. Wir haben das als Checkliste im Abschnitt 4.3 ergänzt, um sicherzustellen, dass Poseidon-Routen vor der Rückschaltung stabil sind."}
{"ts": "144:15", "speaker": "I", "text": "Gab es dabei Konflikte mit bestehenden SOPs aus der Networking-Abteilung?"}
{"ts": "144:20", "speaker": "E", "text": "Ein bisschen, ja. Die SOP NET-SW-07 sieht eigentlich eine andere Reihenfolge vor. Wir mussten mit dem Lead von Poseidon abstimmen, dass wir im DR-Kontext eine Ausnahme fahren dürfen. Das ist jetzt auch im internen Confluence als 'DR Exception Flow' dokumentiert."}
{"ts": "144:30", "speaker": "I", "text": "Okay, verstanden. Wie wirkt sich diese Ausnahme auf Ihre SLA-Metriken aus, insbesondere RTO?"}
{"ts": "144:36", "speaker": "E", "text": "Minimal. Wir reden von vielleicht +3 Minuten im Worst Case, aber der Gewinn an Stabilität ist es wert. Wir haben das mit SLA-Ownern gegengetestet – Ticket SLA-2025-DR-14 – und Freigabe erhalten."}
{"ts": "144:45", "speaker": "I", "text": "Sie haben erwähnt, dass es ein Alerting-Problem gab. Wie haben Sie verhindert, dass so etwas erneut passiert?"}
{"ts": "144:50", "speaker": "E", "text": "Wir haben in Nimbus Observability ein zusätzliches Synthetic Heartbeat eingerichtet, das gezielt die Cross-Region-Links von Poseidon testet. Falls der Dienst länger als 20 Sekunden nicht antwortet, wird ein DR-specific PagerDuty-Workflow getriggert."}
{"ts": "145:00", "speaker": "I", "text": "Welche Risiken sehen Sie aktuell trotz dieser neuen Maßnahmen noch?"}
{"ts": "145:05", "speaker": "E", "text": "Ein Restrisiko bleibt beim BLAST_RADIUS: Wenn ein gleichzeitiger Ausfall in zwei Regionen eintritt, greifen unsere Geo-DNS-Routings nicht mehr sauber. Da arbeiten wir an einem experimentellen Anycast-Fallback, aber das ist noch nicht produktionsreif."}
{"ts": "145:15", "speaker": "I", "text": "Wie priorisieren Sie solche Experimente gegenüber den produktiven Anforderungen?"}
{"ts": "145:20", "speaker": "E", "text": "Wir nutzen ein 70/20/10-Modell: 70% Kapazität für produktive Stabilität, 20% für geplante Verbesserungen, 10% für Forschung. Anycast-Fallback fällt in die 10%. Wir dokumentieren alle Hypothesen in RFC-TITAN-DR-EXP-05."}
{"ts": "145:30", "speaker": "I", "text": "Gibt es da schon erste Testergebnisse?"}
{"ts": "145:35", "speaker": "E", "text": "Ein interner Drill im Lab hat gezeigt, dass wir die Failover-Zeit um weitere 15% senken könnten. Allerdings hatten wir dabei ein Memory-Leak im Border-Gateway-Node. Das müssen wir erst mit Poseidon fixen, bevor wir weitergehen."}
{"ts": "145:45", "speaker": "I", "text": "Letzte Frage dazu: Welche impliziten Regeln leiten Sie bei solchen Trade-offs?"}
{"ts": "145:50", "speaker": "E", "text": "Unschriebene Regel bei uns ist: 'No hero moves in production'. Lieber ein stabiles, wenn auch langsameres DR als eine riskante Beschleunigung. Diese Kultur ist Teil der Werte von Novereon Systems, und das leitet viele meiner Entscheidungen."}
{"ts": "146:00", "speaker": "I", "text": "Im letzten Abschnitt hatten Sie SLA-Compliance angesprochen. Können Sie bitte genauer ausführen, wie Sie im Drill verifizieren, dass sowohl RTO als auch RPO gemessen und eingehalten werden?"}
{"ts": "146:09", "speaker": "E", "text": "Ja, also wir nutzen hier ein kombiniertes Approach – wir haben im Runbook RB-DR-001 klare Messpunkte definiert, und zusätzlich laufen in Nimbus Observability spezielle Drill-Dashboards. Dort sehen wir in Echtzeit, ob wir innerhalb der 45-Minuten-RTO bleiben, und RPO-Metriken werden per Log-Replication-Delta alle 5 Minuten erhoben."}
{"ts": "146:31", "speaker": "I", "text": "Und was passiert, wenn diese Metriken während eines Drills außerhalb der Toleranz geraten?"}
{"ts": "146:36", "speaker": "E", "text": "Dann greift unser Incident Playbook IP-DR-07. Wir haben vorher definierte Escalation Paths – erst zum DR-Lead, dann zum Cloud Ops Duty Manager. In TEST-DR-2025-Q1 hatten wir einen Fall, Ticket DR-INC-4432, wo wir RTO um 3 Minuten überschritten haben. Da haben wir sofort eine Root-Cause-Analyse gestartet."}
{"ts": "146:58", "speaker": "I", "text": "Interessant, können Sie die Ursache aus diesem Ticket kurz skizzieren?"}
{"ts": "147:02", "speaker": "E", "text": "Ja, das war ein Networking Bottleneck im Poseidon Edge-Segment. Die Failover-Routen waren falsch priorisiert, sodass der Traffic erstmal über eine saturierte Leitung lief. Wir haben das in RFC-POS-210 nachgebessert – inzwischen werden DR-Routen höher priorisiert als Bulk-Backup-Traffic."}
{"ts": "147:22", "speaker": "I", "text": "Das zeigt ja, wie stark die Abhängigkeiten wirken. Haben Sie dafür spezielle Monitoring-Korrelationen eingerichtet?"}
{"ts": "147:28", "speaker": "E", "text": "Genau, in Nimbus haben wir jetzt Cross-Domain Alerts. Wenn ein Poseidon-Link-Latency-Alarm getriggert wird, prüft ein Lambda-Skript automatisch, ob gleichzeitig DR-Replication-Lags zunehmen. Falls ja, wird ein Composite Alert ausgelöst und direkt im Drill-Warroom-Channel gepostet."}
{"ts": "147:49", "speaker": "I", "text": "Gab es im aktuellen Drill eine Situation, in der diese Composite Alerts aktiv wurden?"}
{"ts": "147:54", "speaker": "E", "text": "Nein, diesmal nicht. Aber wir haben sie im Staging zwei Wochen vorher simuliert, um sicherzustellen, dass der Alert-Flow und die Reaktion nach Runbook RB-DR-001 Schritt 14 funktionieren."}
{"ts": "148:08", "speaker": "I", "text": "Wie gehen Sie mit den potentiellen Trade-offs zwischen strengen SLAs und den Kosten für redundante Ressourcen in mehreren Regionen um?"}
{"ts": "148:15", "speaker": "E", "text": "Das ist tatsächlich ein Balanceakt. Für kritische Workloads haben wir aktive Standby-Regionen, was teuer ist, aber die RTO auf < 20 Minuten drückt. Für weniger kritische Services akzeptieren wir ein Cold-Standby mit RTO bis zu 60 Minuten, um Kosten um rund 35% zu senken. Diese Entscheidung wurde im Steering Committee dokumentiert, Decision Log DL-DR-2025-02."}
{"ts": "148:37", "speaker": "I", "text": "Gab es Bedenken seitens des Managements zu diesem differenzierten Ansatz?"}
{"ts": "148:41", "speaker": "E", "text": "Ja, vor allem wegen der Kundenkommunikation. Wir mussten in den SLA-Dokumenten klarstellen, welche Services unter welcher Kategorie fallen. Die Rechtsabteilung hat dazu Addendum SLA-DR-2025-A2 erstellt, um Missverständnisse zu vermeiden."}
{"ts": "148:56", "speaker": "I", "text": "Und abschließend – welche Lessons Learned aus TEST-DR-2025-Q1 sind im aktuellen Drill am spürbarsten?"}
{"ts": "149:02", "speaker": "E", "text": "Ganz klar: die Wichtigkeit der End-to-End-Kommunikation. Im letzten GameDay hatten wir technische Probleme schneller gelöst als die Informationskette lief. Jetzt sind Kommunikations-Checkpoints im Runbook Pflicht, und wir sehen, dass das Team dadurch koordinierter agiert, selbst wenn kleine Incidents wie DR-INC-4510 auftreten."}
{"ts": "148:00", "speaker": "I", "text": "Sie hatten vorhin kurz die SLA-Matrix erwähnt – können Sie bitte konkretisieren, wie diese im Kontext des Titan DR Drills überprüft wird?"}
{"ts": "148:06", "speaker": "E", "text": "Ja, klar. Wir nutzen für die Überprüfung eine Kombination aus automatisierten Checks im Nimbus Observability Stack und manuellen Validierungen nach Runbook RB-DR-001 Abschnitt 4.2. Die Checks triggern beim Failover und messen Time-to-Serve gegen unsere RTO-Zielwerte."}
{"ts": "148:14", "speaker": "I", "text": "Und wie fließen die Poseidon Networking Metriken da ein?"}
{"ts": "148:18", "speaker": "E", "text": "Poseidon liefert uns Latenzmessungen und Paketverluststatistiken zwischen den Regionen. Diese Daten werden in Nimbus als zusätzliche Dimension getaggt, sodass wir Korrelationen zwischen Netzwerkpfad-Qualität und RTO-Abweichungen erkennen können."}
{"ts": "148:27", "speaker": "I", "text": "Gab es denn im Drill Abweichungen, die aus diesen Korrelationen ersichtlich wurden?"}
{"ts": "148:33", "speaker": "E", "text": "Ja, am Standort West-2 hatten wir während TEST-DR-2025-Q1 einen temporären Paketverlust von 3 %. Das hat den Failover um ca. 90 Sekunden verzögert. Incident-Ticket DR-INC-452 dokumentiert das, und wir haben daraufhin die BGP-Policy gemäß RFC-POSE-47 angepasst."}
{"ts": "148:44", "speaker": "I", "text": "Das klingt nach einem klaren Trade-off-Entscheidungsfall. Haben Sie in dem Zusammenhang SLA-Risiken akzeptiert?"}
{"ts": "148:50", "speaker": "E", "text": "Teilweise ja. Wir haben entschieden, für West-2 keine sofortige hardwareseitige Redundanz nachzurüsten, um Budget in Q2 für die Ost-Region zu sparen. Das bedeutet, dass das SLA dort im Extremfall punktuell unterschritten werden könnte."}
{"ts": "148:59", "speaker": "I", "text": "Wie haben Sie diese Entscheidung intern kommuniziert?"}
{"ts": "149:04", "speaker": "E", "text": "Über ein DR Change Advisory Board Meeting, dokumentiert im Protokoll CAB-DR-2025-04, sowie durch einen Eintrag in Confluence mit klarer Risikoklassifizierung nach unserem internen Schema RISK-MAP-V3."}
{"ts": "149:12", "speaker": "I", "text": "Gab es Lessons Learned aus diesem Vorgang, die Sie ins Runbook übernommen haben?"}
{"ts": "149:17", "speaker": "E", "text": "Ja, wir haben in RB-DR-001 einen neuen Step 4.2.3 ergänzt: \u000bWenn Netzwerk-Latenz > 200 ms, dann sofort Failover-Pfad B aktivieren, auch wenn Primärregion noch teilfunktional ist. Das verkleinert den Blast Radius bei Netzwerkanomalien."}
{"ts": "149:27", "speaker": "I", "text": "Interessant. Wie validieren Sie, dass dieser neue Step nicht zu unnötigen Umschaltungen führt?"}
{"ts": "149:33", "speaker": "E", "text": "Wir haben dafür in Nimbus einen Canary-Test implementiert, der jede Stunde den Pfad B simuliert und die Umschaltzeit sowie Service-Degradation misst. Nur wenn drei aufeinanderfolgende Messungen über dem Schwellenwert liegen, wird ein echter Failover ausgelöst."}
{"ts": "149:43", "speaker": "I", "text": "Und dieser Canary-Test ist schon produktiv im Einsatz?"}
{"ts": "149:48", "speaker": "E", "text": "Seit zwei Wochen, ja. Bisher keine False Positives, aber wir beobachten noch, ob saisonale Traffic-Spitzen Einfluss haben. Das Monitoring läuft in enger Abstimmung mit dem Poseidon-Team, um etwaige Routing-Änderungen zeitnah einzubeziehen."}
{"ts": "149:36", "speaker": "I", "text": "Lassen Sie uns bitte noch einmal konkret auf die SLA-Überwachung eingehen: Wie wird im Drill verifiziert, dass die im Runbook RB-DR-001 definierten RTO- und RPO-Ziele auch tatsächlich eingehalten werden?"}
{"ts": "149:40", "speaker": "E", "text": "Wir haben im letzten Drill einen automatisierten Check implementiert, der direkt nach dem Failover die Recovery-Zeit misst. Die Messpunkte kommen aus dem Nimbus Observability-Stack, genauer aus dem Modul 'Nimbus Chronos'. Dort werden Timestamps von Event-IDs wie EVT-DR-FA-START und EVT-DR-FA-END geloggt, und unser Skript vergleicht diese mit den im SLA-Dokument DR-SLA-v3.2 hinterlegten Sollwerten."}
{"ts": "149:46", "speaker": "I", "text": "Und wie fließt Poseidon Networking da hinein?"}
{"ts": "149:50", "speaker": "E", "text": "Poseidon liefert uns die Netzwerkverkehrsmetriken, insbesondere die Latenzen zwischen den Regionen FRA und HEL. Wenn wir sehen, dass der L3-Link während des Drills eine Latenzspitze > 120 ms hat, wissen wir, dass das RTO-Risiko steigt. Diese Daten korrelieren wir in Nimbus mit den Applikationsmetriken, um eine End-to-End-Sicht zu erhalten."}
{"ts": "149:56", "speaker": "I", "text": "Gab es denn im aktuellen Drill Abweichungen von den SLA-Standards?"}
{"ts": "150:00", "speaker": "E", "text": "Ja, minimal. Ticket DR-INC-2025-044 dokumentiert einen RPO-Verlust von 3 Minuten bei der Datenbank 'LedgerCore'. Grund war ein Replikationsstau auf dem HEL-Knoten, ausgelöst durch eine kurzfristige Bandbreitenlimitierung im Poseidon Segment seg-hel-02."}
{"ts": "150:06", "speaker": "I", "text": "Wie sind Sie mit dieser Abweichung umgegangen?"}
{"ts": "150:10", "speaker": "E", "text": "Wir haben in der Nachbereitung gemäß Runbook-Abschnitt 5.3 den Failover-Pfad neu priorisiert, sodass kritische DB-Streams Vorrang vor sekundären Logs haben. Zusätzlich wird Poseidon so konfiguriert, dass im DR-Modus QoS-Regeln automatisch angepasst werden."}
{"ts": "150:16", "speaker": "I", "text": "Das klingt nach einem bewussten Trade-off, um SLA-Risiken zu minimieren. Gab es aber auch den umgekehrten Fall, Kosten zu sparen und dafür SLA-Risiken in Kauf zu nehmen?"}
{"ts": "150:20", "speaker": "E", "text": "Ja, im Setup der Standby-Region YUL haben wir uns gegen eine permanente Warm-Standby-Konfiguration entschieden. Stattdessen booten wir Compute-Ressourcen on-demand. Das spart rund 35% der monatlichen Betriebskosten, erhöht aber im Worst-Case das RTO um etwa 90 Sekunden. Dieser Trade-off ist im RFC-DR-2025-12 dokumentiert und vom Steering Committee abgesegnet."}
{"ts": "150:26", "speaker": "I", "text": "Welche Risiken sehen Sie aktuell noch als besonders kritisch?"}
{"ts": "150:30", "speaker": "E", "text": "Ein großes Risiko ist die Abhängigkeit von der Poseidon Core-Variante 4.7, die wir nutzen. Sollte dort ein unbekannter Bug im Routing-Algorithmus liegen, könnten Failover-Routen fehlerhaft gesetzt werden. Wir haben zwar Canary-Tests, aber vollständige Sicherheit gibt es nicht."}
{"ts": "150:36", "speaker": "I", "text": "Lessons Learned aus dem letzten GameDay, TEST-DR-2025-Q1 – was haben Sie übernommen?"}
{"ts": "150:40", "speaker": "E", "text": "Wir haben daraus die sogenannte 'Staggered Failover Sequenz' eingeführt: Nicht mehr alle Services gleichzeitig umschalten, sondern in kritischen Wellen mit 2-Minuten-Abständen. Das hat im aktuellen Drill die Netzwerklast um 18% reduziert und die Latenzspitzen abgeflacht."}
{"ts": "150:46", "speaker": "I", "text": "Gibt es intern auch informelle Regeln, die nicht im Runbook stehen, aber effektiv sind?"}
{"ts": "150:50", "speaker": "E", "text": "Ja, zum Beispiel die \"No-Silent-Failover\"-Regel: Auch wenn der Automatismus funktioniert, wird ein Operator live per Chat informiert und gibt ein 'Green Light'. Das steht so nicht in RB-DR-001, wird aber im Team als Best Practice gelebt, um menschliche Kontrolle zu behalten."}
{"ts": "151:06", "speaker": "I", "text": "Sie hatten vorhin erwähnt, dass die SLA-Übersicht direkt aus Nimbus kommt. Können Sie erläutern, wie Sie die Daten in Echtzeit während des Drills validieren?"}
{"ts": "151:11", "speaker": "E", "text": "Ja, wir nutzen in Nimbus ein spezielles Dashboard, das auf Runbook RB-DR-001 referenziert. Dort sind RTO und RPO als Metriken hinterlegt, und wir haben Webhooks, die bei Abweichungen sofort Tickets im DR-Board eröffnen. Während des Drills sehe ich quasi live, ob die Recovery-Zeiten noch im grünen Bereich sind."}
{"ts": "151:19", "speaker": "I", "text": "Und wie fließt Poseidon Networking in diese Live-Validierung ein?"}
{"ts": "151:24", "speaker": "E", "text": "Poseidon liefert uns die Latenz- und Paketverlustdaten zwischen den Regionen. Wenn z.B. der Failover von Region Alpha nach Region Delta erfolgt, sehe ich über die Poseidon-API sofort, ob der Netzwerkpfad stabil ist. Diese Daten korrelieren wir dann mit den Nimbus-Events, um Ursachen einzugrenzen."}
{"ts": "151:33", "speaker": "I", "text": "Gab es beim letzten Drill relevante Abweichungen?"}
{"ts": "151:37", "speaker": "E", "text": "Ja, im TEST-DR-2025-Q1 hatten wir ein Ticket DR-INC-4721, wo die RTO um 12 Minuten überschritten wurde. Poseidon zeigte gleichzeitig einen Jitter-Anstieg von 40 ms auf 120 ms, was dann als primäre Ursache bestätigt wurde."}
{"ts": "151:46", "speaker": "I", "text": "Wie sind Sie mit dieser Überschreitung umgegangen?"}
{"ts": "151:50", "speaker": "E", "text": "Wir haben im Nachgang eine Änderung über RFC-DR-2025-09 eingereicht, um die Route-Selection-Policies zu optimieren. Das bedeutete kurzfristig mehr Kosten für Premium-Links, aber es war nötig, um das SLA wieder zu erfüllen."}
{"ts": "151:59", "speaker": "I", "text": "Das klingt nach einem klaren Trade-off zwischen Kosten und SLA-Compliance."}
{"ts": "152:03", "speaker": "E", "text": "Genau, und wir haben das auch transparent im Steering Committee kommuniziert. In der DR-Strategie gilt die Regel: SLA-Verfügbarkeit > Kostenoptimierung, außer bei Testszenarien mit explizitem Genehmigungsflag."}
{"ts": "152:12", "speaker": "I", "text": "Gab es Szenarien, wo Sie bewusst das Risiko eingegangen sind?"}
{"ts": "152:16", "speaker": "E", "text": "Ja, beim Drill in Q4 2024 haben wir den sekundären Storage in Region Gamma mit günstigeren Standard-IO-Volumes gefahren. Das hat die RTO um etwa 5 Minuten verlängert, war aber als Kostenexperiment genehmigt."}
{"ts": "152:25", "speaker": "I", "text": "Welche Lessons Learned ziehen Sie aus solchen Experimenten?"}
{"ts": "152:29", "speaker": "E", "text": "Man muss die BLAST_RADIUS-Parameter realistisch halten. Bei uns heißt das, nur nicht-kritische Services für solche Experimente zu nutzen und klare Rollback-Kriterien im Runbook zu haben."}
{"ts": "152:37", "speaker": "I", "text": "Wie stellen Sie sicher, dass diese Kriterien auch im Drill eingehalten werden?"}
{"ts": "152:41", "speaker": "E", "text": "Wir haben im Runbook Checklisten, die von einem zweiten Operator gegengezeichnet werden müssen. Nimbus triggert außerdem einen Pre-Failover-Check, der blockiert, wenn kritische Flags gesetzt sind."}
{"ts": "153:06", "speaker": "I", "text": "Lassen Sie uns noch einmal konkret auf die Lessons Learned vom letzten GameDay eingehen – was war aus Ihrer Sicht der größte Aha-Moment?"}
{"ts": "153:17", "speaker": "E", "text": "Hm, ja, der größte Aha-Moment war tatsächlich, dass unsere automatischen DNS-Umschaltungen laut Runbook RB-DR-001 zwar technisch perfekt funktioniert haben, aber organisatorisch die Freigabeprozesse zu träge waren. Das hat uns im Drill fast drei Minuten an RTO gekostet."}
{"ts": "153:36", "speaker": "I", "text": "Drei Minuten klingt im DR-Kontext nicht dramatisch, aber bei strikten SLAs kann das critical werden, richtig?"}
{"ts": "153:45", "speaker": "E", "text": "Genau, wir hatten im SLA-Dokument SLA-DR-2025-01 eine RTO von 15 Minuten. Im Drill lagen wir bei 17:48. Formal also ein Breach, den wir mit den Stakeholdern diskutieren mussten."}
{"ts": "154:04", "speaker": "I", "text": "Und wie haben Sie das adressiert? Gab es ein Ticket oder eine RFC-Änderung?"}
{"ts": "154:12", "speaker": "E", "text": "Ja, wir haben Change Request RFC-DR-117 eröffnet, um den Approval-Workflow für Failover in die Poseidon Networking Control Plane zu integrieren. Das ist jetzt ein automatisierter Step mit Audit-Log, sodass kein manueller Klick mehr nötig ist."}
{"ts": "154:32", "speaker": "I", "text": "Das heißt, Sie haben Technik und Prozesse enger verzahnt?"}
{"ts": "154:39", "speaker": "E", "text": "Richtig, it's more about aligning human processes with machine speed. Wir nennen das intern 'Ops-at-DR-pace'. Die Herausforderung ist, dass Compliance trotzdem ihre Checks bekommt."}
{"ts": "154:56", "speaker": "I", "text": "Können Sie ein Beispiel geben, wie Compliance jetzt eingebunden wird ohne die RTO zu gefährden?"}
{"ts": "155:04", "speaker": "E", "text": "Klar, im Runbook RB-DR-001 haben wir einen Schritt ergänzt: sobald Poseidon den Failover initiiert, pusht Nimbus Observability einen signierten Event an das Compliance-Dashboard. Die Prüfer sehen den gleichen Status in Echtzeit, no waiting for manual reports."}
{"ts": "155:27", "speaker": "I", "text": "Sie hatten vorhin die Kosten angesprochen. Gab es hier auch eine Entscheidung, die bewusst auf ein SLA-Risiko eingegangen ist?"}
{"ts": "155:37", "speaker": "E", "text": "Ja, beim Storage-Replication-Lag zwischen Region West-1 und East-2 haben wir die asynchrone Replikation gewählt, um 30% Kosten zu sparen. Das erhöht das RPO von 30 Sekunden auf knapp 2 Minuten. Für kritische Datenbanken haben wir das ausgeschlossen, aber für Log-Archive war das acceptable."}
{"ts": "155:59", "speaker": "I", "text": "Wie dokumentieren Sie solche Trade-offs?"}
{"ts": "156:05", "speaker": "E", "text": "Wir führen ein internes DR-Decision-Log, Eintrag DL-DR-2025-07 beschreibt genau diesen Trade-off, inklusive Genehmigung durch den CTO. This way, später gibt es keine Überraschungen."}
{"ts": "156:21", "speaker": "I", "text": "Gibt es für den nächsten Drill schon Anpassungen basierend auf diesen Erkenntnissen?"}
{"ts": "156:29", "speaker": "E", "text": "Ja, wir planen in TEST-DR-2025-Q3 eine Mischstrategie: für mittelwichtige Workloads testen wir eine on-demand synchrone Replikation, die nur bei erhöhtem Nimbus-Alert-Level aktiviert wird. Damit wollen wir sowohl Kosten als auch SLA-Compliance optimieren."}
{"ts": "160:06", "speaker": "I", "text": "Lassen Sie uns noch einmal auf die Lessons Learned aus dem letzten GameDay eingehen. Gab es konkrete Architekturänderungen, die Sie seitdem umgesetzt haben?"}
{"ts": "160:12", "speaker": "E", "text": "Ja, definitiv. Wir haben nach TEST-DR-2025-Q1 das Runbook RB-DR-001 um einen Schritt zur manuellen Validierung der DNS-Cutover ergänzt, weil wir im Drill einen transienten Fehler im automatischen Poseidon Routing hatten."}
{"ts": "160:28", "speaker": "I", "text": "Das klingt nach einer kritischen Anpassung. Wie wirkt sich das auf die RTO aus?"}
{"ts": "160:33", "speaker": "E", "text": "Kurzfristig verlängert es die RTO um etwa 3 Minuten, aber es reduziert das Risiko eines inkonsistenten Failovers signifikant. Die SLA-Vereinbarung erlaubt uns ±5 Minuten Puffer, siehe SLA-Dokument DR-SLA-2024."}
{"ts": "160:51", "speaker": "I", "text": "Gab es Rückmeldungen vom Incident Response Team zu dieser Änderung?"}
{"ts": "160:56", "speaker": "E", "text": "Ja, das IR-Team hat den neuen Schritt in die Checkliste IR-CHK-DR-07 aufgenommen. Sie haben betont, dass die klaren Kriterien im Runbook helfen, Fehlentscheidungen unter Stress zu vermeiden."}
{"ts": "161:11", "speaker": "I", "text": "Wie stellen Sie sicher, dass diese Runbook-Updates auch in allen Regionen synchron ausgerollt werden?"}
{"ts": "161:17", "speaker": "E", "text": "Wir nutzen ein zentrales Git-Repo mit Branch-Protection. Deployments der Runbook-Dokumentation erfolgen via unser internes Tool DocSync, das Änderungen simultan in EU-CENTRAL, US-EAST und AP-SOUTHEAST ausrollt."}
{"ts": "161:34", "speaker": "I", "text": "Und wie überprüfen Sie, dass die Teams vor Ort die aktualisierten Prozesse auch verstehen?"}
{"ts": "161:40", "speaker": "E", "text": "Wir führen sogenannte Micro-Drills durch – kurze, 15-minütige Simulationsübungen. Dabei werden gezielt einzelne Runbook-Schritte getestet und durch Nimbus Observability mit Tags wie TEST-RB-STEP12 geloggt."}
{"ts": "161:57", "speaker": "I", "text": "Gab es bei diesen Micro-Drills Auffälligkeiten?"}
{"ts": "162:02", "speaker": "E", "text": "Einmal ja – in AP-SOUTHEAST wurde der neue DNS-Validierungsschritt übersprungen. Das wurde durch eine Alarmregel in Nimbus (RuleID: NIM-DR-DNS-FAIL) erkannt, und wir haben sofort ein Ticket INC-DR-442 erstellt."}
{"ts": "162:18", "speaker": "I", "text": "Wie wurde das Problem gelöst?"}
{"ts": "162:22", "speaker": "E", "text": "Nach einer kurzen Root-Cause-Analyse stellten wir fest, dass die lokale Runbook-Version veraltet war. Wir haben daraufhin DocSync mit einer Integritätsprüfung versehen, die Hash-Werte der Dokumente vergleicht."}
{"ts": "162:39", "speaker": "I", "text": "Sehen Sie in solchen Integritätsprüfungen einen generellen Mehrwert über DR hinaus?"}
{"ts": "162:44", "speaker": "E", "text": "Ja, absolut. Diese Checks können wir auch für Security-Response-Playbooks nutzen. Das stärkt die gesamte Resilienzstrategie von Novereon Systems, nicht nur Titan DR."}
{"ts": "161:30", "speaker": "I", "text": "Sie hatten vorhin kurz die Integration von Poseidon Networking angesprochen – könnten Sie jetzt noch mal genauer erläutern, wie sich die Routenänderungen im Drill tatsächlich manifestiert haben?"}
{"ts": "161:34", "speaker": "E", "text": "Ja, im Drill haben wir das Runbook RB-DR-001 befolgt, Section 4.2 beschreibt die BGP-Community-Updates. Wir haben innerhalb von 38 Sekunden alle primären Routen auf die Secondary Region umgeleitet. Das war etwas langsamer als im Plan, weil ein Poseidon-Edge-Knoten in Wien einen Graceful Restart brauchte."}
{"ts": "161:42", "speaker": "I", "text": "Gab es dafür einen spezifischen Incident- oder Ticket-Eintrag?"}
{"ts": "161:45", "speaker": "E", "text": "Ja, das war Ticket DR-INC-2025-044. Darin haben wir auch vermerkt, dass die Verzögerung keinen Einfluss auf das SLA hatte, weil wir im Drill-Fenster blieben."}
{"ts": "161:50", "speaker": "I", "text": "Wie haben Sie denn in Nimbus Observability diese konkreten Verzögerungen sichtbar gemacht?"}
{"ts": "161:54", "speaker": "E", "text": "Wir haben spezielle Traces mit Tag `failover_path_switch` aktiviert. Nimbus sammelt die Latenzen pro Hop, sodass wir genau sehen konnten, dass der Hop Vienna→Frankfurt 12 Sekunden länger brauchte. Das war dann im Dashboard DR-LAT-View sofort als orange Warnung sichtbar."}
{"ts": "162:02", "speaker": "I", "text": "Interessant. Und wie fließen diese Daten dann zurück in die Architekturplanung?"}
{"ts": "162:06", "speaker": "E", "text": "Wir machen nach jedem Drill ein Architecture Review, dabei verlinken wir die Nimbus-Exports direkt in unseren Confluence-Abschnitt `Titan DR Learnings`. Daraus resultieren RFCs – in diesem Fall RFC-DR-2025-07 zur Optimierung der Edge-Restart-Policy."}
{"ts": "162:14", "speaker": "I", "text": "Gab es bei dieser RFC-Umsetzung auch Abhängigkeiten zu anderen Projekten, zum Beispiel Athena Storage?"}
{"ts": "162:18", "speaker": "E", "text": "Ja, Athena Storage musste ihre Replikations-Queues anpassen, weil ein schnellerer Failover bedeutet, dass mehr unflush data in der Secondary Region ankommt. Wir haben dazu mit deren Team im Cross-Project-Meeting CP-2025-03 Abstimmungen gemacht."}
{"ts": "162:26", "speaker": "I", "text": "Wie bewerten Sie das Risiko, dass solche Cross-Project-Abstimmungen im Ernstfall zu Verzögerungen führen könnten?"}
{"ts": "162:30", "speaker": "E", "text": "Das Risiko ist da, klar. Wir haben deshalb ein Fallback im Runbook RB-DR-002, wo wir bei Storage-Latenzproblemen auf ein Read-Only-Modell umschalten. Das erfüllt dann nur 80% der SLA, ist aber besser als ein kompletter Ausfall."}
{"ts": "162:38", "speaker": "I", "text": "Das klingt nach einem klassischen Trade-off zwischen Verfügbarkeit und Funktionalität."}
{"ts": "162:41", "speaker": "E", "text": "Genau. In TEST-DR-2025-Q1 haben wir das bewusst durchgespielt. Die Entscheidung, temporär auf Read-Only zu gehen, senkt die operativen Kosten und minimiert den Blast Radius, auch wenn einige Write-Operationen verzögert werden."}
{"ts": "162:49", "speaker": "I", "text": "Wie dokumentieren Sie solche Entscheidungen für zukünftige Drills oder gar für einen echten Disaster Case?"}
{"ts": "162:53", "speaker": "E", "text": "Wir pflegen eine Decision Log Tabelle im Projekt-Wiki, jede Entscheidung hat eine ID, verknüpft mit den Nimbus-Daten, den relevanten Runbook-Abschnitten und den Tickets. Das hilft uns, im Ernstfall nicht lange diskutieren zu müssen."}
{"ts": "162:06", "speaker": "I", "text": "Wir hatten ja schon die Integration mit Nimbus und Poseidon besprochen. Mich würde jetzt interessieren, wie Sie die Lessons Learned aus TEST-DR-2025-Q1 konkret in das aktuelle Drill-Setup übertragen haben."}
{"ts": "162:14", "speaker": "E", "text": "Ja, also eine der größten Anpassungen war, dass wir im Runbook RB-DR-001 die Sequenz für den Storage-Replikationswechsel angepasst haben. Damals im Q1-Test hatten wir 12 Minuten Latenz, weil die Region EU-North erst nach Netzfreigabe von Poseidon geschaltet wurde. Jetzt triggern wir das parallel mit den DNS-Updates, so sparen wir rund 4 Minuten."}
{"ts": "162:28", "speaker": "I", "text": "Das klingt nach einer reinen Prozessoptimierung. Gab es auch Änderungen an der Architektur selbst?"}
{"ts": "162:34", "speaker": "E", "text": "Ja, wir haben die BLAST_RADIUS-Parameter in der Load-Balancer-Konfiguration enger gezogen. Vorher konnten bis zu 40% der Sessions in einer betroffenen Zone hängenbleiben, jetzt ist der Wert auf 15% limitiert, mit automatischem Session-Drain über Poseidon’s FlowControl-Modul."}
{"ts": "162:48", "speaker": "I", "text": "Und wie überprüfen Sie, ob diese Limits in einem echten Failover greifen?"}
{"ts": "162:54", "speaker": "E", "text": "Wir fahren monatlich ein synthetisches Chaos-Experiment. Nimbus Observability zeichnet Metriken wie conn_drain_time und active_session_ratio auf. Wenn die Werte außerhalb der SLO-Bandbreite sind, erstellt das System automatisch ein Ticket im DR-Board, z.B. DR-ALERT-774."}
{"ts": "163:06", "speaker": "I", "text": "Gab es kürzlich einen solchen Ausreißer?"}
{"ts": "163:10", "speaker": "E", "text": "Vor drei Wochen, ja. In der Simulation 'Sim-Fail-EU-North-2025-05' lag der active_session_ratio kurzzeitig bei 22%. Wir haben herausgefunden, dass ein veraltetes Poseidon-Routingprofil geladen war. Das wurde dann laut RFC-DR-2025-07 korrigiert."}
{"ts": "163:26", "speaker": "I", "text": "Wenn Sie solche Fehlerquellen identifizieren, wie gehen Sie mit den Stakeholdern um, die die SLAs im Blick haben?"}
{"ts": "163:33", "speaker": "E", "text": "Wir kommunizieren transparent im wöchentlichen DR-Sync. Die SLA-Owner bekommen ein Impact-Assessment – in diesem Fall war der Effekt minimal, da es ein Drill war. Aber wir dokumentieren es ausführlich in Confluence unter Lessons Learned, mit Verweis auf das Ticket und die Korrekturmaßnahmen."}
{"ts": "163:48", "speaker": "I", "text": "Gibt es aus Ihrer Sicht noch Risiken, die aktuell nicht ganz mitigiert sind?"}
{"ts": "163:53", "speaker": "E", "text": "Ja, das querliegende Thema sind Kosten vs. RTO. Für die Region AP-South haben wir bewusst eine asynchrone Replikation, weil synchrone zu teuer wäre. Das bedeutet im Worst Case ein RPO von 90 Sekunden statt 30. Das ist im SLA akzeptiert, aber wir behalten es im Risk Register DR-RISK-019."}
{"ts": "164:08", "speaker": "I", "text": "Wie reagieren die Kunden, wenn solche Werte kommuniziert werden?"}
{"ts": "164:12", "speaker": "E", "text": "Die meisten Enterprise-Kunden verstehen den Trade-off, solange wir klar belegen, dass wir im Rest des Netzes, besonders in EU und US, strengere Werte erreichen. In unseren Quarterly Reports zeigen wir die regionalen Abweichungen transparent."}
{"ts": "164:24", "speaker": "I", "text": "Letzte Frage: Planen Sie für den nächsten GameDay eine neue Art von Tests?"}
{"ts": "164:29", "speaker": "E", "text": "Ja, wir wollen im Q3-GameDay erstmals Cross-Region-Data-Corruption simulieren. Das heißt, wir testen nicht nur Failover, sondern auch, wie schnell wir fehlerhafte Daten isolieren und aus einer sauberen Kopie wiederherstellen können. Dafür wird RB-DR-004 ergänzt und mit Nimbus’ DataIntegrity-Modul gekoppelt."}
{"ts": "164:30", "speaker": "I", "text": "Könnten Sie bitte noch einmal konkret erläutern, wie das Runbook RB-DR-001 während des Drills zur Anwendung kam?"}
{"ts": "164:38", "speaker": "E", "text": "Ja, klar. RB-DR-001 ist praktisch unser Drehbuch für den kompletten Failover-Prozess. Im Drill haben wir es Schritt für Schritt abgearbeitet – von der initialen Alarmierung über das Poseidon-Netzwerk bis hin zur Validierung via Nimbus-Dashboards. Wir haben die Checklisten im Confluence gespiegelt, um Live-Updates während des Drills zu dokumentieren."}
{"ts": "164:52", "speaker": "I", "text": "Gab es dabei Stellen, wo Sie vom Runbook abweichen mussten?"}
{"ts": "165:00", "speaker": "E", "text": "Ein kleiner Punkt: In Schritt 4 war eigentlich ein manueller DNS-Switch vorgesehen, aber wir haben das automatisierte Trigger-Skript aus Ticket OPS-DR-219 getestet. Das war nicht im Runbook, aber wir wollten die Gelegenheit nutzen."}
{"ts": "165:12", "speaker": "I", "text": "Und wie hat sich diese Automatisierung auf die RTO ausgewirkt?"}
{"ts": "165:18", "speaker": "E", "text": "Positiv, also wir konnten etwa 7 Minuten einsparen. Allerdings mussten wir im Post-Mortem feststellen, dass das Script nicht alle Edge-Cases aus Poseidon Routing Table v3 abdeckte, was bei komplexen BGP-Announcen relevant wird."}
{"ts": "165:34", "speaker": "I", "text": "Sie haben vorhin angesprochen, dass Observability-Daten aus Nimbus einfließen. Können Sie ein Beispiel geben, wie diese Daten eine Anpassung im Drill bewirkt haben?"}
{"ts": "165:42", "speaker": "E", "text": "Klar, während des Drills haben wir über das Nimbus Latency Heatmap-Widget gesehen, dass Region EU-Central ungewöhnlich hohe Latenzen in der Storage-Replication zeigte. Aufgrund dieser Live-Daten haben wir den Failover-Prioritäten-Plan (siehe Tabelle 3 in RB-DR-001) angepasst und auf US-East als primären DR-Standort umgeschaltet."}
{"ts": "165:58", "speaker": "I", "text": "Das klingt nach einem Multi-Hop-Entscheidungsprozess, nicht wahr?"}
{"ts": "166:04", "speaker": "E", "text": "Genau, es war ein Zusammenspiel aus Nimbus-Metriken, Poseidon-Routing-Optionen und den Kostenprofilen aus unserem Capacity Planner. Wir mussten schnell abwägen: bessere Latenz vs. höhere Transitkosten. Letztlich hat die Service-Verfügbarkeit Vorrang bekommen."}
{"ts": "166:18", "speaker": "I", "text": "Gab es auch Lessons Learned, die Sie sofort in die Architektur übernommen haben?"}
{"ts": "166:24", "speaker": "E", "text": "Ja, wir haben nach TEST-DR-2025-Q1 und auch jetzt beschlossen, die DR-Regionen dynamischer per Policy zu wählen. Dazu erweitern wir die Poseidon-API-Integration, sodass Nimbus-Metriken in Echtzeit den bevorzugten Failover-Standort setzen können."}
{"ts": "166:38", "speaker": "I", "text": "Wie gehen Sie dabei mit dem Risiko um, dass dynamische Umschaltungen Instabilität bringen könnten?"}
{"ts": "166:44", "speaker": "E", "text": "Das Risiko ist real. Deshalb haben wir im RFC-DR-2025-07 festgelegt, dass ein Failback erst nach mindestens 30 Minuten stabiler Metriken erlaubt ist. Das reduziert Flip-Flop-Effekte, auch wenn es die RTO im Einzelfall verlängern kann."}
{"ts": "166:58", "speaker": "I", "text": "Also ein klarer Trade-off zwischen Stabilität und Geschwindigkeit."}
{"ts": "167:04", "speaker": "E", "text": "Genau. Wir haben im Steering Committee dokumentiert, dass dieser Trade-off akzeptiert ist, da er SLA-Verletzungen in der Verfügbarkeit stärker reduziert als er sie in der Wiederanlaufzeit erhöht."}
{"ts": "165:06", "speaker": "I", "text": "Könnten Sie bitte noch einmal erläutern, wie genau Runbook RB-DR-001 im Drill angewendet wurde?"}
{"ts": "165:10", "speaker": "E", "text": "Ja, klar. RB-DR-001 ist quasi unser Blueprint. Es enthält Schritt-für-Schritt-Anweisungen für das Failover, inklusive der Checkliste für die Poseidon-Routenaktualisierung und das Auslösen der Storage-Replikation. Wir haben im Drill exakt diese Sequenz genutzt, nur die manuellen Freigaben haben wir verkürzt, um die Reaktionszeit zu messen."}
{"ts": "165:17", "speaker": "I", "text": "Gab es bei der Umsetzung Abweichungen vom Runbook?"}
{"ts": "165:20", "speaker": "E", "text": "Minimal. Wir mussten einen Workaround einbauen, weil eine der Nimbus-Observability-Alerts nicht korrekt getriggert wurde. Laut Ticket DR-ALRT-442 haben wir dann manuell die Log-Streams geprüft, bevor wir zu Schritt 5 gegangen sind."}
{"ts": "165:27", "speaker": "I", "text": "Wie hat sich das auf die RTO ausgewirkt?"}
{"ts": "165:30", "speaker": "E", "text": "Die Verzögerung war etwa 90 Sekunden, was in unserem Drill-SLO von 15 Minuten noch völlig okay ist. Aber wir haben es als Verbesserungspunkt markiert, weil im realen Ernstfall jede Minute zählt."}
{"ts": "165:36", "speaker": "I", "text": "Sie hatten vorhin die Schnittstellen zu Poseidon erwähnt. Können Sie ein Beispiel geben, wie ein Routing-Fehler dort Auswirkungen auf Titan DR hätte?"}
{"ts": "165:40", "speaker": "E", "text": "Klar. Wenn Poseidon eine falsche BGP-Ankündigung in der DR-Region macht, könnten Clients in eine nicht-synchrone Zone geleitet werden. Das hätte direkte RPO-Verletzungen zur Folge. Deshalb ist im Runbook ein Validierungsschritt drin, der über Nimbus-Daten prüft, ob alle Pfade auf die erwarteten Endpunkte zeigen."}
{"ts": "165:47", "speaker": "I", "text": "Welche Metriken beobachten Sie live während eines Drills?"}
{"ts": "165:50", "speaker": "E", "text": "Primär Latenz zwischen den Regionen, Replikations-Lag in Sekunden und den Status der Heartbeats. Zusätzlich haben wir ein internes Dashboard, das den sogenannten BLAST_RADIUS Score berechnet – wie viele Services potenziell betroffen sind."}
{"ts": "165:57", "speaker": "I", "text": "Gab es beim letzten Drill einen hohen BLAST_RADIUS?"}
{"ts": "166:00", "speaker": "E", "text": "Nein, der Score lag konstant unter 0.2, was bedeutet, dass weniger als 20% der kritischen Services betroffen wären. Das ist unser Zielwert laut interner Policy POL-DR-03."}
{"ts": "166:05", "speaker": "I", "text": "Wie reagieren Sie, wenn im Drill ein SLA-Verstoß droht?"}
{"ts": "166:09", "speaker": "E", "text": "Dann greifen wir auf den sogenannten Grace-Mode zurück, der im RFC-DR-2024-12 beschrieben ist. Dort steht, wie wir temporär Ressourcen in der Primärregion wieder hochfahren dürfen, um den Betrieb zu stabilisieren, bevor wir das Failover fortsetzen."}
{"ts": "166:15", "speaker": "I", "text": "Sehen Sie in diesem Vorgehen irgendwelche Risiken?"}
{"ts": "166:19", "speaker": "E", "text": "Ja, definitiv. Das Zurückschalten kann Dateninkonsistenzen erzeugen, wenn nicht alle Writes synchron repliziert wurden. Deshalb ist das immer eine Risikoabwägung – wir dokumentieren das in den Drill-Reports und weisen explizit auf mögliche SLA-Trade-offs hin."}
{"ts": "167:06", "speaker": "I", "text": "Können Sie uns bitte ein konkretes Beispiel nennen, wie Sie das Runbook RB-DR-001 während des letzten Drills angepasst haben?"}
{"ts": "167:11", "speaker": "E", "text": "Ja, im März haben wir im Abschnitt 4.2 des RB-DR-001 die Sequenz für die DNS-Umschaltung geändert, um mit den neuen Poseidon Networking Gateways synchron zu gehen."}
{"ts": "167:18", "speaker": "I", "text": "War das eine rein technische Anpassung oder gab es auch prozessuale Änderungen?"}
{"ts": "167:23", "speaker": "E", "text": "Beides – technisch hieß das, wir mussten die TTL von 300s auf 120s setzen, prozessual wurde im Runbook vermerkt, dass das NOC-Team die Änderung via Ticket DR-OPS-458 bestätigen muss, bevor der Failover-Schritt initiiert wird."}
{"ts": "167:33", "speaker": "I", "text": "Und wie haben Sie überprüft, dass diese Anpassung keine neuen Risiken einführt?"}
{"ts": "167:38", "speaker": "E", "text": "Wir haben einen Staging-Failover in der Sandbox-Region 'eu-central-test' simuliert und mit Nimbus Observability Latency Graphs die Wirkung gemessen; die Metriken blieben innerhalb des SLO-Korridors."}
{"ts": "167:47", "speaker": "I", "text": "Gab es Abhängigkeiten, die Sie dabei überrascht haben?"}
{"ts": "167:52", "speaker": "E", "text": "Ja, die Synchronisation der Poseidon Routen-Maps mit unserem internen Auth-Service in Region AP-SG war langsamer als erwartet, was auf eine veraltete Firmware hinwies."}
{"ts": "168:00", "speaker": "I", "text": "Wie sind Sie mit diesem Firmware-Problem umgegangen, um den Drill nicht zu gefährden?"}
{"ts": "168:05", "speaker": "E", "text": "Wir haben temporär einen Bypass über die West-Region erstellt, dokumentiert in RFC-DR-2025-17, und parallel das Firmware-Update in einem Wartungsfenster eingespielt."}
{"ts": "168:14", "speaker": "I", "text": "Gab es dadurch Auswirkungen auf Ihre RTO/RPO-Werte während des Drills?"}
{"ts": "168:19", "speaker": "E", "text": "Minimal – RTO stieg von 18 auf 22 Minuten, blieb aber unter dem SLA-Limit von 25 Minuten, RPO blieb unverändert bei 5 Sekunden dank asynchroner Replikation."}
{"ts": "168:27", "speaker": "I", "text": "Und welche Lessons Learned haben Sie daraus gezogen?"}
{"ts": "168:32", "speaker": "E", "text": "Dass wir Firmware-Validierungen in den Pre-Drill-Check aufnehmen müssen; das wird jetzt als Pflichtschritt im Runbook Abschnitt 3.1 geführt."}
{"ts": "168:38", "speaker": "I", "text": "Würden Sie sagen, dass dieser Vorfall Ihr Risikoprofil verändert hat?"}
{"ts": "168:43", "speaker": "E", "text": "Definitiv, wir haben den BLAST_RADIUS neu bewertet und für Auth-Services strictere Isolationszonen eingeführt, um kaskadierende Ausfälle zu vermeiden."}
{"ts": "169:42", "speaker": "I", "text": "Lassen Sie uns nochmal genauer auf die Lessons Learned aus dem letzten Drill eingehen. Welche konkreten Änderungen haben Sie danach im Runbook RB-DR-001 vorgenommen?"}
{"ts": "169:49", "speaker": "E", "text": "Wir haben unter anderem die Sequenz der Failover-Schritte angepasst. Früher starteten wir alle Replikations-Checks parallel, jetzt haben wir eine Gate-Logik zwischen Storage-Sync und DNS-Umschaltung eingebaut, um Inkonsistenzen zu vermeiden. Außerdem wurde im Abschnitt 4.2 ein neuer Validierungs-Checkpoint eingefügt, um mit Nimbus Telemetrie-Daten die Konsistenz zwischen den Regionen zu prüfen."}
{"ts": "169:59", "speaker": "I", "text": "Das heißt, Sie haben technische Telemetrie direkt in den Ablauf integriert?"}
{"ts": "170:03", "speaker": "E", "text": "Genau. Wir nutzen jetzt ein internes API-Endpoint, das von Nimbus Observability bereitgestellt wird. Dieses liefert innerhalb von 30 Sekunden nach Storage-Sync eine Hash-Map aller kritischen Datenbanken in beiden Regionen. Falls ein Mismatch größer 0,5% auftritt, blockiert das Runbook automatisch den DNS-Failover."}
{"ts": "170:14", "speaker": "I", "text": "Welche Auswirkungen hatte das auf die RTO im Drill?"}
{"ts": "170:18", "speaker": "E", "text": "Die RTO hat sich im Schnitt um etwa 2,5 Minuten verlängert. Wir haben das bewusst akzeptiert, weil die Daten-Konsistenz in unserem Szenario Priorität hat. Das war auch ein Trade-off, der im Steering Committee Protokoll SC-DR-2025-02 dokumentiert ist."}
{"ts": "170:29", "speaker": "I", "text": "Gab es Rückmeldungen von Stakeholdern zu dieser Entscheidung?"}
{"ts": "170:33", "speaker": "E", "text": "Ja, das Finance-Team war zunächst skeptisch, weil die längere Ausfallzeit im schlimmsten Fall Kosten von ca. 25.000 € pro Minute verursachen kann. Wir haben aber mit ihnen durchgerechnet, dass ein Datenverlust im Terabyte-Bereich, wie er ohne den Check passieren könnte, deutlich teurer wäre."}
{"ts": "170:45", "speaker": "I", "text": "Wie stellen Sie sicher, dass diese neue Gate-Logik im Ernstfall nicht selbst zum Single Point of Failure wird?"}
{"ts": "170:51", "speaker": "E", "text": "Wir haben eine Fallback-Logik implementiert: Falls das Nimbus API länger als 90 Sekunden nicht antwortet, wird ein manueller Override durch den Incident Commander aktiviert. Dieser Schritt ist in Ticket DR-OPS-7781 beschrieben, inklusive der Authentifizierung über unser internes IAM-System."}
{"ts": "171:02", "speaker": "I", "text": "In welchem Maße war Poseidon Networking in diese Änderungen involviert?"}
{"ts": "171:07", "speaker": "E", "text": "Relativ stark. Die DNS-Umschaltung basiert auf Poseidons Anycast-Routing, und die Gate-Logik musste um eine zusätzliche Health-Check-Route erweitert werden. Das haben wir in enger Abstimmung mit dem Poseidon-Team umgesetzt, um keine Routing-Loops zu riskieren."}
{"ts": "171:18", "speaker": "I", "text": "Gab es bei der Integration dieser neuen Checks unerwartete Probleme?"}
{"ts": "171:22", "speaker": "E", "text": "Ja, beim ersten Testlauf hat der Health-Check eine falsche Region als 'healthy' markiert, weil die Metriken verzögert eintrafen. Wir haben daraufhin im Nimbus-Dashboard ein Alert-Delay von 15 Sekunden eingeführt, um Flapping zu vermeiden."}
{"ts": "171:33", "speaker": "I", "text": "Wie wird diese Änderung im operativen Betrieb überwacht?"}
{"ts": "171:37", "speaker": "E", "text": "Über ein zusammengesetztes Dashboard in Nimbus, das sowohl Latenz- als auch Konsistenzmetriken darstellt. Zusätzlich läuft ein wöchentlicher automatischer Drill in der Staging-Umgebung, der die Gate-Logik testet und Berichte an das DR-Review-Board sendet."}
{"ts": "171:22", "speaker": "I", "text": "Könnten Sie bitte noch einmal genauer schildern, wie Sie beim Drill die Schnittstellen zu Poseidon Networking praktisch getestet haben?"}
{"ts": "171:47", "speaker": "E", "text": "Ja, ähm, wir haben im Drill ein Simulationsszenario gefahren, bei dem wir den primären Region-Link bewusst gekappt haben. Poseidon hat dann automatisch den Traffic gemäß Runbook RB-NET-042 auf den sekundären Pfad über die Nordregion umgeleitet."}
{"ts": "172:15", "speaker": "I", "text": "Gab es dabei unerwartete Effekte oder Latenzspitzen?"}
{"ts": "172:34", "speaker": "E", "text": "Minimal, im Bereich von 150 ms zusätzlich, was innerhalb des SLA-Puffers liegt. Allerdings hat das Nimbus Observability Dashboard ein paar False Positives gemeldet, weil die Heartbeat-Konfiguration noch auf alte Thresholds gesetzt war."}
{"ts": "172:59", "speaker": "I", "text": "Mussten Sie dafür ein Incident-Ticket eröffnen?"}
{"ts": "173:14", "speaker": "E", "text": "Genau, Ticket IT-DR-2025-117. Darin haben wir die Schwellenwerte angepasst und gleich den Abgleich mit der Poseidon API automatisiert."}
{"ts": "173:41", "speaker": "I", "text": "Wie haben Sie die Anpassung getestet, bevor Sie sie produktiv gesetzt haben?"}
{"ts": "174:02", "speaker": "E", "text": "Wir haben in der Staging-Umgebung einen Re-Drill durchgeführt, mit identischer Topologie wie in Produktion, und die Anpassung per Canary Deployment ausgerollt. Die SLIs für Failover Detection lagen danach konstant unter 8 Sekunden."}
{"ts": "174:31", "speaker": "I", "text": "Das klingt strukturiert. Gab es noch weitere Learnings aus diesem Teil des Drills?"}
{"ts": "174:49", "speaker": "E", "text": "Ja, wir haben erkannt, dass die enge Kopplung zwischen Nimbus Alerting und Poseidon Routing zwar Effizienz bringt, aber im Fehlerfall auch eine Kaskade auslösen könnte. Wir planen deshalb eine Circuit Breaker Logik einzubauen."}
{"ts": "175:18", "speaker": "I", "text": "Welche Risiken sehen Sie, falls dieser Circuit Breaker nicht rechtzeitig kommt?"}
{"ts": "175:36", "speaker": "E", "text": "Worst-case könnten unnötige Failover ausgelöst werden, was unsere RTO erhöht und potenziell Kunden-Workloads beeinträchtigt. Das ist in RFC-DR-2025-07 als mittleres Risiko eingestuft."}
{"ts": "176:01", "speaker": "I", "text": "Planen Sie dafür temporäre Mitigationsmaßnahmen?"}
{"ts": "176:17", "speaker": "E", "text": "Ja, wir setzen vorläufig manuelle Approval-Steps im Runbook RB-DR-001, Schritt 4.2, ein. Das verlangsamt zwar den automatischen Failover leicht, verhindert aber Fehlentscheidungen des Systems."}
{"ts": "176:43", "speaker": "I", "text": "Wie kommunizieren Sie solche Änderungen intern?"}
{"ts": "177:02", "speaker": "E", "text": "Über das wöchentliche DR-Changelog im Confluence-Space TITAN-DR, plus ein kurzes Briefing im Operations-Standup. Wichtig ist, dass alle Runbook-Änderungen auch in der zentralen Audit-Tabelle DR_CHANGES protokolliert werden."}
{"ts": "181:22", "speaker": "I", "text": "Könnten Sie bitte genauer auf das Runbook RB-DR-001 eingehen und wie es bei der letzten Übung angewendet wurde?"}
{"ts": "181:36", "speaker": "E", "text": "Ja, klar. RB-DR-001 beschreibt Schritt für Schritt den Failover-Prozess zwischen unserer Primär- und Sekundärregion. Bei der letzten Übung haben wir es mit Ticket DR-OPS-547 abgeglichen, um sicherzustellen, dass auch die neuesten Netzwerkpfad-Änderungen aus Poseidon berücksichtigt sind."}
{"ts": "181:56", "speaker": "I", "text": "Gab es Abweichungen vom Runbook, die Sie dokumentieren mussten?"}
{"ts": "182:06", "speaker": "E", "text": "Eine kleine Abweichung, ja. Wir mussten die Reihenfolge zweier DNS-Switch-Schritte tauschen, weil die Latenz zwischen den Regionen auf 220ms anstieg. Das haben wir im Runbook unter Abschnitt 4.3 ergänzt, mit Verweis auf Messdaten aus Nimbus."}
{"ts": "182:28", "speaker": "I", "text": "Wie haben Sie diese Anpassung vorab getestet?"}
{"ts": "182:38", "speaker": "E", "text": "Wir haben in der Staging-Umgebung ein simuliertes Latenzprofil injiziert. Dabei nutzten wir das Tool LatSim v2, das auch in unserem DR-Testplan vermerkt ist. Die Resultate flossen in RFC-DR-2025-09 ein, zur Genehmigung durch den DR-Lenkungsausschuss."}
{"ts": "182:59", "speaker": "I", "text": "Interessant. Beeinflusst diese Latenz-Thematik auch Ihre SLO-Definitionen?"}
{"ts": "183:10", "speaker": "E", "text": "Definitiv. Unser SLO für Failover Completion liegt bei 90 Sekunden. Mit der DNS-Reihenfolge-Anpassung konnten wir verhindern, dass wir diesen Wert reißen. Wir haben das im Observability-Dashboard als neuen KPI hinterlegt."}
{"ts": "183:30", "speaker": "I", "text": "Wie stellen Sie sicher, dass solche Lessons Learned dauerhaft im Team verankert werden?"}
{"ts": "183:41", "speaker": "E", "text": "Wir führen nach jedem Drill ein Post-Mortem durch, dokumentieren in Confluence und binden die Erkenntnisse in die nächste Iteration des Runbooks ein. Zusätzlich gibt es monatliche Brown-Bag-Sessions im DR-Team, um informell Wissen auszutauschen."}
{"ts": "184:01", "speaker": "I", "text": "Gab es bei der Integration mit Poseidon Networking noch offene Punkte?"}
{"ts": "184:12", "speaker": "E", "text": "Ja, ein offener Punkt war die automatische Routen-Neuberechnung. Poseidon hatte hier ein Timeout-Problem, das wir nur durch temporäre statische Routen umgehen konnten. Das ist als Risk-ID DR-NET-88 im Risikoregister vermerkt."}
{"ts": "184:33", "speaker": "I", "text": "Wie hoch schätzen Sie das Risiko ein, dass dieses Timeout im Ernstfall auftritt?"}
{"ts": "184:44", "speaker": "E", "text": "Mittlere Wahrscheinlichkeit, aber mit potenziell hohem Impact. Deshalb arbeiten wir mit dem Poseidon-Team an einem Firmware-Update, das laut Plan bis Q3 eingespielt wird."}
{"ts": "185:02", "speaker": "I", "text": "Könnte es kurzfristig zu einem Trade-off kommen zwischen schneller Behebung und Systemstabilität?"}
{"ts": "185:13", "speaker": "E", "text": "Ja, wenn wir das Update vor Abschluss aller Tests einspielen, könnten unvorhergesehene Nebenwirkungen auftreten. Deshalb ist der aktuelle Beschluss, bei DR-Drills die statischen Routen als Fallback zu nutzen, bis die Tests abgeschlossen sind – das ist dokumentiert in Entscheidungsprotokoll DEC-DR-2025-04."}
{"ts": "187:22", "speaker": "I", "text": "Lassen Sie uns bitte noch einmal konkret auf die Lessons Learned aus dem letzten Drill eingehen – was hat sich für Sie in der Praxis als besonders wertvoll herausgestellt?"}
{"ts": "187:34", "speaker": "E", "text": "Eine der wichtigsten Erkenntnisse war, dass unser Runbook RB-DR-001 in einigen Schritten zu linear gedacht war. Wir haben daraus abgeleitet, dass wir parallele Checklisten für Netzwerk-Pfade mit Poseidon gleich zu Beginn anstoßen, um die Failover-Zeit um etwa 90 Sekunden zu reduzieren."}
{"ts": "187:50", "speaker": "I", "text": "Interessant – und wie haben Sie das im System dokumentiert, damit es im nächsten Drill nicht verloren geht?"}
{"ts": "188:02", "speaker": "E", "text": "Wir haben ein Addendum zum Runbook erstellt, Version RB-DR-001-A2, und ein internes Ticket im Change-Tracker (CHG-4482) hinterlegt. Zusätzlich gab es ein Walkthrough-Meeting mit dem Operationsteam, damit auch implizite Handgriffe, die nicht im Text stehen, verbal vermittelt wurden."}
{"ts": "188:21", "speaker": "I", "text": "Gab es bei der Umsetzung dieser Änderungen auch Abhängigkeiten zu anderen Projekten, die Sie berücksichtigen mussten?"}
{"ts": "188:32", "speaker": "E", "text": "Ja, insbesondere zu Nimbus Observability. Wir mussten die neuen Logging-Events für parallele Checks in deren Pipeline integrieren. Ohne diese Anpassung hätten wir keine durchgängige Telemetrie für die geänderten Failover-Schritte gehabt."}
{"ts": "188:50", "speaker": "I", "text": "Wie haben Sie in dieser Phase sichergestellt, dass die SLA-Metriken trotz der Prozessänderungen eingehalten wurden?"}
{"ts": "189:02", "speaker": "E", "text": "Wir haben vorab simulierte Failover in einer Staging-Umgebung durchgeführt. Dabei wurden RTO und RPO mit den Zielwerten aus SLA-2025-DR verglichen: RTO ≤ 15 min, RPO ≤ 60 s. Die Simulationen lagen im Schnitt bei 14:10 min RTO und 45 s RPO."}
