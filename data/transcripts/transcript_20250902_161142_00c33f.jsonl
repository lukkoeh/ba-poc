{"ts": "00:00", "speaker": "I", "text": "Könnten Sie bitte kurz Ihre Hauptverantwortlichkeiten im Hera QA Platform Projekt skizzieren, damit wir ein gemeinsames Bild haben?"}
{"ts": "01:45", "speaker": "E", "text": "Ja klar, also meine Rolle ist QA Lead für P-HER, was bedeutet, dass ich die gesamte Teststrategie definiere und koordiniere. Dazu gehört im Build-Phase-Kontext sowohl das Setup der Unified Test Orchestration als auch die Analyse von flaky Tests. We also ensure SLA compliance for defect turnaround times — currently 48h for high severity per QA-SLA-202."}
{"ts": "04:10", "speaker": "I", "text": "Und wie ist Ihr Team strukturiert, arbeiten Sie eher funktionsübergreifend oder innerhalb eines dedizierten QA-Bereichs?"}
{"ts": "06:00", "speaker": "E", "text": "Wir haben eine Matrix-Struktur: drei QA Engineers sind direkt für Hera, weitere zwei werden aus dem Platform-Testpool bei Bedarf zugesteuert. Cross-team syncs mit SRE passieren wöchentlich, um etwaige Infrastruktur- oder Pipeline-Änderungen früh zu erkennen."}
{"ts": "08:20", "speaker": "I", "text": "Welche SLOs oder SLAs gelten aktuell konkret für QA-bezogene Deliverables?"}
{"ts": "10:15", "speaker": "E", "text": "Neben der genannten 48h Defect-Turnaround haben wir ein SLO, dass 95% der geplanten regression suite vor jedem merge-to-main läuft und grün ist. This is tracked in our internal 'QA-Dash P-HER' and alerts feed into Nimbus Observability for visibility."}
{"ts": "13:00", "speaker": "I", "text": "Lassen Sie uns zur Teststrategie gehen. Wie priorisieren Sie Testfälle basierend auf einer Risikoanalyse im aktuellen Build-Status?"}
{"ts": "15:10", "speaker": "E", "text": "Wir nutzen die risk_based_testing Methodik laut POL-QA-014: Features mit hoher Nutzer-Exposure und komplexen Integrationen, z. B. mit Atlas Mobile, bekommen Priorität 1. We feed production incident patterns from P-NIM into our risk matrix to adjust coverage dynamically."}
{"ts": "18:30", "speaker": "I", "text": "Können Sie ein praktisches Beispiel geben, wie POL-QA-014 umgesetzt wird?"}
{"ts": "21:00", "speaker": "E", "text": "Klar, im letzten Sprint hatten wir RFC-1770 Änderungen an der Test Execution API. Laut POL-QA-014 haben wir dafür 12 exploratory tests zusätzlich eingeplant, weil die Änderung mehrere Downstream-Consumer betrifft. We linked them to JIRA-PHER-482 in our traceability tool, so any defects auto-link back to the RFC."}
{"ts": "24:20", "speaker": "I", "text": "Das bringt mich zur Traceability: wie stellen Sie sicher, dass Anforderungen aus RFC-1770 bis zu Testfällen und Defect Reports rückverfolgbar sind?"}
{"ts": "27:10", "speaker": "E", "text": "Wir nutzen intern 'TraceLinker', der via API sowohl zu unserem Confluence-RFC-Space als auch zu TestRail und JIRA connectet. Jeder Testfall bekommt ein RFC-Feld, und Defects ziehen sich diese ID automatisch. This prevents orphaned defects without context."}
{"ts": "31:00", "speaker": "I", "text": "Gab es Fälle, in denen fehlende Traceability zu Verzögerungen geführt hat?"}
{"ts": "33:40", "speaker": "E", "text": "Ja, im Beta-Release von Hera v0.7 hatten wir zwei Criticals, die keiner RFC zugeordnet waren. Die Root Cause Analysis in RB-QA-051 zeigte, dass fehlende Links zu einer Woche Verzögerung führten, weil das Fix-Team die Impact-Analyse nicht machen konnte."}
{"ts": "37:20", "speaker": "I", "text": "Gibt es Schnittstellen zu P-NIM (Nimbus Observability) für Testmetriken, und wie beeinflusst das Ihre Planung?"}
{"ts": "40:00", "speaker": "E", "text": "Ja, das ist ein wichtiger Multi-Hop: Test result KPIs werden an P-NIM gepusht, dort mit Live-Service-Metriken von Atlas Mobile korreliert. If error rates spike in Atlas features we just tested, our plan adapts within the same sprint — this closed-loop feedback is crucial for Build-phase agility."}
{"ts": "90:00", "speaker": "I", "text": "Wir hatten gerade über die Integration mit Atlas Mobile gesprochen. Können Sie bitte genauer erklären, wie diese Abhängigkeit praktisch Ihre Sprint-Planung beeinflusst?"}
{"ts": "90:05", "speaker": "E", "text": "Ja, also, weil Atlas Mobile im selben Release-Zyklus hängt, müssen wir die Test-Suites, especially die device-specific ones, früher einfrieren. Das bedeutet, wir haben ein festes Cut-off nach Sprint 3, um noch Zeit für Cross-App regression runs zu haben."}
{"ts": "90:18", "speaker": "I", "text": "Verstehe. Nutzen Sie für diese Cross-App-Runs spezielle Runbooks oder sind das eher adhoc-Prozesse?"}
{"ts": "90:24", "speaker": "E", "text": "Nee, das ist in RB-QA-051 festgeschrieben. Da steht genau drin, welche Device Pools, welche OS-Versionen und welche Nimbus Observability Hooks gesetzt werden müssen, um Testmetriken live zu tracken."}
{"ts": "90:36", "speaker": "I", "text": "Und wie binden Sie die Live-Metriken aus Nimbus in Ihre Risikoanalyse ein?"}
{"ts": "90:41", "speaker": "E", "text": "Wir haben im Hera QA Dashboard ein Panel, das direkt die P-NIM API abfragt. So sehen wir anomaly scores in near-real-time und können risk-based testing dynamically adjusten, zum Beispiel wenn Crash-Raten auf bestimmten Devices plötzlich steigen."}
{"ts": "90:54", "speaker": "I", "text": "Gab es ein jüngstes Beispiel, wo Sie aufgrund solcher Live-Daten den Testplan geändert haben?"}
{"ts": "91:00", "speaker": "E", "text": "Ja, im letzten Sprint hat ein Beta-Build von Atlas Mobile auf Android 13 plötzlich Memory-Leaks gezeigt, was in Nimbus als Heap Usage Spike aufgetaucht ist. Wir haben dann gemäß POL-QA-014 zusätzliche Stress-Tests eingeschoben."}
{"ts": "91:14", "speaker": "I", "text": "Interessant. Wie dokumentieren Sie solche kurzfristigen Planänderungen?"}
{"ts": "91:19", "speaker": "E", "text": "Das läuft bei uns als Change-Note in JIRA, verlinkt mit dem Original-Ticket und den relevanten Runs aus RB-QA-051. Sometimes we also append a short post-mortem if it impacted the sprint goal."}
{"ts": "91:30", "speaker": "I", "text": "Welche SLAs gelten dabei? Müssen Sie solche Änderungen noch von einem anderen Team approven lassen?"}
{"ts": "91:36", "speaker": "E", "text": "Ja, unsere interne SLA-QA-06 sagt, dass Planänderungen über 8 Stunden Testaufwand vom Platform Lead sign-off brauchen. Anything under that we can decide autonomously."}
{"ts": "91:47", "speaker": "I", "text": "Und wie wirken sich diese Abhängigkeiten auf Ihr Defect-Management aus?"}
{"ts": "91:52", "speaker": "E", "text": "Wenn Bugs aus Cross-App-Tests kommen, taggen wir sie mit 'XAPP' und ordnen sie sowohl dem Hera QA Projekt P-HER als auch dem Atlas Mobile Board zu. Das sichert Traceability bis zurück zu RFC-1770."}
{"ts": "92:04", "speaker": "I", "text": "Gab es schon Fälle, wo fehlende oder falsche Tags zu Verzögerungen geführt haben?"}
{"ts": "92:09", "speaker": "E", "text": "Leider ja. Vor zwei Monaten hat ein fehlendes XAPP-Tag dazu geführt, dass ein kritischer Crash-Fix erst im nächsten Release Cycle kam. Wir haben daraus gelernt und eine Pre-Close Checklist in RB-QA-051 ergänzt."}
{"ts": "96:00", "speaker": "I", "text": "Können Sie uns jetzt ein konkretes Beispiel geben, wo Sie zwischen Testabdeckung und dem Release-Termin abwägen mussten?"}
{"ts": "96:08", "speaker": "E", "text": "Ja, das war im Build-Sprint 14. Wir hatten laut RB-QA-051 einen Coverage-Threshold von 85 %, aber durch eine späte Änderung aus RFC-1822 im Atlas Mobile SDK war klar, dass wir nur auf 78 % kommen würden, wenn wir den Release-Termin halten wollen. Wir mussten also entscheiden: delay oder scope cut."}
{"ts": "96:25", "speaker": "I", "text": "Und wie haben Sie diese Entscheidung dokumentiert?"}
{"ts": "96:28", "speaker": "E", "text": "Wir haben ein Decision Log in Confluence gepflegt, verlinkt mit Ticket QA-DEC-47. Darin steht die Risikoabschätzung gemäß POL-QA-014 und die Abzeichnung durch den Release Manager. Zusätzlich ein Verweis auf den Runbook-Abschnitt RNBK-HER-05 für spätere Audits."}
{"ts": "96:44", "speaker": "I", "text": "Gab es dazu auch eine formale Risikoanalyse, oder war das eher ad hoc?"}
{"ts": "96:48", "speaker": "E", "text": "Es war formalisiert: Wir haben den Risk Score mit unserem internen Tool aus P-NIM berechnet, basierend auf Failure Probability × Impact. Das Tool zieht Metriken aus der Hera QA Platform und Nimbus Observability-KPIs."}
{"ts": "97:02", "speaker": "I", "text": "Wie reagieren Stakeholder, wenn Sie so einen Scope Cut vorschlagen?"}
{"ts": "97:06", "speaker": "E", "text": "Zunächst skeptisch, deshalb bringen wir immer ein 'what-if' Szenario mit: Eine Tabelle, in der wir zeigen, wie der Mean Time to Detect für relevante Defects steigt, wenn wir Tests weglassen. This makes the risk tangible."}
{"ts": "97:20", "speaker": "I", "text": "Haben Sie ein Beispiel für eine Lesson Learned, die Ihre aktuelle Entscheidungsfindung beeinflusst?"}
{"ts": "97:24", "speaker": "E", "text": "Nach Release 2.3 hatten wir einen Production Incident, ID INC-HER-3321. Damals war ein Low-Priority Testfall aus dem Regression Set gestrichen worden, der aber einen kritischen Payment-Bug entdeckt hätte. Seitdem markieren wir Payment-Flows immer als High-Risk in POL-QA-014."}
{"ts": "97:42", "speaker": "I", "text": "Interessant. Beeinflusst das auch, welche Tools Sie einsetzen?"}
{"ts": "97:46", "speaker": "E", "text": "Ja, wir haben Atlas Mobile Test Harness erweitert, um Payment-API-Mocks direkt im CI zu fahren. Zusätzlich haben wir in Hera QA Platform Alerts hinterlegt, die bei Payment-Test Failure sofort via P-NIM an den On-Call pushen."}
{"ts": "98:00", "speaker": "I", "text": "Wie stellen Sie sicher, dass diese Alerts nicht zu False Positives führen?"}
{"ts": "98:03", "speaker": "E", "text": "Wir haben eine dreistufige Filterlogik implementiert: Erstens Thresholds aus RB-QA-051, zweitens ein Heuristik-Modul (internes Script hera_fp_filter.py), drittens manuelle Review im QA-Slack-Channel. This keeps noise low but reaction fast."}
{"ts": "98:18", "speaker": "I", "text": "Gibt es für diese Themen regelmäßige Retros?"}
{"ts": "98:22", "speaker": "E", "text": "Ja, alle zwei Sprints führen wir eine QA-Retro durch, protokolliert im Template QA-RETRO-Form v3. Dort dokumentieren wir auch Trade-offs, Lessons Learned und verlinken zu relevanten RFCs oder Incident Reports. So bleibt das Wissen nicht nur im Kopf einzelner Leute."}
{"ts": "112:00", "speaker": "I", "text": "Vielleicht können wir da noch etwas tiefer gehen – when you had to cut down scope for the Hera QA Platform tests, what were the exact risk mitigation steps you documented?"}
{"ts": "112:10", "speaker": "E", "text": "Ja, also im Fall vom Build-Sprint 14 haben wir ein Risk Acceptance Ticket, QA-RAT-032, erstellt. Dort steht explizit, welche Testfälle aus POL-QA-014 wir verschoben haben und wie wir mit temporären Canary Deployments kompensieren."}
{"ts": "112:25", "speaker": "I", "text": "And did you involve SRE to adjust monitoring thresholds in Nimbus Observability during that mitigation?"}
{"ts": "112:34", "speaker": "E", "text": "Genau, wir hatten ein gemeinsames Runbook RB-QA-051 Appendix B, wo wir die Alert-Thresholds für Login- und Payment-Flows im P-NIM Modul temporär gelockert haben, um false positives zu vermeiden."}
{"ts": "112:49", "speaker": "I", "text": "Gab es da einen formalen Review-Prozess oder war das eher ad hoc?"}
{"ts": "112:57", "speaker": "E", "text": "Formell schon – wir haben eine Mini-RFC, RFC-1822, eingereicht, die im QA-Change-Board wöchentlich geprüft wurde. Trotzdem, die initiale Entscheidung war innerhalb von 2 Stunden, weil sonst der Release-Train verpasst worden wäre."}
{"ts": "113:15", "speaker": "I", "text": "So you accepted a potential increase in defect leakage to production?"}
{"ts": "113:21", "speaker": "E", "text": "Ja, aber nur in low criticality Bereichen. Die SLOs für Critical Bugs (Severity 1) bleiben bei max. 0,5% Leakage, das ist auch im SLA-QA-2023 hinterlegt."}
{"ts": "113:36", "speaker": "I", "text": "Wie wurde das dem Product Owner kommuniziert?"}
{"ts": "113:42", "speaker": "E", "text": "Über den wöchentlichen QA-Risk-Report. Da gibt es eine Section 'Deferred Tests & Compensations'. Wir haben dort den Link zu QA-RAT-032 und die geplanten Post-Release Tests dokumentiert."}
{"ts": "113:58", "speaker": "I", "text": "And any lessons learned from that sprint that you're applying now in Build phase?"}
{"ts": "114:05", "speaker": "E", "text": "Absolut, wir haben jetzt eine Heuristik: wenn der zu verschiebende Test eine Abdeckungslücke >15% in einem Modul erzeugt, muss ein zusätzlicher Canary plus Rollback-Plan im Runbook verankert werden."}
{"ts": "114:21", "speaker": "I", "text": "Wie setzen Sie das praktisch um, gerade mit den Atlas Mobile Dependencies?"}
{"ts": "114:29", "speaker": "E", "text": "Wir cross-referencen in Jira alle Atlas-Module, die betroffen sind, und nutzen die Integration zu Hera QA Platform, um sofortige Smoke Tests gegen die Atlas Staging-API zu triggern."}
{"ts": "114:44", "speaker": "I", "text": "Sounds like a tighter feedback loop. Any remaining risk you can't fully mitigate?"}
{"ts": "114:52", "speaker": "E", "text": "Ja, die Abhängigkeit von externen Payment-Gateways. Da können wir nur mit synthetischen Testaccounts arbeiten, was nicht immer alle Edge Cases abbildet – das ist im Risk Registry als QA-RISK-219 dokumentiert."}
{"ts": "120:00", "speaker": "I", "text": "Lassen Sie uns nochmal zu den Integrationen mit P-NIM zurückkommen. Wie genau fließen die Observability-Daten in Ihre Testmetriken ein?"}
{"ts": "120:18", "speaker": "E", "text": "Also, wir nutzen den P-NIM Stream Adapter, der im RB-QA-051 dokumentiert ist. Der zieht Logs und Metriken aus Nimbus Observability in unser Hera Dashboard. That way we can correlate flaky tests mit real-time infra anomalies."}
{"ts": "120:45", "speaker": "I", "text": "Verstehe, und wie gehen Sie mit Latenzen um, wenn P-NIM Daten verspätet liefert?"}
{"ts": "121:02", "speaker": "E", "text": "Da haben wir ein Fallback im Runbook RB-QA-051 Abschnitt 4.2: If metrics are delayed >90s, we cache the last valid snapshot und markieren die Tests als 'pending infra confirmation'. Das reduziert Fehlalarme drastisch."}
{"ts": "121:28", "speaker": "I", "text": "Sie hatten vorhin Atlas Mobile erwähnt. Können Sie ein Multi-Hop Beispiel geben, wie ein Bug dort durch Hera QA erkannt wurde?"}
{"ts": "121:45", "speaker": "E", "text": "Klar, das war Ticket QA-882. Ein UI-Bug in Atlas Mobile triggered ein Memory Leak im Backend, was dann in Nimbus Alerts hochging. Hera verband das via Trace-ID aus RFC-1770, sodass wir die Ursache schnell isolieren konnten."}
{"ts": "122:15", "speaker": "I", "text": "Das klingt nach einem komplexen Pfad. Gab es dafür spezielle Traceability-Einstellungen?"}
{"ts": "122:32", "speaker": "E", "text": "Ja, wir mussten in unserem Test Management Tool die Cross-System Trace Keys aktivieren. Das ist im internen Guidance-Note GN-QA-12 beschrieben und war vorher gar nicht Standard, was echt... äh, hinderlich war."}
{"ts": "123:00", "speaker": "I", "text": "Kommen wir auf Entscheidungen zurück: In der letzten Sprintplanung, welche Abdeckung mussten Sie streichen, um den Release-Termin zu halten?"}
{"ts": "123:18", "speaker": "E", "text": "Wir haben die Low-Risk Regression Suite für Legacy Reports gestrichen. The decision was based on risk scoring per POL-QA-014 und dokumentiert in RFC-1891, mit Verweis auf Impact <1% laut historischem Defect Log."}
{"ts": "123:45", "speaker": "I", "text": "Gab es dabei Bedenken seitens Stakeholder?"}
{"ts": "124:00", "speaker": "E", "text": "Ja, der Product Owner war initially uncomfortable, aber wir haben Ticket DEC-QA-77 erstellt mit allen Metriken, SLA-Check und Rollback-Plan, um das Risiko transparent zu machen."}
{"ts": "124:28", "speaker": "I", "text": "Und wie dokumentieren Sie Lessons Learned aus solchen Entscheidungen? Nutzen Sie ein zentrales Repository?"}
{"ts": "124:44", "speaker": "E", "text": "Genau, wir pflegen das QA Decision Log im Confluence Space 'HERA-QA', linked zu den jeweiligen RFCs und Runbooks. That way future teams sehen den Kontext sofort."}
{"ts": "125:10", "speaker": "I", "text": "Letzte Frage: Welche Risiken sehen Sie aktuell für den nächsten Release?"}
{"ts": "125:30", "speaker": "E", "text": "Hauptsächlich die Abhängigkeit von einem neuen P-NIM Adapter, der noch nicht load-getestet ist. Wir haben Risk ID RSK-204 angelegt, mitigations im RB-QA-059 dokumentiert, und... ja, wir beobachten das täglich im Build-Monitor."}
{"ts": "136:00", "speaker": "I", "text": "Sie hatten vorhin angedeutet, dass einige dieser Lessons Learned auch durch Schnittstellenprobleme mit P-NIM entstanden sind. Können Sie das ein bisschen ausführen?"}
{"ts": "136:18", "speaker": "E", "text": "Ja, genau. In der Build-Phase hatten wir im April den Fall, dass Testmetriken aus P-NIM nicht rechtzeitig synchronisiert wurden. This delayed our flaky test analysis by almost 48 hours, und das hat uns gezwungen, die Priorisierung ad hoc zu ändern."}
{"ts": "136:42", "speaker": "I", "text": "Und wie haben Sie das gelöst? Gab es einen Workaround im Runbook RB-QA-051?"}
{"ts": "137:00", "speaker": "E", "text": "RB-QA-051 hat tatsächlich einen Abschnitt 'Fallback Metrics', der besagt, dass wir im Notfall auf lokal aggregierte Logs zurückgreifen dürfen. That required manual parsing of the JSON output, was fehleranfällig ist, aber immerhin konnten wir die risk-based testing slots füllen."}
{"ts": "137:26", "speaker": "I", "text": "Klingt nach einem hohen manuellen Aufwand. Hat das die SLA-Ziele gefährdet?"}
{"ts": "137:40", "speaker": "E", "text": "Ja, das SLO 'Analyse innerhalb von 24h' aus POL-QA-014 wurde in diesem Sprint verfehlt. Wir mussten das im Sprint-Review transparent machen, und es gibt jetzt ein Jira-Ticket QA-7731, um diese Fallback-Prozesse zu automatisieren."}
{"ts": "138:05", "speaker": "I", "text": "Wenn wir auf Atlas Mobile schauen: gibt es dort ähnliche Risiken für die Testplanung?"}
{"ts": "138:20", "speaker": "E", "text": "Atlas Mobile hat eine andere Art von Dependency. The build artifacts often change schema without pre-announcement, und dadurch brechen unsere Integrationstests, weil die Mock-Daten nicht mehr passen."}
{"ts": "138:44", "speaker": "I", "text": "Wie dokumentieren Sie diese Art von Schema-Breaks?"}
{"ts": "139:00", "speaker": "E", "text": "Wir haben ein internes Schema-Registry-Log, und jedes unerwartete Breaking Change wird als Incident-Typ 'QA-BLOCKER' im Confluence-Board vermerkt. Zusätzlich erstellen wir ein Subticket unter dem übergeordneten RFC-1770-Thread, damit die Traceability gewahrt bleibt."}
{"ts": "139:28", "speaker": "I", "text": "Gibt es einen konkreten Fall, bei dem fehlende Traceability wirklich zu Verzögerungen führte?"}
{"ts": "139:44", "speaker": "E", "text": "Ja, im Mai hatten wir einen Defect DF-992 ohne Verknüpfung zur ursprünglichen User Story. That meant we couldn't determine the impacted module quickly, und die Entwickler mussten Code-Reviews manuell durchsuchen."}
{"ts": "140:10", "speaker": "I", "text": "Sie sagten vorhin, dass solche Fälle in Ihre aktuellen Entscheidungen einfließen. Können Sie ein Beispiel nennen, wie?"}
{"ts": "140:26", "speaker": "E", "text": "Klar. Wir haben jetzt im QA-Gate eine Pflichtprüfung: every defect ticket must link to at least one requirement ID aus RFC-1770. Fehlt diese, wird das Ticket nicht in den 'Ready for QA'-Status verschoben."}
{"ts": "140:50", "speaker": "I", "text": "Wie reagieren die Entwickler darauf? Gibt es da Friktionen?"}
{"ts": "141:10", "speaker": "E", "text": "Anfangs gab es Widerstand, weil es mehr Formulararbeit bedeutet. But after we showed them the time saved in root cause analysis—up to 3 hours per incident—they accepted it as a fair trade-off."}
{"ts": "145:00", "speaker": "I", "text": "Lassen Sie uns noch mal kurz auf die Integration mit Nimbus Observability eingehen – wie fließen die dort erhobenen Metriken konkret in Ihre Testentscheidungen ein?"}
{"ts": "145:10", "speaker": "E", "text": "Also, wir haben einen direkten Feed aus P-NIM, der per gRPC Stream in unser QA-Dashboard eingespeist wird. Die Error Rate vom letzten Build wird mit den risk scores aus POL-QA-014 verheiratet, sodass wir high-risk Bereiche gezielter re-testen."}
{"ts": "145:22", "speaker": "I", "text": "Und diese Verknüpfung, die Sie beschreiben – passiert die automatisch oder muss das Team manuell mappen?"}
{"ts": "145:30", "speaker": "E", "text": "In 80% der Fälle auto-mapped anhand von Service IDs aus RFC-1770. Für exotische Module, wo Atlas Mobile APIs involviert sind, machen wir ein manuelles Mapping, documented in RB-QA-051 Appendix C."}
{"ts": "145:45", "speaker": "I", "text": "Könnten Sie ein Beispiel geben, bei dem diese Mapping-Logik einen Bug schneller auffindbar gemacht hat?"}
{"ts": "145:55", "speaker": "E", "text": "Ja, Ticket QA-8721. Da hat ein Spike in der Login-Latenz in Atlas Mobile direkt ein flag in unserem risk-based Testplan ausgelöst. Wir konnten vor dem Release den Thread-Leak fixen."}
{"ts": "146:10", "speaker": "I", "text": "Interessant. Gab es umgekehrt auch mal falsche Positivmeldungen durch diese Integration?"}
{"ts": "146:18", "speaker": "E", "text": "Ja, einmal im Sprint 12 – false positive wegen eines Testdaten-Generators, der in P-NIM als realer Traffic gewertet wurde. Lesson learned: wir taggen jetzt synthetic traffic explizit in den Observability Streams."}
{"ts": "146:35", "speaker": "I", "text": "Das klingt nach einer Anpassung, die nicht nur QA betrifft. Mussten Sie dafür ein RFC erstellen?"}
{"ts": "146:43", "speaker": "E", "text": "Genau, RFC-1822. Da haben wir zusammen mit SRE definiert, wie synthetic_traffic_flag im Protokoll gesetzt wird. War ein kleiner Trade-off: Minimal mehr Overhead, aber deutlich weniger noise in den Metriken."}
{"ts": "146:58", "speaker": "I", "text": "Wie haben Sie diesen Trade-off intern vermittelt, gerade in Bezug auf den Release-Termin?"}
{"ts": "147:07", "speaker": "E", "text": "Wir haben im Steering Committee eine Impact-Matrix gezeigt: extra 2h Build-Zeit vs. 15% weniger false positives. Die Entscheidung fiel pro Qualität, documented in Jira QADEC-44."}
{"ts": "147:23", "speaker": "I", "text": "Gab es von Managementseite Bedenken wegen der zusätzlichen Build-Zeit?"}
{"ts": "147:30", "speaker": "E", "text": "Kurze Diskussion, ja. Aber wir hatten Belege aus den letzten drei Sprints, dass false positives jeweils 6-8h Analyse-Aufwand erzeugten. Das hat die Entscheidung klar gemacht."}
{"ts": "147:45", "speaker": "I", "text": "Wenn Sie jetzt zurückblicken: Würden Sie den gleichen Weg nochmal gehen oder alternative Maßnahmen prüfen?"}
{"ts": "147:54", "speaker": "E", "text": "Ich würde es wieder tun, aber eventuell früher im Projektzyklus, um weniger Kontextwechsel im Team zu verursachen. Frühzeitige Integration mit P-NIM hätte einige Iterationen gespart."}
{"ts": "150:00", "speaker": "I", "text": "Sie hatten vorhin die Integration mit Atlas Mobile erwähnt. Können Sie bitte genauer erklären, wie diese Schnittstelle in Ihrer Testplanung berücksichtigt wird?"}
{"ts": "150:05", "speaker": "E", "text": "Ja, klar. Wir haben im Hera QA Platform Build ein dediziertes Integration-Testpaket, das auf die Atlas Mobile APIs zielt. Interessanterweise müssen wir dort sowohl funktionale Tests als auch Latenz-Tests fahren, weil die Atlas-Services manchmal unter Last degradieren. This means in our sprint plans, wir reservieren mindestens zwei Tage nur für Cross-Team Debugging in Kooperation mit dem Atlas DevOps."}
{"ts": "150:20", "speaker": "I", "text": "Und gibt es da eine direkte Abhängigkeit zu den Nimbus Observability-Daten?"}
{"ts": "150:25", "speaker": "E", "text": "Ja, genau, hier kommt der Multi-Hop-Aspekt ins Spiel. Wir injizieren Testmetriken aus Atlas-Integration-Tests in P-NIM, das ist unser Nimbus Observability Projekt. Von dort werden Alerts an das Hera QA Dashboard weitergeleitet. So können wir Korrelationen zwischen mobilen Performance-Drops und Back-End-Flaky Tests herstellen. Without P-NIM, we'd be blind to temporal spikes."}
{"ts": "150:45", "speaker": "I", "text": "Das klingt komplex. Gibt es spezielle Runbooks, die dieses Zusammenspiel dokumentieren?"}
{"ts": "150:50", "speaker": "E", "text": "Ja, im RB-QA-051 ist genau beschrieben, wie wir die Observability-Feeds anzapfen. Steps 4 bis 9 sind critical: Dort steht, wie man die WebSocket-Streams aus P-NIM filtert, bevor man sie ins Hera QA Data Lake schreibt. We also have a fallback procedure in case the WebSocket lags exceed 3 seconds, documented under RB-QA-051-AppendixB."}
{"ts": "151:10", "speaker": "I", "text": "Wie stellen Sie sicher, dass diese Integrationspunkte auch in den Testplänen aktuell bleiben?"}
{"ts": "151:15", "speaker": "E", "text": "Wir haben ein monatliches Traceability-Review-Meeting. Dort gleichen wir alle Integrations-Testfälle mit der jeweils aktuellen Version von RFC-1770 und den Atlas API Specs ab. Also, if Atlas changes an endpoint, wir müssen das sofort in den Hera QA Test Suites reflektieren, sonst riskieren wir False Negatives."}
{"ts": "151:35", "speaker": "I", "text": "Gab es einen Vorfall, bei dem fehlende Aktualisierung zu einem Problem geführt hat?"}
{"ts": "151:40", "speaker": "E", "text": "Leider ja. Ticket QA-4287 beschreibt einen Fall im letzten Quartal: Atlas hatte den Auth-Flow angepasst, aber unser Smoke-Test nutzte noch die alte Token-Route. That caused a cascade von 17 fehlgeschlagenen Tests, die nichts mit der eigentlichen Codequalität zu tun hatten. Wir haben daraus gelernt, einen API-Change-Webhook zu abonnieren."}
{"ts": "152:00", "speaker": "I", "text": "Wie fließen solche Lessons Learned in Ihre Build-Phase-Entscheidungen ein?"}
{"ts": "152:05", "speaker": "E", "text": "Wir dokumentieren jede größere Abweichung als Decision Record im Confluence-Bereich 'Hera-Build-Decisions'. Da steht dann drin: was ist passiert, welcher Trade-off wurde akzeptiert, welche mitigations. Based on QA-4287, haben wir z.B. entschieden, API-Mock-Suites parallel zu den Live-Tests zu fahren, auch wenn das mehr Maintenance bedeutet."}
{"ts": "152:25", "speaker": "I", "text": "Aber das erhöht ja den Aufwand, oder?"}
{"ts": "152:30", "speaker": "E", "text": "Ja, absolut. Wir mussten zwei zusätzliche QA Engineers einplanen, und das Budget anpassen. However, die SLA für Testfeedback ist 4 Stunden; ohne die Mock-Suites hätten wir diese SLA bei API-Änderungen nicht mehr gehalten."}
{"ts": "152:45", "speaker": "I", "text": "Wie dokumentieren Sie diese SLA-Einhaltung?"}
{"ts": "152:50", "speaker": "E", "text": "Wir loggen die Zeitstempel vom Commit bis zum Testreport automatisch. Das fließt in unser QA-Monitoring-Board ein. Every month, wir exportieren einen Report, der mit dem internen SLA-Dokument verknüpft ist, ID SLA-QA-2023-04. Dort sieht man, ob die 4h eingehalten wurden, und falls nicht, welche Root Cause wir identifiziert haben."}
{"ts": "152:00", "speaker": "I", "text": "Bevor wir zu den Lessons Learned kommen—könnten Sie, äh, noch mal erläutern, wie genau Sie die SLAs für QA-Deliverables aktuell im Build-Phase-Kontext interpretieren?"}
{"ts": "152:15", "speaker": "E", "text": "Klar, also, wir haben für P-HER eine SLA von 48 Stunden für die Bereitstellung von Testresultaten nach einem Merge-Window. In der Build-Phase heißt das, wir müssen oft parallelisieren—und wir nutzen dabei das Runbook RB-QA-051 als Leitplanke, um nicht gegen die Policy POL-QA-014 zu verstoßen."}
{"ts": "152:38", "speaker": "I", "text": "Und diese Policy, ist die rein intern, oder gibt es Schnittstellen zu anderen Projekten?"}
{"ts": "152:50", "speaker": "E", "text": "Die ist intern, aber sie referenziert Metriken, die wir via Nimbus Observability (P-NIM) einspielen. Zum Beispiel fließen dort die flaky test rates aus Hera in ein gemeinsames Dashboard, das auch Atlas Mobile sehen kann."}
{"ts": "153:12", "speaker": "I", "text": "Ah, das erklärt die Cross-Projekt-Abhängigkeit. Wie stellen Sie sicher, dass diese Daten konsistent und rückverfolgbar bleiben?"}
{"ts": "153:26", "speaker": "E", "text": "Wir koppeln jedes Testpaket mit einer eindeutigen QA-Build-ID, die in Jira-Tickets wie QA-5243 vermerkt wird. Diese ID taucht dann auch in den P-NIM Metrics Streams auf, wodurch die Traceability bis zu RFC-1770-Anforderungen gewährleistet bleibt."}
{"ts": "153:49", "speaker": "I", "text": "Sie haben vorhin das Runbook RB-QA-051 erwähnt—wie oft müssen Sie davon im Tagesgeschäft abweichen?"}
{"ts": "154:01", "speaker": "E", "text": "Selten, aber wenn, dann dokumentieren wir eine Ausnahme in einem Exception-Log im Confluence-Bereich des Projekts. Ein Beispiel ist Ticket QA-5378, wo wir einen Testfall wegen einer Atlas Mobile API-Änderung temporär deaktivieren mussten."}
{"ts": "154:24", "speaker": "I", "text": "Klingt nach einem klassischem Multi-Hop-Impact—Atlas ändert API, Hera muss Test anpassen, Metrics in Nimbus weichen ab."}
{"ts": "154:37", "speaker": "E", "text": "Genau, und das war tricky, weil die SLA-Uhr trotzdem lief. Wir mussten also Risiko priorisieren: High-Impact Tests für Payment-Flows blieben aktiv, Low-Impact wurden verschoben."}
{"ts": "154:54", "speaker": "I", "text": "Wie, äh, gehen Sie bei solchen Priorisierungen methodisch vor? Gibt es ein Tool oder ist das eher Erfahrungswert?"}
{"ts": "155:07", "speaker": "E", "text": "Wir nutzen einen Risk Scoring Guide aus POL-QA-014, ergänzt durch unser internes Heuristik-Worksheet. Das Worksheet enthält ungeschriebene Regeln, z.B. dass Änderungen an Auth-Providern immer als 'High' eingestuft werden, auch wenn Code-Diff klein ist."}
{"ts": "155:30", "speaker": "I", "text": "Letzte Woche gab es ja eine Diskussion im Steering Committee—ging es da um so eine Auth-Provider-Änderung?"}
{"ts": "155:42", "speaker": "E", "text": "Ja, RFC-1821. Da mussten wir entscheiden, ob wir vollständige Regression machen oder nur targeted tests. Aufgrund der Deadline haben wir den Trade-off in QA-5421 dokumentiert: 70% Coverage statt 95%, mit Hinweis auf geplanten Follow-up-Testlauf."}
{"ts": "156:05", "speaker": "I", "text": "Wie wird sichergestellt, dass solche Trade-offs nicht untergehen und in die Lessons Learned einfließen?"}
{"ts": "156:20", "speaker": "E", "text": "Wir führen am Ende jedes Sprints ein QA-Retrospective mit Risk Review durch. Die Top-3 Abweichungen vom Standardprozess werden in einem 'Risk Ledger' festgehalten, das als Input für die Planung der nächsten Build-Phase dient."}
{"ts": "160:00", "speaker": "I", "text": "Lassen Sie uns noch einmal genauer auf die Integration mit P-NIM eingehen. Wie fließen die Observability-Daten konkret in Ihre Testmetriken ein?"}
{"ts": "160:06", "speaker": "E", "text": "Wir haben da ein internes Mapping, das in RB-QA-051 beschrieben ist. The P-NIM hooks push latency and error rate data directly into our Hera dashboards, und wir korrelieren diese dann mit Flaky-Test-Reports."}
{"ts": "160:15", "speaker": "I", "text": "Nutzen Sie diese Korrelation auch, um Prioritäten bei Regressionstests zu setzen?"}
{"ts": "160:21", "speaker": "E", "text": "Ja, genau. If we see a spike in error rate from Nimbus Observability in a module, we elevate related regression cases to 'critical'. Das ist auch in POL-QA-014 als Best Practice verankert."}
{"ts": "160:32", "speaker": "I", "text": "Wie stellen Sie sicher, dass dabei die Traceability zu RFC-1770 gewahrt bleibt?"}
{"ts": "160:38", "speaker": "E", "text": "Wir taggen die Testfälle mit den RFC-IDs im Test-Case-Management-Tool. That way, any defect linked to that test retains the RFC reference, und wir können bis zur ursprünglichen Anforderung zurückverfolgen."}
{"ts": "160:49", "speaker": "I", "text": "Gab es schon Situationen, in denen fehlende Tags zu Verzögerungen führten?"}
{"ts": "160:54", "speaker": "E", "text": "Leider ja, im Build 1.6 hatten wir ein Ticket QA-3421, bei dem ein fehlender Tag dazu führte, dass der Defect nicht in die Scope-Freeze-Liste aufgenommen wurde. It delayed the hotfix by two days."}
{"ts": "161:05", "speaker": "I", "text": "Und wie haben Sie das Problem adressiert?"}
{"ts": "161:09", "speaker": "E", "text": "Wir haben einen Pre-merge Hook eingeführt, der prüft, ob alle neuen Testfälle RFC- und Risk-Tags enthalten. Plus, wir schulen die QA-Engineers regelmäßig dazu."}
{"ts": "161:18", "speaker": "I", "text": "Kommen wir zu Atlas Mobile – beeinflusst die Mobile-Integration Ihre Testpriorisierung?"}
{"ts": "161:24", "speaker": "E", "text": "Definitiv. Mobile clients haben andere Latenz- und Battery-Constraints. We run a separate risk assessment for Atlas-related features, und oft müssen wir zusätzliche soak tests einplanen."}
{"ts": "161:35", "speaker": "I", "text": "Gab es da schon kritische Trade-offs zwischen Testtiefe und Release-Datum?"}
{"ts": "161:40", "speaker": "E", "text": "Ja, im Fall von Atlas Sync 2.1 war klar, dass vollständige battery drain tests drei Tage extra bedeuten würden. We documented the decision in RFC-1892 and accepted partial coverage to meet the client delivery window."}
{"ts": "161:53", "speaker": "I", "text": "Wie haben Sie die Risiken dabei kommuniziert?"}
{"ts": "161:57", "speaker": "E", "text": "Wir haben ein Risk Log im Confluence gepflegt, verlinkt auf RFC-1892 und das Jira-Issue REL-778. Das erlaubt SRE und Product sofort zu sehen, welche Qualitätsaspekte bewusst verschoben wurden."}
{"ts": "161:36", "speaker": "I", "text": "Könnten Sie vielleicht noch einmal erläutern, wie genau das Runbook RB-QA-051 Ihnen bei diesen Abwägungen hilft?"}
{"ts": "161:42", "speaker": "E", "text": "Ja, klar. RB-QA-051 beschreibt Schritt für Schritt, wie wir bei knappen Zeitfenstern die Test Suite auf die risikorelevantesten Module reduzieren. It maps directly to our risk scoring model aus POL-QA-014 and gibt klare Thresholds, wann ein Testblock geskippt werden darf."}
{"ts": "161:54", "speaker": "I", "text": "Und diese Thresholds, sind die statisch oder werden die dynamisch angepasst?"}
{"ts": "162:00", "speaker": "E", "text": "Teilweise dynamisch. Wir haben eine Integration mit P-NIM, also Nimbus Observability, die uns Live-Telemetriedaten liefert. If a subsystem shows rising error rates, dann bumpen wir automatisch den Risk Score in unserer QA Orchestration."}
{"ts": "162:12", "speaker": "I", "text": "Das heißt, es gibt eine direkte Feedbackschleife aus Produktion ins Build-Phase-Testen hinein?"}
{"ts": "162:17", "speaker": "E", "text": "Genau. Das ist besonders wichtig für Module, die auch Atlas Mobile anbindet. Wenn dort in der Beta-App Fehlermuster auftauchen, triggern wir sofort targeted Regression Tests im Hera QA Platform orchestrator."}
{"ts": "162:28", "speaker": "I", "text": "Wie dokumentieren Sie solche spontanen Regressionen?"}
{"ts": "162:33", "speaker": "E", "text": "Wir hängen sie an das ursprüngliche RFC, z.B. RFC-1770, als Ergänzung und verlinken das zu den Jira-Tickets. Zusätzlich pflegen wir in Confluence eine Change Log Section, wo wir die ad-hoc Testeinsätze mit Datum, Risk Score und Impact versehen."}
{"ts": "162:46", "speaker": "I", "text": "Gab es in letzter Zeit ein Beispiel, wo diese Kette besonders gut funktioniert hat?"}
{"ts": "162:51", "speaker": "E", "text": "Ja, im Ticket QA-5231. Da hat P-NIM uns einen Spike bei API-Latenzen gemeldet, noch bevor der geplante Testzyklus startete. Wir haben innerhalb von 30 Minuten gezielt die API-Tests hochpriorisiert, Fehler gefunden und an das Dev-Team zurückgemeldet."}
{"ts": "163:04", "speaker": "I", "text": "Und dadurch konntet ihr den Release-Termin halten?"}
{"ts": "163:08", "speaker": "E", "text": "Ja, das war einer dieser lucky saves. Without that feedback loop, wäre der Defekt erst post-release aufgefallen, und das hätte unser SLA von 99,5% availability gefährdet."}
{"ts": "163:18", "speaker": "I", "text": "Sie erwähnten Lessons Learned – haben Sie das in die nächste Iteration der Teststrategie übernommen?"}
{"ts": "163:23", "speaker": "E", "text": "Absolut. Wir haben im Review Meeting den Trigger Threshold für API-Latenzen gesenkt und im RB-QA-051 ergänzt, dass API-Regressionen bei Atlas Mobile Integrationspunkten immer bevorzugt werden."}
{"ts": "163:34", "speaker": "I", "text": "Klingt nach einem klaren Trade-off zugunsten kritischer Pfade. Gibt es Risiken, dass dadurch niedrigere Prioritätstests zu lange liegenbleiben?"}
{"ts": "163:41", "speaker": "E", "text": "Ja, das ist die Kehrseite. Wir dokumentieren das Risiko im Risk Log RSK-HER-08. Dort steht, dass non-critical modules maximal zwei Sprints ohne vollständige Regression bleiben dürfen, sonst wird ein Stop-Release-Flag gesetzt."}
{"ts": "162:06", "speaker": "I", "text": "Wenn wir jetzt noch einmal auf die Risk-Based-Testing-Methodik schauen: gibt es Punkte, wo Sie bewusst von POL-QA-014 abweichen, um kurzfristige Ziele zu erreichen?"}
{"ts": "162:11", "speaker": "E", "text": "Ja, manchmal. Wir haben z.B. in der letzten Build-Sprintwoche ein paar Low-Risk-Testfälle aus der Suite genommen, um dafür ein zusätzliches End-to-End-Szenario für Atlas Mobile zu bauen. That was logged under Ticket QA-8734 with a deviation note."}
{"ts": "162:19", "speaker": "I", "text": "Und wie wird so eine Abweichung dokumentiert? Nutzen Sie ein formales Freigabeprozedere?"}
{"ts": "162:24", "speaker": "E", "text": "Genau, wir hängen an das Ticket die Approval Comments vom QA Governance Board an, plus einen Verweis auf Runbook RB-QA-051 Kapitel 4.2. This way, the deviation is traceable and auditable."}
{"ts": "162:32", "speaker": "I", "text": "Hat die Integration mit Nimbus Observability hier eine Rolle gespielt?"}
{"ts": "162:38", "speaker": "E", "text": "Ja, wir konnten über den P-NIM Feed sofort sehen, dass die Ausführungszeit der Regression Suite um 18% gesunken ist. That metric gave us confidence to proceed without jeopardizing SLA-TRT-02 for test turnaround time."}
{"ts": "162:47", "speaker": "I", "text": "Können Sie ein Beispiel geben, wo fehlende Traceability zu einer Verzögerung geführt hat?"}
{"ts": "162:53", "speaker": "E", "text": "Im Februar hatten wir bei RFC-1770 Section 5 keine klare Zuordnung zu den Testfällen TF-HER-221 bis -225. That gap caused a 3-day slip because defects DEF-992 und DEF-997 were not linked in JIRA-QA correctly."}
{"ts": "163:02", "speaker": "I", "text": "Wie haben Sie darauf reagiert?"}
{"ts": "163:07", "speaker": "E", "text": "Wir haben eine Quickfix-Convention \"req_link\" eingeführt, documented in Confluence, und in RB-QA-051 ergänzt. Since then, our traceability score—gemessen in unserem Internal QA KPI Dashboard—ist von 82% auf 95% gestiegen."}
{"ts": "163:17", "speaker": "I", "text": "Gab es in letzter Zeit eine schwierige Entscheidung zwischen Testabdeckung und Release-Termin, die besonders riskant war?"}
{"ts": "163:23", "speaker": "E", "text": "Ja, im Build 1.8 Release. Wir mussten auf zwei Security Regression Tests verzichten, um den Atlas Mobile API Freeze einzuhalten. That was documented in RFC-1852 with an explicit Risk Acceptance signed by the CISO."}
{"ts": "163:33", "speaker": "I", "text": "Und welche Lessons Learned haben Sie daraus gezogen?"}
{"ts": "163:38", "speaker": "E", "text": "Wir haben gelernt, dass wir im Sprint-0 bereits Security Test Hooks in die CI Pipeline integrieren müssen. Otherwise, we risk last-minute cuts that compromise our security posture."}
{"ts": "163:46", "speaker": "I", "text": "Wie stellen Sie jetzt sicher, dass diese Hooks frühzeitig da sind?"}
{"ts": "163:51", "speaker": "E", "text": "Wir haben ein Pre-Build Checklist Item eingeführt, mandatory per POL-QA-014 Addendum A, und SRE setzt in Jenkins einen Gate-Job, der ohne Security Hook keine Build-Promotion zulässt."}
{"ts": "164:42", "speaker": "I", "text": "Bevor wir jetzt in die Integrationsdetails gehen – können Sie mir kurz sagen, wie die Schnittstelle zu P-NIM für Testmetriken aktuell aussieht?"}
{"ts": "164:47", "speaker": "E", "text": "Klar, also wir haben einen Exporter, der nightly die QA-Metriken aus Hera in den P-NIM-Kanal 'qa.hera.metrics' pusht. Das ist ein YAML-basiertes Payload-Format gemäß RB-QA-051, und wir nutzen dort ein Mapping, das direkt auf die risk_score Felder aus POL-QA-014 referenziert."}
{"ts": "164:55", "speaker": "I", "text": "Und diese risk_scores, sind die rein aus den Testresultaten oder auch aus externen Logs?"}
{"ts": "165:00", "speaker": "E", "text": "Mixed – etwa 70 % kommen aus internen Testläufen, 30 % sind Correlation Scores mit Nimbus Observability Logs. Das heißt, wenn wir im P-NIM Error-Spikes sehen, fließt das direkt in die Priorisierung für den nächsten Testzyklus ein."}
{"ts": "165:09", "speaker": "I", "text": "Okay, das heißt, Sie verknüpfen quasi die Build-Phase Tests mit dem Live-Monitoring."}
{"ts": "165:13", "speaker": "E", "text": "Genau, und das ist einer der multi-hop Links, die wir intern 'triangulated risk path' nennen – von RFC-1770 über Testfall-Mapping in Jira-HQP bis hin zu Live-Daten aus P-NIM. Ohne diese Kette wäre die Risikoanalyse deutlich flacher."}
{"ts": "165:23", "speaker": "I", "text": "Das klingt komplex. Gab's da mal Probleme mit Traceability?"}
{"ts": "165:27", "speaker": "E", "text": "Ja, in Ticket HQP-342 hatten wir einen Fall, wo ein Atlas Mobile API Change nicht sauber in RFC-1770 nachgezogen wurde. Das führte dazu, dass die Testfälle in unserem Orchestrator noch auf alte Payload-Formate zielten – zwei Tage Verzögerung."}
{"ts": "165:38", "speaker": "I", "text": "Wie haben Sie das gefixt?"}
{"ts": "165:40", "speaker": "E", "text": "Wir haben einen Runbook-Abschnitt RB-QA-051/sect.4 ergänzt, der einen Pre-Merge Check gegen die Atlas Mobile API Schemas vorschreibt. Seitdem müssen alle QA-relevanten RFCs ein 'schema_diff' Feld enthalten."}
{"ts": "165:51", "speaker": "I", "text": "Klingt nach einer klaren Lesson Learned."}
{"ts": "165:53", "speaker": "E", "text": "Ja, und es war auch eine Entscheidung mit Trade-off: Wir haben dafür die Review-Zeit um ca. 4 Stunden pro RFC verlängert. Aber die Risikobewertung aus RSK-REP-19 hat gezeigt, dass wir damit potenziell 12–16 Stunden Firefighting im Release sparen."}
{"ts": "166:04", "speaker": "I", "text": "Das ist also ein klassisches Beispiel, wo mehr upfront QA Zeit langfristig den Release-Termin absichert."}
{"ts": "166:08", "speaker": "E", "text": "Exactly, und wir dokumentieren solche Entscheidungen immer in einem 'Decision Log' Confluence Space, verlinkt mit den Jira-Tickets. Das hilft uns auch, bei künftigen Builds schnell zu sehen, welche Trade-offs schon gemacht wurden."}
{"ts": "166:17", "speaker": "I", "text": "Gibt es da eine formale Abnahme durch das Release Board?"}
{"ts": "166:21", "speaker": "E", "text": "Ja, der Release Board Checkpoint 'RB-QA-GO' muss jedes Mal ein grünes Signal haben, wenn wir einen solchen Trade-off dokumentieren. Ohne das Go wird der Merge in den Release Branch blockiert – das steht so auch in SLA-QA-07."}
{"ts": "170:42", "speaker": "I", "text": "Bevor wir zum Abschluss kommen, würde mich interessieren, how you actually link the Hera QA Platform’s flaky test analytics to the Nimbus Observability KPIs, gerade im Kontext der Build-Phase."}
{"ts": "171:05", "speaker": "E", "text": "Ja, also wir haben eine direkte Pipeline, die die Flaky-Test-Daten in das P-NIM-Dashboard schiebt. This is configured via RB-QA-051, und dort ist definiert, welche Event-Typen als 'instabil' gelten. Die KPI 'Test Reliability Index' in Nimbus wird dann automatisch aktualisiert."}
{"ts": "171:32", "speaker": "I", "text": "Und diese Event-Typen – sind die manuell kuratiert oder rein datengetrieben aus der Hera QA Engine?"}
{"ts": "171:47", "speaker": "E", "text": "Mixed approach: Wir haben heuristische Filter, zum Beispiel bestimmte Timeout-Patterns, aber auch eine ML-Komponente, die aus vergangenen Runs lernt. Das Mapping zu Nimbus-KPIs ist dann in einem internen Confluence-Doc dokumentiert, plus als Appendix im RFC-1770."}
{"ts": "172:15", "speaker": "I", "text": "Makes sense. In Bezug auf Atlas Mobile – beeinflusst diese Integration, äh, konkret Ihre Testfall-Priorisierung?"}
{"ts": "172:33", "speaker": "E", "text": "Definitiv. Atlas Mobile liefert uns mobile client footprints, und wenn wir sehen, dass eine API-Änderung primär den mobilen Traffic betrifft, dann priorisieren wir die zugehörigen Tests höher. This is embedded in our risk scoring model gemäß POL-QA-014."}
{"ts": "173:00", "speaker": "I", "text": "Und dieses Risk Scoring – fließt das auch in Ihre SLA-Planung ein?"}
{"ts": "173:17", "speaker": "E", "text": "Ja, wir haben ein SLA von maximal 48 Stunden zur Behebung von High-Risk-Defects in der Build-Phase. The risk score determines defect priority in JIRA, und wir taggen solche Tickets mit 'SLA-48h' für automatische Alerts."}
{"ts": "173:44", "speaker": "I", "text": "Gab es denn mal einen Fall, wo fehlende Traceability zwischen RFC-1770 und einem Testfall zu einer SLA-Verfehlung geführt hat?"}
{"ts": "174:02", "speaker": "E", "text": "Leider ja, Ticket QA-3421. There we had a missing link in XRay, sodass der Defect nicht eindeutig der Anforderung aus RFC-1770 zugeordnet wurde. Das führte zu einem 14-stündigen Delay, documented in Post-Mortem PM-QA-019."}
{"ts": "174:33", "speaker": "I", "text": "Wie gehen Sie heute vor, um solche Gaps zu vermeiden?"}
{"ts": "174:49", "speaker": "E", "text": "Wir haben einen nightly Traceability-Check implementiert, der alle Testfälle gegen die RFC-Matrix validiert. Wenn ein Link fehlt, schlägt der Build in der QA-Staging-Pipeline fehl. This is strict but it saved us twice last month."}
{"ts": "175:17", "speaker": "I", "text": "Klingt restriktiv, aber effektiv. Any trade-offs you had to accept with this strictness?"}
{"ts": "175:34", "speaker": "E", "text": "Ja, wir mussten akzeptieren, dass kleine UI-Fixes manchmal blockiert werden, weil die Traceability-Checks sie als unvollständig markieren. Wir dokumentieren solche Exceptions in DEV-EXC-Log und holen die Links nach dem Merge nach."}
{"ts": "176:02", "speaker": "I", "text": "Und wie bewerten Sie das Risiko solcher Ausnahmen? Geht das in Ihre Lessons Learned ein?"}
{"ts": "176:22", "speaker": "E", "text": "Absolutely. Jede Ausnahme wird im Quarterly Risk Review bewertet, mit Referenz auf die Runbooks und betroffene SLAs. Das fließt dann wieder in die Anpassung von POL-QA-014 ein, so dass wir langfristig weniger Ausnahmen brauchen."}
{"ts": "179:42", "speaker": "I", "text": "Lassen Sie uns noch etwas tiefer in die Integration mit P-NIM eintauchen — how exactly do you pull observability data into your QA dashboards?"}
{"ts": "179:50", "speaker": "E", "text": "Wir nutzen eine interne Bridge namens 'NIM-QA Adapter'. Der zieht Metriken über die Test Execution IDs und mapped sie gegen unsere Testfall-IDs aus RB-QA-051. So kann ich in Echtzeit sehen, ob ein Test, der gerade im Hera Cluster läuft, auch im P-NIM Error Budget auftaucht."}
{"ts": "180:06", "speaker": "I", "text": "So you actually link Error Budgets directly back to test case performance?"}
{"ts": "180:10", "speaker": "E", "text": "Ja, genau. Das kam aus einer Lessons Learned Session nach Release 3.1. Damals hatten wir einen kritischen Drop in der Availability, den wir nicht früh genug mit QA-Daten korrelieren konnten."}
{"ts": "180:24", "speaker": "I", "text": "Und wie wirkt sich die Atlas Mobile Integration auf dieses Setup aus?"}
{"ts": "180:29", "speaker": "E", "text": "Atlas Mobile hat spezielle UI-Tests, die extrem flaky sind wegen variabler Netzwerkbedingungen. Wir haben deshalb im risk_based_testing Scoring aus POL-QA-014 einen Faktor für 'mobile volatility' ergänzt."}
{"ts": "180:43", "speaker": "I", "text": "Interesting — und das fließt automatisch in die Priorisierung?"}
{"ts": "180:46", "speaker": "E", "text": "Yes. Der Adapter bewertet jede Test Suite täglich neu und verschiebt bei Bedarf Ressourcen von low risk API Checks zu high risk mobile workflows."}
{"ts": "180:58", "speaker": "I", "text": "Gab es Fälle, wo diese Umpriorisierung Probleme mit dem Release-Termin verursacht hat?"}
{"ts": "181:03", "speaker": "E", "text": "Einmal, bei Ticket QA-4278, mussten wir zwei Tage länger testen, weil die mobile Checkout-Tests plötzlich 35 % Fail Rate hatten. Wir haben das in RFC-1802 als akzeptierten Trade-off dokumentiert."}
{"ts": "181:18", "speaker": "I", "text": "How do you document the rationale so stakeholders accept the delay?"}
{"ts": "181:22", "speaker": "E", "text": "Wir nutzen ein Decision Log im Confluence Space 'Hera-QA-Risks'. Da gibt es eine Vorlage mit Feldern für Impact, Probability, Links zu relevanten Runbooks wie RB-QA-051 und zu Tickets. Das erhöht Transparenz."}
{"ts": "181:38", "speaker": "I", "text": "Und welche impliziten Regeln beeinflussen diese Entscheidungen?"}
{"ts": "181:42", "speaker": "E", "text": "Intern gilt: Sicherheit und Compliance Bugs haben always Vorrang, egal was der Terminplan sagt. Das steht so nicht in jedem SLA, aber es ist ein ungeschriebenes Gesetz im QA-Team."}
{"ts": "181:55", "speaker": "I", "text": "Das heißt, wenn z. B. ein Security Test in Atlas Mobile fehlschlägt…"}
{"ts": "181:58", "speaker": "E", "text": "…dann wird sofort ein Hotfix-Branch erstellt und wir blocken das Release, bis der Fix durch unsere Hera QA Pipeline läuft und P-NIM keine neuen Incidents loggt. Das ist zwar teuer in der Time-to-Market, aber reduziert langfristig Risiko."}
{"ts": "187:42", "speaker": "I", "text": "Könnten Sie vielleicht noch konkreter beschreiben, wie Sie diese Lessons Learned in aktuelle Runbooks wie RB-QA-051 einfließen lassen?"}
{"ts": "188:10", "speaker": "E", "text": "Ja, klar. In RB-QA-051 haben wir jetzt z.B. eine Section 'Risk Re-Evaluation' eingebaut, die auf den Findings aus Ticket QA-9824 basiert. That means we require a mid-sprint checkpoint where risk scores are recalculated, especially if P-NIM metrics show anomalies."}
{"ts": "188:42", "speaker": "I", "text": "Verstehe. Das heißt, Sie binden Observability-Daten direkt in die QA-Prozesse ein?"}
{"ts": "189:01", "speaker": "E", "text": "Genau, wir ziehen die Error Rate und Latency Trends aus Nimbus Observability, mappen die auf unsere Testmodule, und justieren dann die Priorisierung. This cross-linking is actually aligned with clause 4.3 of RFC-1770."}
{"ts": "189:28", "speaker": "I", "text": "Wie wird diese Priorisierung dokumentiert? Gibt es dafür ein spezielles Format?"}
{"ts": "189:46", "speaker": "E", "text": "Wir nutzen ein internes YAML-basiertes Template, nennt sich QA-PRIOR-01. It includes fields for risk score, SLA impact, and trace references back to the original requirement ID in RFC-1770."}
{"ts": "190:12", "speaker": "I", "text": "Und wie gehen Sie mit widersprüchlichen Signalen um, z.B. wenn die Risikoanalyse ein anderes Bild zeigt als die SRE-Metriken?"}
{"ts": "190:33", "speaker": "E", "text": "Dann machen wir ein sogenanntes 'Dual Review' Meeting. One QA Lead and one SRE Lead review the data sets separately, dann wird gemeinsam entschieden, welches Signal schwerer wiegt. Das ist zwar nicht im offiziellen Runbook, aber eine bewährte Praxis."}
{"ts": "190:59", "speaker": "I", "text": "Klingt nach einem pragmatischen Ansatz. Gab es in letzter Zeit einen Fall, wo das den Release-Plan beeinflusst hat?"}
{"ts": "191:17", "speaker": "E", "text": "Ja, bei Build 2024.05 haben wir durch ein Dual Review eine Blocker-Story in Atlas Mobile verschoben, um erst Performance-Tests zu intensivieren. It delayed the release by two days, but prevented a high-severity defect in production."}
{"ts": "191:44", "speaker": "I", "text": "Wie haben Sie dieses Delay gegenüber dem Management gerechtfertigt?"}
{"ts": "192:01", "speaker": "E", "text": "Wir haben auf unsere SLA-QA-02 verwiesen, die eine Maximalabweichung von Release Dates um drei Tage erlaubt, wenn eine Risiko-Eskalation dokumentiert ist. We attached evidence from P-NIM dashboards and the QA-PRIOR-01 sheet."}
{"ts": "192:27", "speaker": "I", "text": "Gibt es Lessons Learned daraus, die Sie jetzt schon in die nächsten Builds übertragen?"}
{"ts": "192:44", "speaker": "E", "text": "Absolut, wir haben einen neuen Trigger definiert: Wenn die Abweichung zwischen Risiko-Score und Observability-Rank >30% beträgt, muss automatisch ein Dual Review stattfinden. This is now part of RB-QA-051 v2."}
{"ts": "193:11", "speaker": "I", "text": "Das scheint die Konsistenz in den Entscheidungen zu erhöhen. Gibt es noch offene Risiken, die Sie nicht mit solchen Mechanismen abfangen können?"}
{"ts": "193:32", "speaker": "E", "text": "Ja, inter-system dependencies, besonders zu externen APIs, sind tricky. Even with best traceability, we can't always predict contract changes. Dafür haben wir jetzt einen wöchentlichen Sync mit dem Integration Team eingeführt, um frühzeitig Warnungen zu bekommen."}
{"ts": "195:42", "speaker": "I", "text": "Könnten Sie bitte etwas genauer beschreiben, wie diese Lessons Learned, äh, konkret in den aktuellen Build-Sprints umgesetzt wurden?"}
{"ts": "195:48", "speaker": "E", "text": "Ja, also wir haben ein internes Template in Confluence, das aus RB-QA-051 abgeleitet ist, wo wir für jede Sprint-Planung einen Abschnitt 'Risikoableitung' haben. This section forces us to link every high-risk test gap to either an RFC clause like RFC-1770 §4.2 or an existing Jira ticket, so there is no orphan risk item."}
{"ts": "195:58", "speaker": "I", "text": "Und wie stellen Sie sicher, dass diese Verlinkungen nicht im Laufe der Zeit veralten?"}
{"ts": "196:03", "speaker": "E", "text": "Wir haben ein kleines Python-Skript im QA-Tools-Repo, das einmal pro Woche läuft und prüft, ob alle Referenzen in den Risikoabschnitten noch auf offene oder abgeschlossene, aber relevante Tickets zeigen. If a link is broken or points to a closed non-relevant task, we get a Slack alert in #hera-qa-alerts."}
{"ts": "196:14", "speaker": "I", "text": "Das klingt nach einer Mischung aus automatischer und manueller Kontrolle. Gibt es dafür ein Runbook?"}
{"ts": "196:19", "speaker": "E", "text": "Ja, RB-QA-051 beschreibt den manuellen Review-Prozess. Step 3 im Runbook beschreibt: 'Cross-check automated link validation results with sprint backlog' – and we typically do that on Thursdays before the risk review meeting."}
{"ts": "196:29", "speaker": "I", "text": "Sie hatten vorhin Nimbus Observability erwähnt – können Sie erläutern, wie diese Plattform in Ihre Testmetriken eingebunden ist?"}
{"ts": "196:35", "speaker": "E", "text": "Klar, über das P-NIM API exportieren wir Testlaufzeiten und Flakiness-Raten direkt in Nimbus. There, SRE uses custom dashboards to spot trends that might indicate infrastructure-related flakiness, which we then tag back in our QA Platform."}
{"ts": "196:46", "speaker": "I", "text": "Und beeinflusst das Ihre Priorisierung bei den Tests?"}
{"ts": "196:50", "speaker": "E", "text": "Absolut, wenn Nimbus zeigt, dass ein bestimmter Service in Atlas Mobile erhöhte Latenzen hat, erhöhen wir den Risiko-Score für Tests, die diesen Service berühren. Das ist im risk_based_testing Sheet direkt mit dem Observability-Feed verknüpft."}
{"ts": "196:59", "speaker": "I", "text": "Das heißt, Sie haben eine Art Closed Loop zwischen Observability und Testplanung?"}
{"ts": "197:03", "speaker": "E", "text": "Genau. We call it 'Obs-QA loop'. It's documented in RFC-1770 Annex B. It ensures that monitoring data informs QA priorities within a single sprint cycle."}
{"ts": "197:12", "speaker": "I", "text": "Gibt es Risiken, dass dadurch andere Tests vernachlässigt werden?"}
{"ts": "197:16", "speaker": "E", "text": "Ja, das ist ein klassischer Trade-off. Sometimes focusing too much on observability-driven hotspots means we under-test low-visibility components. Wir dokumentieren das explizit in den Sprint-Retros unter 'Deferred Low-Risk Coverage', und wenn es kritisch wird, geht ein RFC an den Change Advisory Board."}
{"ts": "197:28", "speaker": "I", "text": "Und wie wird entschieden, ob so ein Deferred Coverage akzeptabel ist?"}
{"ts": "197:33", "speaker": "E", "text": "Das hängt vom SLA-Level der Komponente ab. Für Level-1 Components erlaubt POL-QA-014 maximal zwei aufeinanderfolgende Sprints ohne vollständige Abdeckung, andernfalls muss ein Hotfix-Testzyklus eingeschoben werden, auch wenn das den Release-Termin schiebt."}
{"ts": "203:42", "speaker": "I", "text": "Wenn wir jetzt auf die Integration mit Nimbus Observability schauen – gab es da im Build-Status P-HER schon die ersten Cross-System Issues?"}
{"ts": "203:55", "speaker": "E", "text": "Ja, tatsächlich. Wir haben in Sprint 12 ein Problem gehabt, wo die Testmetriken nicht sauber in P-NIM synchronisiert wurden – war ein Mapping-Fehler im internen Schema. Das fiel erst auf, als unser Risk-based Dashboard leere Heatmaps zeigte."}
{"ts": "204:14", "speaker": "I", "text": "War das in einem Ticket dokumentiert?"}
{"ts": "204:18", "speaker": "E", "text": "Ja, das ist T-QA-9821. Da steht auch drin, wie wir einen Hotfix über RB-QA-051 eingespielt haben, um die Query-Felder aus der TestResult-Collection temporär zu mappen."}
{"ts": "204:36", "speaker": "I", "text": "Und hat dieser Hotfix die SLA für QA-Metrik-Updates verletzt?"}
{"ts": "204:42", "speaker": "E", "text": "Kurzzeitig ja – unser SLA ist <4h Update-Latenz für Metriken. In diesem Fall waren es fast 9 Stunden. Wir haben das im Post-Mortem vermerkt und in den Runbook-Abschnitt 5.3 ergänzt."}
{"ts": "205:03", "speaker": "I", "text": "Können Sie erklären, wie Atlas Mobile Ihre Testplanung beeinflusst – gerade in Bezug auf diese Metrik-Schnittstellen?"}
{"ts": "205:12", "speaker": "E", "text": "Sure – Atlas Mobile liefert UI-Komponenten, die wir in Hera simulieren. Wenn deren API-Version ändert, muss unser Test-Orchestrator angepasst werden, sonst fallen die Mock-Sessions im CI durch. Wir haben dafür eine Atlas→Hera Compatibility Matrix in Confluence."}
{"ts": "205:34", "speaker": "I", "text": "Wie verzahnt sich das mit der Risikoanalyse, die Sie vorhin erwähnt haben?"}
{"ts": "205:39", "speaker": "E", "text": "Das ist ein klassischer Multi-Hop: Wir nehmen die API-Change-Logs von Atlas, mappen sie auf betroffene Testfälle laut RFC-1770, bewerten das Risiko nach POL-QA-014 und setzen Prioritäten im Testplan. So verhindern wir, dass Low-Risk-Features den Critical Path blockieren."}
{"ts": "206:02", "speaker": "I", "text": "Gab es mal einen Fall, wo diese Kette nicht funktioniert hat?"}
{"ts": "206:07", "speaker": "E", "text": "Ja, im Herbst-Release – API v5.2 kam ohne Vorwarnung. Unser Mapping war outdated, Risk Score falsch niedrig. Ergebnis: Zwei High-Severity Bugs gingen ins UAT. Lessons Learned: jetzt wöchentliche Atlas API-Checks im Runbook verankert."}
{"ts": "206:28", "speaker": "I", "text": "Das klingt nach einem klaren Trade-off zwischen Aufwand und Sicherheit."}
{"ts": "206:33", "speaker": "E", "text": "Genau, wir mussten entscheiden: mehr Automatisierung für den Check (höhere Build-Kosten) oder manuelle Reviews (langsamer, günstiger). Wir haben in RFC-1822 den Automationsweg beschlossen, weil das Risiko im Release-Fenster zu hoch war."}
{"ts": "206:54", "speaker": "I", "text": "Und wie dokumentieren Sie den Impact solcher Entscheidungen?"}
{"ts": "206:59", "speaker": "E", "text": "Wir pflegen pro Entscheidung einen Decision Record im QA-Wiki, verlinken auf die zugrundeliegenden Tickets wie T-QA-9821 und referenzieren relevante Runbook-Abschnitte. Damit ist für Audits klar, warum welcher Trade-off gewählt wurde."}
{"ts": "211:42", "speaker": "I", "text": "Könnten Sie bitte noch etwas genauer beschreiben, wie Sie konkret mit den Schnittstellen zu P-NIM umgehen, wenn Sie Testmetriken aus Hera extrahieren?"}
{"ts": "211:56", "speaker": "E", "text": "Ja, also wir haben da eine REST-basierte API-Bridge, äh, zwischen Hera QA Platform und Nimbus Observability. Die Metriken wie pass rate, test duration und flaky ratio werden im 15-Minuten-Intervall gepusht. Wir nutzen dazu die interne Spec aus RB-QA-051, die definiert, welche Felder mandatory sind."}
{"ts": "212:20", "speaker": "I", "text": "And how do you ensure that those pushed metrics comply with the agreed SLOs?"}
{"ts": "212:32", "speaker": "E", "text": "We actually have a validation hook in our pipeline. Wenn die Payload nicht den Thresholds aus SLA-QA-202 entspricht, wird der Push blockiert und ein Ticket in JIRA-HERA erstellt—typischerweise mit dem Tag 'METRIC-BLOCK'."}
{"ts": "212:55", "speaker": "I", "text": "Wie beeinflusst die Integration mit Atlas Mobile Ihre Testplanung konkret?"}
{"ts": "213:06", "speaker": "E", "text": "Atlas Mobile hat eine andere Release-Cadence, alle zwei Wochen. Das zwingt uns, in Hera parallel zwei Test-Suites zu warten: eine regressionslastige für den monatlichen Hera-Release und eine leichtgewichtige Smoke-Suite synced mit Atlas. Das ist in POL-QA-014 als Sonderfall dokumentiert."}
{"ts": "213:32", "speaker": "I", "text": "Do these dual suites create extra maintenance overhead?"}
{"ts": "213:42", "speaker": "E", "text": "Absolutely, ja. Wir haben dafür ein kleines Script 'sync_suites.py' gebaut, das shared steps in beiden Suites aktualisiert. Sonst wäre die Divergenz zu hoch, und wir würden bei Traceability in RFC-1770 brechen."}
{"ts": "214:05", "speaker": "I", "text": "Welche Abhängigkeiten bergen Ihrer Ansicht nach das größte Risiko für die QA-Ziele?"}
{"ts": "214:17", "speaker": "E", "text": "Das größte Risiko ist die Abhängigkeit von den Monitoring-Daten aus P-NIM. Fällt der Metrik-Push aus, können wir keine risk-based Repriorisierung durchführen. In Ticket HERA-2198 mussten wir z. B. manuell Daten nachpflegen, was 2 Tage Verzögerung brachte."}
{"ts": "214:44", "speaker": "I", "text": "How do you document such incidents for future mitigation?"}
{"ts": "214:54", "speaker": "E", "text": "Wir schreiben ein Post-Mortem im Confluence, mapping die Root Cause zu einem Abschnitt in RB-QA-051. Außerdem fügen wir einen Preventive Action Task ins Backlog, mit Verweis auf das Incident-Ticket."}
{"ts": "215:15", "speaker": "I", "text": "Gab es zuletzt einen Trade-off zwischen Testabdeckung und Termintreue, der besonders schwierig war?"}
{"ts": "215:27", "speaker": "E", "text": "Ja, beim Build 1.8. Wir mussten 12% der geplanten exploratory tests streichen, um den Atlas-integrierten Release-Day zu halten. Das wurde in RFC-1795 dokumentiert, mit Risiko-Level 'Medium', basierend auf Lessons Learned aus Build 1.6."}
{"ts": "215:52", "speaker": "I", "text": "And what safeguards did you put in place after that?"}
{"ts": "216:02", "speaker": "E", "text": "Wir haben einen pre-release risk gate eingeführt, das die must-have Tests aus der Risikoanalyse gegen den Release-Termin abgleicht. Falls wir unter 85% must-have coverage fallen, wird der Release-Kandidat automatisch geblockt, es sei denn, ein Director signiert ein Override."}
{"ts": "219:42", "speaker": "I", "text": "Bevor wir tiefer ins Risiko-Management einsteigen, könnten Sie bitte noch einmal kurz skizzieren, wie die Schnittstellen zu P-NIM im Testmetriken-Kontext aussehen?"}
{"ts": "220:05", "speaker": "E", "text": "Klar, ähm, also wir haben für P-NIM, also Nimbus Observability, ein dedicated API-Feed, der unsere Build-Phase-Metriken wie flaky test rate, mean test latency und coverage drift exportiert. Die Integration ist in RB-QA-051 dokumentiert, und das Polling läuft alle 15 Minuten."}
{"ts": "220:34", "speaker": "I", "text": "Das heißt, Sie ziehen dann die Daten auch in Ihre Risikoanalysen hinein?"}
{"ts": "220:48", "speaker": "E", "text": "Genau, we aggregate them into our risk-based dashboard. Dort haben wir eine Mapping-Tabelle aus POL-QA-014, die definiert, wie ein bestimmter Metrikenwert auf ein Risikoniveau gemappt wird, und das fließt direkt in die Testfall-Priorisierung."}
{"ts": "221:15", "speaker": "I", "text": "Gibt es da eine Verbindung zu Atlas Mobile, oder sind das separate Streams?"}
{"ts": "221:29", "speaker": "E", "text": "Teilweise verbunden. Atlas Mobile liefert z.B. device fragmentation stats, die wir über einen Connector an dieselbe Pipeline hängen. So können wir z.B. sehen: hoher flaky rate auf bestimmten Devices + hoher business impact = Test hoch priorisieren. Das ist sozusagen der multi-hop link zwischen zwei Subsystemen."}
{"ts": "221:58", "speaker": "I", "text": "Verstehe, also Sie verknüpfen Observability-Metriken mit Mobile-spezifischen KPIs, um Risiken zu gewichten."}
{"ts": "222:10", "speaker": "E", "text": "Ja, und das war ein großer Fortschritt seit wir RFC-1770 umgesetzt haben. Vorher waren diese Streams siloed, und wir haben delayed responses bei kritischen Bugs gehabt."}
{"ts": "222:31", "speaker": "I", "text": "Hatten Sie konkrete Fälle, in denen fehlende Integration zu spürbaren Problemen führte?"}
{"ts": "222:46", "speaker": "E", "text": "Ja, Ticket QA-BLD-338 zeigt das: ein Memory-Leak bug auf low-end Android devices wurde erst nach zwei Sprints entdeckt, weil Atlas-Daten nicht ins Risiko-Dashboard flossen. Das hätte mit der heutigen Pipeline maximal 3 Stunden gedauert."}
{"ts": "223:12", "speaker": "I", "text": "Im aktuellen Build, gab es eine ähnliche Entscheidung, bei der Sie Abdeckung zugunsten von Terminen anpassen mussten?"}
{"ts": "223:27", "speaker": "E", "text": "Ja, beim Sprint 42 Cut-off. Wir hatten eine neue Test-Suite für den Payment-Flow geplant. Based on risk scoring war sie medium, aber das Release-Termin war fix. Wir haben die Suite in den Smoke-Test-Plan nach Release verschoben und das in RFC-1822 als Trade-off geloggt."}
{"ts": "223:58", "speaker": "I", "text": "Wie dokumentieren Sie solche Trade-offs genau, also neben RFCs?"}
{"ts": "224:12", "speaker": "E", "text": "Neben den RFCs pflegen wir im Confluence ein Lessons Learned Board, linked to Jira Issues. Jede Entscheidung kriegt einen Impact-Score und einen Risk-Matrix-Eintrag, nach Template aus Runbook RB-QA-051."}
{"ts": "224:37", "speaker": "I", "text": "Gab es Feedback aus dem Management zu dieser dokumentierten Vorgehensweise?"}
{"ts": "224:50", "speaker": "E", "text": "Ja, they appreciated the transparency. Management kann so nachvollziehen, warum ein Feature getestet oder deferred wurde. Das schafft Vertrauen, auch wenn wir manchmal bewusst Coverage opfern, um das Release nicht zu gefährden."}
{"ts": "226:42", "speaker": "I", "text": "Lassen Sie uns noch einmal auf die Traceability eingehen – können Sie erläutern, wie genau Sie RFC-1770 im Tooling verankern?"}
{"ts": "227:05", "speaker": "E", "text": "Ja, also im Jira-ähnlichen System haben wir ein Custom Field 'RFC Linkage', das automatisch gefüllt wird, wenn wir aus dem Confluence-RFC-Archiv einen Test Case anlegen. This ensures that each test case is directly tied back to the originating system requirement."}
{"ts": "227:31", "speaker": "I", "text": "Und wie verbinden Sie das mit Bug Reports, gerade in Bezug auf RB-QA-051?"}
{"ts": "227:50", "speaker": "E", "text": "RB-QA-051 beschreibt den Runbook-Prozess für Defect Handling. Wir haben da eine Regel: jeder Defect muss ein Feld 'Originating Requirement' haben, das auf die RFC- oder User Story verweist. Without that link, kein Bug kommt ins Triage-Board."}
{"ts": "228:18", "speaker": "I", "text": "Das klingt strikt. Gab es Fälle, wo diese Traceability gefehlt hat und es zu Verzögerungen kam?"}
{"ts": "228:33", "speaker": "E", "text": "Ja, einmal im Sprint 14, Ticket QA-4821, war der Link auf RFC-1770 nicht gesetzt. We lost almost two days re-tracing the business logic, weil niemand den Ursprung kannte."}
{"ts": "228:58", "speaker": "I", "text": "Verstehe. Kommen wir zu P-NIM – gibt es Schnittstellen für Testmetriken?"}
{"ts": "229:15", "speaker": "E", "text": "Ja, wir pushen unsere flaky test stats über eine gRPC-API direkt in Nimbus Observability. Dort laufen sie in ein Dashboard 'Hera-QA-Flakiness'. This way, SRE can correlate test instability with infra changes."}
{"ts": "229:45", "speaker": "I", "text": "Interessant. Hat die Integration mit Atlas Mobile Einfluss auf Ihre Testplanung?"}
{"ts": "230:02", "speaker": "E", "text": "Absolut, wir müssen mobile client changes mit einplanen. Wenn ein neues Atlas-SDK kommt, priorisieren wir smoke tests für mobile flows. Otherwise, regressions slip into prod."}
{"ts": "230:28", "speaker": "I", "text": "Welche Abhängigkeiten bergen das größte Risiko für Ihre QA-Ziele?"}
{"ts": "230:42", "speaker": "E", "text": "Größtes Risiko ist die Synchronisation mit P-NIM Deployments. Wenn Observability-Agent-Updates kommen, können unsere Testhooks brechen. We mitigate via staging env sync tests."}
{"ts": "231:10", "speaker": "I", "text": "Können Sie ein Beispiel geben für eine schwierige Entscheidung zwischen Testabdeckung und Release-Termin, zusätzlich zu dem eben erwähnten?"}
{"ts": "231:28", "speaker": "E", "text": "Klar, in Build-Phase Meilenstein 3 haben wir 12% der geplanten API-Tests gestrichen, um den Release Candidate rechtzeitig zu liefern. We documented it in Decision Log DEC-QA-019, referencing risk scorecards."}
{"ts": "231:56", "speaker": "I", "text": "Wie dokumentieren Sie solche Trade-offs formell? Nutzen Sie RFCs oder Tickets?"}
{"ts": "232:15", "speaker": "E", "text": "Beides: kleinere Trade-offs landen in Jira-Decision Tickets, größere wie die DEC-QA-019 gehen als formale RFC-Addendum ins Archiv. Das ist wichtig für unsere Lessons Learned Sessions, um beim nächsten Sprint fundierter zu entscheiden."}
{"ts": "235:02", "speaker": "I", "text": "Lassen Sie uns nochmal auf die Integration mit P-NIM zurückkommen – wie genau fließen die Testmetriken dort ein, und welche Anpassungen waren nötig?"}
{"ts": "235:15", "speaker": "E", "text": "Also, wir pushen nightly die Aggregationsdaten aus Hera QA in den P-NIM Metrics Stream. That required a custom exporter, weil unsere Metriken wie 'flaky test ratio' und 'mean time to detect' nicht im Standard-Schema lagen."}
{"ts": "235:36", "speaker": "I", "text": "Gab es da Überschneidungen mit dem Atlas Mobile Projekt?"}
{"ts": "235:44", "speaker": "E", "text": "Ja, die gab's. Atlas Mobile liefert Telemetrie, die wir in der Risikoanalyse nutzen. Zum Beispiel, wenn ein Feature in Atlas nur 2% der User betrifft, adjustieren wir die Testpriorität. Das ist genau dieser multi-hop Link zwischen Observability, Usage Stats und unserer risk_based_testing Methodik."}
{"ts": "236:07", "speaker": "I", "text": "Spannend, und diese Priorisierung – ist die fest in POL-QA-014 verankert?"}
{"ts": "236:15", "speaker": "E", "text": "Teilweise. POL-QA-014 gibt den Rahmen, aber wir haben in RB-QA-051 einen Zusatz-Workflow beschrieben, der externe Metriken wie Atlas-Telemetrie einbindet."}
{"ts": "236:33", "speaker": "I", "text": "Wie stellen Sie sicher, dass diese Zusatz-Workflows nicht an der QA-Governance vorbeigehen?"}
{"ts": "236:40", "speaker": "E", "text": "Wir loggen jede Ausnahme in Tickets vom Typ QA-CUST, und verlinken sie zu den relevanten RFCs – in diesem Fall RFC-1770 – damit die Audit-Trail vollständig ist."}
{"ts": "236:58", "speaker": "I", "text": "Gab es konkrete Incidents, wo fehlende Verknüpfungen Probleme machten?"}
{"ts": "237:06", "speaker": "E", "text": "Ja, im Build 2024.05 hatten wir einen Atlas-Crash-Test nicht getrackt, weil der Bezug zur Anforderung in RFC-1770 fehlte. That caused a three-day slip, documented in ticket INC-QA-221."}
{"ts": "237:28", "speaker": "I", "text": "Und wie haben Sie darauf reagiert?"}
{"ts": "237:34", "speaker": "E", "text": "Wir haben eine Runbook-Erweiterung erstellt, RB-QA-051v2, mit einem verpflichtenden Traceability-Check in der CI-Pipeline. Seitdem kein ähnlicher Vorfall."}
{"ts": "237:53", "speaker": "I", "text": "Können Sie ein Beispiel für einen jüngsten Trade-off geben, der diese Integrationen berücksichtigt?"}
{"ts": "238:00", "speaker": "E", "text": "Sicher, beim letzten Sprint mussten wir 15% der geplanten Cross-App-Tests mit Atlas streichen, um den Release-Termin zu halten. Das Risiko haben wir mit P-NIM-Daten untermauert: nur 0,8% der Core-User wären betroffen. Dokumentiert in DEC-QA-034."}
{"ts": "238:22", "speaker": "I", "text": "Wie fließt so eine Entscheidung in Ihre Lessons Learned ein?"}
{"ts": "238:30", "speaker": "E", "text": "Wir führen sie im LL-Hera-Log zusammen mit den genutzten Metriken, den referenzierten Tickets und der Risikoabschätzung. That way, future sprints can reuse the evidence and rationale without starting from scratch."}
{"ts": "243:22", "speaker": "I", "text": "Lassen Sie uns jetzt konkret auf die Lessons Learned eingehen — was waren die größten Learnings aus den letzten zwei Sprints im Hera QA Platform Build?"}
{"ts": "243:35", "speaker": "E", "text": "Eines der größten Learnings war, dass wir bei der Integrationstestsuite für das Atlas-Mobile-Modul zu viel auf synthetische Testdaten gesetzt haben. In real-life conditions, especially with the mobile network jitter, some flaky tests didn't manifest until late in staging."}
{"ts": "243:56", "speaker": "I", "text": "Und wie haben Sie darauf reagiert? Gab es eine Anpassung Ihrer Runbooks oder Testdatenstrategie?"}
{"ts": "244:07", "speaker": "E", "text": "Ja, wir haben RB-QA-051 ergänzt. Der neue Abschnitt 4.2 verpflichtet uns, in jeder zweiten CI-Pipeline einen Testlauf mit Produktions-Snapshotdaten aus dem Nimbus Observability Data Lake zu fahren. That way we catch environment-specific issues earlier."}
{"ts": "244:28", "speaker": "I", "text": "Interessant. Wie dokumentieren Sie das im Kontext der SLA- und SLO-Vorgaben?"}
{"ts": "244:41", "speaker": "E", "text": "Wir haben ein internes QA-SLO-Dokument, das besagt, dass max. 3% der Testfälle in der Pre-Release-Phase flaky sein dürfen. Any deviation triggers a QA Incident ticket — Format TKT-QA-xxx — mit Root Cause Analysis innerhalb von 48 Stunden."}
{"ts": "245:02", "speaker": "I", "text": "Gab es denn kürzlich so ein Incident Ticket?"}
{"ts": "245:11", "speaker": "E", "text": "Ja, TKT-QA-219 vom letzten Freitag. Das betraf die Payment-API-Tests. The root cause was a misconfigured mock service, which slipped past our risk-based prioritization because it was tagged as low impact — wrongly, as it turned out."}
