{"ts": "00:00", "speaker": "I", "text": "Können Sie bitte zu Beginn die Hauptziele des Titan DR Projekts in Ihren eigenen Worten umreißen?"}
{"ts": "03:15", "speaker": "E", "text": "Ja, gern. Titan DR ist, äh, im Kern unser strategisches Disaster-Recovery-Programm. Ziel ist es, bei einem regionalen Ausfall innerhalb von 15 Minuten wieder produktiv zu sein, also RTO 15 Min., und Datenverluste auf maximal 30 Sekunden zu begrenzen – das ist unser RPO. Das Ganze unter Einhaltung der SLA-ORI-02 für 99,95 % Uptime über alle Regionen."}
{"ts": "06:45", "speaker": "I", "text": "Welche SLA-Parameter spielen hier noch eine Rolle, und wie setzen Sie diese praktisch um?"}
{"ts": "10:20", "speaker": "E", "text": "Neben SLA-ORI-02 ist SLA-HEL-01 wichtig, der definiert, dass kritische Healthchecks in unter 60 Sekunden reagieren müssen. Um das einzuhalten, nutzen wir gestaffelte Health-Endpoints mit proaktiven Liveness Tests, und wir haben im Runbook RB-DR-001 klare Eskalationsketten hinterlegt."}
{"ts": "14:10", "speaker": "I", "text": "Wie stellen Sie im Drill sicher, dass RTO und RPO tatsächlich eingehalten werden?"}
{"ts": "18:05", "speaker": "E", "text": "Das machen wir mit simulierten Ausfällen und präzisem Timestamp-Logging. Im Drill-Log DR-DRILL-LOG-24-03 sehen Sie z.B., dass vom Erstalarm bis zum vollständigen Traffic-Switch nach AP-South nur 12:47 Minuten vergangen sind, und die Datenreplikation lag maximal 24 Sekunden zurück."}
{"ts": "22:40", "speaker": "I", "text": "Kommen wir zur Architektur: Welche Regionen sind aktiv und wie ist der Failover-Pfad definiert?"}
{"ts": "27:00", "speaker": "E", "text": "Primär läuft alles in EU-Central, sekundär in AP-South. Der Failover-Pfad ist so definiert, dass Poseidon Networking binnen 90 Sekunden die Routen anpasst. Nimbus Observability triggert die Automation, wenn drei kritische KPIs über Schwellwert liegen."}
{"ts": "31:20", "speaker": "I", "text": "Wie integriert sich RB-DR-001 konkret in den operativen Ablauf bei einem Ausfall?"}
{"ts": "36:15", "speaker": "E", "text": "RB-DR-001 beschreibt Schritt für Schritt: Detection durch Nimbus, Routing-Update durch Poseidon, Validierung per Synthetic Checks, und finale Freigabe durch den Incident Commander. Das Runbook wurde zuletzt nach RFC-DR-2024-12 angepasst."}
{"ts": "41:00", "speaker": "I", "text": "Gab es in TEST-DR-2025-Q1 ein spezielles Beispiel, wo Sie ein Problem entdeckt haben?"}
{"ts": "45:35", "speaker": "E", "text": "Ja, beim GameDay im Januar haben wir im Ticket DR-FIND-88 dokumentiert, dass die DNS-Propagation nach AP-South 4 Minuten länger dauerte als geplant, weil ein Poseidon-Edge-Node eine veraltete Config hatte."}
{"ts": "50:10", "speaker": "I", "text": "Wie priorisieren Sie solche Findings?"}
{"ts": "54:55", "speaker": "E", "text": "Wir nutzen eine Matrix aus SLA-Impact und Eintrittswahrscheinlichkeit. DR-FIND-88 war High Impact/Medium Likelihood, daher Priorität P1 und sofortige RFC-Erstellung."}
{"ts": "59:20", "speaker": "I", "text": "Gab es Situationen, in denen Kostenreduktion mit erhöhtem Risiko einherging?"}
{"ts": "90:00", "speaker": "E", "text": "Ja, wir haben 2024 kurz überlegt, die Warm-Standby-Kapazität in AP-South um 20 % zu reduzieren, um monatlich 15 T€ zu sparen. Vesta FinOps hat uns aber auf Basis von DR-RISK-42 gezeigt, dass dadurch das RTO-Risiko auf 19 Minuten steigen könnte, was SLA-HEL-01 verletzen würde. Daher haben wir die Reduktion verworfen."}
{"ts": "90:00", "speaker": "I", "text": "Wir hatten ja vorhin über die Abhängigkeit zu Nimbus Observability gesprochen. Können Sie mir mal erklären, wie genau das Monitoring im Drill-Fall den Failover-Prozess triggert?"}
{"ts": "90:06", "speaker": "E", "text": "Ja, klar. Nimbus Observability hat bei uns sogenannte Health-Streams, die kontinuierlich die Heartbeats aus allen Regionen prüfen. Wenn der Stream für EU-Central mehr als 20 Sekunden ohne Response ist, wird über das interne Event-Bus-System ein Signal an Poseidon Networking geschickt, das dann die BGP-Routen umschwenkt. Dieser Trigger ist im Runbook RB-DR-001 als Schritt 4 dokumentiert."}
{"ts": "90:17", "speaker": "I", "text": "Verstehe, und diese 20 Sekunden sind aus welchem Grund gewählt?"}
{"ts": "90:21", "speaker": "E", "text": "Das ist ein Trade-off zwischen false positives und der Einhaltung des RTO. Kürzere Schwellen würden bei kurzzeitigen Netzwerkglitches zu unnötigen Umschaltungen führen, längere würden das RTO von 15 Minuten aus SLA-HEL-01 gefährden. Wir haben das in RFC-DR-2024-19 so begründet und in zwei GameDays validiert."}
{"ts": "90:33", "speaker": "I", "text": "Gab es in diesen GameDays, neben dem DNS-Latenzproblem, weitere Findings, die Sie als kritisch einstufen würden?"}
{"ts": "90:38", "speaker": "E", "text": "Ja, in TEST-DR-2025-Q1 hatten wir auch eine unerwartete Verzögerung beim Sync des Object Stores. Das war ein Multi-hop Problem: Poseidon Networking hatte zwar sauber umgeschaltet, aber in AP-South war ein Storage-Cluster auf einer älteren Firmware, was den Sync um ca. 90 Sekunden verzögert hat. Das haben wir dann im Ticket DR-STOR-45 festgehalten und als Patch in der Firmware-Upgrade-Policy verankert."}
{"ts": "90:50", "speaker": "I", "text": "Wie fließen solche Lessons Learned eigentlich in die operative Vorbereitung für den nächsten Drill ein?"}
{"ts": "90:54", "speaker": "E", "text": "Wir haben dafür den Prozess LL-DR-IMP. Der sieht vor, dass jedes Critical Finding innerhalb von zwei Wochen in ein Runbook oder eine SOP mündet. Im Beispiel des Storage-Syncs haben wir RB-DR-003 angelegt, wo jetzt explizit vor jedem Drill die Firmwarestände abgefragt werden."}
{"ts": "91:05", "speaker": "I", "text": "Kommen wir noch kurz zu den Kosten — wie wirkt sich denn die Firmware-Upgrade-Policy auf das Budget aus?"}
{"ts": "91:09", "speaker": "E", "text": "Das ist überschaubar, aber messbar: Wir haben ca. 4% höhere OPEX im Storage-Bereich, weil wir teilweise vorzeitig Hardware aus dem Lifecycle nehmen. Vesta FinOps hat das in Report FIN-DR-2025-M3 mit den Kosten durch potenzielle SLA-Verletzungen verglichen, und das Risiko war teurer als das frühere Refresh."}
{"ts": "91:21", "speaker": "I", "text": "Gab es auch Diskussionen darüber, dieses Risiko einfach zu akzeptieren, um Kosten zu sparen?"}
{"ts": "91:25", "speaker": "E", "text": "Ja, im Architekturboard im Februar. Wir haben Szenario-Analysen aus DR-RISK-12 präsentiert: Worst Case wären Vertragsstrafen von ca. 180k €, während die Mehrkosten bei 45k € pro Jahr liegen. Das Board hat einstimmig für die sichere Variante gestimmt."}
{"ts": "91:36", "speaker": "I", "text": "Alles klar. Welche Risiken sehen Sie noch, die trotz aller Maßnahmen bestehen bleiben?"}
{"ts": "91:40", "speaker": "E", "text": "Residual Risk ist vor allem die Abhängigkeit von externen DNS-Providern. Selbst mit Multi-Provider-Setup könnten simultane Incidents zu Verzögerungen führen. Das ist in DR-RISK-15 dokumentiert, und wir haben dafür nur ein mitigierendes Monitoring, keine vollwertige technische Lösung."}
{"ts": "91:51", "speaker": "I", "text": "Haben Sie für DR-RISK-15 schon eine Entscheidungsvorlage erarbeitet?"}
{"ts": "91:55", "speaker": "E", "text": "Ja, Decision Memo DM-DR-07. Darin schlagen wir vor, 2026 ein internes Anycast-DNS als Fallback zu pilotieren. Evidenz aus TEST-DR-2025-Q2 soll zeigen, ob der Aufwand gerechtfertigt ist."}
{"ts": "96:00", "speaker": "I", "text": "Wir hatten vorhin schon das DNS-Latenzproblem aus dem letzten Drill erwähnt. Mich würde jetzt interessieren, welche finalen Entscheidungen Sie in Bezug auf die DNS-Strategie getroffen haben."}
{"ts": "96:12", "speaker": "E", "text": "Ja, wir haben nach dem Drill beschlossen, im Failover-Fall auf einen Anycast-basierten Resolver umzuschalten. Das steht jetzt auch als Änderung in RFC-DR-58, abgestimmt mit Poseidon Networking, damit wir im Ernstfall unter 200 ms bleiben. Die Entscheidung war nicht trivial, weil die Umstellung Zusatzkosten von etwa 8 % verursacht."}
{"ts": "96:38", "speaker": "I", "text": "Gab es dazu interne Diskussionen, ob diese Mehrkosten gerechtfertigt sind?"}
{"ts": "96:46", "speaker": "E", "text": "Ja, Vesta FinOps war zunächst skeptisch. Wir haben aber DR-RUN-042 als Evidenz herangezogen – das Runbook beschreibt explizit, wie DNS-Verzögerungen die RTO von 4 Stunden gefährden. Mit dieser Dokumentation konnten wir die Notwendigkeit klar machen."}
{"ts": "97:08", "speaker": "I", "text": "Welche Risiken bleiben trotz des Anycast-Ansatzes bestehen?"}
{"ts": "97:16", "speaker": "E", "text": "Es bleibt das Risiko, dass bei einem globalen Routing-Fehler mehrere Anycast-Knoten gleichzeitig ausfallen. SLAs wie SLA-ORI-02 decken das nicht explizit ab, und wir müssten dann auf unseren sekundären DNS-Pfad im RB-DR-001 zurückfallen."}
{"ts": "97:36", "speaker": "I", "text": "Und dieser sekundäre Pfad – ist der ebenfalls in allen Regionen getestet?"}
{"ts": "97:43", "speaker": "E", "text": "Ja, wir haben in TEST-DR-2025-Q2 ein Failover von EU-Central direkt nach US-West simuliert, um genau diese Kette zu prüfen. Das Ticket DR-TEST-89 dokumentiert, dass der Fallback in 3 Minuten aktiv war, was unter dem internen RPO-Ziel von 5 Minuten liegt."}
{"ts": "98:05", "speaker": "I", "text": "Okay, und wie fließen diese Lessons Learned in zukünftige Drills ein?"}
{"ts": "98:12", "speaker": "E", "text": "Wir haben ein Update für das Master-Runbook RB-DR-001 initiiert und neue Prüfschritte in den GameDay-Plan integriert. Jeder neue Drill muss jetzt explizit die DNS-Fallback-Kette testen, auch wenn das Hauptszenario anders liegt."}
{"ts": "98:33", "speaker": "I", "text": "Gab es auch organisatorische Entscheidungen, die aus diesen technischen Findings resultierten?"}
{"ts": "98:41", "speaker": "E", "text": "Ja, wir haben ein kleines DR-Response-Team formiert, das im Ernstfall dediziert für DNS und Netzwerk-Routing zuständig ist. Vorher war das verteilt, jetzt ist es gebündelt – das reduziert die Reaktionszeit, wie wir in DR-ARCH-81 festgehalten haben."}
{"ts": "99:02", "speaker": "I", "text": "Wie stellen Sie sicher, dass dieses Team auch in anderen kritischen Bereichen einspringen kann, falls nötig?"}
{"ts": "99:10", "speaker": "E", "text": "Wir haben Cross-Training-Sessions eingeführt. Dabei lernen die DNS-Spezialisten auch die Prozeduren aus Storage- und Compute-Failover, basierend auf RB-DR-014 und RB-DR-019. So vermeiden wir Silos."}
{"ts": "99:30", "speaker": "I", "text": "Abschließend: Welche offene Entscheidung halten Sie aktuell für die kritischste?"}
{"ts": "99:38", "speaker": "E", "text": "Das ist die Frage, ob wir die US-West-Region als dauerhaften Warm-Standby aktiv halten oder nur als Cold-Standby. Warm-Standby erfüllt RTO leichter, kostet aber 20 % mehr. Wir haben dazu eine Entscheidungsvorlage in DEC-DR-2025-07 und wollen beim nächsten Steering Committee final abstimmen."}
{"ts": "112:00", "speaker": "I", "text": "Könnten Sie noch einmal präzisieren, welche Detailrisiken nach den letzten Architekturänderungen bestehen bleiben?"}
{"ts": "112:08", "speaker": "E", "text": "Ja, also trotz der Anpassungen in DR-ARCH-77 bleibt das Risiko bestehen, dass bei extremen Netzwerkpartitionen zwischen EU-Central und AP-South die Failover-Ketten nicht vollständig durchlaufen. Insbesondere die Abhängigkeit vom Poseidon Networking Control Plane kann im Worst Case zu zusätzlichen 90 Sekunden Verzögerung führen."}
{"ts": "112:25", "speaker": "I", "text": "Wie dokumentieren Sie solch ein Restrisiko, damit es auch im nächsten Audit präsent ist?"}
{"ts": "112:33", "speaker": "E", "text": "Wir führen das im Risk Register RR-DR-2025 unter Eintrag RISK-NTWK-12. Dort verlinken wir sowohl den Auszug aus dem Runbook RB-DR-001, Abschnitt 4.3, als auch die Simulationsergebnisse vom Drill im März, die das Verhalten unter Paketverlust zeigen."}
{"ts": "112:50", "speaker": "I", "text": "Gab es Überlegungen, dieses Risiko technisch zu mitigieren, auch wenn es Kosten verursacht?"}
{"ts": "112:58", "speaker": "E", "text": "Wir haben mit Vesta FinOps eine Optionsanalyse gemacht. Variante A wäre ein permanenter Warm-Standby in einer dritten Region, laut Kalkulation +27% OPEX. Variante B, die wir aktuell bevorzugen, ist eine optimierte Anycast-DNS-Konfiguration, die auf Basis der Nimbus Observability Metriken dynamisch reagiert."}
{"ts": "113:17", "speaker": "I", "text": "Welche Evidenz stützt die Entscheidung für Variante B?"}
{"ts": "113:23", "speaker": "E", "text": "Ticket DR-DEC-45 enthält die vollständige Kosten-Nutzen-Analyse, plus Screenshots aus dem Testlauf vom 15. April, wo die DNS-Latenz um 38% reduziert wurde. Das ist auch in der Lessons Learned Sektion unseres letzten GameDay-Protokolls vermerkt."}
{"ts": "113:40", "speaker": "I", "text": "Wie stellen Sie sicher, dass diese Lessons Learned nicht verloren gehen?"}
{"ts": "113:47", "speaker": "E", "text": "Wir haben intern die Policy, dass alle GameDay-Findings innerhalb von fünf Arbeitstagen in ein RFC überführt werden müssen, wenn sie architekturrelevant sind. In diesem Fall wurde RFC-DR-2025-07 erstellt, der das Anycast-DNS-Design und die Implementierungsdetails beschreibt."}
{"ts": "114:05", "speaker": "I", "text": "Und welche Rolle spielt Poseidon Networking bei der Umsetzung von RFC-DR-2025-07?"}
{"ts": "114:12", "speaker": "E", "text": "Poseidon stellt die BGP-Announcements und die Health-Check-Integration bereit. Ohne diese Module könnte Anycast nicht zuverlässig zwischen den Regionen umschalten. Wir mussten aber den Health-Check-Intervall von 30 auf 15 Sekunden senken, um die RTO-Anforderung aus SLA-ORI-02 zu erfüllen."}
{"ts": "114:29", "speaker": "I", "text": "Gab es Bedenken, dass kürzere Intervalle zu mehr False Positives führen?"}
{"ts": "114:35", "speaker": "E", "text": "Ja, das war Teil der Risikoabwägung. Laut unseren Nimbus Observability Logs steigt die Wahrscheinlichkeit für einen Fehlalarm um etwa 4%. Wir haben dafür ein Threshold-Tuning eingebaut, dokumentiert in RB-DR-001 Appendix C."}
{"ts": "114:52", "speaker": "I", "text": "Werden diese Anpassungen noch vor dem nächsten Drill produktiv geschaltet?"}
{"ts": "115:00", "speaker": "E", "text": "Ja, Rollout ist für KW 21 geplant, nach Abschluss der Staging-Validierung. Wir wollen das im Juni-Drill unter Realbedingungen messen, um zu sehen, ob die RTO/RPO-Ziele stabil eingehalten werden."}
{"ts": "128:00", "speaker": "I", "text": "Lassen Sie uns bitte konkret auf die Umsetzung der SLA-HEL-01 im Drill eingehen. Wie haben Sie im letzten Durchlauf sichergestellt, dass die Health Checks tatsächlich in unter 15 Sekunden reagieren?"}
{"ts": "128:27", "speaker": "E", "text": "Wir haben im Drill ein angepasstes Monitoring-Profile aus Nimbus Observability verwendet. Dabei wurden die Health Endpoints in beiden Regionen parallel angesteuert. Zusätzlich gab es im Runbook RB-DR-001 einen Abschnitt, der die Eskalation bei >12 Sekunden Antwortzeit vorsieht."}
{"ts": "128:58", "speaker": "I", "text": "Gab es bei der Umsetzung Abweichungen zu den Vorgaben in RB-DR-001?"}
{"ts": "129:15", "speaker": "E", "text": "Nur minimal – wir haben die Eskalationskette um einen Schritt verkürzt, da der Drill simuliert hat, dass Poseidon Networking bereits eine Vorwarnung gegeben hatte. Dadurch konnten wir den Pager direkt auslösen, ohne den Level-2 Check zu warten."}
{"ts": "129:48", "speaker": "I", "text": "Das klingt nach einer Prozessoptimierung. Haben Sie diese Änderung auch dokumentiert?"}
{"ts": "130:05", "speaker": "E", "text": "Ja, es gibt eine Ergänzung im Ticket DR-ARCH-77, Unterpunkt 4.2. Dort ist vermerkt, dass bei Vorwarnung aus Poseidon das manuelle Bestätigen entfällt und direkt in Failover-Modus gewechselt wird."}
{"ts": "130:34", "speaker": "I", "text": "Wie hat sich diese Anpassung auf die RTO-Zeit ausgewirkt?"}
{"ts": "130:50", "speaker": "E", "text": "Sehr positiv – wir lagen im Drill bei 4 Minuten 42 Sekunden, also deutlich unter der SLA-ORI-02 Vorgabe von 7 Minuten. Das wurde auch durch unsere Zeitstempel im Nimbus Dashboard belegt."}
{"ts": "131:18", "speaker": "I", "text": "Gab es Auswirkungen auf die Kostenstruktur, weil Sie hier zusätzliche Checks parallelisiert haben?"}
{"ts": "131:35", "speaker": "E", "text": "Ja, kurzfristig sind die Observability-Kosten um etwa 3 % gestiegen. Wir haben das mit Vesta FinOps besprochen und als akzeptabel eingestuft, da der Performance-Gewinn den Mehraufwand rechtfertigt."}
{"ts": "132:02", "speaker": "I", "text": "Haben Sie nach dem Drill weitere Optimierungen identifiziert, die Sie in die nächste Iteration übernehmen wollen?"}
{"ts": "132:19", "speaker": "E", "text": "Wir planen, den DNS-Lookup-Pfad weiter zu verkürzen. In TEST-DR-2025-Q1 hatten wir ja die 180ms Latenz. Mit einem Caching-Layer in beiden Regionen könnten wir auf unter 80ms kommen."}
{"ts": "132:49", "speaker": "I", "text": "Würde das auch Änderungen an der Poseidon Networking Konfiguration erfordern?"}
{"ts": "133:04", "speaker": "E", "text": "Genau, wir müssten dort die TTL-Parameter anpassen und eine neue Routing-Regel in Poseidon hinterlegen. Das würde dann als RFC-DR-015 vorbereitet und mit dem Networking-Team abgestimmt werden."}
{"ts": "133:31", "speaker": "I", "text": "Sehen Sie Risiken, wenn diese TTL-Reduzierung live geht?"}
{"ts": "133:47", "speaker": "E", "text": "Ja, es besteht ein geringes Risiko erhöhter DNS-Query-Last auf den Resolvern. Das haben wir im Risikoabschnitt von DR-ARCH-77 vermerkt, mit der Maßgabe, die Last in den ersten 72 Stunden nach Rollout aktiv zu überwachen."}
{"ts": "144:00", "speaker": "I", "text": "Lassen Sie uns bitte noch einmal konkret auf die Failover-Architektur eingehen: wie sieht der aktuelle Ablauf vom Ausfall in EU-Central bis zur Aktivierung in AP-South aus?"}
{"ts": "144:05", "speaker": "E", "text": "Der Ablauf ist im Runbook RB-DR-001 Schritt für Schritt dokumentiert. Sobald Nimbus Observability einen kritischen Health-Check-Fehler meldet, triggert Poseidon Networking das automatisierte Routing-Update. Innerhalb von ca. 90 Sekunden wird der Traffic via Anycast auf AP-South umgeleitet."}
{"ts": "144:12", "speaker": "I", "text": "Und wie stellen Sie in diesem Szenario sicher, dass SLA-ORI-02, also der maximale RTO, eingehalten wird?"}
{"ts": "144:17", "speaker": "E", "text": "Wir haben bei der letzten Übung durch kontinuierliches Monitoring den RTO bei 3 Minuten gehalten, was unter dem SLA-Limit von 5 Minuten liegt. Die Trigger-Skripte wurden aus Ticket DR-ARCH-77 heraus optimiert, um Latenzen beim DNS-Update zu reduzieren."}
{"ts": "144:25", "speaker": "I", "text": "Gab es in den GameDay-Tests Punkte, an denen diese Kette unterbrochen wurde?"}
{"ts": "144:30", "speaker": "E", "text": "Ja, in TEST-DR-2025-Q1 hatten wir eine 45-sekündige Verzögerung, weil ein Poseidon-Router in Frankfurt ein fehlerhaftes Firmware-Update hatte. Das haben wir durch einen zusätzlichen Pre-Check in RB-DR-001 abgefangen."}
{"ts": "144:38", "speaker": "I", "text": "Wie priorisieren Sie solche Findings aus Tests im Verhältnis zu anderen Projekten?"}
{"ts": "144:42", "speaker": "E", "text": "Wir nutzen eine interne Severity-Matrix, die auf SLA-Verstößen basiert. Findings, die SLA-HEL-01 oder ORI-02 gefährden, gehen immer als P1 in unser Change-Backlog, selbst wenn das bedeutet, dass andere Feature-Deployments warten müssen."}
{"ts": "144:50", "speaker": "I", "text": "Könnten Sie ein Beispiel für eine solche Abwägung nennen?"}
{"ts": "144:54", "speaker": "E", "text": "Im März haben wir eine geplante Poseidon-Bandbreiten-Erweiterung verschoben, um zuerst den DNS-Failover-Mechanismus zu fixen. Das war eine bewusste Entscheidung, da das Risiko eines SLA-Bruchs höher bewertet wurde als der Nutzen der Bandbreitensteigerung."}
{"ts": "145:02", "speaker": "I", "text": "Wie fließen solche Entscheidungen in die Zusammenarbeit mit Vesta FinOps ein?"}
{"ts": "145:07", "speaker": "E", "text": "Wir dokumentieren jede Entscheidung in unserem FinOps-Board. Vesta prüft dann, ob die Kostensteigerung durch z. B. zusätzliche Hot-Standby-Ressourcen im Verhältnis zum Risiko steht. In diesem Fall wurde die Mehrbelastung von 4 % akzeptiert."}
{"ts": "145:15", "speaker": "I", "text": "Welche Risiken sehen Sie trotz aller getroffenen Maßnahmen noch?"}
{"ts": "145:19", "speaker": "E", "text": "Ein Restrisiko bleibt bei geopolitischen Ausfällen der AP-South-Region. Wir haben zwar ein drittes, kaltes Standby in US-East dokumentiert (siehe DR-ARCH-77, Abschnitt 4.3), aber das würde den RTO auf etwa 12 Minuten erhöhen."}
{"ts": "145:28", "speaker": "I", "text": "Und wie ist hier die strategische Entscheidung gefallen?"}
{"ts": "145:33", "speaker": "E", "text": "Wir haben beschlossen, das US-East-Standby nur bei gleichzeitigen Ausfällen von EU-Central und AP-South zu aktivieren, um Kosten zu sparen. Das wurde in RFC-DR-12 festgehalten und von der Geschäftsführung freigegeben."}
{"ts": "146:00", "speaker": "I", "text": "Könnten Sie bitte noch einmal die genaue Sequenz im Falle eines EU-Central Ausfalls skizzieren, bevor AP-South übernimmt?"}
{"ts": "146:04", "speaker": "E", "text": "Ja, klar. Also sobald unser Monitoring, angebunden an Nimbus Observability, einen kritischen Ausfall nach den Parametern aus SLA-ORI-02 detektiert, wird gemäß RB-DR-001 der Failover-Prozess getriggert. Zuerst geht das Signal an Poseidon Networking, das die BGP-Ankündigungen aktualisiert."}
{"ts": "146:12", "speaker": "E", "text": "Danach, innerhalb von etwa 90 Sekunden, übernehmen die AP-South-Cluster die Last. Die DNS-Propagation ist ein kritischer Punkt; hier hatten wir im GameDay ein Latenzproblem, das unsere RTO fast verletzt hätte."}
{"ts": "146:20", "speaker": "I", "text": "Das war der Vorfall im TEST-DR-2025-Q1, richtig? Können Sie kurz schildern, was genau schief lief?"}
{"ts": "146:24", "speaker": "E", "text": "Genau. Wir hatten in der Simulation festgestellt, dass der TTL-Wert in der DNS-Zone nicht wie im Runbook spezifiziert auf 30 Sekunden stand, sondern auf 300 Sekunden. Das führte zu verzögerten Client-Umleitungen."}
{"ts": "146:32", "speaker": "I", "text": "Und wie wurde das priorisiert?"}
{"ts": "146:34", "speaker": "E", "text": "Wir haben das als P1-Finding klassifiziert, weil es direkt SLA-HEL-01 gefährdete. Innerhalb von zwei Tagen gab es ein RFC, das in allen Zonen den korrekten TTL-Wert setzt. Die Umsetzung ist jetzt in RB-DR-001 vermerkt."}
{"ts": "146:42", "speaker": "I", "text": "Welche Rolle spielte Vesta FinOps bei der Behebung?"}
{"ts": "146:45", "speaker": "E", "text": "FinOps hat uns geholfen, die Kosten für die niedrigeren TTLs zu kalkulieren. Kürzere TTLs bedeuten mehr DNS-Queries und damit höhere Kosten. Wir mussten abwägen zwischen Kostensteigerung und RTO-Sicherheit."}
{"ts": "146:54", "speaker": "I", "text": "Gab es da einen Kompromiss?"}
{"ts": "146:56", "speaker": "E", "text": "Ja, wir haben für kritische Zonen eine TTL von 30 Sekunden und für weniger kritische 120 Sekunden definiert. Das ist auch in Ticket DR-ARCH-77 dokumentiert, inklusive der Kostenschätzung."}
{"ts": "147:04", "speaker": "I", "text": "Wie interagieren Poseidon Networking und Nimbus Observability konkret während des Failovers?"}
{"ts": "147:07", "speaker": "E", "text": "Poseidon empfängt von Nimbus ein strukturiertes Alert-Event im JSON-Format, das den Ausfallzustand beschreibt. Daraufhin ändert Poseidon die Routen und gibt Feedback an Nimbus, damit dieser das Event schließen kann, sobald die Systeme stabil sind."}
{"ts": "147:15", "speaker": "I", "text": "Gibt es dabei Abhängigkeiten, die Risiken bergen?"}
{"ts": "147:18", "speaker": "E", "text": "Ja. Wenn die API von Nimbus verzögert reagiert, verzögert sich auch das Routing-Update. Wir haben deshalb in RB-DR-001 einen manuellen Override beschrieben, falls die Automatisierung hängt."}
{"ts": "147:26", "speaker": "I", "text": "Verstanden, danke für die detaillierte Ausführung."}
{"ts": "148:00", "speaker": "I", "text": "Könnten Sie bitte genauer ausführen, wie sich die Multi-Region-Strategie konkret auf die monatlichen Betriebskosten auswirkt?"}
{"ts": "148:05", "speaker": "E", "text": "Ja, also wir sehen durch die permanente Synchronisation zwischen EU-Central und AP-South etwa 18 % höhere Compute-Kosten. Hinzu kommen, äh, interregionale Datenübertragungen, die laut Vesta FinOps Analyse im DR-COST-12 Report rund 4 TB pro Tag betragen."}
{"ts": "148:14", "speaker": "I", "text": "Und welche Optimierungen haben Sie geprüft, um diese Zahlen zu senken, ohne die SLA-Parameter zu verletzen?"}
{"ts": "148:19", "speaker": "E", "text": "Wir haben in RFC-DR-2025-04 evaluiert, ob eine inkrementelle Replikation mit Batch-Übertragung außerhalb der Peak-Zeiten möglich ist. Das würde Kosten sparen, könnte aber das RPO von 15 Minuten unter SLA-ORI-02 gefährden."}
{"ts": "148:28", "speaker": "I", "text": "Gab es schon Tests in dieser Richtung?"}
{"ts": "148:32", "speaker": "E", "text": "Im Testlauf TEST-DR-2025-Q2-Prep haben wir einmal auf 30-Minuten-Batches umgestellt. Ergebnis: Kosten runter um fast 9 %, aber beim simulierten Ausfall lag der Datenverlust bei 22 Minuten – nicht akzeptabel laut Runbook RB-DR-001."}
{"ts": "148:42", "speaker": "I", "text": "Verstehe. Gab es Situationen, in denen Sie bewusst höhere Kosten in Kauf genommen haben, um Risiken zu minimieren?"}
{"ts": "148:46", "speaker": "E", "text": "Ja, das betraf unser DNS-Failover. Nach dem GameDay-Latenzproblem haben wir in AP-South einen zusätzlichen Anycast-Knoten aufgebaut. Kostet pro Monat ca. 1 500 €, reduziert aber die Umstellungszeit im Drill um fast 40 %."}
{"ts": "148:56", "speaker": "I", "text": "Wie binden Sie Vesta FinOps in solche Entscheidungen ein?"}
{"ts": "149:01", "speaker": "E", "text": "Wir haben ein wöchentliches Review, in dem wir geplante Änderungen mit deren Cost-Monitoring-Dashboard abgleichen. Dort wird jede Maßnahme gegen Budgetlinien und SLA-Risiken bewertet."}
{"ts": "149:10", "speaker": "I", "text": "Welche großen Architekturentscheidungen haben Sie zuletzt getroffen?"}
{"ts": "149:14", "speaker": "E", "text": "Die größte war der Verzicht auf einen dritten, inaktiven DR-Standort in SA-East. Analysen in DR-ARCH-89 zeigten, dass die Added Value für RTO minimal war, die Kosten aber 25 % höher. Wir bleiben bei zwei Regionen und investieren stattdessen in automatisierte Netzwerkpfad-Optimierung mit Poseidon."}
{"ts": "149:26", "speaker": "I", "text": "Welche Risiken bleiben trotz dieser Maßnahmen bestehen?"}
{"ts": "149:30", "speaker": "E", "text": "Ein Restrisiko ist die gleichzeitige Beeinträchtigung beider Regionen durch globale Netzwerkstörungen. Laut unserem Risiko-Register DR-RISK-15 ist das zwar sehr unwahrscheinlich, aber nicht null."}
{"ts": "149:38", "speaker": "I", "text": "Können Sie konkrete Evidenz nennen, die Ihre Entscheidungen stützt?"}
{"ts": "149:42", "speaker": "E", "text": "Ja, neben DR-COST-12 und DR-ARCH-89 gibt es Protokolle aus TEST-DR-2025-Q2-Prep und die aktualisierte Fassung des RB-DR-001, die genau zeigen, wie sich die Latenz- und Kostenänderungen auf RTO/RPO ausgewirkt haben."}
{"ts": "152:00", "speaker": "I", "text": "Lassen Sie uns jetzt bitte konkret über die Kostenseite sprechen: Wie genau wirkt sich unsere Multi-Region-Strategie auf das Kostenprofil von Titan DR aus?"}
{"ts": "152:06", "speaker": "E", "text": "Ja, also die permanent aktive Zweitregion erhöht die Infrastrukturkosten um etwa 38 % im Vergleich zu einem aktiven-passiven Setup. Diese Zahl stammt direkt aus dem Ticket DR-COST-12, wo wir die monatlichen Compute-, Storage- und Netzwerkgebühren aufgeschlüsselt haben."}
{"ts": "152:14", "speaker": "I", "text": "Und wie sind diese Mehrkosten gegenüber den Performance- und Resilienzvorteilen abzuwägen?"}
{"ts": "152:20", "speaker": "E", "text": "Wir haben mit Vesta FinOps eine Matrix erstellt, die die SLA-Parameter – insbesondere SLA-ORI-02 mit einem RTO von 15 Minuten – gegen die Kostenkurve legt. Das zeigt, dass wir ohne Multi-Region die Latenz im Failover-Fall verdoppeln würden."}
{"ts": "152:27", "speaker": "I", "text": "Gab es Erwägungen, auf ein aktives-passives Modell zu wechseln, um Kosten zu sparen?"}
{"ts": "152:32", "speaker": "E", "text": "Ja, in RFC-DR-2025-04 haben wir genau das simuliert. Die Simulation zeigte jedoch, dass im Drill-Szenario TEST-DR-2025-Q1 die Recovery-Zeit auf 27 Minuten stieg – also klar unter SLA-Niveau."}
{"ts": "152:40", "speaker": "I", "text": "Das heißt, Sie haben sich bewusst für höhere Kosten entschieden, um die Latenz niedrig zu halten?"}
{"ts": "152:45", "speaker": "E", "text": "Genau. Wir haben die Entscheidung dokumentiert, inklusive der Risikoanalyse im Runbook RB-DR-001, Abschnitt 4.3. Dort steht, dass wir <120 ms Cross-Region-Latenz beibehalten, auch wenn dies ca. 12 000 € Mehrkosten pro Quartal bedeutet."}
{"ts": "152:54", "speaker": "I", "text": "Wie ist Vesta FinOps in diesen Entscheidungsprozess eingebunden?"}
{"ts": "152:59", "speaker": "E", "text": "Vesta FinOps liefert wöchentliche Kosten-Reports und identifiziert Optimierungspotenziale, z. B. Reserved Instances für Standby-Kapazitäten. Diese werden dann gegen die Performance-Anforderungen aus SLA-HEL-01 geprüft."}
{"ts": "153:07", "speaker": "I", "text": "Gab es in dieser Zusammenarbeit konkrete Einsparungen, ohne die Performance zu gefährden?"}
{"ts": "153:12", "speaker": "E", "text": "Ja, wir konnten durch Anpassung der Storage-Klassen in der Zweitregion 8 % einsparen, ohne das RPO von 5 Minuten zu verletzen. Das wurde in DR-COST-12 als \"Low Risk/High Save\" markiert."}
{"ts": "153:20", "speaker": "I", "text": "Welche Risiken bestehen trotz dieser Maßnahmen weiterhin?"}
{"ts": "153:24", "speaker": "E", "text": "Ein Restrisiko bleibt die plötzliche Kostensteigerung bei Cross-Region-Data-Transfer im Katastrophenfall. Das ist in unserer Risikoübersicht RISK-DR-19 erfasst und hat ein mittleres Impact-Level."}
{"ts": "153:32", "speaker": "I", "text": "Wurde dieses Risiko in den finalen Architekturentscheidungen berücksichtigt?"}
{"ts": "153:37", "speaker": "E", "text": "Ja, wir haben dafür eine Budget-Reserve von 15 % im DR-OPEX vorgesehen, wie in RFC-DR-2025-04, Kapitel 6, beschrieben. Das erlaubt uns, die Latenzwerte stabil zu halten, ohne in einer Krise plötzlich über Budget zu laufen."}
{"ts": "153:36", "speaker": "I", "text": "Lassen Sie uns jetzt gezielt auf die Kostendetails eingehen – wie hat die Multi-Region-Strategie konkret unsere Betriebsausgaben verändert?"}
{"ts": "153:40", "speaker": "E", "text": "Also, wir haben in DR-COST-12 klar aufgeführt, dass der Betrieb der zweiten aktiven Region etwa 28 % Mehrkosten pro Quartal verursacht. Das liegt primär an den kontinuierlichen Replikations-Streams und den Warm-Standby-Kapazitäten."}
{"ts": "153:45", "speaker": "I", "text": "Und wie haben Sie diese Zahlen mit Vesta FinOps diskutiert?"}
{"ts": "153:50", "speaker": "E", "text": "Wir hatten im Februar eine gemeinsame Review-Session, in der wir die Metriken aus dem Observability-Stack (über Nimbus) mit den Kostenkennzahlen aus Vesta FinOps korreliert haben. Dadurch konnten wir z. B. nachweisen, dass eine Reduktion der aktiven Knoten in der Sekundärregion unter einen Threshold den RTO um 42 Sekunden verlängert hätte."}
{"ts": "153:56", "speaker": "I", "text": "Gab es Szenarien, in denen Sie bewusst auf Kostenoptimierung verzichtet haben?"}
{"ts": "154:00", "speaker": "E", "text": "Ja, in RFC-DR-2025-04 haben wir dokumentiert, dass wir an den Latenz-SLAs aus SLA-ORI-02 festhalten – das bedeutet dedizierte Interconnects zwischen den Regionen. Das verursacht Mehrkosten, aber hält die Latenz im Drill unter 120 ms, was für bestimmte Finanzkunden kritisch ist."}
{"ts": "154:05", "speaker": "I", "text": "Wie fließen solche Entscheidungen in zukünftige Budgetplanungen ein?"}
{"ts": "154:09", "speaker": "E", "text": "Wir markieren sie in den FinOps-Dashboards als 'business critical' Ausgaben. Das ist eine interne Kategorie, die von Kostensenkungsmaßnahmen ausgenommen ist, solange kein neues Risiko-Assessment anderes empfiehlt."}
{"ts": "154:14", "speaker": "I", "text": "Haben Sie auch Performance-Metriken, die zeigen, dass sich diese Investitionen lohnen?"}
{"ts": "154:18", "speaker": "E", "text": "Ja, die Benchmarks aus TEST-DR-2025-Q1 zeigen, dass die Failover-Zeit bei unter 3 Minuten liegt und die Transaktionsverluste deutlich unter dem RPO von 60 Sekunden bleiben. Ohne den dedizierten Interconnect hätten wir doppelte Latenz und bis zu 3 % mehr verlorene Transaktionen."}
{"ts": "154:23", "speaker": "I", "text": "Gab es hierzu Gegenargumente aus dem Controlling?"}
{"ts": "154:27", "speaker": "E", "text": "Natürlich, das Controlling hat vorgeschlagen, den Traffic in der Sekundärregion über günstigeres Public Peering zu routen. Aber unser Risiko-Assessment in DR-COST-12 hat klar gezeigt, dass dadurch die SLA-HEL-01 für Recovery Time nicht mehr erfüllt würde."}
{"ts": "154:33", "speaker": "I", "text": "Welche Risiken bleiben trotz dieser Maßnahmen bestehen?"}
{"ts": "154:37", "speaker": "E", "text": "Ein Restrisiko ist die mögliche Unterbrechung beider Regionen durch einen großflächigen Cloud-Anbieter-Ausfall. Wir mitigieren das teilweise mit Offsite-Backups, aber ein simultaner Fail der Interconnects und Core-Regionen würde unser RTO sprengen."}
{"ts": "154:43", "speaker": "I", "text": "Wie halten Sie diese Restrisiken transparent?"}
{"ts": "154:47", "speaker": "E", "text": "Wir führen sie in unserem Risiko-Register unter 'High Impact/Low Probability' auf und verlinken auf relevante Tickets wie RISK-DR-2025-07. Außerdem werden sie in den Lessons Learned-Sessions nach jedem Drill nochmals diskutiert."}
{"ts": "155:06", "speaker": "I", "text": "Kommen wir nun zu den Kosten- und Performance-Abwägungen, speziell im Kontext der Multi-Region-Strategie für Titan DR. Können Sie kurz skizzieren, welche Hauptkostenfaktoren hier wirken?"}
{"ts": "155:10", "speaker": "E", "text": "Ja, also die größten Posten sind eindeutig die kontinuierliche Synchronisation der Daten zwischen den Regionen, also die Cross-Region-Replication, und die Bereithaltung der warm-standby Ressourcen. Dazu kommen noch die erhöhten Observability-Kosten, weil wir in jeder Region ein volles Stack-Setup fahren."}
{"ts": "155:17", "speaker": "I", "text": "Wie fließt Vesta FinOps in diese Analyse ein?"}
{"ts": "155:21", "speaker": "E", "text": "Wir bekommen von Vesta FinOps monatlich einen Drill-Down-Report, der nicht nur Kosten pro Service ausweist, sondern auch mit den in DR-COST-12 hinterlegten Zielwerten vergleicht. Anhand dieser Zahlen können wir sehen, ob z. B. der Traffic zwischen Region Nord und Region West über dem Budget liegt."}
{"ts": "155:28", "speaker": "I", "text": "Gab es denn Situationen, in denen Kostendruck und Performance-Anforderungen in Konflikt geraten sind?"}
{"ts": "155:32", "speaker": "E", "text": "Ja, ganz deutlich beim Thema Latenz. In RFC-DR-2025-04 haben wir dokumentiert, dass wir bewusst teurere Direct-Link-Verbindungen beibehalten, um die Latenz zwischen den aktiven Regionen unter 80ms zu halten. Eine Reduzierung auf günstigere VPN-basierte Links hätte zwar 15% Kosten gespart, aber die RTO-Tests im Drill gezeigt, dass wir dann über den SLA-ORI-02 Grenzwerten lägen."}
{"ts": "155:40", "speaker": "I", "text": "Das heißt, Sie haben sich hier klar für Performance entschieden?"}
{"ts": "155:44", "speaker": "E", "text": "Genau. Die Bewertung im Risikoregister, Eintrag DR-COST-12-P3, zeigt, dass ein SLA-Bruch im Katastrophenfall mehr kosten würde als die Einsparung. Wir haben das auch mit dem Notfallteam und den Stakeholdern aus Compliance abgestimmt."}
{"ts": "155:51", "speaker": "I", "text": "Wie gehen Sie vor, um solche Entscheidungen transparent zu kommunizieren?"}
{"ts": "155:55", "speaker": "E", "text": "Wir haben ein internes Decision Log, in dem jede größere Architekturentscheidung mit Verweis auf die relevanten Runbooks und RFCs dokumentiert wird. Für diesen Fall verlinken wir auf RB-DR-001, Abschnitt 4.3, und hängen die Kostenabschätzung als Anhang an."}
{"ts": "156:02", "speaker": "I", "text": "Und wie sieht es mit Optimierungen aus, die trotz Beibehaltung der Performance möglich sind?"}
{"ts": "156:06", "speaker": "E", "text": "Wir prüfen aktuell, ob wir in den Off-Peak-Zeiten die Replikationsfrequenz leicht drosseln können, ohne die RPO-Anforderung von 5 Minuten aus SLA-HEL-01 zu verletzen. Vesta FinOps hat hier ein Simulationstool bereitgestellt, mit dem wir unterschiedliche Lastprofile durchspielen."}
{"ts": "156:13", "speaker": "I", "text": "Gibt es Lessons Learned aus vergangenen Drills, die in diesen Kosten-Performance-Kontext eingeflossen sind?"}
{"ts": "156:17", "speaker": "E", "text": "Ja, aus TEST-DR-2025-Q1 haben wir mitgenommen, dass zu aggressive Kostensenkungen bei Storage-IOPS zu verlängerten Recovery-Zeiten führen. Das wurde damals im Ticket DR-INC-431 dokumentiert und in RB-DR-002 als Anti-Pattern gekennzeichnet."}
{"ts": "156:24", "speaker": "I", "text": "Wie verankern Sie solche Anti-Patterns langfristig im Teamwissen?"}
{"ts": "156:28", "speaker": "E", "text": "Wir führen vierteljährlich Knowledge-Sharing-Sessions durch, in denen genau solche Fälle besprochen werden. Zusätzlich pflegen wir eine Wiki-Seite 'DR Do's and Don'ts', die verpflichtend in der Onboarding-Checkliste für neue Engineers steht."}
{"ts": "156:42", "speaker": "I", "text": "Lassen Sie uns bitte noch einmal auf die finale Architekturentscheidung zurückkommen, die Sie im Kontext von Titan DR getroffen haben."}
{"ts": "156:47", "speaker": "E", "text": "Ja, das war im Zuge der Überprüfung von RFC-DR-2025-04. Wir hatten die Option, die Latenz von 85 ms auf unter 70 ms zu bringen, indem wir eine zusätzliche Edge-Region aktivieren. Das hätte jedoch laut Kalkulation von Vesta FinOps die laufenden Kosten um etwa 22 % erhöht."}
{"ts": "156:58", "speaker": "I", "text": "Und Sie haben sich dagegen entschieden, korrekt?"}
{"ts": "157:00", "speaker": "E", "text": "Genau, wir blieben bei den 85 ms, weil das innerhalb der SLA-ORI-02 Vorgabe von ≤ 90 ms liegt. Die Risikoanalyse aus DR-COST-12 zeigte, dass die Ausfallwahrscheinlichkeit nicht signifikant sinkt durch die zusätzliche Region."}
{"ts": "157:12", "speaker": "I", "text": "Gab es dabei Bedenken seitens des Operations-Teams?"}
{"ts": "157:15", "speaker": "E", "text": "Einige, ja. Das Ops-Team hatte Sorge, dass bei einem großflächigen Ausfall die Re-Routing-Zeit länger sein könnte. Aber RB-DR-001 deckt genau diesen Ablauf ab und sieht vor, dass wir innerhalb von 4 Minuten den Traffic vollständig umgeleitet haben."}
{"ts": "157:27", "speaker": "I", "text": "Wie verifizieren Sie das in den Drills?"}
{"ts": "157:30", "speaker": "E", "text": "Wir haben im Drill TEST-DR-2025-Q1 gezielt eine Core-Region abgeschaltet und über Poseidon Networking ein Failover in die sekundäre Region getriggert. Nimbus Observability hat uns bestätigt, dass der Traffic nach 3 Minuten 42 Sekunden wieder bei 100 % lag."}
{"ts": "157:45", "speaker": "I", "text": "Hatten Sie in diesem Drill auch Kostenmetriken im Blick?"}
{"ts": "157:48", "speaker": "E", "text": "Ja, parallel lief ein Cost-Monitor aus dem Vesta FinOps Dashboard. Wir haben dort ein Spike Alert bei +18 % Kosten gesehen, der nach dem Failover-Prozess innerhalb von 24 Stunden wieder normalisierte."}
{"ts": "158:00", "speaker": "I", "text": "Interessant, und wie fließt das in künftige Optimierungen ein?"}
{"ts": "158:04", "speaker": "E", "text": "Wir haben eine Empfehlung in RFC-DR-2025-06 aufgenommen, die vorsieht, den Cold-Standby-Betrieb energieeffizienter zu gestalten. Das könnte die Spike-Kosten bei Failover um ca. 5 % senken."}
{"ts": "158:15", "speaker": "I", "text": "Gab es in der Analyse einen Punkt, an dem Sicherheit versus Kosten besonders abgewogen wurde?"}
{"ts": "158:19", "speaker": "E", "text": "Ja, beim Thema Cross-Region-Encryption. Eine Always-on-Verschlüsselung hätte pro Monat zusätzliche 14 000 € bedeutet. Wir nutzen nun situative Aktivierung je nach Bedrohungslage, dokumentiert in DR-SEC-07."}
{"ts": "158:31", "speaker": "I", "text": "Wie stellen Sie sicher, dass diese situative Aktivierung im Ernstfall greift?"}
{"ts": "158:35", "speaker": "E", "text": "Das ist im Runbook RB-DR-004 hinterlegt. Nimbus Observability überwacht Security-Events, und sobald ein Threat-Level über 'Medium' steigt, wird automatisch ein API-Call zur Aktivierung der Verschlüsselung in allen aktiven Regionen ausgelöst."}
{"ts": "158:18", "speaker": "I", "text": "Können Sie, äh, bitte noch einmal erläutern, wie genau RB-DR-001 im Drill-Szenario ausgelöst wird?"}
{"ts": "158:23", "speaker": "E", "text": "Ja, sicher. RB-DR-001 wird automatisch getriggert, sobald das Monitoring aus Nimbus Observability einen `region_health=false` Status meldet. Das Skript prüft dann die Prioritätenliste im Runbook und leitet den Traffic über Poseidon Networking an die definierte sekundäre Region weiter."}
{"ts": "158:31", "speaker": "I", "text": "Und diese sekundäre Region, ist das derzeit noch `eu-central-2` oder haben Sie das im Lauf der Tests geändert?"}
{"ts": "158:36", "speaker": "E", "text": "Im Moment, für Titan DR Drill, ist `eu-central-2` aktiv als Backup. Wir hatten im Prototyp auch `us-east-1` als tertiäre Option, aber das erhöht die Latenz deutlich, daher nur im Notfall."}
{"ts": "158:44", "speaker": "I", "text": "Bezieht sich das auf die Latenz-Constraint aus SLA-ORI-02?"}
{"ts": "158:48", "speaker": "E", "text": "Genau. SLA-ORI-02 definiert maximal 250 ms RTT im Failover-Modus. In TEST-DR-2025-Q1 hatten wir bei US-Fallback eher 380 ms, das war außerhalb der Toleranz."}
{"ts": "158:56", "speaker": "I", "text": "Wie haben Sie das Finding damals priorisiert?"}
{"ts": "159:00", "speaker": "E", "text": "Wir haben es als P2 eingestuft, weil es nur im tertiären Szenario auftritt. Laut unserem GameDay-Priorisierungsschema werden P1 innerhalb von 7 Tagen gefixt, P2 innerhalb von 30 Tagen – in dem Fall haben wir die Optimierung in RFC-DR-2025-04 aufgenommen."}
{"ts": "159:09", "speaker": "I", "text": "Gab es dafür eine Anpassung im Runbook?"}
{"ts": "159:13", "speaker": "E", "text": "Ja, Runbook RB-DR-001 wurde um einen Hinweis ergänzt: Vor Fallback auf US-Region muss geprüft werden, ob die Latenz-SLA temporär gelockert werden kann, basierend auf Abstimmung mit dem Incident Commander."}
{"ts": "159:22", "speaker": "I", "text": "Das klingt nach einem Trade-off zwischen Verfügbarkeit und Performance."}
{"ts": "159:25", "speaker": "E", "text": "Absolut, und genau da kam Vesta FinOps ins Spiel. Wir mussten entscheiden: höhere Kosten für zusätzliche europäische Kapazitäten versus Risiko höherer Latenz. In DR-COST-12 haben wir dokumentiert, dass wir die Latenzvorgaben beibehalten – trotz Mehrkosten von ca. 18 %. "}
{"ts": "159:36", "speaker": "I", "text": "Und welche Risiken bleiben trotz dieser Entscheidung bestehen?"}
{"ts": "159:40", "speaker": "E", "text": "Das Hauptrisiko ist, dass bei simultanen Ausfällen in beiden europäischen Regionen der US-Fallback zwangsläufig genutzt werden muss. Das würde SLA-ORI-02 verletzen und könnte Vertragsstrafen nach SLA-HEL-01 auslösen."}
{"ts": "159:49", "speaker": "I", "text": "Wie wollen Sie dieses Restrisiko adressieren?"}
{"ts": "159:52", "speaker": "E", "text": "Wir planen ein Proof-of-Concept mit einer dritten EU-Region, `eu-north-1`, um den US-Fallback zu vermeiden. Das geht als RFC-DR-2025-07 in den Architektur-Review nächste Woche."}
{"ts": "159:54", "speaker": "I", "text": "Wir hatten vorhin kurz die RTO- und RPO-Ziele erwähnt. Können Sie bitte konkret sagen, wie diese im Drill für Titan DR überprüft werden?"}
{"ts": "159:59", "speaker": "E", "text": "Ja, also wir haben für P‑TIT definierte Werte aus SLA‑ORI‑02, sprich RTO von maximal 90 Minuten und RPO von 5 Minuten. Im Drill messen wir beides über das Monitoring-Setup von Nimbus Observability und korrelieren die Zeitstempel mit den Alerts, damit wir exakte Gap‑Analysen machen können."}
{"ts": "160:05", "speaker": "I", "text": "Und wie wird das in der Architektur technisch sichergestellt, gerade in Hinblick auf die Multi‑Region‑Strategie?"}
{"ts": "160:10", "speaker": "E", "text": "Indem wir in der aktiven Region EU‑West laufend Snapshots auf die sekundäre Region US‑East replizieren. Poseidon Networking stellt dabei die dedizierten VPN‑Tunnels mit garantierter Bandbreite sicher. RB‑DR‑001 beschreibt Schritt für Schritt, wie der Umschaltprozess unter Last erfolgt."}
{"ts": "160:16", "speaker": "I", "text": "Gab es beim letzten Drill konkrete Probleme entlang dieser Kette?"}
{"ts": "160:21", "speaker": "E", "text": "Ja, in TEST‑DR‑2025‑Q1 hatten wir einen 7‑Minuten‑Lag in der Replikation, weil in Poseidon ein Routing-Update verzögert wurde. Das hat Nimbus zwar erkannt, aber erst nach Überschreiten des RPO‑Fensters Alarm geschlagen."}
{"ts": "160:27", "speaker": "I", "text": "Wie haben Sie darauf reagiert?"}
{"ts": "160:31", "speaker": "E", "text": "Wir haben daraus das RFC‑DR‑2025‑04 gestartet, um in RB‑DR‑001 einen zusätzlichen Pre‑Failover‑Check einzubauen, der die Replikationslatenz gegen den Zielwert prüft, bevor der Switch initiiert wird."}
{"ts": "160:38", "speaker": "I", "text": "Klingt sinnvoll. Haben Sie solche Lessons Learned standardisiert?"}
{"ts": "160:42", "speaker": "E", "text": "Ja, wir haben ein internes Template, in dem jedes Finding aus einem GameDay mit Quelle, Auswirkung, Maßnahmen und Referenzen auf Runbooks dokumentiert wird. TEST‑DR‑2025‑Q1 hat so drei neue Einträge in RB‑DR‑001 erzeugt."}
{"ts": "160:48", "speaker": "I", "text": "Wie wirkt sich das alles auf die Kosten aus, gerade wenn man zusätzliche Prüfungen und Ressourcen einplant?"}
{"ts": "160:53", "speaker": "E", "text": "Die Kosten steigen kurzfristig um etwa 4 %, weil wir mehr Monitoring und Bandbreite vorhalten. Mit Vesta FinOps prüfen wir aber, ob sich das durch geringere Ausfallzeiten und SLA‑Bonuszahlungen langfristig amortisiert."}
{"ts": "160:59", "speaker": "I", "text": "Also kein unmittelbarer Kostenschnitt, sondern ein Performance‑Bias?"}
{"ts": "161:03", "speaker": "E", "text": "Genau. Wir priorisieren hier Performance und Sicherheit, weil DR‑COST‑12 klar aufzeigt, dass ein einziger SLA‑Bruch teurer wäre als die laufenden Zusatzkosten."}
{"ts": "161:08", "speaker": "I", "text": "Gibt es noch offene Risiken, die Sie trotz dieser Maßnahmen sehen?"}
{"ts": "161:12", "speaker": "E", "text": "Ja, das Hauptrisiko bleibt die Abhängigkeit von den interkontinentalen Links. Auch mit Poseidon‑Optimierungen kann ein kompletter Transatlantik‑Ausfall nicht völlig ausgeschlossen werden. Das ist im DR‑Risiko‑Register unter ID R‑NET‑07 erfasst."}
{"ts": "161:27", "speaker": "I", "text": "Sie hatten vorhin die Verknüpfung zu Poseidon Networking erwähnt. Mich würde interessieren, wie konkret die Routing-Policies im Failover-Fall greifen."}
{"ts": "161:33", "speaker": "E", "text": "Ja, also ähm, bei einem Ausfall in Region West greift zuerst die statische Policy aus RB-DR-001, Abschnitt 4.2, die die Anycast-IPs auf die Backup-Region Ost umleitet. Diese ist wiederum im Poseidon Control Plane als High-Priority Path hinterlegt."}
{"ts": "161:46", "speaker": "I", "text": "Und Nimbus Observability, wie wird das da eingebunden?"}
{"ts": "161:51", "speaker": "E", "text": "Nimbus liefert uns in near-real-time die Health-Metriken der Gateways, und triggert den Runbook-Step 5: 'Validate Endpoint Reachability'. Ohne diese Telemetrie müssten wir viel manuell prüfen, was im Drill zu RTO-Verzögerungen führen könnte."}
{"ts": "162:05", "speaker": "I", "text": "Gab es im TEST-DR-2025-Q1 einen Fall, wo genau dieser Mechanismus versagt hat?"}
{"ts": "162:10", "speaker": "E", "text": "Ja, Ticket DR-INC-77 zeigt, dass Nimbus damals die Latenzanomalie zu spät erkannt hat. Das führte zu einer 3‑Minuten-Überschreitung des RTO gemäß SLA-ORI-02. Danach haben wir ein Alert-Tuning eingeführt, siehe RFC-DR-2025-04."}
{"ts": "162:26", "speaker": "I", "text": "Wie priorisieren Sie solche Findings nach einem GameDay?"}
{"ts": "162:31", "speaker": "E", "text": "Wir bewerten nach Impact auf RTO/RPO und SLA-Verletzungsrisiko. In einer Matrix, die im Confluence im Abschnitt Titan DR gefunden wird, ist DR-INC-77 als 'High Impact / High Probability' markiert worden, deshalb sofortige Umsetzung."}
{"ts": "162:45", "speaker": "I", "text": "Kommen wir nochmal zu den Kosten: Welche Optimierungen wurden zuletzt diskutiert?"}
{"ts": "162:50", "speaker": "E", "text": "Wir haben überlegt, die Warm-Standby-Kapazität in Region Nord von 100% auf 70% zu senken. Das spart laut Vesta FinOps Kalkulation etwa 18% in monatlichen Compute-Kosten, erhöht aber das Risiko, dass bei gleichzeitigen Lastspitzen das RTO von 15 Minuten nicht gehalten wird."}
{"ts": "163:05", "speaker": "I", "text": "Wie sind Sie da vorgegangen, um diese Trade-offs zu bewerten?"}
{"ts": "163:10", "speaker": "E", "text": "Wir haben ein Szenario in der Testumgebung simuliert, basierend auf Lastprofilen aus den letzten 12 Monaten. Ergebnis im Testprotokoll TP-DR-2025-07: Bei 70% Kapazität steigt die Recovery-Zeit auf 17-18 Minuten, also knapp über SLA."}
{"ts": "163:24", "speaker": "I", "text": "Wurde diese Maßnahme dann verworfen?"}
{"ts": "163:28", "speaker": "E", "text": "Ja, wir haben im Architektur-Board Meeting vom 12.05.2025 beschlossen, die Kapazität bei 100% zu belassen. Entscheidung ist dokumentiert in Decision Log DL-DR-2025-05, mit Verweis auf die Risiken und die Kostendaten aus Vesta."}
{"ts": "163:42", "speaker": "I", "text": "Welche Risiken bleiben denn trotz aller Maßnahmen bestehen?"}
{"ts": "163:47", "speaker": "E", "text": "Residual Risk RR-DR-2025-02 beschreibt die Abhängigkeit von einem einzigen Cloud-Transit-Anbieter zwischen Ost und Nord. Fällt der aus, greifen wir zwar auf Satellite Link Backup zurück, aber RTO würde auf 25 Minuten steigen. Das ist außerhalb SLA, wir haben es aber als 'Low Probability' eingestuft."}
{"ts": "162:07", "speaker": "I", "text": "Kommen wir jetzt bitte zu den jüngsten Architekturentscheidungen. Was war die letzte große Änderung, die Sie im Titan DR vorgenommen haben, und warum?"}
{"ts": "162:12", "speaker": "E", "text": "Die letzte große Änderung war die Einführung eines aktiven Read-Replica-Clusters in der Region Nord-2. Hintergrund war eine Analyse aus dem Drill im März, die zeigte, dass unser primärer Failover-Pfad über Ost-1 in Spitzenlastzeiten 18 % höhere Latenzen hatte als im SLA-ORI-02 erlaubt."}
{"ts": "162:21", "speaker": "I", "text": "Und wie haben Sie das technisch umgesetzt?"}
{"ts": "162:26", "speaker": "E", "text": "Wir haben auf Basis des Runbooks RB-DR-004 den Datenstrom via Poseidon Networking optimiert und die Replikations-Intervalle nach RFC-DR-2025-07 angepasst. Dadurch konnten wir die RTO von 12 auf 9 Minuten senken."}
{"ts": "162:34", "speaker": "I", "text": "Gab es dabei Risiken, die Sie in Kauf nehmen mussten?"}
{"ts": "162:38", "speaker": "E", "text": "Ja, wir mussten akzeptieren, dass die laufenden Betriebskosten um ca. 4 % steigen, weil die Read-Replica permanent aktiv ist. Das ist im Ticket OPS-DR-558 als 'Kostenrisiko-Kategorie B' dokumentiert."}
{"ts": "162:46", "speaker": "I", "text": "Wie haben Sie diese Entscheidung gegenüber Vesta FinOps begründet?"}
{"ts": "162:50", "speaker": "E", "text": "Mit einer Kosten-Nutzen-Rechnung: Der potenzielle Verlust durch SLA-Verletzung bei einem Ausfall in Primär-Region wurde auf 240k EUR pro Stunde geschätzt. Die Mehrkosten für die aktive Replica liegen bei ca. 3,5k EUR pro Monat – das ist vertretbar."}
{"ts": "162:59", "speaker": "I", "text": "Gibt es noch offene Risiken trotz dieser Maßnahme?"}
{"ts": "163:03", "speaker": "E", "text": "Ja, das Risiko eines gleichzeitigen Ausfalls von Primär- und Backup-Region bleibt bestehen, wenn auch mit 0,03 % Eintrittswahrscheinlichkeit laut Risk-Assessment RA-DR-2025-Q2."}
{"ts": "163:10", "speaker": "I", "text": "Wie würden Sie in so einem Szenario vorgehen?"}
{"ts": "163:14", "speaker": "E", "text": "Unser Plan C, dokumentiert in RB-DR-010, sieht vor, temporär auf die Cloud-Burst-Kapazitäten in West-3 auszuweichen. Das wurde beim letzten GameDay nur in einer Simulation getestet, nicht live."}
{"ts": "163:22", "speaker": "I", "text": "Haben Sie Belege, dass dieser Plan funktioniert?"}
{"ts": "163:26", "speaker": "E", "text": "Ja, die Simulation vom 12. April ist im Drill-Bericht TEST-DR-2025-Q2-ANNEX dokumentiert. Nimbus Observability zeigte, dass 95 % der Services innerhalb von 15 Minuten wieder verfügbar waren."}
{"ts": "163:34", "speaker": "I", "text": "Wird dieser Plan C noch weiter verbessert?"}
{"ts": "163:38", "speaker": "E", "text": "Absolut. Wir haben eine RFC in Vorbereitung, RFC-DR-2025-15, die den automatischen DNS-Failover via Poseidon Routing innerhalb von 60 Sekunden triggern soll. Das würde das verbleibende Risiko weiter senken."}
{"ts": "164:07", "speaker": "I", "text": "Sie hatten vorhin DR-OPS-441 erwähnt. Können Sie vielleicht noch genauer ausführen, wie dieses Ticket in den Entscheidungsprozess eingeflossen ist?"}
{"ts": "164:15", "speaker": "E", "text": "Ja, gern. In DR-OPS-441 haben wir die Beobachtung dokumentiert, dass beim Drill im März die DNS-Propagation zwischen Region West-2 und Central-1 knapp 90 Sekunden länger dauerte als unser Zielwert laut SLA-ORI-02. Das war der Auslöser, die TTL-Werte im Runbook RB-DR-001 umzustellen."}
{"ts": "164:29", "speaker": "I", "text": "Das heißt, die Anpassung der TTL-Werte war eine direkte Folge dieser Messung?"}
{"ts": "164:34", "speaker": "E", "text": "Genau. Wir haben über RFC-DR-19 eine Herabsetzung von 300s auf 120s beantragt. Das minimiert zwar die Latenz beim Failover, erhöht aber den Query-Traffic auf die Nameserver. Hier haben wir mit Vesta FinOps abgewogen."}
{"ts": "164:48", "speaker": "I", "text": "Wie wurde dieses Mehr an Traffic in den Kostenmodellen berücksichtigt?"}
{"ts": "164:54", "speaker": "E", "text": "Wir haben im Kostensheet CS-DR-Q2-25 einen Anstieg um etwa 4,5% bei den DNS-Kosten kalkuliert. Vesta FinOps hat das akzeptiert, weil wir dadurch den RTO gemäß SLA-HEL-01 um fast 40 Sekunden verbessern konnten."}
{"ts": "165:07", "speaker": "I", "text": "Gab es dazu auch eine Risikoanalyse?"}
{"ts": "165:12", "speaker": "E", "text": "Ja, Risikoanalyse RA-DR-77. Dort wurde als Restrisiko festgehalten, dass kürzere TTLs bei globalen Resolvern nicht immer sofort greifen, also die Verbesserung nicht in allen Märkten gleich spürbar ist."}
{"ts": "165:24", "speaker": "I", "text": "Wie reagieren Sie darauf operativ, wenn in bestimmten Märkten längere Ausfälle gemeldet werden?"}
{"ts": "165:30", "speaker": "E", "text": "Wir haben in RB-DR-001 einen Abschnitt 'Regional Overrides' ergänzt. Damit kann das NOC-Team gezielt in betroffenen Märkten die CDN-Konfiguration anpassen, um den Traffic schneller in die verfügbare Region zu lenken."}
{"ts": "165:44", "speaker": "I", "text": "Das klingt nach einer engen Verzahnung mit dem CDN-Team?"}
{"ts": "165:49", "speaker": "E", "text": "Absolut. Wir haben eine Schnittstelle zu Poseidon Networking, die es erlaubt, BGP-Communities dynamisch zu setzen. Darüber steuern wir die Routenankündigungen pro Region. Das ist im Drill-Skript DS-POSE-09 beschrieben."}
{"ts": "166:02", "speaker": "I", "text": "Wie testen Sie, dass diese Overrides im Ernstfall tatsächlich greifen?"}
{"ts": "166:07", "speaker": "E", "text": "Wir simulieren im Quartalstest TEST-DR-2025-Q2 gezielte Regional-Blackholes. Nimbus Observability trackt dann die Latenz und den Traffic Shift. In Q1 konnten wir so in 95% der Fälle die Umschaltung unter 60 Sekunden erreichen."}
{"ts": "166:21", "speaker": "I", "text": "Gibt es noch offene Punkte, die Sie für den nächsten Drill angehen möchten?"}
{"ts": "166:27", "speaker": "E", "text": "Ja, wir wollen die Cross-Region-Datenbank-Replikation von asynchron auf semi-synchron umstellen. Das steht als RFC-DR-22 im Review, um das RPO von aktuell 90 Sekunden auf unter 30 zu senken, ohne den Durchsatz massiv zu reduzieren."}
{"ts": "165:43", "speaker": "I", "text": "Lassen Sie uns noch einmal konkret auf die DNS-Propagation eingehen – welche Szenarien betrachten Sie als kritisch?"}
{"ts": "165:48", "speaker": "E", "text": "Kritisch sind vor allem die Fälle, in denen TTL-Werte in externen Resolvern länger gecachet werden als geplant. Das kann bei einem Failover dazu führen, dass Clients noch auf die Primärregion zeigen, obwohl RB-DR-001 bereits den Umschaltprozess abgeschlossen hat."}
{"ts": "165:56", "speaker": "I", "text": "Und wie wird das im Rahmen des Drills simuliert?"}
{"ts": "166:00", "speaker": "E", "text": "Wir nutzen im TEST-DR-2025-Q1 einen Mix aus internen Test-Resolvern und externen Cloud-DNS-Providern. Dabei setzen wir absichtlich hohe TTLs, um Worst-Case-Szenarien zu beobachten. Die Messpunkte werden im Ticket DR-OPS-512 dokumentiert."}
{"ts": "166:08", "speaker": "I", "text": "Sie hatten vorhin die Cross-Region-Datenkonsistenz erwähnt – wie sieht der aktuelle Stand aus?"}
{"ts": "166:13", "speaker": "E", "text": "Wir replizieren synchron für kritische Tabellen im Core-Billing-Service, asynchron für Logs und Metriken. Das reduziert Latenz, birgt aber das Risiko, dass im Failover die letzten Sekunden an Logdaten fehlen. Diese Abwägung ist in RFC-DR-2025-07 festgehalten."}
{"ts": "166:22", "speaker": "I", "text": "Gab es Überlegungen, diese asynchrone Replikation aufzugeben?"}
{"ts": "166:27", "speaker": "E", "text": "Ja, aber die Performance-Einbußen wären signifikant. Unsere Latenz-Tests zeigen, dass wir bei synchroner Replikation in beiden Regionen +40ms hätten, was SLA-HEL-01 gefährden würde."}
{"ts": "166:35", "speaker": "I", "text": "Wie gehen Sie im Drill mit dieser Lücke um?"}
{"ts": "166:39", "speaker": "E", "text": "Wir markieren potenziell unvollständige Datensätze via Flag im Audit-Trail. Das Incident-Playbook PB-DR-INC-03 beschreibt die manuelle Nachführung aus den asynchronen Streams."}
{"ts": "166:46", "speaker": "I", "text": "Können Sie abschätzen, wie oft diese manuelle Nachführung im letzten Jahr nötig war?"}
{"ts": "166:50", "speaker": "E", "text": "In produktiven Failovern gar nicht, in Drills etwa zweimal. Die Lessons Learned sind jeweils in Confluence unter DR-LL-Section-B dokumentiert."}
{"ts": "166:57", "speaker": "I", "text": "Welche Maßnahmen stehen als nächstes auf der Roadmap, um diese Rest-Risiken zu reduzieren?"}
{"ts": "167:02", "speaker": "E", "text": "Wir planen, einen GeoDNS-Dienst mit schnellerer Propagation zu evaluieren und für Logs eine semi-synchrone Replikation zu testen. Dafür erstellen wir ein Pilotprojekt-Ticket DR-PILOT-09 mit Vesta FinOps zur Kostenbewertung."}
{"ts": "167:11", "speaker": "I", "text": "Sehen Sie Zielkonflikte zwischen diesen Maßnahmen und den Budgetvorgaben?"}
{"ts": "167:15", "speaker": "E", "text": "Definitiv. Die GeoDNS-Lösung kostet voraussichtlich 15% mehr pro Monat. Ohne erkennbare SLA-Verbesserung wird Vesta FinOps das ablehnen. Wir müssen daher belastbare Metriken aus dem Pilot liefern, um die Entscheidung zu stützen."}
{"ts": "166:43", "speaker": "I", "text": "Wir hatten vorhin die Latenzreduktion durch direkte Inter-Region Links angesprochen. Können Sie jetzt noch einmal schildern, wie sich das konkret auf das DNS-Propagation-Risiko auswirkt?"}
{"ts": "166:58", "speaker": "E", "text": "Ja, also durch den direkten Link sinkt zwar die Zeit bis die Daten repliziert sind, aber die DNS-Änderung selbst hängt weiter an den TTL-Werten. Wir haben diese in RB-DR-001 auf 60 Sekunden gesetzt, um im Drill schneller umschalten zu können."}
{"ts": "167:22", "speaker": "I", "text": "Gab es dabei Probleme in der letzten Übung, TEST-DR-2025-Q2?"}
{"ts": "167:34", "speaker": "E", "text": "Minimal – im Ticket DR-OPS-441 ist dokumentiert, dass einige Legacy-Resolver die TTLs gecacht haben, obwohl sie abgelaufen waren. Das führte zu 90 Sekunden Verzögerung bei etwa 5% der Requests."}
{"ts": "167:54", "speaker": "I", "text": "Wie haben Sie diese Erkenntnis in Ihre Runbooks aufgenommen?"}
{"ts": "168:06", "speaker": "E", "text": "Wir haben in RB-DR-001 einen zusätzlichen Schritt ergänzt: Pre-announce CNAME mit parallelem Healthcheck in Nimbus Observability. So wird ein Teil der Clients schon vor dem eigentlichen Failover umgeleitet."}
{"ts": "168:28", "speaker": "I", "text": "Und wie interagiert das mit der Cross-Region-Datenkonsistenz, die Sie als zweites Restrisiko genannt hatten?"}
{"ts": "168:42", "speaker": "E", "text": "Das ist ein Trade-off: Wenn wir schneller umleiten, besteht eine kurze Phase, in der die Zielregion noch nicht alle Writes hat. In SLA-HEL-01 ist dafür ein RPO von 30 Sekunden definiert, den wir einhalten müssen – deshalb lassen wir erst bei <15 Sekunden Lags den Switch zu."}
{"ts": "169:08", "speaker": "I", "text": "Wurde dieser Schwellenwert empirisch ermittelt oder rein aus SLA-Anforderungen abgeleitet?"}
{"ts": "169:20", "speaker": "E", "text": "Beides: Wir haben in drei Drills gemessen, dass unter 15 Sekunden Lag keine Inkonsistenzen auf Applikationsebene sichtbar waren. Das passt genau in den SLA-Rahmen, daher haben wir ihn formalisiert."}
{"ts": "169:42", "speaker": "I", "text": "Gab es Überlegungen, den RPO noch weiter zu reduzieren, um auf der sicheren Seite zu sein?"}
{"ts": "169:55", "speaker": "E", "text": "Ja, aber das hätte eine durchgehende synchrone Replikation erfordert, was die Latenz zwischen den Regionen deutlich erhöht hätte. Die Kosten laut Vesta FinOps wären um ca. 40% gestiegen."}
{"ts": "170:16", "speaker": "I", "text": "Wie wurde dieser Kosten-Performance-Konflikt entschieden?"}
{"ts": "170:27", "speaker": "E", "text": "Im Architekturboard haben wir mit Verweis auf RFC-DR-2025-07 beschlossen, den asynchronen Modus beizubehalten, aber die Monitoring-Frequenz zu verdoppeln, um schneller reagieren zu können."}
{"ts": "170:48", "speaker": "I", "text": "Gibt es weitere Runbooks oder Tickets, die diese Entscheidung untermauern?"}
{"ts": "171:03", "speaker": "E", "text": "Neben RB-DR-001 auch RB-DR-004, das den Lag-Monitoring-Prozess beschreibt, und Ticket DR-OPS-452, wo die Entscheidung mit Messdaten der letzten vier Drills hinterlegt ist."}
{"ts": "174:43", "speaker": "I", "text": "Lassen Sie uns jetzt auf die Multi-Region-Strategie eingehen: Wie wirkt sich das konkret auf unsere Kostenstruktur aus, gerade während eines Drills?"}
{"ts": "174:49", "speaker": "E", "text": "Während eines Drills fahren wir alle Regionen parallel hoch, was die Compute- und Storage-Kosten um etwa 35 % über den Normalwert hebt. Wir haben dafür mit Vesta FinOps ein temporäres Budget-Tagging etabliert, das in den FinOps-Dashboards (vgl. Report FIN-DR-03) sichtbar ist."}
{"ts": "174:59", "speaker": "I", "text": "Gab es Optimierungen, die Sie erwogen haben, um diese Spitze zu glätten?"}
{"ts": "175:04", "speaker": "E", "text": "Ja, wir haben evaluiert, ob wir in der Warm-Standby-Region statt vollem Provisioning auf 'Just-in-time Scaling' umstellen (RFC-DR-221). Allerdings hätte das unser RTO von 4 Stunden nach SLA-ORI-02 auf über 6 Stunden verlängert."}
{"ts": "175:15", "speaker": "I", "text": "Also klare Abwägung zwischen Kostenersparnis und SLA-Einhaltung."}
{"ts": "175:18", "speaker": "E", "text": "Genau. Wir haben uns für SLA-Konformität entschieden, weil SLA-HEL-01 besonders für Healthcare-Kunden kritisch ist. Dort ist ein Datenverlust (RPO) von mehr als 15 Minuten nicht akzeptabel."}
{"ts": "175:28", "speaker": "I", "text": "Wie binden Sie in solchen Entscheidungen Abhängigkeiten zu Poseidon Networking ein?"}
{"ts": "175:34", "speaker": "E", "text": "Poseidon liefert das Cross-Region-Routing. Wenn wir auf Just-in-time Scaling gehen würden, müsste Poseidon dynamische Routen in unter 10 Minuten setzen. In TEST-DR-2025-Q1 hat das in einem Szenario 17 Minuten gedauert (siehe Ticket NET-POSE-118)."}
{"ts": "175:47", "speaker": "I", "text": "Das heißt, die Routing-Latenz war ein limitierender Faktor, der gegen die Kostenoptimierung sprach."}
{"ts": "175:51", "speaker": "E", "text": "Richtig. Unser Runbook RB-DR-001 sieht aktuell einen festen Warmstart vor, um genau diese Latenzrisiken auszuschließen."}
{"ts": "175:58", "speaker": "I", "text": "Haben Sie auch Performance-Trade-offs im Bereich Storage geprüft?"}
{"ts": "176:02", "speaker": "E", "text": "Ja, wir haben überlegt, ob wir in der Passiv-Region auf günstigere HDD-Tiers umsteigen. Aber in einem Drill-Szenario DR-OPS-441 hat sich gezeigt, dass der Rehydrierungsprozess von kalten Daten 2 Stunden länger dauerte."}
{"ts": "176:14", "speaker": "I", "text": "Das hätte RTO und möglicherweise RPO verletzt."}
{"ts": "176:17", "speaker": "E", "text": "Genau, deshalb blieb es bei SSD-Replicas. Die höheren Kosten nehmen wir in Kauf, um konsistente und schnelle Failover zu sichern."}
{"ts": "176:24", "speaker": "I", "text": "Können Sie eine Entscheidung nennen, die Sie trotz höherer Kosten bewusst getroffen haben und die durch Evidenz gestützt ist?"}
{"ts": "176:30", "speaker": "E", "text": "Ein Beispiel ist das Beibehalten der synchronen Cross-Region-Replikation. Das belegen wir mit Ticket DR-ARCH-552 und dem Ergebnisprotokoll von GameDay 2025-Q2, wo asynchrone Replikation zu 43 Sekunden Datenlücke führte – für uns inakzeptabel."}
{"ts": "182:43", "speaker": "I", "text": "Wir hatten ja gerade die Kosten-Performance-Thematik angerissen. Können Sie mir bitte noch einmal schildern, wie Sie bei Titan DR konkret mit Vesta FinOps zusammenarbeiten?"}
{"ts": "182:52", "speaker": "E", "text": "Ja, klar. Wir haben wöchentliche Stand-ups mit dem Vesta FinOps-Team. Dort vergleichen wir die aktuellen Ausgaben aus dem Multi-Region Betrieb gegen die im Runbook RB-FIN-004 definierten Zielwerte. Wenn zum Beispiel die Kosten pro Stunde in der aktiven Region über den Schwellenwert aus SLA-ORI-02 steigen, triggern wir eine Optimierungsanalyse."}
{"ts": "183:09", "speaker": "I", "text": "Und diese Analyse, ist die rein technisch oder fließen da auch Risiken ein?"}
{"ts": "183:14", "speaker": "E", "text": "Beides. Technisch prüfen wir etwa, ob wir Instanztypen downgraden können, ohne den RTO von 15 Minuten zu reißen. Gleichzeitig bewerten wir mit dem Risk Register DR-RISK-07, ob dadurch ein höheres Ausfallrisiko entsteht."}
{"ts": "183:27", "speaker": "I", "text": "Gab es zuletzt so eine Situation?"}
{"ts": "183:31", "speaker": "E", "text": "Ja, im Drill TEST-DR-2025-Q2. Da haben wir erwogen, die Standby-Kapazität in Region Ost um 20% zu reduzieren. Laut Ticket OPS-DR-552 hätten wir dadurch 1.200 € pro Monat gespart, aber die Failover-Zeit stieg in den Simulationsläufen auf 17 Minuten. Das wäre ein SLA-Verstoß gewesen."}
{"ts": "183:48", "speaker": "I", "text": "Verstehe. Wie dokumentieren Sie solche Entscheidungen?"}
{"ts": "183:53", "speaker": "E", "text": "Wir nutzen dafür RFC-Dokumente, in diesem Fall RFC-DR-2025-08. Dort steht die Kostenanalyse, die Performance-Metriken und die Risikoeinschätzung drin. Das Dokument liegt dann im Confluence-Bereich 'Titan DR / Architekturentscheidungen'."}
{"ts": "184:07", "speaker": "I", "text": "Wie gehen Sie mit Findings aus solchen Szenarien um, gerade wenn sie sich wiederholen?"}
{"ts": "184:13", "speaker": "E", "text": "Wenn ein Pattern mehrfach auftritt, erstellen wir ein spezifisches Runbook. Zum Beispiel RB-DR-015 beschreibt jetzt genau, wie wir Kapazitätsanpassungen in Standby-Regionen vornehmen und gleichzeitig die RTO-Schwelle überwachen."}
{"ts": "184:26", "speaker": "I", "text": "Das klingt nach einer recht reifen Prozesslandschaft. Gab es in letzter Zeit Fälle, wo Sie trotz Runbook improvisieren mussten?"}
{"ts": "184:33", "speaker": "E", "text": "Ja, während des GameDay im März kam es in der Primärregion zu einer unerwarteten Routing-Anomalie. Laut RB-DR-001 hätten wir den Traffic in 12 Minuten umleiten sollen, aber wir mussten auf Ad-hoc-Skripte aus dem Poseidon Networking-Team zurückgreifen, weil der Standardpfad blockiert war."}
{"ts": "184:50", "speaker": "I", "text": "Und hat das Auswirkungen auf künftige Architekturentscheidungen?"}
{"ts": "184:54", "speaker": "E", "text": "Definitiv. Wir planen im nächsten Quartal ein Update des Routing-Layers, um alternative Pfade vorzudefinieren. Das wird als RFC-DR-2025-12 eingereicht, zusammen mit einer Budgetfreigabe, die wir mit Vesta FinOps abstimmen."}
{"ts": "185:07", "speaker": "I", "text": "Welche Risiken bleiben nach Umsetzung dieser Maßnahmen noch bestehen?"}
{"ts": "185:12", "speaker": "E", "text": "Ein Restrisiko bleibt bei gleichzeitigen Ausfällen in beiden aktiven Regionen. Das ist im DR-RISK-14 dokumentiert. Die Eintrittswahrscheinlichkeit ist laut unserer Analyse 0,5% pro Jahr, aber der Impact wäre hoch. Daher haben wir klare Eskalationspfade im Runbook RB-DR-999 hinterlegt."}
{"ts": "186:43", "speaker": "I", "text": "Lassen Sie uns noch einmal auf die Lessons Learned aus dem letzten Drill zurückkommen – speziell aus TEST-DR-2025-Q1. Gab es ein Finding, das Sie als besonders kritisch eingestuft haben?"}
{"ts": "186:54", "speaker": "E", "text": "Ja, das gravierendste war ein unerwartetes Throttling im Asynchronous Replication Layer zwischen der Region Nord-1 und West-2. Das hat unser RPO von 5 Minuten temporär auf 9 Minuten verschoben, was SLA-ORI-02 verletzen könnte, wenn es nicht gefixt würde."}
{"ts": "187:09", "speaker": "I", "text": "Und wie sind Sie mit diesem Problem umgegangen?"}
{"ts": "187:14", "speaker": "E", "text": "Wir haben sofort eine Ad-hoc-Taskforce gebildet, Runbook RB-DR-001-Sec3 aktiviert, und parallel eine RFC-DR-145 erstellt. Danach haben die Kollegen aus Poseidon Networking die Bandbreitenpolicies angepasst, um Bursts besser zu handhaben."}
{"ts": "187:32", "speaker": "I", "text": "Gab es dabei Abhängigkeiten zu anderen Projekten wie Nimbus Observability?"}
{"ts": "187:37", "speaker": "E", "text": "Absolut. Ohne Nimbus hätten wir das Throttling nicht so schnell sichtbar gemacht. Der Anomalie-Alarm AL-OBS-778 ist genau aus dem Metrikfeed von Nimbus gekommen, noch bevor die Applikationsschicht Fehler meldete."}
{"ts": "187:52", "speaker": "I", "text": "Wie priorisieren Sie solche Findings aus GameDays?"}
{"ts": "187:57", "speaker": "E", "text": "Wir nutzen eine interne Matrix in JIRA-Board DR-OPS, die Impact-Kategorie, SLA-Relevanz und Wiederholungswahrscheinlichkeit gewichtet. Der Throttling-Fall landete direkt als P1-Ticket DRBUG-9025."}
{"ts": "188:12", "speaker": "I", "text": "Und wie fließen solche Learnings in die Dokumentation zurück?"}
{"ts": "188:17", "speaker": "E", "text": "Nach Abschluss des Fixes haben wir RB-DR-001 um einen neuen Troubleshooting-Abschnitt ergänzt und in der Wissensdatenbank KB-DR-Perf-21 hinterlegt. Das ist jetzt fester Bestandteil künftiger Drills."}
{"ts": "188:31", "speaker": "I", "text": "Gab es bei der Umsetzung Zielkonflikte zwischen Kosten und Performance?"}
{"ts": "188:36", "speaker": "E", "text": "Ja, die dauerhafte Erhöhung der Bandbreitenreserve hätte monatlich ca. 4.500 € gekostet. Stattdessen haben wir ein dynamisches Scaling auf Netzwerkebene implementiert, das nur bei Drill- oder Failover-Events greift."}
{"ts": "188:51", "speaker": "I", "text": "Wie binden Sie Vesta FinOps in solche Entscheidungen ein?"}
{"ts": "188:56", "speaker": "E", "text": "Wir haben ein wöchentliches Alignment-Meeting, in dem wir Cost-Benefit-Szenarien durchgehen. Für das Scaling gab es ein gemeinsames Sheet, das die Lastprofile aus Nimbus mit den Kostensätzen von Vesta kombiniert."}
{"ts": "189:11", "speaker": "I", "text": "Bleiben trotz dieser Maßnahmen Risiken bestehen?"}
{"ts": "189:16", "speaker": "E", "text": "Ja, Residual Risk RR-DR-07 besagt, dass bei simultanen Multi-Region-Ausfällen das dynamische Scaling nicht schnell genug greift. Das ist im Risk-Register dokumentiert und im Runbook als Ausnahmefall markiert."}
{"ts": "194:43", "speaker": "I", "text": "Lassen Sie uns jetzt noch einmal auf die Lessons Learned aus dem letzten Drill zurückkommen. Gab es Punkte, die Sie sofort in die Architektur eingearbeitet haben?"}
{"ts": "194:58", "speaker": "E", "text": "Ja, wir haben direkt nach TEST-DR-2025-Q1 die Sequenz im Failover-Skript RB-DR-001 angepasst. Es gab einen Schritt, der in der Priorisierung zu spät kam – konkret das Umschalten der internen DNS in Region Nord. Das haben wir im Runbook v4.2 jetzt nach vorn gezogen."}
{"ts": "195:20", "speaker": "I", "text": "Wie haben Sie diese Änderung validiert, bevor sie produktiv ging?"}
{"ts": "195:31", "speaker": "E", "text": "Wir haben einen kontrollierten Simulationslauf auf der Staging-Umgebung gefahren, inklusive 12-minütigem kompletten Netzwerkcut zu Region Ost. Die Metriken im Nimbus Observability-Board haben uns gezeigt, dass die DNS-Umschaltung nun in unter 45 Sekunden erfolgt."}
{"ts": "195:55", "speaker": "I", "text": "Interessant. Und wie fließt das in die SLA-Parameter, insbesondere SLA-ORI-02, ein?"}
{"ts": "196:07", "speaker": "E", "text": "SLA-ORI-02 hat ein RTO von 15 Minuten und ein RPO von 5 Minuten. Durch die Anpassung verkürzen wir den Pfad, was uns mehr Puffer gibt, um verbleibende Tasks bei komplexeren Incidents – wie Storage-Replikationsfehlern – zu handeln."}
{"ts": "196:28", "speaker": "I", "text": "Gab es beim Drill auch Interaktionen mit Poseidon Networking, die unvorhergesehene Effekte hatten?"}
{"ts": "196:39", "speaker": "E", "text": "Ja, beim Umschalten des BGP-Peering hat Poseidon einen Backup-Link aktiviert, der 30 ms höhere Latenz brachte. Das war per se tolerierbar, aber in Kombination mit einem langsamen Storage-ACK hat sich das summiert. Wir haben jetzt ein Pre-Warm-Skript hinzugefügt, das diese Links früh initialisiert."}
