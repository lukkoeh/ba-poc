{"ts": "00:00", "speaker": "I", "text": "Können Sie mir bitte zunächst den aktuellen Stand von Atlas Mobile aus Ihrer Sicht schildern?"}
{"ts": "01:15", "speaker": "E", "text": "Ja, klar. Also, wir sind aktuell mitten in der Pilotphase, mit etwa 120 internen Testnutzern. Der Kern ist die Cross-Platform-App, bei der wir gerade die Offline-Synchronisation in realen Nutzungsszenarien beobachten. Ich als UX Lead bin dafür verantwortlich, dass die Nutzererfahrung sowohl auf iOS als auch Android konsistent ist, auch wenn Feature Flags unterschiedliche Pfade aktivieren."}
{"ts": "05:00", "speaker": "I", "text": "Und welche Hauptverantwortlichkeiten haben Sie konkret in dieser Phase?"}
{"ts": "06:10", "speaker": "E", "text": "Zum einen leite ich die Definition unserer UX-Standards, zum Beispiel in DS-ATLAS v2, und überprüfe deren Umsetzung. Zum anderen koordiniere ich mit Mobile-Engineering, wenn neue Komponenten unter einem Flag getestet werden. Außerdem arbeite ich eng mit QA zusammen, um sicherzustellen, dass die Test-Cases auch die UX-Kriterien abbilden. Das ist in unserem Runbook RB-UX-014 festgehalten."}
{"ts": "10:45", "speaker": "I", "text": "Wie stellen Sie sicher, dass neue UI-Komponenten aus DS-ATLAS v2 konsistent ausgerollt werden?"}
{"ts": "12:00", "speaker": "E", "text": "Wir haben im Deployment-Plan eine Synchronisierung zwischen dem UI-Library-Repository und dem Feature-Flag-Management. Über ein internes Tool namens FlagSync können wir komponentenspezifische Flags setzen, die nur in bestimmten Builds aktiv sind. QA prüft das gegen die Style-Guidelines, die in DS-ATLAS v2 dokumentiert sind."}
{"ts": "16:30", "speaker": "I", "text": "Gibt es Prozesse oder Runbooks, die Sie bei der Aktivierung oder Deaktivierung bestimmter Features unterstützen?"}
{"ts": "18:00", "speaker": "E", "text": "Ja, das ist im Runbook RB-FLG-002 definiert. Dort ist beschrieben, dass vor Aktivierung ein dreistufiger Check erfolgt: visuelles Review durch UX, Funktionstest durch QA und Performance-Messung durch das Observability-Team. Erst wenn alle drei Checks bestehen, wird das Flag in der Staging-Umgebung aktiviert."}
{"ts": "22:15", "speaker": "I", "text": "Wie gehen Sie mit der Herausforderung um, dass Feature Flags unterschiedliche UI-Zustände erzeugen können?"}
{"ts": "23:45", "speaker": "E", "text": "Das ist tatsächlich tricky. Wir nutzen Snapshot-Tests, die alle möglichen Flag-Kombinationen durchspielen. Zusätzlich haben wir ein internes Dokument, den 'State Matrix Plan', der beschreibt, welche UI-Zustände zulässig sind. Der Abgleich passiert dann halbautomatisch über ein Script, das wir zusammen mit Engineering entwickelt haben."}
{"ts": "28:20", "speaker": "I", "text": "Arbeiten Sie mit dem Nimbus Observability Team zusammen, um UX-bezogene Metriken zu sammeln?"}
{"ts": "30:00", "speaker": "E", "text": "Genau, wir haben mit Nimbus ein eigenes Dashboard eingerichtet, das Metriken wie 'Time to Interactive', Scroll-Jitter und Offline-Queue-Länge erfasst. Die Daten kommen aus der Telemetrie, die im Mobile Core integriert ist, und werden nach User-Segment gefiltert. So können wir sehen, ob z.B. die Offline-Erfahrung bei Low-End-Geräten schlechter ist."}
{"ts": "36:15", "speaker": "I", "text": "Wie nutzen Sie quantitative Daten, um qualitative UX-Forschung zu ergänzen?"}
{"ts": "38:00", "speaker": "E", "text": "Wir identifizieren aus den Telemetrie-Daten zunächst Anomalien, z.B. längere Ladezeiten bei bestimmten Flag-Kombinationen. Dann führen wir gezielte User-Interviews mit den betroffenen Testnutzern durch. Dieses Vorgehen ist in unserem Forschungsplan FP-ATL-05 beschrieben und hilft uns, die Zahlen mit konkreten Erlebnissen zu hinterlegen."}
{"ts": "44:30", "speaker": "I", "text": "Können Sie ein Beispiel nennen, wo Observability-Daten direkt zu einer UX-Änderung führten?"}
{"ts": "46:00", "speaker": "E", "text": "Ja, im März hatten wir Ticket UX-BUG-221, wo die Offline-Queue bei schwachem Netz überlief. Die Metriken zeigten eine 40% höhere Abbruchrate. Wir haben dann den Sync-Indikator im UI deutlicher gemacht und eine Retry-Logik eingebaut. Seitdem ist die Abbruchrate in diesem Szenario um 70% gesunken."}
{"ts": "90:00", "speaker": "I", "text": "Sie hatten vorhin ja schon den Zusammenhang zwischen DS-ATLAS v2, Feature Flags und Observability erläutert. Mich würde jetzt interessieren: welche spezifischen Risiken sehen Sie aktuell für die UX-Qualität, wenn Atlas Mobile von der Pilot- in die Scale-Phase geht?"}
{"ts": "90:18", "speaker": "E", "text": "Das größte Risiko ist aus meiner Sicht die Fragmentierung der UI-State-Logik. Wir haben im Pilot zehn aktive Feature Flags, die teilweise tief in die Navigation eingreifen. Wenn wir das in die Scale-Phase mit mehr Plattformvarianten tragen, steigt die Wahrscheinlichkeit von Inkonsistenzen. In Ticket UX-432 haben wir schon dokumentiert, dass zwei Flags in Kombination zu einem unzugänglichen Menü geführt haben."}
{"ts": "90:48", "speaker": "I", "text": "Und wie gehen Sie mit solchen Erkenntnissen um? Gibt es einen etablierten Prozess, um diese Risiken systematisch zu mitigieren?"}
{"ts": "91:02", "speaker": "E", "text": "Ja, wir haben im Runbook RB-UX-07 einen Abschnitt „Flag Interaction Matrix“ eingeführt. Darin werden alle möglichen Kombinationen visualisiert und mit Accessibility-Checks versehen. Bevor ein Flag in die Staging-Umgebung geht, muss diese Matrix aktualisiert werden. Das reduziert die Gefahr, dass ein Edge Case erst in Produktion auffällt."}
{"ts": "91:28", "speaker": "I", "text": "Gab es in letzter Zeit konkrete Trade-offs zwischen Performance und Accessibility, die Sie dokumentiert haben?"}
{"ts": "91:41", "speaker": "E", "text": "Ja, ein prominentes Beispiel ist die Offline-Sync-Animation. Wir hatten eine sehr flüssige Lottie-Animation, die aber auf älteren Geräten zu merklichen Frame-Drops führte. In RFC-UX-19 haben wir entschieden, die Animation auf eine statische Illustration zu reduzieren, wenn das Gerät unter einer definierten GPU-Leistung liegt. Das verschlechtert minimal die visuelle Klarheit, verbessert aber die Bedienbarkeit für Screenreader-Nutzer deutlich."}
{"ts": "92:12", "speaker": "I", "text": "Das heißt, Sie priorisieren in solchen Fällen Barrierefreiheit über optische Perfektion?"}
{"ts": "92:20", "speaker": "E", "text": "Genau. Unsere Heuristik lautet: „Zugänglichkeit first, Ästhetik second, solange die Markenidentität nicht verletzt wird.“ Wir haben das auch in den UX-SLAs (SLA-UX-2.3) verankert, die besagen, dass keine Funktion ausgeliefert werden darf, die WCAG 2.1 Level AA unterschreitet, selbst wenn Performance-Gewinne locken."}
{"ts": "92:46", "speaker": "I", "text": "Wie planen Sie, diese dokumentierten Trade-offs in künftigen Iterationen zu adressieren?"}
{"ts": "92:58", "speaker": "E", "text": "Wir wollen adaptive Assets einführen: also mehrere Varianten einer Animation oder Komponente, die je nach Gerät und Nutzerpräferenz automatisch gewählt werden. Dazu wird das Telemetrie-Modul erweitert, um Device Capability Scores in Echtzeit zu liefern. Das ist in Jira-EPIC UX-ADAPT-01 abgebildet, mit Ziel-Q3 für die ersten Prototypen."}
{"ts": "93:28", "speaker": "I", "text": "Klingt spannend. Gibt es Risiken bei der Umsetzung dieser adaptiven Ansätze?"}
{"ts": "93:37", "speaker": "E", "text": "Ja, vor allem Komplexität in der Testmatrix. Jede zusätzliche Asset-Variante multipliziert den Testaufwand, besonders bei Offline-Funktionalität. Wir haben mit QA vereinbart, einen automatisierten Snapshot-Vergleich in den CI-Pipelines zu integrieren, um visuelle Regressionen früh zu erkennen."}
{"ts": "94:02", "speaker": "I", "text": "Wie behalten Sie bei so vielen Variablen den Überblick in der Pilot- zu Scale-Transition?"}
{"ts": "94:12", "speaker": "E", "text": "Wir setzen auf ein zentrales UX-Dashboard, das aus dem Nimbus Observability Backend gespeist wird. Dort laufen Feature Flag Status, Accessibility Scores und Performance Metriken in einer Ansicht zusammen. Das erlaubt uns, Trends zu sehen und frühzeitig Trade-offs neu zu bewerten."}
{"ts": "94:38", "speaker": "I", "text": "Zum Abschluss: welche nächsten Schritte stehen für Sie in den kommenden vier Wochen an, um die UX-Risiken weiter zu minimieren?"}
{"ts": "94:50", "speaker": "E", "text": "Wir finalisieren die Flag Interaction Matrix für die letzten drei Pilotfeatures, rollen das adaptive Asset-Framework im internen Beta-Channel aus und führen einen Accessibility-Bug-Bash mit gemischten Teams durch. Ziel ist, vor der Scale-Entscheidung alle kritischen UX-Issues aus den Tickets UX-400 bis UX-450 zu schließen."}
{"ts": "98:00", "speaker": "I", "text": "Lassen Sie uns bitte noch einmal konkret auf die Risiken eingehen, die Sie beim Übergang von der Pilot- zur Scale-Phase sehen."}
{"ts": "98:09", "speaker": "E", "text": "Ja, also das größte Risiko ist tatsächlich, dass unsere Offline-Sync-Logik unter höherer Last nicht mehr stabil bleibt. In der Pilotphase hatten wir laut Ticket UX-2145 maximal 500 gleichzeitige Nutzer, im Scale reden wir von 10.000 plus."}
{"ts": "98:25", "speaker": "I", "text": "Und wie spiegelt sich das in Ihrer UX-Planung wider?"}
{"ts": "98:32", "speaker": "E", "text": "Wir haben bereits im Runbook RB-ATL-OPS-07 festgelegt, wie wir visuelle Feedback-Elemente anpassen, sobald Syncs länger als 3 Sekunden dauern. Das verhindert 'UI Freeze' Wahrnehmung."}
{"ts": "98:48", "speaker": "I", "text": "Gab es dabei Zielkonflikte zwischen Performance und Accessibility?"}
{"ts": "98:55", "speaker": "E", "text": "Ja, klar. Zum Beispiel beim Einsatz von Skeleton Screens: Performance-mäßig super, aber für Screenreader initially verwirrend. Wir haben deshalb in RFC-UX-098 beschrieben, wie ARIA-Live Regionen ergänzt werden."}
{"ts": "99:12", "speaker": "I", "text": "Wie dokumentieren Sie diese Trade-offs intern?"}
{"ts": "99:18", "speaker": "E", "text": "Über das Decision Log in Confluence, gekoppelt an Jira. Für obiges Beispiel siehe DEC-ATL-202, wo sowohl Lighthouse Metriken als auch Accessibility Audits verlinkt sind."}
{"ts": "99:35", "speaker": "I", "text": "Gibt es auch Risiken in Bezug auf Feature Flags?"}
{"ts": "99:42", "speaker": "E", "text": "Ja, wenn Flags gleichzeitig aktiviert werden, kann es zu ungetesteten UI-Kombinationen kommen. Unser Runbook RB-ATL-FLG-03 sieht dafür eine Staging-Testmatrix vor."}
{"ts": "99:58", "speaker": "I", "text": "Wie wollen Sie diese Risiken in den kommenden Iterationen adressieren?"}
{"ts": "100:04", "speaker": "E", "text": "Wir planen, das Observability-Dashboard um UX-Latency-KPIs zu erweitern, sodass wir bei Rollouts in Echtzeit sehen, ob Nutzer länger auf Interaktionen warten müssen."}
{"ts": "100:18", "speaker": "I", "text": "Und was ist mit der Schulung der Teams?"}
{"ts": "100:24", "speaker": "E", "text": "Das ist Teil unseres Maßnahmepakets MA-ATL-Q4. Darin ist ein Training für Dev und QA zu barrierefreien Patterns vorgesehen, um Missverständnisse bei der Implementierung zu vermeiden."}
{"ts": "100:38", "speaker": "I", "text": "Können Sie abschließend sagen, wie Sie Erfolg in der Scale-Phase messen werden?"}
{"ts": "100:44", "speaker": "E", "text": "Ja, wir kombinieren quantitative Metriken wie Task Completion Time und Error Rate mit qualitativen Remote-Interviews. Unser SLA sieht vor, dass 95% der kritischen Interaktionen unter 2 Sekunden bleiben, bei gleichzeitiger Einhaltung der WCAG-2.1-AA-Standards."}
{"ts": "114:00", "speaker": "I", "text": "Kommen wir noch einmal auf die Risiken zurück, die Sie beim Übergang vom Pilot- in den Scale-Betrieb sehen. Können Sie das etwas konkretisieren?"}
{"ts": "114:07", "speaker": "E", "text": "Ja, klar. Ein zentrales Risiko ist, dass unser Offline-Sync-Mechanismus unter höherer Last nicht konstant die Latenzzeiten einhält, die wir im SLA-Entwurf für Scale (max. 2,5 Sekunden pro Delta-Sync) festgelegt haben. Das könnte die wahrgenommene Performance beeinträchtigen."}
{"ts": "114:17", "speaker": "I", "text": "Und wie wirkt sich das auf die Accessibility aus?"}
{"ts": "114:21", "speaker": "E", "text": "Bei Screenreadern zum Beispiel haben wir in einigen Tests festgestellt, dass verzögerte Syncs zu veralteten Statusmeldungen führen. Das ist in Ticket UX-178 dokumentiert und im Runbook RB-UX-05 gibt es einen Abschnitt, wie man Synchronisationsmeldungen priorisiert, um diese Diskrepanz zu vermeiden."}
{"ts": "114:35", "speaker": "I", "text": "Gab es da schon einen Trade-off, den Sie bewusst eingegangen sind?"}
{"ts": "114:39", "speaker": "E", "text": "Ja, wir haben in Sprint 22 die Entscheidung getroffen, den Rendering-Thread für visuelle Komponenten zu priorisieren, um optisch flüssig zu bleiben, was aber bedeutet, dass ARIA-Live-Region-Updates manchmal 200–300 ms später kommen. Das ist in unserem internen Design Decision Log DDL-14 vermerkt."}
{"ts": "114:53", "speaker": "I", "text": "Wie planen Sie, das in den nächsten Iterationen zu verbessern?"}
{"ts": "114:58", "speaker": "E", "text": "Wir wollen die Live-Region-Updates in einen separaten Worker auslagern, der vom Rendering entkoppelt ist. Dafür haben wir bereits ein Proof-of-Concept im Branch 'ux/a11y-worker' und planen in Iteration 25 ein Performance-Benchmarking."}
{"ts": "115:10", "speaker": "I", "text": "Welche Rolle spielt dabei die Zusammenarbeit mit dem Nimbus Observability Team?"}
{"ts": "115:14", "speaker": "E", "text": "Eine große. Wir haben zusammen Metriken definiert, z. B. 'a11y_update_lag_ms', die wir in den Telemetrie-Streams aufnehmen. So sehen wir im Atlas-Dashboard sofort, wenn die Verzögerung über 250 ms steigt."}
{"ts": "115:26", "speaker": "I", "text": "Nutzen Sie diese Daten auch, um Entscheidungen live während eines Deployments zu treffen?"}
{"ts": "115:31", "speaker": "E", "text": "Ja, wir haben in Runbook RB-DEP-09 einen Abschnitt 'Canary UX Check', der vorschreibt, dass bei mehr als 5 % Sessions mit Lag über 300 ms der Rollout gestoppt und das Feature-Flag zurückgesetzt wird. Das ist uns im letzten Pilot-Release passiert, und wir konnten so größeren Impact vermeiden."}
{"ts": "115:47", "speaker": "I", "text": "Gibt es weitere Maßnahmen, um die Risiken beim Scaling zu mindern?"}
{"ts": "115:51", "speaker": "E", "text": "Wir planen eine Staffelung der Feature-Flag-Aktivierungen nach Nutzersegmenten. Zuerst die 'Beta-Plus'-Gruppe mit hoher Fehlertoleranz, dann sukzessive breitere Segmente. Das erlaubt uns, UX-bezogene Anomalien früh zu erkennen und gegenzusteuern."}
{"ts": "116:03", "speaker": "I", "text": "Wie dokumentieren Sie diese Entscheidungen für das gesamte Team?"}
{"ts": "116:07", "speaker": "E", "text": "Alle relevanten Beschlüsse landen im Confluence-Bereich 'Atlas UX Decisions', verlinkt mit Jira-Tickets wie UX-178 oder PER-44, und mit Querverweisen auf die betroffenen Runbooks. So können QA, DevOps und UX jederzeit nachvollziehen, welche Trade-offs bewusst eingegangen wurden."}
{"ts": "116:00", "speaker": "I", "text": "Könnten Sie noch einmal konkret erläutern, wie Sie die Learnings aus der Pilotphase dokumentieren, bevor Sie in die Scale-Phase gehen?"}
{"ts": "116:10", "speaker": "E", "text": "Ja, wir haben dafür ein internes Confluence-Space, in dem jedes Subteam sein Kapitel hat. Mein UX-Teil enthält z.B. eine Tabelle mit allen Feature-Flag-Tests, referenziert auf die entsprechenden Tickets wie UX-ATL-549, und ergänzt um qualitative Notizen aus den Remote-Usability-Tests."}
{"ts": "116:32", "speaker": "I", "text": "Und wie fließen diese Notizen in die Arbeit der Mobile-Engineering-Kollegen ein?"}
{"ts": "116:40", "speaker": "E", "text": "Wir haben wöchentlich ein Sync-Meeting mit Mobile-Engineering und QA. Dort gehe ich die UX-Änderungsvorschläge durch, und wir verlinken direkt in JIRA auf die Dev-Tasks, etwa DEV-ATL-334, sodass klar ist, welche UI-States in welchem Branch getestet wurden."}
{"ts": "117:02", "speaker": "I", "text": "Gab es in letzter Zeit ein Beispiel, wo solche direkte Verlinkung besonders wertvoll war?"}
{"ts": "117:10", "speaker": "E", "text": "Ja, beim Offline-Sync-Indikator. Wir hatten drei verschiedene Designs in DS-ATLAS v2 unter unterschiedlichen Flags. Ohne diese klare Ticket-Verknüpfung hätten wir vermutlich einen älteren Entwurf versehentlich in den Main-Merge übernommen."}
{"ts": "117:28", "speaker": "I", "text": "Sie hatten vorhin erwähnt, dass Sie auch mit dem Nimbus Observability Team arbeiten. Inwiefern half das bei der Offline-Sync-Optimierung?"}
{"ts": "117:38", "speaker": "E", "text": "Das war ein klassischer Multi-Hop: Wir haben aus den Telemetriedaten gesehen, dass 18% der Nutzer während des Syncs das App-Tab wechseln. In Kombination mit unserem UX-Testfeedback deutete das darauf hin, dass der Ladeindikator zu wenig auffällig war. Daraus entstand dann RFC-ATL-88 zur Anpassung der visuellen Hierarchie."}
{"ts": "118:05", "speaker": "I", "text": "Wie haben Sie diese Anpassung priorisiert?"}
{"ts": "118:12", "speaker": "E", "text": "Wir nutzen eine gewichtete Entscheidungsmatrix im UX-Board, die sowohl den Impact auf Core-Journeys als auch die technischen Risiken bewertet. Beim Indikator lag der gewichtete Score bei 8,5 von 10, also klar hohe Priorität."}
{"ts": "118:30", "speaker": "I", "text": "Gab es dabei Abhängigkeiten zu anderen Komponenten?"}
{"ts": "118:36", "speaker": "E", "text": "Ja, das Icon-Set in DS-ATLAS v2 musste erweitert werden, was wiederum im Build-Pipeline-Runbook RN-DS-41 beschrieben ist. Wir mussten sicherstellen, dass die neuen SVGs in beiden Mobile-Plattformen kompatibel sind."}
{"ts": "118:55", "speaker": "I", "text": "Wie stellen Sie sicher, dass die Accessibility-Anforderungen auch bei solchen Änderungen nicht untergehen?"}
{"ts": "119:02", "speaker": "E", "text": "Wir haben im QA-Runbook RN-QA-12 einen Accessibility-Checkpoint, der vor jedem Feature-Flag-Enable auf der Staging-Umgebung durchgeführt wird. Dazu gehört u.a. ein Test mit Screenreadern, der sicherstellt, dass neue Indikatoren korrekt angekündigt werden."}
{"ts": "119:22", "speaker": "I", "text": "Und wenn bei diesem Check ein Problem gefunden wird?"}
{"ts": "119:28", "speaker": "E", "text": "Dann wird das Flag nicht aktiviert, und wir erstellen ein Blocker-Ticket, z.B. UX-ATL-572, das erst geschlossen werden muss, bevor das Feature live geht. Diese Policy ist fest in unserer internen SLA-Doku UX-SLA-03 verankert."}
{"ts": "132:00", "speaker": "I", "text": "Bevor wir zum Abschluss kommen, würde ich gern noch einmal auf die Zusammenarbeit mit dem Nimbus Observability Team eingehen. Gab es in letzter Zeit ein konkretes Beispiel, wo deren Daten Ihre UX-Entscheidungen beeinflusst haben?"}
{"ts": "132:12", "speaker": "E", "text": "Ja, tatsächlich. Vor drei Wochen hatten wir in den Telemetrie-Dashboards von Nimbus einen ungewöhnlichen Anstieg der Offline-Sync-Fehler. Laut Runbook OBS-UX-04 prüfen wir dann zuerst die Client-Logs und die Sync-Queue-Latenzen. Dabei fiel auf, dass Nutzer mit Screen-Readern häufiger in diesen Fehlerzustand liefen."}
{"ts": "132:32", "speaker": "I", "text": "Das klingt nach einer interessanten Verbindung zwischen Accessibility und technischen Metriken. Wie haben Sie darauf reagiert?"}
{"ts": "132:42", "speaker": "E", "text": "Wir haben daraufhin ein Hotfix-Flag gesetzt, um die Reihenfolge der UI-Elemente im Offline-Modus anzupassen. Das war in Feature Flag ATLAS-OFF-ACC-12 dokumentiert, und wir haben es innerhalb von 48 Stunden ausgerollt, weil unser SLA für kritische Accessibility-Bugs bei 72 Stunden liegt."}
{"ts": "133:02", "speaker": "I", "text": "Gab es eine formale Auswertung danach?"}
{"ts": "133:10", "speaker": "E", "text": "Ja, wir haben ein Post-Mortem erstellt, PM-UX-2024-05, in dem wir die Korrelation zwischen der Screen-Reader-Fokus-Logik und den Sync-Threads detailliert beschrieben haben. Das führte zu einer Ergänzung im Designsystem DS-ATLAS v2, Kapitel 5.3: 'Accessible Offline States'."}
{"ts": "133:34", "speaker": "I", "text": "Interessant. Bedeutet das auch Änderungen an Ihren Testplänen?"}
{"ts": "133:42", "speaker": "E", "text": "Genau, wir haben in unserem QA-Runbook QA-OFF-ACC-02 einen neuen Schritt eingeführt: simulierte Offline-Szenarien mit aktivierten Screen-Readern. Das ist jetzt Pflicht in jeder Pilot-Build-Review."}
{"ts": "134:00", "speaker": "I", "text": "Wie fließen solche Erkenntnisse in die Kommunikation mit den Stakeholdern ein?"}
{"ts": "134:08", "speaker": "E", "text": "Wir bereiten monatliche UX-Health-Reports vor, in denen auch Observability-Funde enthalten sind. Für den genannten Fall haben wir einen Abschnitt 'Cross-impact Accessibility & Sync' eingefügt, um die Produktowner für die Wechselwirkungen zu sensibilisieren."}
{"ts": "134:28", "speaker": "I", "text": "Gibt es dabei auch eine Priorisierung mit Blick auf die Roadmap?"}
{"ts": "134:36", "speaker": "E", "text": "Ja, wir nutzen ein internes Scoring, das sowohl die Häufigkeit des Problems als auch den Schweregrad für die Nutzbarkeit bewertet. In diesem Fall 8 von 10, was bedeutet, dass es maximal eine Iteration dauern darf, bis ein permanenter Fix im Mainline-Code ist."}
{"ts": "134:56", "speaker": "I", "text": "Und wie stellen Sie sicher, dass solche Fixes nicht unbeabsichtigt andere Teile der App beeinträchtigen?"}
{"ts": "135:04", "speaker": "E", "text": "Wir haben dafür Canary-Releases mit gezieltem Rollout an 5% der Pilotnutzer. Die Observability-Metriken werden dabei im 15-Minuten-Intervall geprüft, und bei Regressionen zieht ein automatisches Rollback-Skript, gemäß Deployment-Runbook DEP-ATLAS-07."}
{"ts": "135:24", "speaker": "I", "text": "Das klingt sehr robust. Haben Sie abschließend noch ein Beispiel, wo eine klein wirkende UX-Änderung messbar positive Effekte hatte?"}
{"ts": "135:36", "speaker": "E", "text": "Ja, die Umstellung der Sync-Statusanzeige von reinem Text auf ein kombiniertes Icon-Text-Element. Laut Nimbus-Logs ging die Fehlbedienungsrate um 15% zurück, und das Nutzerfeedback in den wöchentlichen Surveys war durchweg positiv."}
{"ts": "136:00", "speaker": "I", "text": "Bevor wir abschließen, würde mich noch interessieren, ob Sie für den nächsten Sprint konkrete UX-Metriken festlegen, um die Wirksamkeit der Anpassungen zu messen."}
{"ts": "136:05", "speaker": "E", "text": "Ja, wir haben im Sprint-Backlog für Sprint 14 die Metriken 'Time to Interactive' und 'Offline Success Rate' als Key UX Indicators vermerkt. Diese sind in unserem Runbook RB-UX-14.2 dokumentiert und werden über das Nimbus Observability Dashboard getrackt."}
{"ts": "136:15", "speaker": "I", "text": "Und wie binden Sie das Team daran, diese Werte wirklich kontinuierlich zu beobachten?"}
{"ts": "136:20", "speaker": "E", "text": "Wir haben eine tägliche Stand-up-Agenda-Erweiterung eingeführt, in der der QA-Lead kurz die aktuellen Werte aus der letzten Build-Pipeline nennt. Das ist keine harte KPI-Kontrolle, sondern eher ein Awareness-Mechanismus, der uns frühzeitig auf Abweichungen hinweist."}
{"ts": "136:33", "speaker": "I", "text": "Gab es da schon einen Fall, wo so eine Abweichung unmittelbar zu einer Designanpassung geführt hat?"}
{"ts": "136:38", "speaker": "E", "text": "Ja, letzte Woche hat der QA-Lead gemeldet, dass die Offline Success Rate im Beta-Test Build 0.9.21 um 12% gefallen ist. Wir haben daraufhin im UX-Team die Retry-Mechanik für den Sync-Button aus Ticket UX-421 neu gestaltet, um visuell klarer zu kommunizieren, wann ein erneuter Versuch sinnvoll ist."}
{"ts": "136:54", "speaker": "I", "text": "Spannend. Nutzen Sie für solche Fälle ein spezielles Protokoll oder eher Ad-hoc-Entscheidungen?"}
{"ts": "137:00", "speaker": "E", "text": "Wir orientieren uns an der Richtlinie GL-UX-Offline-03, die vorsieht, dass bei Drops über 10% innerhalb von zwei Builds sofort ein Hotfix-Designreview stattfinden muss. Das haben wir diesmal genau so umgesetzt."}
{"ts": "137:14", "speaker": "I", "text": "Das heißt, diese Richtlinie ist auch Teil des Onboarding neuer Teammitglieder?"}
{"ts": "137:18", "speaker": "E", "text": "Genau, im Onboarding-Handbuch Kapitel 5.2 ist sie enthalten. Neue Designer*innen müssen in der zweiten Woche eine Simulation durchlaufen, bei der sie anhand historischer Daten wie aus Ticket UX-388 einen solchen Drop analysieren und Gegenmaßnahmen entwerfen."}
{"ts": "137:34", "speaker": "I", "text": "Wie fließt das Feedback aus diesen Simulationen wieder in den realen Prozess ein?"}
{"ts": "137:39", "speaker": "E", "text": "Wir dokumentieren Verbesserungsvorschläge in Confluence unter 'UX Process Improvements'. Alle drei Sprints machen wir ein Retro-Meeting, bei dem wir prüfen, ob etwas davon in die offiziellen Runbooks übernommen wird."}
{"ts": "137:52", "speaker": "I", "text": "Noch eine letzte Frage: gibt es eine geplante Verknüpfung zwischen diesen UX-Kennzahlen und den SLAs, die Sie in der Scale-Phase erreichen wollen?"}
{"ts": "137:58", "speaker": "E", "text": "Ja, wir haben im Entwurf des SLA-Dokuments für Scale-Phase festgelegt, dass die Time to Interactive unter 1,5 Sekunden liegen muss und die Offline Success Rate über 95%. Diese Werte sind direkt aus unseren Pilotmetriken abgeleitet und validiert."}
{"ts": "138:12", "speaker": "I", "text": "Und wenn diese Werte nicht erreicht werden?"}
{"ts": "138:16", "speaker": "E", "text": "Dann greift ein Eskalationspfad: Zunächst ein internes UX-Taskforce-Meeting innerhalb von 24 Stunden, dann eine enge Abstimmung mit Engineering für mögliche Code-Optimierungen. Im Extremfall pausieren wir den Roll-out betroffener Feature Flags, wie in Runbook RB-FF-Stop-02 beschrieben."}
{"ts": "142:00", "speaker": "I", "text": "Bevor wir abschließen, eine Nachfrage: Wie gehen Sie eigentlich vor, wenn in der Beta-Observability-Daten auftauchen, die nicht direkt mit unseren festgelegten UX-Metriken korrelieren?"}
{"ts": "142:08", "speaker": "E", "text": "Das passiert öfter, gerade bei Atlas Mobile. Wir haben im Runbook RB-OBS-44 einen Abschnitt 'Out-of-Scope Signals'. Da definieren wir, wie man z.B. anomale Latenzspitzen aus dem Nimbus-Stream prüft, auch wenn sie nicht in den UX-KPIs sind. Wir nutzen dann ein ad-hoc Query-Template in der Telemetrie-Console und vergleichen mit den QA-Bugreports."}
{"ts": "142:22", "speaker": "I", "text": "Und wenn sich daraus doch ein UX-relevanter Effekt ergibt?"}
{"ts": "142:26", "speaker": "E", "text": "Dann eskalieren wir es in den UX-Debrief-Channel, verlinken das Observability-Ticket – meist im Format OBS-UX-### – und ziehen, je nach Schwere, einen Hotfix-Flag in der nächsten Nightly-Build."}
{"ts": "142:38", "speaker": "I", "text": "Gab es in der Pilotphase ein konkretes Beispiel?"}
{"ts": "142:42", "speaker": "E", "text": "Ja, OBS-UX-219. Da hatten wir plötzlich 18% Abbruchrate im Offline-Sync im Testmarkt Südwest. Die Latenzen waren ok, aber die Retry-Logik hat das UI blockiert. Wir haben innerhalb von 48 Stunden einen Patch eingespielt, Feature Flag 'syncUIv2' aktiviert und mit DS-ATLAS v2-Komponenten neu ausgerollt."}
{"ts": "142:58", "speaker": "I", "text": "Interessant. Wie haben Sie das mit QA koordiniert?"}
{"ts": "143:02", "speaker": "E", "text": "Über das gemeinsame QA-UX-Standup. Da teilen wir die Observability-Diagramme, QA bringt die reproduzierten Steps, und wir einigen uns, ob es in den nächsten Regression-Test-Katalog kommt. In dem Fall kam es unter 'Critical Offline UI Flows' rein."}
{"ts": "143:15", "speaker": "I", "text": "Haben Sie für solche Hotfixes eine SLA definiert?"}
{"ts": "143:19", "speaker": "E", "text": "Ja, im Pilot hatten wir eine interne SLA von 72 Stunden für 'Severe UX Degradation'. OBS-UX-219 lag mit 48 Stunden darunter. In der Scale-Phase wollen wir auf 48 Stunden SLA runtergehen."}
{"ts": "143:31", "speaker": "I", "text": "Klingt ambitioniert. Welche Risiken sehen Sie dabei?"}
{"ts": "143:35", "speaker": "E", "text": "Das Hauptrisiko ist, dass wir bei zu engem SLA Abkürzungen in der Accessibility-Validierung nehmen. Das widerspricht eigentlich unserem Runbook RB-A11Y-02, wo mindestens zwei Geräteklassen geprüft werden sollen. Wir müssen da den Spagat schaffen."}
{"ts": "143:50", "speaker": "I", "text": "Wäre ein gestaffeltes Rollout eine Option?"}
{"ts": "143:54", "speaker": "E", "text": "Ja, da gibt es die 'phasedFlagDeploy'-Routine. Erst 10% der Pilot-User, dann 50% nach 24h, und Vollausrollung nur, wenn keine kritischen Observability-Hits kommen. Damit können wir Accessibility-Checks parallel abschließen."}
{"ts": "144:08", "speaker": "I", "text": "Das heißt, Observability ist hier quasi der Gatekeeper?"}
{"ts": "144:12", "speaker": "E", "text": "Genau. Wir haben für die Scale-Phase schon einen RFC-Entwurf RFC-UXOBS-07, der beschreibt, wie UX-Metriken und technische Telemetrie in einem Gate-Score zusammenfließen. Nur wenn der Score > 85% liegt, darf das Feature-Flag voll aktiviert werden."}
{"ts": "144:00", "speaker": "I", "text": "Bevor wir abschließen, würde ich gern noch auf das Thema Observability zurückkommen: Haben sich seit der letzten Iteration neue UX-Metriken ergeben, die Sie in der Nimbus-Pipeline integriert haben?"}
{"ts": "144:04", "speaker": "E", "text": "Ja, wir haben im letzten Sprint ein neues Event-Tagging für Offline-Fehlermeldungen eingeführt. Das war RFC-42 im internen Confluence. Dadurch können wir in Nimbus nicht nur die Fehlerquote sehen, sondern auch, welche UI-Komponente betroffen ist."}
{"ts": "144:09", "speaker": "I", "text": "Und wie fließt das dann konkret zurück ins Designteam?"}
{"ts": "144:13", "speaker": "E", "text": "Wir haben ein wöchentliches Sync mit dem Data-Team. Dort stellen die Analyst*innen Heatmaps aus den Event-Daten vor. Basierend darauf haben wir z.B. die Position des Retry-Buttons in der Offline-Ansicht angepasst (Ticket UX-234)."}
{"ts": "144:18", "speaker": "I", "text": "Klingt nach einem guten Kreislauf. Gab es dabei technische Herausforderungen?"}
{"ts": "144:22", "speaker": "E", "text": "Ein Problem war die Latenz. Nimbus aggregiert die Daten stündlich, für manche UX-Tests brauchen wir near real-time. Wir haben daher ein Light-Mode-Logging in der App aktiviert, das lokal Puffert und bei Sync hochlädt."}
{"ts": "144:28", "speaker": "I", "text": "Wie wirkt sich das auf die Performance des Offline-Syncs aus?"}
{"ts": "144:32", "speaker": "E", "text": "Minimal, weil wir die Logging-Payload komprimieren. Im Runbook RB-ATL-07 ist genau dokumentiert, wie wir den Threshold bei 150KB setzen, um keine merkbaren Delays zu erzeugen."}
{"ts": "144:38", "speaker": "I", "text": "Haben Sie auch Accessibility-Events im Logging?"}
{"ts": "144:42", "speaker": "E", "text": "Teilweise. Wir loggen z.B., ob Screenreader-Fokus korrekt gesetzt wurde. Das war eine Forderung aus der Accessibility-Richtlinie AC-11. So konnten wir feststellen, dass Feature Flag FF-OFFACC in 8% der Fälle den Fokus verlor."}
{"ts": "144:48", "speaker": "I", "text": "Interessant, und wie sind Sie damit umgegangen?"}
{"ts": "144:52", "speaker": "E", "text": "Wir haben das Flag temporär deaktiviert und in Sprint 21 eine Korrektur implementiert. Das hat Priorität P1 bekommen, weil es direkt die Nutzbarkeit für sehbehinderte User*innen betrifft."}
{"ts": "144:58", "speaker": "I", "text": "Planen Sie, solche Events künftig automatisiert zu testen?"}
{"ts": "145:02", "speaker": "E", "text": "Ja, wir evaluieren gerade ein Tool namens A11ySim, das in unserer Staging-App Screenreader-Events simuliert. Wenn's stabil läuft, integrieren wir es in die CI-Pipeline, ähnlich wie wir's bei den Feature-Flag-UI-Tests schon tun."}
{"ts": "145:08", "speaker": "I", "text": "Zum Abschluss: Gibt es Lessons Learned aus der Observability-Integration, die Sie auch anderen Projekten bei Novereon empfehlen würden?"}
{"ts": "145:12", "speaker": "E", "text": "Definitiv: frühzeitig Metriken definieren, die sowohl technische als auch UX-relevante Fragen beantworten. Und enge Schleifen zwischen Data, Engineering und UX schaffen. Das steht jetzt auch so in unserem internen Leitfaden LG-UX-05."}
{"ts": "145:36", "speaker": "I", "text": "Bevor wir abschließen, würde mich interessieren, ob Sie in Bezug auf die Observability-Daten schon konkrete Dashboards für die nächste Phase vorbereitet haben."}
{"ts": "145:44", "speaker": "E", "text": "Ja, wir haben im Nimbus Observability Portal ein Dashboard namens \"UX Atlas KPIs\" angelegt. Das ist nach dem Runbook RB-OBS-UX-04 konfiguriert und zeigt Metriken wie First Meaningful Paint, Offline-Sync-Latenz und Accessibility-Error-Rate an."}
{"ts": "145:58", "speaker": "I", "text": "Und werden diese Metriken auch in SLAs oder SLOs für die Scale-Phase einfließen?"}
{"ts": "146:02", "speaker": "E", "text": "Teilweise, ja. Für die Pilotphase hatten wir nur interne Benchmarks. Ab Scale-Phase wollen wir laut RFC-ATLAS-SLO-07 einen Zielwert von unter 2,5 Sekunden für den First Meaningful Paint verbindlich machen."}
{"ts": "146:16", "speaker": "I", "text": "Gab es in der Vergangenheit schon Situationen, wo solche Observability-Metriken zu sofortigen UX-Änderungen geführt haben?"}
{"ts": "146:21", "speaker": "E", "text": "Ja, ein Beispiel war Ticket UX-4321: Wir sahen einen Anstieg der Fehlerquote bei Screenreader-Nutzung nach Aktivierung des Feature Flags FF-ATLAS-OFFLINE-BETA. Das hat uns veranlasst, die Navigationshierarchie in der Offline-Ansicht zu überarbeiten."}
{"ts": "146:38", "speaker": "I", "text": "Ah, das heißt, die Feature Flags werden aktiv in Monitoring-Alerts eingebunden?"}
{"ts": "146:42", "speaker": "E", "text": "Genau. Wir taggen alle Events mit dem aktiven Flag-Set, sodass wir im Dashboard nach Flag-Kombinationen filtern können. Das ist im Flag-Management-Runbook RB-FLAG-03 dokumentiert."}
{"ts": "146:55", "speaker": "I", "text": "Wie gehen Sie damit um, wenn die Daten und das qualitative Nutzerfeedback auseinanderdriften?"}
{"ts": "147:00", "speaker": "E", "text": "Wir haben dafür ein wöchentliches Alignment-Meeting mit Research. Wenn z. B. Telemetrie gute Ladezeiten zeigt, Nutzer aber subjektiv von 'langsamer App' sprechen, schauen wir in Session-Replays und prüfen, ob vielleicht ein visuelles Feedback fehlt."}
{"ts": "147:15", "speaker": "I", "text": "Das klingt nach einem hohen Abstimmungsaufwand, aber auch nach einer guten Fehlerabsicherung."}
{"ts": "147:19", "speaker": "E", "text": "Ja, es kostet Zeit, reduziert aber das Risiko, blinde Flecken zu übersehen. Gerade bei komplexen Funktionen wie Offline Sync wollen wir kein Risiko für die Barrierefreiheit eingehen."}
{"ts": "147:30", "speaker": "I", "text": "Sehen Sie für die Kommunikation dieser Metriken noch Verbesserungspotenzial?"}
{"ts": "147:34", "speaker": "E", "text": "Wir überlegen, die wichtigsten KPIs direkt im Designsystem-Portal DS-ATLAS zu spiegeln, damit Designer sie im Kontext der Komponenten sehen. Das würde auch helfen, bei Flag-Änderungen schneller zu reagieren."}
{"ts": "147:48", "speaker": "I", "text": "Spannend, das wäre ja eine unmittelbare Verbindung von Design und Observability."}
{"ts": "147:52", "speaker": "E", "text": "Genau, und es würde den Kreislauf schließen: von der Komponentengestaltung über Feature Flags und Pilotdaten bis hin zu den Anpassungen für die Scale-Phase."}
{"ts": "147:36", "speaker": "I", "text": "Bevor wir zum Abschluss kommen, würde mich noch interessieren, ob Sie in der Pilotphase spezifische SLAs für UX-Aspekte definiert haben."}
{"ts": "147:42", "speaker": "E", "text": "Ja, wir haben sogenannte UX-SLAs formuliert, äh, zum Beispiel: Reaktionszeit bei Touch-Interaktionen maximal 150 ms unter normalen Netzbedingungen. Das steht auch im Runbook RB-ATL-UX-04."}
{"ts": "147:55", "speaker": "I", "text": "Und wie prüfen Sie, ob diese Werte eingehalten werden? Nutzen Sie dafür das Observability-Setup?"}
{"ts": "148:02", "speaker": "E", "text": "Genau, wir haben mit dem Nimbus Team einen Dashboard-Widget gebaut, das die Client-Side Latenzen misst und mit den SLAs abgleicht. Alerts triggern, wenn wir über 10 % Abweichung sehen."}
{"ts": "148:15", "speaker": "I", "text": "Gab es zuletzt solche Alerts?"}
{"ts": "148:19", "speaker": "E", "text": "Vor zwei Wochen, bei einem Beta-Build mit aktiviertem Feature Flag 'offline-share'. Die Latenz stieg, weil die Cache-Verwaltung noch nicht optimiert war. Ticket UX-ATL-227 dokumentiert das."}
{"ts": "148:33", "speaker": "I", "text": "Interessant. Haben Sie daraus direkt Anpassungen ins Design übernommen?"}
{"ts": "148:37", "speaker": "E", "text": "Ja, wir haben den visuellen Ladeindikator angepasst, um wahrgenommene Wartezeit zu verringern, und die asynchrone Pre-Loading-Strategie im Code vorgeschlagen."}
{"ts": "148:49", "speaker": "I", "text": "Wie binden Sie dabei QA ein?"}
{"ts": "148:53", "speaker": "E", "text": "QA bekommt von uns ein Test-Grid, in dem UI-Zustände mit und ohne Flag dokumentiert sind. Sie prüfen das gegen die Design-Spezifikation aus DS-ATLAS v2."}
{"ts": "149:06", "speaker": "I", "text": "Also eine Art Living Document?"}
{"ts": "149:09", "speaker": "E", "text": "Genau, in Confluence als 'Atlas UX State Matrix'. Wir aktualisieren es nach jedem Merge in den Pilot-Branch."}
{"ts": "149:18", "speaker": "I", "text": "Das klingt nach viel Koordinationsaufwand. Gibt es automatisierte Checks?"}
{"ts": "149:22", "speaker": "E", "text": "Teilweise. Wir nutzen ein Skript aus dem Tooling-Repo, das Screenshots in allen Flag-Kombinationen erzeugt und visuell diffed. QA schaut dann nur noch bei Abweichungen rein."}
{"ts": "149:35", "speaker": "I", "text": "Sehr effizient. Denken Sie, dass dieses Setup auch in der Scale-Phase tragfähig ist?"}
{"ts": "149:40", "speaker": "E", "text": "Mit ein paar Erweiterungen ja – vor allem müssen wir die Alert-Logik anpassen, damit sie bei größeren Userzahlen nicht zu viele False Positives produziert."}
{"ts": "152:00", "speaker": "I", "text": "Lassen Sie uns vielleicht noch kurz auf die Lessons Learned aus dem Pilot eingehen – was würden Sie sagen, war die wichtigste UX-Erkenntnis?"}
{"ts": "152:05", "speaker": "E", "text": "Die größte Erkenntnis war, dass unsere Nutzer:innen sehr sensibel auf Latenz im Offline-Sync reagieren. Das steht zwar in keinem SLA, aber wir haben es aus Session-Logs im Nimbus-Dashboard abgeleitet. Die Runbook-Referenz RB-ATL-07 hat uns geholfen, Sync-Indikatoren visuell zu optimieren."}
{"ts": "152:15", "speaker": "I", "text": "Also eher implizite Heuristiken als harte Vertragswerte?"}
{"ts": "152:19", "speaker": "E", "text": "Genau. Im Pilot haben wir gelernt, dass 3 Sekunden gefühlte Wartezeit schon kritisch sind, auch wenn technisch bis zu 8 Sekunden erlaubt wären. Da greifen wir lieber auf Micro-Animations zurück, wie im DS-ATLAS v2 Styleguide beschrieben."}
{"ts": "152:28", "speaker": "I", "text": "Wie haben Sie dieses Feedback dann ins Designsystem zurückgespielt?"}
{"ts": "152:33", "speaker": "E", "text": "Wir haben ein RFC-Dokument, RFC-ATL-UI-22, erstellt, das von Mobile-Engineering und QA gegengezeichnet wurde. Darin sind neue Komponentenstates enthalten, die unter einem Feature Flag 'sync-feedback' ausgerollt werden."}
{"ts": "152:44", "speaker": "I", "text": "Klingt nach enger Verzahnung. Gab es technische Hürden bei der Umsetzung dieses Flags?"}
{"ts": "152:49", "speaker": "E", "text": "Ja, wir mussten den Flag-Parser anpassen, damit er auch UI-spezifische Payloads versteht. Im Ticket ATLAS-FE-1129 ist dokumentiert, wie wir das Schema erweitert haben."}
{"ts": "152:59", "speaker": "I", "text": "Und testen Sie solche UI-Änderungen auch wieder im Offline-Modus?"}
{"ts": "153:03", "speaker": "E", "text": "Absolut, wir nutzen dafür das Offline-Test-Skript aus Runbook RB-ATL-05, Schritt 4 bis 7. Interessanterweise zeigt sich, dass Accessibility-Hinweise im Offline-Banner die wahrgenommene Performance verbessern."}
{"ts": "153:15", "speaker": "I", "text": "Das ist spannend – also eine Art psychologischer Performance-Boost?"}
{"ts": "153:19", "speaker": "E", "text": "Genau, das ist nicht offiziell in den KPIs, aber wir haben in den Observability-Daten einen Rückgang der Abbruchraten um 12% gesehen, nachdem wir den Banner-Text optimiert haben."}
{"ts": "153:28", "speaker": "I", "text": "Würden Sie sagen, dass solche impliziten Effekte künftig mehr Gewicht bekommen sollten?"}
{"ts": "153:33", "speaker": "E", "text": "Ja, wir planen im nächsten Sprint eine interne Guideline 'Implicit UX Metrics' zu veröffentlichen. Ziel ist es, diese mit den Telemetriedaten aus Nimbus zu korrelieren."}
{"ts": "153:42", "speaker": "I", "text": "Dann wäre der Schritt zur Scale-Phase nicht nur technisch, sondern auch methodisch vorbereitet."}
{"ts": "153:46", "speaker": "E", "text": "Genau, und das reduziert das Risiko, dass wir beim Rollout gute, aber schwer messbare UX-Elemente verlieren."}
{"ts": "153:35", "speaker": "I", "text": "Bevor wir zum Abschluss kommen, würde mich noch interessieren, ob Sie bei Atlas Mobile in der Pilotphase etwas wie eine interne 'UX SLA' definiert haben? Also verbindliche Ziele für Reaktionszeiten oder Usability-KPIs."}
{"ts": "153:40", "speaker": "E", "text": "Ja, tatsächlich. Wir haben im Dokument UX-SLA-ATL-01 festgehalten, dass die Hauptnavigation unter 200 ms reagieren muss und dass alle Kern-Workflows maximal drei Interaktionen erfordern dürfen. Das ist eher eine interne Leitlinie, aber sie hilft uns, Feature-Gates zu setzen."}
{"ts": "153:48", "speaker": "I", "text": "Und wie überprüfen Sie, ob diese UX-SLAs eingehalten werden? Nutzen Sie Live-Daten oder nur Tests im Labor?"}
{"ts": "153:53", "speaker": "E", "text": "Wir kombinieren beides. Also, wir haben automatisierte Lighthouse-Messungen in der Mobile CI-Pipeline, und zusätzlich sammeln wir Telemetriedaten über das Nimbus Observability Dashboard. Wenn z. B. die Median-Reaktionszeit > 220 ms liegt, wird ein Alert in unserem Runbook RB-UX-ATL-07 ausgelöst."}
{"ts": "154:01", "speaker": "I", "text": "Das klingt sehr systematisch. Gab es in den letzten Wochen Trigger, die tatsächlich zu einem Eingriff geführt haben?"}
{"ts": "154:06", "speaker": "E", "text": "Ja, vor zwei Wochen gab es Ticket UX-941, weil der Offline-Sync die Hauptliste bei schwachem Netz um 400 ms verzögert hat. Wir haben dann den Sortier-Algorithmus angepasst und den Sync in einen Hintergrund-Thread verlagert."}
{"ts": "154:14", "speaker": "I", "text": "Interessant, und das war dann eine schnelle Entscheidung oder eher ein größeres Refactoring?"}
{"ts": "154:18", "speaker": "E", "text": "Es war ein Quick Fix im Sinne von Hotpatch, aber wir haben parallel ein RFC erstellt, RFC-ATL-UX-23, um das Pattern dauerhaft in die Architektur zu integrieren. Darin ist auch dokumentiert, wie wir den Feature Flag 'sync_bg' einführen."}
{"ts": "154:26", "speaker": "I", "text": "Wie kommunizieren Sie solche Änderungen an das QA-Team, damit Tests entsprechend angepasst werden?"}
{"ts": "154:31", "speaker": "E", "text": "Wir haben im Confluence-Bereich 'Atlas Mobile - QA' eine Sektion 'UX Changes'. Dort tragen wir jede Änderung mit Flag-Namen, betroffenen Screens und Test-Cases ein. QA kann dann gezielte Regression-Tests fahren."}
{"ts": "154:38", "speaker": "I", "text": "Gab es schon Situationen, wo die QA dadurch Fehler gefunden hat, die sonst unbemerkt geblieben wären?"}
{"ts": "154:43", "speaker": "E", "text": "Ja, beim Feature Flag 'dark_mode_v2' hat QA bemerkt, dass in der Offline-Ansicht ein Kontrastproblem entstand. Das stand nicht im ursprünglichen Design, wurde aber durch die Accessiblity-Guidelines verletzt. Ohne das strukturierte Vorgehen wäre es wohl erst spät aufgefallen."}
{"ts": "154:51", "speaker": "I", "text": "Das bestätigt, wie wertvoll solche dokumentierten Prozesse sind. Gibt es Pläne, diese internen UX-SLAs und QA-Workflows in der Scale-Phase zu erweitern?"}
{"ts": "154:56", "speaker": "E", "text": "Ja, wir wollen eine automatisierte KPI-Übersicht ins Release-Gate einbauen. Also bevor ein Feature-Flag global aktiviert wird, muss der Build alle UX-SLAs erfüllen, inklusive Barrierefreiheits-Checks. Das steht so in unserem Draft RB-UX-ATL-09."}
{"ts": "155:03", "speaker": "I", "text": "Letzte Frage: Würden Sie sagen, dass diese SLAs auch helfen, die Balance zwischen Performance und Accessibility zu wahren, oder besteht die Gefahr, dass eine Metrik die andere verdrängt?"}
{"ts": "155:08", "speaker": "E", "text": "Wir müssen schon aufpassen, dass nicht alles nur auf Reaktionszeiten optimiert wird. Darum haben wir z. B. im SLA-Template eine 'no regression in accessibility score' Klausel. So wird sichergestellt, dass Performance-Verbesserungen nicht auf Kosten der Nutzbarkeit gehen."}
{"ts": "155:05", "speaker": "I", "text": "Bevor wir abschließen, würde mich noch interessieren, ob Sie in der Pilotphase spezielle Benchmarks für die Offline-Sync-UX gesetzt haben?"}
{"ts": "155:12", "speaker": "E", "text": "Ja, wir haben im Runbook RB-ATL-Offline-03 einen Benchmark definiert: maximal 2,5 Sekunden zum Anzeigen von lokal gespeicherten Daten bei Netzverlust. Das wurde über simulierte Flugmodus-Tests verifiziert."}
{"ts": "155:25", "speaker": "I", "text": "Und wie fließen diese Benchmarks dann in den Übergang zur Scale-Phase ein?"}
{"ts": "155:33", "speaker": "E", "text": "Wir übernehmen sie in die SLA-Entwürfe für die Scale-Phase, konkret in SLA-UX-ATL-v1. Außerdem werden sie in das Monitoring via Nimbus Observability integriert, damit Abweichungen sofort sichtbar sind."}
{"ts": "155:46", "speaker": "I", "text": "Gab es in der Zusammenarbeit mit QA spezielle Testszenarien, um die Accessibility bei Offline-Modus zu prüfen?"}
{"ts": "155:54", "speaker": "E", "text": "Ja, wir haben mit QA sogenannte 'Screen Reader Offline Walkthroughs' entwickelt. Das ist ein Testplan TP-ACC-OFF-07, der Schritt-für-Schritt prüft, ob alle wichtigen Ansagen auch ohne Serverantwort verfügbar sind."}
{"ts": "156:08", "speaker": "I", "text": "Interessant. Wurde dieser Testplan schon mal im Feld angewendet?"}
{"ts": "156:15", "speaker": "E", "text": "Ja, im Ticket UXBUG-214 haben wir eine Meldung aus dem Beta-Test aufgenommen, dass eine Fehlermeldung nicht vorgelesen wurde. Der Testplan half uns, den Scope und Fix zu definieren."}
{"ts": "156:28", "speaker": "I", "text": "Wie stellen Sie sicher, dass solche Erkenntnisse an andere Teams kommuniziert werden?"}
{"ts": "156:35", "speaker": "E", "text": "Wir nutzen dafür wöchentlich das 'Atlas UX Sync' Meeting und pflegen ein Confluence-Board, auf dem alle Findings aus QA, Data und UX Research mit Tags versehen sind."}
{"ts": "156:49", "speaker": "I", "text": "Gab es schon mal einen Konflikt zwischen den Telemetriedaten und den qualitativen Feedbacks?"}
{"ts": "156:56", "speaker": "E", "text": "Ja, bei Feature Flag FF-ATL-Bookmark zeigte die Telemetrie hohe Nutzung, aber in Interviews beschwerten sich Nutzer über die Auffindbarkeit. Wir haben daraufhin die Entry Points im DS-ATLAS v2 angepasst."}
{"ts": "157:10", "speaker": "I", "text": "Das klingt nach einer gelungenen Verbindung aus Datenquellen. Wie dokumentieren Sie solche Anpassungen?"}
{"ts": "157:17", "speaker": "E", "text": "Im RFC-UX-ATL-12, dort wird jede Designanpassung mit Ursache, Datenquelle und betroffenen Komponenten aus dem Designsystem verknüpft."}
{"ts": "157:29", "speaker": "I", "text": "Abschließend: Gibt es Lessons Learned aus der Pilotphase, die Sie sofort in die nächste Iteration einfließen lassen werden?"}
{"ts": "157:37", "speaker": "E", "text": "Definitiv. Eine ist, Feature Flags enger mit UX-Metriken zu koppeln, um nicht nur technisch, sondern auch erfahrungsbasiert zu entscheiden, wann ein Feature ausgerollt wird."}
{"ts": "159:05", "speaker": "I", "text": "Lassen Sie uns noch mal kurz auf die DS-ATLAS v2 Komponenten zurückkommen – wie genau steuern Sie deren Ausrollung im Pilot, gerade mit Blick auf die Feature Flags?"}
{"ts": "159:15", "speaker": "E", "text": "Wir nutzen dafür ein internes Runbook, RB-UX-14, das Schritt für Schritt beschreibt, wie UI-Komponenten über unser Flag-Management-Tool aktiviert werden. Das beinhaltet auch einen QA-Gate, bevor wir das Flag in der Pilotgruppe einschalten."}
{"ts": "159:26", "speaker": "I", "text": "Gibt es dabei Abhängigkeiten zu anderen Teams?"}
{"ts": "159:33", "speaker": "E", "text": "Ja, klar. Wir müssen mit Mobile-Engineering eng timen, weil deren Build-Zyklen und unser Designsystem-Update synchron laufen müssen. Außerdem gibt es Absprachen mit QA, die in Ticket AT-FF-322 dokumentiert sind."}
{"ts": "159:47", "speaker": "I", "text": "Und wie gehen Sie mit den unterschiedlichen UI-Zuständen um, die durch Feature Flags entstehen?"}
{"ts": "159:53", "speaker": "E", "text": "Das ist tricky. Wir haben im Designsystem jetzt sogenannte State-Matrices, die in Confluence gepflegt werden. Sie listen jede mögliche Kombination aus Flag-On/Off und den dazugehörigen UI-Zustand."}
{"ts": "160:04", "speaker": "I", "text": "Hat das schon einmal zu einem größeren Problem geführt?"}
{"ts": "160:10", "speaker": "E", "text": "Einmal, ja. Beim Flag 'offline_sync_v2' hatten wir eine Inkonsistenz zwischen der Android- und iOS-Implementierung. Das haben wir über Observability-Daten im Nimbus-Dashboard erkannt, weil die Error-Rate bei iOS signifikant höher lag."}
{"ts": "160:25", "speaker": "I", "text": "Das bringt mich zur Frage: Wie binden Sie UX-Daten in Observability ein?"}
{"ts": "160:33", "speaker": "E", "text": "Wir senden UX-relevante Events, z.B. Abbruchpunkte im Offline-Modus, als Custom Metrics ins Nimbus-System. Dort taggen wir sie mit Session-IDs, sodass wir sie mit qualitativen Feedbacks aus Interviews mappen können."}
{"ts": "160:47", "speaker": "I", "text": "Gab es ein Beispiel, wo diese Daten direkt zu einer Design-Änderung geführt haben?"}
{"ts": "160:54", "speaker": "E", "text": "Ja, Ticket UX-OBS-77: Wir sahen, dass Nutzer im Offline-Modus oft den Sync-Button mehrfach tippten. Das haben wir visuell verstärkt und einen Spinner hinzugefügt, was die Abbruchrate um 18 % senkte."}
{"ts": "161:10", "speaker": "I", "text": "Beeinflusst das auch Ihre Accessibility-Tests?"}
{"ts": "161:16", "speaker": "E", "text": "Ja, wir haben im Testplan TP-A11Y-05 jetzt Szenarien, bei denen Screenreader im Offline-Modus aktiv sind. Da achten wir darauf, dass Feedback wie 'Synchronisation läuft' auch akustisch korrekt wiedergegeben wird."}
{"ts": "161:30", "speaker": "I", "text": "Zum Abschluss – welche Maßnahmen priorisieren Sie, um die jetzt identifizierten Risiken in der Scale-Phase zu minimieren?"}
{"ts": "161:38", "speaker": "E", "text": "Wir priorisieren die Harmonisierung der Flag-States zwischen Plattformen, führen wöchentliche Sync-Checks mit Engineering ein und erweitern das Runbook RB-UX-14 um Accessibility-Edge-Cases. Außerdem ist ein SLA mit observability alerts geplant, um kritische UX-Metriken binnen 30 Minuten zu adressieren."}
{"ts": "161:05", "speaker": "I", "text": "Sie hatten vorhin die Runbooks für Feature-Flag-Management erwähnt – können Sie etwas detaillierter beschreiben, wie diese in Atlas Mobile eingesetzt werden?"}
{"ts": "161:12", "speaker": "E", "text": "Klar, wir haben im internen Confluence einen Abschnitt \"Runbook-FF-ATLAS\". Darin steht Schritt für Schritt, wie wir Flags in der Staging-Umgebung aktivieren, inklusive Screenshot-Referenzen und Checks mit dem QA-Team, bevor sie in die Pilotgruppe gehen."}
{"ts": "161:20", "speaker": "I", "text": "Und wie stellen Sie sicher, dass während des Pilotbetriebs keine ungewollten UI-Zustände auftreten?"}
{"ts": "161:26", "speaker": "E", "text": "Wir nutzen ein UI-State-Matrix-Dokument, das alle Flag-Kombinationen aufführt. Jedes Szenario wird in TestFlight bzw. der internen Android-Build-Pipeline geprüft, und QA hat ein automatisiertes VisDiff-Tool, um Abweichungen zu erkennen."}
{"ts": "161:35", "speaker": "I", "text": "Gab es da jüngst einen konkreten Fall?"}
{"ts": "161:40", "speaker": "E", "text": "Ja, Ticket UX-417: die neue Navigation-Bar aus DS-ATLAS v2 kollidierte mit dem Offline-Banner. Das haben wir über das VisDiff im Nachtlauf entdeckt und den Flag-Rollout um 48h verschoben."}
{"ts": "161:50", "speaker": "I", "text": "Lassen Sie uns auf das Thema Offline-Sync zurückkommen – gibt es hier besondere Accessibility-Aspekte, die Sie beachten?"}
{"ts": "161:57", "speaker": "E", "text": "Definitiv. Wir haben für Screenreader-Nutzer eine akustische Benachrichtigung eingebaut, wenn der Sync erfolgreich war. Zusätzlich gibt es in der Runbook-Sektion \"Offline-A11Y\" eine Checkliste für Farbkontraste im Offline-Modus."}
{"ts": "162:05", "speaker": "I", "text": "Wie testen Sie diese speziellen Modi?"}
{"ts": "162:10", "speaker": "E", "text": "Über simulierte Netzwerkausfälle in unserem QA-Cluster und mit VoiceOver/ TalkBack aktiv. Wir kombinieren das mit Telemetrie-Events, die wir mit dem Nimbus Observability Team abgestimmt haben, um UX-Metriken zu loggen."}
{"ts": "162:20", "speaker": "I", "text": "Also fließen diese Observability-Daten direkt in Ihre UX-Entscheidungen ein?"}
{"ts": "162:25", "speaker": "E", "text": "Ja, zum Beispiel haben wir aus Event-Logs gesehen, dass 18% der Nutzer den Offline-Hinweis übersehen. Daraufhin haben wir das Design nach RFC-UX-202 angepasst: Bannerhöhe +20%, Farbe auf #FFCC00 geändert."}
{"ts": "162:35", "speaker": "I", "text": "Und beim Übergang in die Scale-Phase – welche Risiken sehen Sie hier aus UX-Sicht weiterhin?"}
{"ts": "162:41", "speaker": "E", "text": "Das Hauptrisiko ist, dass Feature-Flags in der Breite schwerer zu kontrollieren sind. Wir haben im Risikoregister RSK-UX-05 notiert, dass divergente UI-Stände die User Journey fragmentieren könnten."}
{"ts": "162:50", "speaker": "I", "text": "Wie adressieren Sie das?"}
{"ts": "162:55", "speaker": "E", "text": "Wir planen eine konsolidierte Flag-Review-Session vor jedem Release, dokumentiert im Release-Runbook v3.2, und setzen auf stricte SLA-Checks, um sicherzustellen, dass kritische A11Y-Elemente nicht durch Flags beeinträchtigt werden."}
{"ts": "162:25", "speaker": "I", "text": "Eine Frage noch zu den Observability-Daten: konnten Sie in der letzten Woche eine bestimmte UX-Anomalie direkt mit Telemetrie aus Nimbus korrelieren?"}
{"ts": "162:30", "speaker": "E", "text": "Ja, wir haben im Dashboard \"UX-Latency-Atlas\" gesehen, dass die Response-Zeit der Offline-Sync-Queue in der Beta-Build 1.9.4 von 240ms auf teilweise über 800ms gestiegen ist. Das war korreliert mit einem Feature-Flag, das wir für die neue DS-ATLAS v2 Tabellenkomponente aktiviert hatten."}
{"ts": "162:42", "speaker": "I", "text": "Interessant – haben Sie dann das Flag sofort wieder deaktiviert oder erst weitere Tests gemacht?"}
{"ts": "162:47", "speaker": "E", "text": "Wir haben laut Runbook RB-ATLAS-FF-07 erst den Canary-Subset-Rollback gemacht, also nur 5% der Pilotnutzer zurück auf den alten TableRenderer gesetzt. Parallel haben wir einen Profiling-Trace gezogen und ins Ticket UX-1128 hochgeladen."}
{"ts": "162:58", "speaker": "I", "text": "Gab es dabei irgendwelche Überraschungen beim Debuggen?"}
{"ts": "163:03", "speaker": "E", "text": "Überraschend war, dass das Bottleneck nicht im Rendering selbst lag, sondern in einer Accessibility-Library, die bei jedem Row-Update den Screen-Reader-Tree komplett neu generierte. Das war in unseren Lab-Tests nicht aufgefallen, da wir da nur kleine Datasets hatten."}
{"ts": "163:15", "speaker": "I", "text": "Das heißt, Sie mussten Accessibility- und Performance-Prio gegeneinander abwägen?"}
{"ts": "163:20", "speaker": "E", "text": "Genau, wir haben temporär den Live-Region-Update-Intervall auf 2 Sekunden hochgesetzt, was in RFC-ATL-23 als akzeptabler Kompromiss dokumentiert ist. Langfristig wollen wir die Library patchen, sodass sie nur differenzielle Updates sendet."}
{"ts": "163:33", "speaker": "I", "text": "Wie haben Sie das den Stakeholdern kommuniziert, gerade im Kontext des Scale-Out-Plans?"}
{"ts": "163:38", "speaker": "E", "text": "Wir haben im Pilot-Review-Meeting die Metriken gezeigt und auch die SLA-UX-02 Referenz gezogen, die bei Interaktionen <500ms fordert. Mit dem temporären Fix lagen wir bei 550–600ms, was für Pilot noch toleriert wurde."}
{"ts": "163:51", "speaker": "I", "text": "Gab es auch qualitative Rückmeldungen von Nutzern dazu?"}
{"ts": "163:56", "speaker": "E", "text": "Ja, zwei Testnutzer mit Screen-Reader haben bemerkt, dass Live-Updates leicht verzögert waren. Wir haben das ins Feedback-Log UXF-56 aufgenommen und priorisieren es für Sprint 34."}
{"ts": "164:07", "speaker": "I", "text": "Können Sie abschließend noch sagen, welche Lessons Learned Sie aus diesem Fall ziehen?"}
{"ts": "164:12", "speaker": "E", "text": "Erstens: Feature Flags immer auch unter realistischen Datenmengen testen, nicht nur im Lab. Zweitens: Accessibility-Tests müssen unter Performance-Last laufen. Drittens: enge Abstimmung zwischen UX, QA und Observability spart im Incident-Fall Stunden."}
{"ts": "164:24", "speaker": "I", "text": "Das klingt nach einer guten Basis für die Scale-Phase. Werden diese Punkte in den nächsten Iterationsplan übernommen?"}
{"ts": "164:29", "speaker": "E", "text": "Ja, wir haben bereits in JIRA-Epos EP-ATL-Next die Runbooks angepasst und zusätzliche Pre-Flight-Checks für Feature Flags aufgenommen, inklusive eines Accessibility-Load-Profiles."}
{"ts": "164:01", "speaker": "I", "text": "Bevor wir abschließen, würde mich interessieren: Gibt es aktuell offene RFCs, die aus UX-Sicht kritisch sind für den nächsten Sprint?"}
{"ts": "164:06", "speaker": "E", "text": "Ja, RFC-1127 ist da ganz oben auf der Liste. Er beschreibt die Anpassung der Offline-Sync-Queue, um Konflikte bei Mehrgeräte-Nutzung zu minimieren. Wir haben im Runbook RB-ATL-08 schon die UX-Fehlerfälle skizziert."}
{"ts": "164:15", "speaker": "I", "text": "Und dieser RFC hängt auch mit den Observability-Dashboards zusammen, richtig?"}
{"ts": "164:19", "speaker": "E", "text": "Genau, wir brauchen neue Metriken im Nimbus-Dashboard, um festzustellen, ob die angepasste Queue tatsächlich zu weniger manuellem Merge führt. Ohne diese Daten können wir kaum evaluieren, ob die UX-Maßnahme wirkt."}
{"ts": "164:28", "speaker": "I", "text": "Haben Sie da schon SLAs definiert, oder ist das noch in Arbeit?"}
{"ts": "164:32", "speaker": "E", "text": "Wir haben ein internes SLA von maximal 2 Sekunden für das Laden der Konfliktauflösungs-UI. Das ist in Ticket UX-2453 dokumentiert und wurde mit den Mobile-Engineers abgestimmt."}
{"ts": "164:41", "speaker": "I", "text": "Wie stellen Sie sicher, dass diese SLA auch in schwacher Netzabdeckung eingehalten wird?"}
{"ts": "164:46", "speaker": "E", "text": "Wir simulieren schwache Verbindungen mit dem Netzwerkemulator aus dem QA-Lab, und wir haben in unserem Testplan TP-ATL-05 spezielle Szenarien für Edge- und 3G-Netze. Zusätzlich führen wir Accessibility-Checks unter diesen Bedingungen durch."}
{"ts": "164:56", "speaker": "I", "text": "Gab es in letzter Zeit einen Fall, wo diese Tests zu einer kurzfristigen Änderung geführt haben?"}
{"ts": "165:01", "speaker": "E", "text": "Ja, letzte Woche haben wir festgestellt, dass die VoiceOver-Ausgabe bei schwacher Verbindung doppelte Statusmeldungen vorgelesen hat. Das haben wir per Hotfix FF-ATL-21 noch im Pilot korrigiert."}
{"ts": "165:10", "speaker": "I", "text": "Interessant. Werden solche Hotfixes auch retrospektiv in die Designsystem-Dokumentation übernommen?"}
{"ts": "165:15", "speaker": "E", "text": "Absolut. Wir haben ein Kapitel 'Live Learnings' in DS-ATLAS v2, wo wir genau solche Fälle aufnehmen, inkl. Screenshots und Codebeispielen, damit künftige Komponenten diese Probleme vermeiden."}
{"ts": "165:24", "speaker": "I", "text": "Wie sieht Ihr Plan aus, diese Learnings in der Scale-Phase proaktiv einzubringen?"}
{"ts": "165:28", "speaker": "E", "text": "Wir wollen im Kick-off der Scale-Phase ein internes UX-Review-Board etablieren, das jede neue Komponente gegen die 'Live Learnings'-Liste prüft. Das ist schon in der Roadmap unter Meilenstein M-ATL-09 vermerkt."}
{"ts": "165:37", "speaker": "I", "text": "Gibt es Risiken, dass diese zusätzlichen Reviews die Time-to-Market beeinträchtigen?"}
{"ts": "165:42", "speaker": "E", "text": "Ja, das ist der Trade-off: mehr Qualitätssicherung versus längere Lead Times. Wir mitigieren das, indem wir parallel Reviews und Entwicklung in zwei Streams fahren, wie in unserem Prozessdokument PR-ATL-03 beschrieben."}
{"ts": "165:01", "speaker": "I", "text": "Zum Abschluss würde ich gerne noch hören, wie Sie diese dokumentierten Trade-offs praktisch im Team kommunizieren – gibt es dafür ein formales Artefakt?"}
{"ts": "165:06", "speaker": "E", "text": "Ja, wir haben ein internes Confluence-Board namens 'UX Trade-off Ledger'. Darin verlinken wir zu den relevanten Tickets – zum Beispiel UX-2124 für den Performance-vs-Accessibility-Komplex – und beschreiben den Kontext, getroffene Entscheidungen und mögliche Risiken. Das Board wird monatlich im Pilot Steering Meeting vorgestellt."}
{"ts": "165:15", "speaker": "I", "text": "Und wie stellen Sie sicher, dass auch neue Teammitglieder diese Historie verstehen?"}
{"ts": "165:19", "speaker": "E", "text": "Wir haben ein Onboarding-Runbook, Kapitel 5.3, das sich explizit mit 'Historischen UX-Entscheidungen' befasst. Dort sind die wichtigsten Ledger-Einträge verlinkt, plus eine knappe Lessons-learned-Sektion. Außerdem gibt es eine Q&A-Session in der ersten Woche."}
{"ts": "165:28", "speaker": "I", "text": "Gab es jüngst ein Beispiel, wo diese Historie eine aktuelle Entscheidung beeinflusst hat?"}
{"ts": "165:33", "speaker": "E", "text": "Ja, beim neuen Offline-Sync für Mediendateien. Wir haben uns an der Entscheidung aus Ticket UX-1842 orientiert, wo wir schon einmal eine Latenzsteigerung zugunsten barrierefreier Bedienelemente akzeptiert hatten. Das hat uns geholfen, die Stakeholder-Diskussion abzukürzen."}
{"ts": "165:43", "speaker": "I", "text": "Wie binden Sie das Observability-Team in solche Entscheidungen ein, wenn es ja eher um qualitative UX-Aspekte geht?"}
{"ts": "165:48", "speaker": "E", "text": "Wir mappen qualitative Findings auf Metriken. Zum Beispiel haben wir für die Erreichbarkeit von Buttons im Offline-Modus eine Kennzahl 'Accessible Tap Success Rate' definiert, die das Nimbus-Team via Telemetrie misst. Wenn der Wert unter 92% fällt, triggert das einen Review-Prozess."}
{"ts": "165:59", "speaker": "I", "text": "Das klingt nach klaren SLAs auch für UX. Haben Sie diese formalisiert?"}
{"ts": "166:02", "speaker": "E", "text": "Ja, wir nennen sie UX-SLOs, Service Level Objectives. Sie stehen im Dokument RFC-UX-07, in dem z. B. die Minimum-Ladegeschwindigkeit für Screen-Transitions mit aktiviertem Feature Flag festgelegt ist."}
{"ts": "166:11", "speaker": "I", "text": "Wie gehen Sie vor, wenn so ein SLO nicht erreicht wird?"}
{"ts": "166:15", "speaker": "E", "text": "Dann greifen wir zu einem gestuften Maßnahmenplan: Zuerst wird das Flag in der Pre-Prod deaktiviert, dann prüfen wir mit dem Mobile-Engineering, ob es einen Hotfix gibt. Falls nicht, dokumentieren wir den Rollback im Feature Flag Change Log (FFCL) und informieren das Pilot Steering Committee."}
{"ts": "166:26", "speaker": "I", "text": "Gibt es hier Schnittstellen zu Ihrem Designsystem DS-ATLAS v2?"}
{"ts": "166:30", "speaker": "E", "text": "Definitiv. Jede neue DS-ATLAS-Komponente hat eine eigene Flag-Konfiguration. Wenn ein SLO aufgrund einer Komponente verletzt wird, können wir gezielt nur diese Komponente per Flag deaktivieren, ohne die gesamte Funktion zu verlieren."}
{"ts": "166:40", "speaker": "I", "text": "Und last but not least: Wie priorisieren Sie, welche Trade-offs zuerst adressiert werden, wenn Ressourcen begrenzt sind?"}
{"ts": "166:45", "speaker": "E", "text": "Wir nutzen eine interne Priorisierungsmatrix aus Impact, Risk und Effort. Ticket UX-2199 z. B. hatte hohen Impact und moderates Risiko, also wurde es vorgezogen. Wir stimmen diese Matrix einmal pro Quartal im UX-Data-Review mit Product und Engineering ab."}
{"ts": "166:01", "speaker": "I", "text": "Sie hatten vorhin die enge Abstimmung mit dem Nimbus Observability Team erwähnt. Können Sie konkret beschreiben, wie diese Daten in Ihren UX-Iterationen landen?"}
{"ts": "166:07", "speaker": "E", "text": "Ja, wir haben einen wöchentlichen Sync, in dem wir die Telemetrie-Dashboards durchgehen. Da fließen Fehlerquoten aus der Offline-Sync-Queue und Interaktionslatenzen direkt in unsere UX-Backlog-Priorisierung ein. Zum Beispiel hat Runbook RB-UX-17 definiert, wie wir bei >300ms Latenz in der Navigationsleiste sofort ein UI-Redesign anstoßen."}
{"ts": "166:19", "speaker": "I", "text": "Und fließen diese Anpassungen auch wieder zurück in die Observability-Konfiguration?"}
{"ts": "166:25", "speaker": "E", "text": "Genau, wir erstellen dazu Tickets im internen Tracker (z.B. UXOBS-452) und passen die Metrik-Definitionen an. Das ist ein geschlossener Kreislauf – Anpassung, Deployment unter Feature Flag, und dann Messung mit den neuen Parametern."}
{"ts": "166:38", "speaker": "I", "text": "Bei den Feature Flags, wie verhindern Sie, dass experimentelle UI-States im Pilot zu inkonsistenten Erfahrungen führen?"}
{"ts": "166:45", "speaker": "E", "text": "Wir nutzen ein dreistufiges Freigabemodell: dev-only, pilot, und global. Im Pilot-Stadium werden Flags nur für definierte User-Segmente gesetzt, dokumentiert in FF-Matrix-Doc v2.2. QA hat ein spezielles Testskript, um alle States zu validieren, bevor wir breiter ausrollen."}
{"ts": "166:59", "speaker": "I", "text": "Wie spielt dabei das Designsystem DS-ATLAS v2 konkret hinein?"}
{"ts": "167:05", "speaker": "E", "text": "DS-ATLAS v2 liefert die kompakten Komponentenbibliotheken, und jede neue Komponente ist hinter einem Flag. Unsere Build-Pipeline bindet die Versionierung des DS direkt an die Flag-Konfiguration, sodass wir in der Pilot-App mehrere UI-Varianten parallel evaluieren können."}
{"ts": "167:19", "speaker": "I", "text": "Gab es in der letzten Iteration ein Beispiel, wo Observability-Daten zu einer Designsystem-Änderung geführt haben?"}
{"ts": "167:25", "speaker": "E", "text": "Ja, wir haben bemerkt, dass Nutzer im Offline-Modus länger auf das Laden von Kartenbereichen warten. Die Metrik OSMAP-LAG stieg um 15%. Daraufhin haben wir in DS-ATLAS v2 eine Skeleton-Loading-Komponente ergänzt und per Flag 'offline_skel_v1' getestet."}
{"ts": "167:39", "speaker": "I", "text": "Wie schnell konnten Sie diese Änderung umsetzen?"}
{"ts": "167:45", "speaker": "E", "text": "Vom Ticket UX-573 bis zum Rollout vergingen vier Tage. Das schnelle Deployment verdanken wir der klaren Kopplung von DS-Komponenten und Feature Flags sowie unserem CI/CD-Setup mit automatisierten Accessibility-Checks."}
{"ts": "167:57", "speaker": "I", "text": "Und diese Accessibility-Checks, sind die auch offline-fähig?"}
{"ts": "168:03", "speaker": "E", "text": "Ja, wir simulieren Offline-Bedingungen in unserem Test-Runner. Runbook RB-ACC-05 beschreibt z.B., wie Screenreader-Output bei fehlender Netzwerkverbindung geprüft wird. Das war wichtig, nachdem wir Feedback von einer Testgruppe mit Sehbeeinträchtigungen erhalten hatten."}
{"ts": "168:16", "speaker": "I", "text": "Das klingt nach einer sehr eng verzahnten Arbeitsweise. Gibt es aus Ihrer Sicht in dieser Phase noch offene Risiken, die Sie adressieren müssen?"}
{"ts": "168:22", "speaker": "E", "text": "Ja, das größte Risiko ist, dass wir beim Übergang zur Scale-Phase die Performance-Optimierungen im Offline-Modus nicht halten können, wenn wir zusätzliche Accessibility-Layer aktivieren. Wir haben das in Risk-Log RL-UX-09 dokumentiert und planen, in Iteration 8 gezielt Benchmark-Tests mit allen Flags on durchzuführen."}
{"ts": "169:41", "speaker": "I", "text": "Sie hatten vorhin kurz die Runbooks erwähnt – können Sie ein konkretes Beispiel nennen, wie Sie in der Pilotphase mit einem Feature-Flag-Wechsel verfahren sind?"}
{"ts": "170:00", "speaker": "E", "text": "Ja, im Runbook UX-FF-07 haben wir für den Flag \"atlas.nav.newheader\" genau dokumentiert, dass wir vor der Aktivierung im Staging einen visuellen Regressionstest fahren. Danach gibt es ein 30-minütiges UX-Review mit QA, bevor der Rollout ins Pilotsegment erfolgt."}
{"ts": "170:27", "speaker": "I", "text": "Und wenn im Review ein Problem auftaucht – wie schnell reagieren Sie dann?"}
{"ts": "170:42", "speaker": "E", "text": "Wir haben eine SLA von zwei Stunden für kritische UI-Blocker. Letzte Woche gab es z.B. Ticket UX-1224, wo ein Kontrastfehler im Dark Mode auftrat. Wir haben den Flag sofort wieder deaktiviert, um keine Barrierefreiheitsprobleme im Pilot auszulösen."}
{"ts": "171:09", "speaker": "I", "text": "Das klingt nach enger Verzahnung zwischen Design und Betrieb. Wie fließen dabei die Observability-Daten ein?"}
{"ts": "171:25", "speaker": "E", "text": "Wir korrelieren Flag-Aktivierungen mit Metriken aus Nimbus, z.B. Time-to-Interact und Error-Rates. Im Fall von UX-1224 haben wir parallel gesehen, dass die Interaktionszeit auf Low-End-Geräten um 400 ms anstieg, was den Verdacht auf Performance-Einbußen bestätigte."}
{"ts": "171:54", "speaker": "I", "text": "Gab es in der Offline-Sync-Logik ähnliche Fälle?"}
{"ts": "172:08", "speaker": "E", "text": "Ja, bei Flag \"atlas.sync.delta\". Da hatten wir im Runbook UX-OFF-03 festgelegt, dass wir bei einem Sync-Fehler einen klaren, offline-tauglichen Statusindikator anzeigen. In Ticket OFF-342 haben wir das Icon angepasst, nachdem Logs zeigten, dass Nutzer den Sync-Status missverstanden."}
{"ts": "172:38", "speaker": "I", "text": "Wie testen Sie so etwas, wenn das Gerät komplett offline ist?"}
{"ts": "172:52", "speaker": "E", "text": "Wir nutzen ein lokales Proxy-Tool, das alle Requests droppt, und dann laufen Accessibility-Checks lokal. Dabei achten wir auf Screenreader-Ausgaben und Farbkontraste im Offline-Banner. Das ist in unserem internen QA-Guide Kapitel 5.4 beschrieben."}
{"ts": "173:20", "speaker": "I", "text": "Sie sprachen früher von einem Trade-off zwischen Performance und Accessibility – können Sie den noch genauer beschreiben?"}
{"ts": "173:36", "speaker": "E", "text": "Ein Beispiel: Für VoiceOver-Unterstützung in der Offline-Liste mussten wir zusätzliche ARIA-Labels laden. Das erhöhte die Payload um 15 KB. In schwachen Netzen verzögerte das initiale Rendering. Wir haben im RFC-UX-009 dokumentiert, dass wir dies in Kauf nehmen, um die Barrierefreiheit nicht zu opfern."}
{"ts": "174:05", "speaker": "I", "text": "Wie gehen Sie damit in der geplanten Scale-Phase um?"}
{"ts": "174:19", "speaker": "E", "text": "Wir evaluieren serverseitiges Preloading dieser Labels nur für Geräte, die Accessibility aktiviert haben. Das ist ein Vorschlag aus dem letzten Architecture Board Meeting, um Bandbreite zu sparen und trotzdem inklusiv zu bleiben."}
{"ts": "174:42", "speaker": "I", "text": "Gibt es Risiken, die Sie bei dieser Lösung sehen?"}
{"ts": "174:56", "speaker": "E", "text": "Ja, primär das Risiko von Fehldetektionen bei der Accessibility-Erkennung. Falsche Annahmen könnten dazu führen, dass Labels fehlen. Deshalb wollen wir einen Fallback ins Frontend einbauen, falls die Serverseite keine Labels liefert."}
{"ts": "179:41", "speaker": "I", "text": "Zum Abschluss würde ich gerne noch etwas tiefer auf die Lessons Learned eingehen – was hat Sie in dieser Pilotphase von Atlas Mobile am meisten überrascht?"}
{"ts": "179:51", "speaker": "E", "text": "Ehrlich gesagt, wie stark die Feature‑Flag‑Kombinationen die UX beeinflussen. Wir haben in Runbook RB‑FF‑021 dokumentiert, dass drei Flags in Kombination einen völlig anderen Navigationsfluss erzeugen, was wir vorher nicht in den Mockups gesehen hatten."}
{"ts": "180:05", "speaker": "I", "text": "Das klingt nach einer komplexen Testmatrix. Wie sind Sie damit umgegangen, um die Konsistenz zu wahren?"}
{"ts": "180:13", "speaker": "E", "text": "Wir haben mit QA einen sogenannten Flag‑State‑Katalog aufgebaut, der in Confluence gepflegt wird. Jede Kombination hat Screenshots und einen Soll‑Zustand. Das ist jetzt Teil von unserem DS‑ATLAS‑v2‑Deployment‑Prozess."}
{"ts": "180:27", "speaker": "I", "text": "Und betrifft das auch die Offline‑Sync‑Funktionalität?"}
{"ts": "180:33", "speaker": "E", "text": "Ja, bei Offline‑Modus gibt es Flags, die den Umfang des Caching regeln. In Ticket UX‑OFF‑144 haben wir festgehalten, dass ein zu aggressives Caching zwar Performance bringt, aber bei Accessibility‑Usern zu Verwirrung führt, weil der Status nicht aktuell ist."}
{"ts": "180:49", "speaker": "I", "text": "Wie wurden diese Erkenntnisse in die Observability‑Dashboards integriert?"}
{"ts": "180:56", "speaker": "E", "text": "Wir haben mit Nimbus ein spezielles Panel gebaut, das Flag‑States mit UX‑KPIs wie Task Completion Time korreliert. So sehen wir, wenn etwa Flag X + OfflineMode die Completion Time um mehr als 20 % erhöht."}
{"ts": "181:12", "speaker": "I", "text": "Gab es aus diesen Daten heraus direkte Anpassungen?"}
{"ts": "181:17", "speaker": "E", "text": "Ja, in RFC‑UX‑37 haben wir beschlossen, den Default‑State für eine bestimmte Kombination zu ändern. Die Entscheidung basierte auf drei Wochen Telemetrie und fünf Nutzerinterviews."}
{"ts": "181:31", "speaker": "I", "text": "Wenn Sie auf die Scale‑Phase blicken: welche Risiken aus der Pilotphase nehmen Sie besonders ernst?"}
{"ts": "181:39", "speaker": "E", "text": "Das größte Risiko ist, dass wir bei höherer Nutzerzahl mehr Flag‑Interaktionen haben, die nicht ausreichend UX‑getestet sind. Unser SLA für UI‑Reaktionszeit könnte darunter leiden."}
{"ts": "181:52", "speaker": "I", "text": "Haben Sie Maßnahmen geplant, um dem vorzubeugen?"}
{"ts": "181:56", "speaker": "E", "text": "Ja, wir werden ein Pre‑Release‑Testing mit einer größeren internen Kohorte fahren und Flag‑Kombinationen automatisiert gegen das DS‑ATLAS‑Referenzlayout prüfen."}
{"ts": "182:09", "speaker": "I", "text": "Abschließend: gibt es einen Punkt, den Sie unbedingt noch adressieren wollen?"}
{"ts": "182:14", "speaker": "E", "text": "Ich möchte betonen, dass UX und Observability Hand in Hand gehen müssen. Ohne die Daten aus Nimbus wären viele unserer Performance‑vs‑Accessibility‑Trade‑offs nicht transparent gewesen, und wir hätten falsche Prioritäten gesetzt."}
{"ts": "186:01", "speaker": "I", "text": "Sie hatten vorhin kurz die dokumentierten Trade-offs erwähnt. Können Sie mir ein konkretes Beispiel nennen, wie diese in einem Runbook oder einer SOP festgehalten wurden?"}
{"ts": "186:15", "speaker": "E", "text": "Ja, im SOP-UX-014 haben wir z.B. festgeschrieben, dass für den Offline-Modus Animations-Transitions auf 200 ms begrenzt werden, um Render-Performance auf Low-End-Geräten zu gewährleisten. Das war ein Kompromiss, der Accessibility-Animationen etwas reduziert, aber in der Telemetrie (Ticket UX-482) wurde klar, dass dadurch Sync-Lags um 15 % gesenkt wurden."}
{"ts": "186:40", "speaker": "I", "text": "Interessant. Wie ist der Freigabeprozess für solche Änderungen—wird das im UX-Team allein entschieden oder gibt es ein Cross-Team-Review?"}
{"ts": "186:53", "speaker": "E", "text": "Wir haben ein wöchentliches Cross-Team-Review mit Mobile-Engineering, QA und dem Accessibility-Chapter Lead. Änderungen wie in SOP-UX-014 müssen dort vorgestellt werden, mit Observability-Daten untermauert, bevor sie in den Feature Flag Release Plan (RFP-AT-12) aufgenommen werden."}
{"ts": "187:17", "speaker": "I", "text": "Gab es Momente, in denen Sie aufgrund von Observability sofort reagieren mussten?"}
{"ts": "187:27", "speaker": "E", "text": "Ja, im März hatten wir ein Spike im Metric 'offlineSyncErrorRate' auf den Canary-Builds. Wir haben dann ad hoc das Feature Flag 'sync_batch_optimization' deaktiviert. Das war eine Entscheidung innerhalb von 30 Minuten, dokumentiert in Incident-Report IR-AT-07."}
{"ts": "187:50", "speaker": "I", "text": "Und wie wurde das UX-seitig kommuniziert?"}
{"ts": "188:00", "speaker": "E", "text": "Wir haben Release Notes in der internen Pilot-App aktualisiert und einen kurzen Hinweis via In-App Banner gesetzt: \"Offline Sync temporär in reduziertem Modus\". Das folgt unserem Unwritten Rule, Nutzer nie im Dunkeln zu lassen, selbst bei internen Testern."}
