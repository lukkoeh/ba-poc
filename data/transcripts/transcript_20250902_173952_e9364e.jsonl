{"ts": "00:00", "speaker": "I", "text": "Können Sie bitte die aktuelle Multi-Region-Architektur für Titan DR kurz umreißen und wie sie im Drill-Phase-Kontext operiert?"}
{"ts": "03:15", "speaker": "E", "text": "Ja, gern. Titan DR nutzt drei aktive Regionen in Europa und Nordamerika, verbunden über Poseidon Networking mit redundanten mTLS-Tunneln. Im Drill-Phase-Szenario simulieren wir Failover nach Runbook RB-DR-001 in einer kontrollierten Umgebung. Die Steuerung erfolgt über das Orchestrator-Cluster, das im Primary und Secondary simultan läuft."}
{"ts": "06:30", "speaker": "I", "text": "Welche Ihrer Architekturentscheidungen wurden besonders durch die RTO- und RPO-Vorgaben beeinflusst?"}
{"ts": "09:45", "speaker": "E", "text": "Die Vorgaben sind RTO ≤ 30 Minuten und RPO ≤ 5 Minuten. Das zwang uns zur synchrone Replikation kritischer Daten über Helios Datalake Streams. Async wäre günstiger, aber hätte das RPO verletzt."}
{"ts": "13:00", "speaker": "I", "text": "Wie sind die Verantwortlichkeiten zwischen Cloud Architecture und Security Architecture verteilt?"}
{"ts": "16:10", "speaker": "E", "text": "Cloud Architecture definiert Infrastruktur, Replikationspfade und Runbook-Inhalte. Security Architecture ist für die Durchsetzung von POL-SEC-001 und das Design der mTLS-Policies zuständig. Bei DR-Events arbeiten beide eng gemäß DR-RACI-Matrix zusammen."}
{"ts": "19:30", "speaker": "I", "text": "Wie genau wird der Runbook RB-DR-001 im Ernstfall angewendet?"}
{"ts": "23:00", "speaker": "E", "text": "RB-DR-001 ist in drei Phasen gegliedert: Detection via Nimbus OBS Alerts, Transition mit kontrolliertem DNS-Swap und Resource Promotion im Secondary. Jeder Schritt wird durch ein Ticket im System PRO-DR markiert, z.B. INC-DR-2025-044."}
{"ts": "26:15", "speaker": "I", "text": "Und welche Authentifizierungs- und Autorisierungsmechanismen greifen beim Umschalten?"}
{"ts": "29:50", "speaker": "E", "text": "Im Umschaltprozess nutzen wir ein temporäres JIT Access Token, das nur 45 Minuten gültig ist, ausgestellt via unserem IAM-Service. Zugriff ist auf das DR-Team beschränkt, per Role 'dr-operator' im Policy-Store."}
{"ts": "33:10", "speaker": "I", "text": "Wie stellen Sie sicher, dass während des Failovers keine unverschlüsselten Datenströme entstehen?"}
{"ts": "37:00", "speaker": "E", "text": "Alle Streams laufen über TLS 1.3, enforced durch Poseidon Policy Engine. Wir haben zusätzlich einen Pre-Check im Runbook, der Verbindungen ohne gültiges Zertifikat sofort unterbindet, bevor DNS umgeschaltet wird."}
{"ts": "40:20", "speaker": "I", "text": "Wie wird POL-SEC-001 während eines DR-Events durchgesetzt?"}
{"ts": "44:00", "speaker": "E", "text": "POL-SEC-001, unser Security Baseline Policy-Dokument, ist im Orchestration-Layer fest verdrahtet. Selbst wenn ein Operator einen manuellen Befehl absetzt, blockiert der Policy-Enforcer Aktionen, die Baseline verletzen würden."}
{"ts": "48:20", "speaker": "I", "text": "Welche Audit-Artefakte werden nach einem Failover generiert und wie fließen diese in die AUD-Logs?"}
{"ts": "54:00", "speaker": "E", "text": "Nach Failover werden drei Artefakte generiert: das vollständige Command-Log, die mTLS-Handshake-Logs und das Incident Summary PDF. Diese werden als EventType 'DR-Audit' an unser AUD-Log-Cluster gestreamt und sind revisionssicher archiviert."}
{"ts": "90:00", "speaker": "I", "text": "Sie hatten vorhin erwähnt, dass Poseidon Networking im DR-Failover spezielle mTLS-Policies lädt. Können Sie das bitte einmal konkret am Beispiel eines Region-Umschalts erläutern?"}
{"ts": "90:15", "speaker": "E", "text": "Ja, im Runbook RB-DR-001 ist ein Schritt 4.2 definiert, der ein API-Call an Poseidon auslöst. Dadurch wird das aktuelle Zertifikats-Bundle rotiert und die CN-Filter für die Zielregion angepasst. Das verhindert, dass alte Sessions aus der Quellregion sich nach dem Umschalten noch verbinden können."}
{"ts": "90:42", "speaker": "I", "text": "Und diese Anpassung, greift die sofort oder gibt es da eine Latenz?"}
{"ts": "90:50", "speaker": "E", "text": "Wir haben im letzten Drill eine durchschnittliche Propagation Time von 2,3 Sekunden gemessen. Das ist in unseren SLA-DR-2024 als akzeptabel definiert, weil in dieser Zeit keine kritischen Transaktionen unverschlüsselt laufen."}
{"ts": "91:05", "speaker": "I", "text": "Wie wirkt sich ein Ausfall des Helios Datalake auf diese DR-Strategie aus, gerade wenn Metadaten fehlen könnten?"}
{"ts": "91:18", "speaker": "E", "text": "Das ist tricky: Wenn Helios nicht verfügbar ist, fällt die automatische Dataset-Validierung aus. In RB-DR-001 haben wir dafür einen Fallback beschrieben, der statische Snapshots aus der letzten Stunde in einer separaten S3-ähnlichen Bucket-Struktur nutzt. Das erhöht den Recovery Point um max. +15 Minuten."}
{"ts": "91:42", "speaker": "I", "text": "Gab es Testfälle, in denen Nimbus OBS Daten für Optimierungen geliefert hat?"}
{"ts": "91:50", "speaker": "E", "text": "Ja, im GameDay TEST-DR-2025-Q1 gab es ein Incident TCK-DR-583, wo wir Latenzspitzen von 120ms gesehen haben. Nimbus OBS zeigte uns, dass dies auf eine suboptimale Netzwerkpfadwahl zurückzuführen war. Wir haben daraufhin im Poseidon Routing-Profil 'dr_secondary' die Weightings angepasst."}
{"ts": "92:15", "speaker": "I", "text": "Interessant. Bedeutet das, Sie haben dynamische Routing-Profile je nach DR-Phase?"}
{"ts": "92:23", "speaker": "E", "text": "Genau, es gibt 'dr_primary', 'dr_secondary' und 'recovery'. Jede Phase hat eigene mTLS- und QoS-Parameter. Das Mapping ist in CFG-DR-NET-07 dokumentiert."}
{"ts": "92:38", "speaker": "I", "text": "Wie balancieren Sie dabei Performance und Kosten?"}
{"ts": "92:46", "speaker": "E", "text": "Wir nutzen Cost Baselines aus POL-FIN-007. Für DR halten wir nur 60% der Compute-Kapazität warm. Die restlichen 40% werden als Spot-ähnliche Instanzen im Failover hochgefahren. Das spart jährlich rund 180k €, ohne die RTO von 12 Minuten zu gefährden."}
{"ts": "93:08", "speaker": "I", "text": "Gab es nach dem letzten GameDay Optimierungen in der Verschlüsselungspipeline?"}
{"ts": "93:16", "speaker": "E", "text": "Ja, wir haben den Cipher-Suite Negotiation-Timeout von 500ms auf 200ms reduziert, basierend auf Observability-Metriken. Das war RFC-DR-042, genehmigt von SecArch."}
{"ts": "93:32", "speaker": "I", "text": "Klingt nach enger Verzahnung zwischen Architektur und Security. Wie werden solche Änderungen auditiert?"}
{"ts": "93:40", "speaker": "E", "text": "Jede Änderung erzeugt ein AUD-Log-Event mit Change-ID, verknüpft mit dem Ticket aus unserem DR-Board. Diese werden wöchentlich von Compliance gegen POL-SEC-001 geprüft."}
{"ts": "96:00", "speaker": "I", "text": "Sie hatten vorhin angedeutet, dass beim letzten GameDay bestimmte Sicherheitsmaßnahmen bewusst Vorrang hatten. Können Sie das bitte konkretisieren?"}
{"ts": "96:15", "speaker": "E", "text": "Ja, im GameDay TEST-DR-2025-Q1 haben wir beim Failover von Region West nach Region Ost die Netzsegmentierung strenger gezogen, als es POL-NET-004 vorgibt. Das führte zu höheren Latenzen, war aber in Übereinstimmung mit unserem Ziel, jedes potenzielle Datenleck zu verhindern."}
{"ts": "96:38", "speaker": "I", "text": "Das heißt, Sie haben bewusst Performance geopfert?"}
{"ts": "96:45", "speaker": "E", "text": "Genau. Wir hätten die mTLS-Handshake-Zeit verringern können, indem wir temporär auf schwächere Cipher Suites umstellen, aber das hätte gegen POL-SEC-001 verstoßen, und die Compliance-Risiken wären signifikant gewesen."}
{"ts": "97:10", "speaker": "I", "text": "Gab es dafür ein formales RFC oder wurde das ad hoc entschieden?"}
{"ts": "97:18", "speaker": "E", "text": "Wir haben RFC-DR-2025-07 im Vorfeld erstellt, in dem genau diese Priorisierung beschrieben wurde. Das wurde auch im Change Advisory Board abgesegnet."}
{"ts": "97:35", "speaker": "I", "text": "Welche Risiken sind dabei noch offen geblieben?"}
{"ts": "97:42", "speaker": "E", "text": "Ein Restrisiko besteht in der Replikationsverzögerung zwischen den Regionen. Wenn der BLAST_RADIUS erweitert wird, um schneller zu recovern, könnten inkonsistente Schreibvorgänge auftreten. Das ist im Risiko-Register unter RSK-DR-014 dokumentiert."}
{"ts": "98:05", "speaker": "I", "text": "Und wie mitigieren Sie das aktuell?"}
{"ts": "98:12", "speaker": "E", "text": "Wir setzen auf verzögertes Promoten der sekundären Datenbanken, laut Runbook RB-DR-001 Abschnitt 4.3. Außerdem läuft ein automatischer Data Integrity Check über Nimbus OBS Logs, um Anomalien zu erkennen."}
{"ts": "98:35", "speaker": "I", "text": "Gibt es dafür SLAs, wie schnell diese Checks abgeschlossen sein müssen?"}
{"ts": "98:42", "speaker": "E", "text": "Ja, SLA-DR-002 schreibt vor, dass die Integritätsprüfung innerhalb von 7 Minuten nach Failover beginnen und binnen 15 Minuten abgeschlossen sein muss."}
{"ts": "99:00", "speaker": "I", "text": "Wie stellen Sie sicher, dass in einem echten Notfall diese Fristen eingehalten werden?"}
{"ts": "99:08", "speaker": "E", "text": "Wir haben in den letzten drei Drills die Zeit gestoppt und in Ticketing-System TKT-DR-889 dokumentiert. Alle lagen unter der 15-Minuten-Marke. Zusätzlich haben wir Alerting in Poseidon Networking konfiguriert, um bei Verbindungsverlust sofort den Check zu triggern."}
{"ts": "99:32", "speaker": "I", "text": "Gibt es aus Ihrer Sicht noch Designalternativen, die Sie prüfen wollen?"}
{"ts": "99:40", "speaker": "E", "text": "Wir erwägen, statt den BLAST_RADIUS zu vergrößern, regionalisierte Micro-Recovery-Zonen einzuführen. Das würde die Replikationslast reduzieren, erfordert aber laut meinem Entwurf für RFC-DR-2025-12 eine komplette Anpassung unserer Routing-Policies in Poseidon Networking."}
{"ts": "112:00", "speaker": "I", "text": "Lassen Sie uns jetzt noch einmal gezielt auf die Integration mit Poseidon Networking eingehen – speziell, wie mTLS-Policies während eines DR-Events angepasst werden."}
{"ts": "112:08", "speaker": "E", "text": "Ja, also im Failover-Szenario triggert RB-DR-001 den Step 'NetPolicySync', der wiederum das Poseidon Control Plane über API-Call PN-API-12 anspricht. Dort wird eine temporäre mTLS-Policy geladen, die in POL-NET-004 dokumentiert ist."}
{"ts": "112:22", "speaker": "I", "text": "Und das funktioniert automatisiert, oder braucht es manuelle Freigaben?"}
{"ts": "112:26", "speaker": "E", "text": "Automatisiert, solange der Auth-Token vom DR-Controller noch gültig ist. Falls nicht, greift Just-In-Time Access gemäss POL-SEC-002, das erfordert eine Freigabe durch den Security Duty Officer."}
{"ts": "112:38", "speaker": "I", "text": "Wie wirkt sich das auf die Recovery Time aus, gerade wenn Security erst freigeben muss?"}
{"ts": "112:43", "speaker": "E", "text": "In unseren Tests mit Ticket SIM-DR-042 lag die zusätzliche Zeit bei maximal 90 Sekunden. Das ist noch innerhalb des RTO von 15 Minuten."}
{"ts": "112:54", "speaker": "I", "text": "Können Sie ein Beispiel geben, wie Nimbus OBS Daten für DR-Optimierungen genutzt wurden?"}
{"ts": "113:00", "speaker": "E", "text": "Ja, im Drill TEST-DR-2025-Q2 haben wir Latenzspitzen in Region EU-Central erkannt, bevor der Failover ausgelöst wurde. Nimbus' Metrik 'StreamLagSec' hat uns erlaubt, proaktiv den Switch einzuleiten und so 2 Minuten Downtime zu vermeiden."}
{"ts": "113:14", "speaker": "I", "text": "Interessant. Gab es da auch Korrelationen mit dem Helios Datalake?"}
{"ts": "113:19", "speaker": "E", "text": "Genau, OBS-Alerts wurden gegen Helios' ETL-Health-Feed gemappt. Als wir sahen, dass ETL-Jobs für kritische Reports verzögert waren, haben wir das als zusätzlichen Trigger im DR-Decision-Tree hinterlegt."}
{"ts": "113:33", "speaker": "I", "text": "Das heißt, Sie verlinken Netzwerk-, Daten- und Compute-Signale in einer einzigen Entscheidungslogik?"}
{"ts": "113:38", "speaker": "E", "text": "Richtig. Das ist im DR-RFC-017 beschrieben – es kombiniert Signals aus Poseidon, Helios und Compute-Scheduler Orion. Multi-Hop-Korrelation ist der Schlüssel, um Fehlauslösungen zu vermeiden."}
{"ts": "113:52", "speaker": "I", "text": "Lassen Sie uns zum Abschluss über ein aktuelles Risiko sprechen, das Sie noch nicht vollständig mitigiert haben."}
{"ts": "113:57", "speaker": "E", "text": "Ein Punkt ist das Cross-Region IAM Drift. Während des letzten GameDay haben wir gesehen, dass ein RoleBinding in AP-Southeast nicht synchron war. Das führte zu einem 3-minütigen Delay bei einem kritischen Batch-Job."}
{"ts": "114:09", "speaker": "I", "text": "Welche Maßnahmen planen Sie dagegen?"}
{"ts": "114:14", "speaker": "E", "text": "Wir arbeiten an einem Pre-Flight-Check im Runbook, der IAM-Diffs erkennt und automatisch ein Sync-Job startet. Zusätzlich planen wir einen wöchentlichen Drift-Report in AUD-Logs, um Trends früh zu sehen."}
{"ts": "120:00", "speaker": "I", "text": "Lassen Sie uns noch einmal auf die Integration mit Poseidon Networking eingehen. Gab es konkrete Fälle, in denen mTLS-Policy-Anpassungen während eines DR-Drills durchgeführt wurden?"}
{"ts": "120:35", "speaker": "E", "text": "Ja, im Drill vom März haben wir Ticket NET-DR-042 geöffnet, um eine temporäre mTLS-Policy zwischen Region West und Region Nord zu aktivieren. Das war notwendig, weil Runbook RB-DR-001 in Abschnitt 4.3 vorsieht, dass bei asynchroner Replikation zusätzliche Verschlüsselungsschichten aktiviert werden, bevor der Traffic umgeleitet wird."}
{"ts": "121:08", "speaker": "I", "text": "Hat das Auswirkungen auf Latenz oder Durchsatz gehabt?"}
{"ts": "121:22", "speaker": "E", "text": "Minimal. Wir haben etwa 8 ms zusätzliche Latenz gemessen. Laut SLA-Sheet SLA-DR-2025 dürfen wir bis zu 15 ms in einem Failover-Fenster zulassen. Durchsatz blieb stabil, weil Poseidon die Cipher-Suites hardwarebeschleunigt verhandelt."}
{"ts": "121:56", "speaker": "I", "text": "Und wie verhält sich das, wenn gleichzeitig Helios Datalake beeinträchtigt ist?"}
{"ts": "122:15", "speaker": "E", "text": "Das ist komplex: Helios liefert Metadaten für die Datenvalidierung im DR. Fällt er aus, greift laut RB-DR-001 Fallback 5.1 ein – wir nutzen dann statische Validierungsregeln aus der DR-Config. Diese sind weniger flexibel, aber sichern, dass keine korrupten Daten in die Zielregion repliziert werden."}
{"ts": "122:50", "speaker": "I", "text": "Können Sie das mit einem Beispiel aus einem Testlauf unterlegen?"}
{"ts": "123:07", "speaker": "E", "text": "Im Test CASE-DR-HEL-07 haben wir Helios absichtlich vom Netz genommen. Die DR-Engine hat sofort auf statische Checks umgeschaltet, Audit-Log AUD-DR-20250315 zeigt den Fallback-Trigger und die entsprechenden Policy-IDs, alles konform zu POL-SEC-001."}
{"ts": "123:42", "speaker": "I", "text": "Wie spielt Nimbus OBS in solchen Szenarien hinein?"}
{"ts": "124:00", "speaker": "E", "text": "Nimbus OBS liefert Metriken wie Replikations-Lag und Paketverlust. Bei HEL-07 haben wir im OBS-Dashboard einen Spike gesehen, was uns half, die Recovery-Skripte gemäß RFC-DR-019 zu optimieren. Das ist ein gutes Beispiel für cross-system learning."}
{"ts": "124:36", "speaker": "I", "text": "Kommen wir auf die Kosten: Wie stellen Sie sicher, dass zusätzliche Sicherheitslagen nicht aus dem Budget laufen?"}
{"ts": "124:52", "speaker": "E", "text": "Wir haben gemäß POL-FIN-007 ein DR-Budget definiert. Jede nicht-routinemäßige Resource-Allocation muss durch meinen Approval-Flow in der Cloud-Governance-Console. Außerdem setzen wir Quotas in den IaC-Templates, sodass selbst in einer Stressphase keine unkontrollierte Skalierung passiert."}
{"ts": "125:28", "speaker": "I", "text": "Gab es nach TEST-DR-2025-Q1 Optimierungen, die sowohl Kosten als auch Performance betreffen?"}
{"ts": "125:43", "speaker": "E", "text": "Ja, wir haben die Warm-Standby-Instanzen in Region Süd von m5.xlarge auf m5.large reduziert, weil OBS-Daten zeigten, dass CPU-Auslastung im Failover-Fall bei 55% bleibt. Das spart laut FIN-Report DR-Q1 etwa 12% laufende Kosten."}
{"ts": "126:15", "speaker": "I", "text": "Zum Abschluss: Welche offenen Risiken sehen Sie aktuell noch?"}
{"ts": "126:30", "speaker": "E", "text": "Das größte Risiko ist derzeit, dass wir bei einem simultanen Ausfall von Poseidon und Helios zwei kritische Pfade verlieren. Unser Mitigation-Plan in DR-RFC-022 sieht vor, dass wir in solchen Fällen den BLAST_RADIUS bewusst erweitern – mehr Systeme gleichzeitig hochziehen – um Recovery-Zeit zu sparen. Das ist ein Trade-off zwischen Komplexität und Schnelligkeit, der von der Geschäftsführung abgesegnet wurde."}
{"ts": "134:00", "speaker": "I", "text": "Sie hatten vorhin die bewusste Erhöhung des BLAST_RADIUS angesprochen. Können Sie noch einmal konkret ausführen, wie das im Rahmen des letzten Drill mit RB-DR-001 koordiniert wurde?"}
{"ts": "134:05", "speaker": "E", "text": "Ja, im Drill-Szenario Q1/2025 haben wir im Runbook RB-DR-001 die Umschalt-Logs bewusst so konfiguriert, dass der Failover-Trigger simultan auf mehrere Availability Zones wirkte, um die Recovery Time Objective, also die RTO von 45 Minuten, sicher einzuhalten."}
{"ts": "134:14", "speaker": "I", "text": "Und das hat Auswirkungen auf die Security-Architektur gehabt?"}
{"ts": "134:18", "speaker": "E", "text": "Genau, weil POL-SEC-001 dadurch im Event-Handler an mehreren Stellen gleichzeitig enforced werden musste. We had to ensure mTLS sessions were re-established without manual intervention, sonst hätten wir temporär unverschlüsselte Streams riskiert."}
{"ts": "134:28", "speaker": "I", "text": "Gab es dabei Schnittstellen, die besonders anfällig waren – etwa zu Poseidon Networking?"}
{"ts": "134:33", "speaker": "E", "text": "Ja, die Schnittstellen für die dynamische Anpassung der mTLS-Policies im Poseidon Controller. Wir mussten deren API-Rate-Limits vorab mit dem Networking-Team abstimmen, um nicht in eine Throttling-Situation zu laufen, die den Failover verzögert."}
{"ts": "134:42", "speaker": "I", "text": "Das klingt nach einem koordinierten Multi-Hop-Workflow – wie haben Sie die Abhängigkeiten zu Helios Datalake dabei berücksichtigt?"}
{"ts": "134:47", "speaker": "E", "text": "Helios liefert kritische Metadaten für die DR-Entscheidungsbäume. During the drill, wir mussten sicherstellen, dass auch bei einem regionalen Ausfall die Datalake-Replica in der sekundären Region konsistent ist, sonst hätte der Orchestrator falsche Prioritäten gesetzt."}
{"ts": "134:57", "speaker": "I", "text": "Wie haben Sie Compliance-Aspekte wie Audit Trails in dieser simultanen Umschaltung beachtet?"}
{"ts": "135:02", "speaker": "E", "text": "Alle Failover-Events wurden in AUD-Logs mit Event-ID-Pattern DR-FLR-* geschrieben, inklusive Timestamp, auslösender User oder Service-Account und der jeweils betroffenen Subsysteme. Das erfüllt die Nachweispflicht nach POL-CMP-004."}
{"ts": "135:11", "speaker": "I", "text": "Aber wenn schnelle manuelle Eingriffe nötig sind – wie verhindern Sie Missbrauch von privilegierten Zugriffen?"}
{"ts": "135:16", "speaker": "E", "text": "Wir setzen auf JIT Access nach POL-SEC-009. That means, selbst im DR-Event muss ein Break-Glass-Request im Ticket-System GEN-TIC vorliegen, z.B. Ticket DR-EMG-224, und er wird post-mortem von Security überprüft."}
{"ts": "135:26", "speaker": "I", "text": "Kommen wir auf Kostenaspekte: Wie balancieren Sie redundant vorzuhaltende Ressourcen mit SLA-Anforderungen?"}
{"ts": "135:31", "speaker": "E", "text": "Wir haben Budgets gemäß POL-FIN-007, die nur 120% der minimalen benötigten DR-Kapazität erlauben. Im Gegenzug haben wir Cold-Standby-Ressourcen, die innerhalb von 10 Minuten per IaC deploybar sind – reduziert laufende Kosten, hält aber RTO knapp ein."}
{"ts": "135:40", "speaker": "I", "text": "Und welche Optimierungen haben Sie nach dem letzten GameDay umgesetzt?"}
{"ts": "135:45", "speaker": "E", "text": "Wir haben die Parallelisierung der Datenreplikation optimiert, indem wir im Runbook RB-DR-001 einen zusätzlichen Schritt zur Batch-Verarbeitung eingeführt haben. That reduced initial sync time by roughly 20%, ohne die Sicherheit zu kompromittieren."}
{"ts": "136:00", "speaker": "I", "text": "Lassen Sie uns jetzt noch einmal gezielt auf die Integration mit Poseidon Networking eingehen. Wie genau wird im Failover-Fall sichergestellt, dass die mTLS-Policies synchron bleiben?"}
{"ts": "136:20", "speaker": "E", "text": "Im Runbook RB-DR-001 ist unter Schritt 4.3 definiert, dass die Poseidon-API für Policy-Pulls aus der Quellregion einen Trigger erhält, sobald Event DR-FE-TRIG-07 aus dem Control Plane geloggt wird. Das minimiert Race Conditions, aber wir haben auch ein Fallback-Skript im Git-Repo infra-tools/poseidon-sync, falls die Automatisierung hängt."}
{"ts": "136:45", "speaker": "I", "text": "Gab es dazu schon Testläufe, bei denen Sie Abweichungen feststellen konnten?"}
{"ts": "137:00", "speaker": "E", "text": "Ja, beim Drill TEST-DR-2025-Q1 hatten wir in Ticket DR-INC-445 eine 12-Sekunden-Differenz zwischen Policy-Update und Traffic-Shift. Das wurde später durch eine Queue-Priorisierung in Poseidon 2.5.7 gefixt."}
{"ts": "137:25", "speaker": "I", "text": "Okay, und wie hängt das mit Helios Datalake zusammen?"}
{"ts": "137:40", "speaker": "E", "text": "Das ist die Multi-Hop-Verknüpfung: Helios zieht Observability-Daten aus Nimbus OBS via secure feed, und diese Daten enthalten Latenz- und Error-Rate-Metriken, die ins DR-Dashboard fließen. Wenn Helios ausfällt, verlieren wir diesen Feedback-Loop und müssen auf die lokale Metrikaggregation in Titan DR umschalten, was in RFC-DR-2024-09 beschrieben ist."}
{"ts": "138:10", "speaker": "I", "text": "Verstehe. Das heißt, ein Ausfall im Datalake beeinflusst indirekt die Failover-Entscheidungen?"}
{"ts": "138:25", "speaker": "E", "text": "Genau. Ohne die aggregierten Daten aus Helios verzögern sich automatisierte Umschaltungen, weil wir mehr manuelle Threshold-Checks einbauen müssen. Das kostet im Schnitt 20-30 Sekunden laut AUD-Report vom letzten Drill."}
{"ts": "138:50", "speaker": "I", "text": "Kommen wir zu den Kosten: Sie hatten ja POL-FIN-007 erwähnt. Wie setzen Sie das konkret im Multi-Region-Setup um?"}
{"ts": "139:05", "speaker": "E", "text": "Wir haben für jede Region ein Budget-Quota im Cloud Controller definiert – 120% vom Peak-Load der letzten 6 Monate. Das ist in den Budget-Files BGT-DR-EMEA.yaml und BGT-DR-APAC.yaml dokumentiert. Überschreitungen triggern einen Alert an das FinOps-Team."}
{"ts": "139:30", "speaker": "I", "text": "Aber Sie sagten vorhin, manchmal gehen Sie bewusst über diese Budgets?"}
{"ts": "139:45", "speaker": "E", "text": "Ja, das sind genau die Trade-offs: Bei kritischen Security-Patches oder Zero-Day-Events überschreiten wir das Limit bewusst, um zusätzliche Redundanz hochzufahren. Dafür gibt es ein Emergency Override nach POL-SEC-001 Abschnitt 7.2, das im Change-Log markiert wird."}
{"ts": "140:15", "speaker": "I", "text": "Welches Risiko sehen Sie aktuell als das größte, das noch offen ist?"}
{"ts": "140:30", "speaker": "E", "text": "Ehrlich gesagt ist es die Abhängigkeit von den Cross-Region-Auth-Tokens. Wenn das Signatur-Backend in einer Region hängt, wie in Incident SEC-AUTH-2025-02, müssen wir manuell auf statische Notfall-Zertifikate ausweichen. Das ist sicherheitskritisch und erhöht den BLAST_RADIUS temporär."}
{"ts": "140:55", "speaker": "I", "text": "Und wie planen Sie, das zu mitigieren?"}
{"ts": "141:10", "speaker": "E", "text": "Wir evaluieren gerade ein verteiltes Signing-Modul, das in jeder Region unabhängig Token signieren kann, mit anschließender asynchroner Konsistenzprüfung. Das ist in POC-DR-AUTH-REPL v1.2 dokumentiert, und soll im Q3 in eine Staging-Umgebung."}
{"ts": "145:00", "speaker": "I", "text": "Können Sie bitte nochmal konkret schildern, wie POL-SEC-001 während eines DR-Events technisch durchgesetzt wird?"}
{"ts": "145:04", "speaker": "E", "text": "Ja, sicher. POL-SEC-001 schreibt bei uns vor, dass selbst im Notfall alle Zugriffspfade durch die zentrale IAM-Layer laufen müssen. Das heißt, auch wenn Runbook RB-DR-001 Schritt 7 den manuellen Failover auslöst, werden mTLS-Zertifikate erneut validiert, und ein Just-in-Time Access Ticket aus dem System SEC-JIT-22 ist zwingend. Wir haben dafür Hooks in den Orchestrator integriert, die das erzwingen."}
{"ts": "145:12", "speaker": "I", "text": "Und wie gehen Sie mit der Latenz um, die diese zusätzlichen Checks in einer ohnehin kritischen Situation verursachen?"}
{"ts": "145:16", "speaker": "E", "text": "Das war tatsächlich ein Trade-off. Wir haben im Drill Q4/2024 gemessen, dass die Authentifizierungsschicht im Worst Case 1,8 Sekunden hinzufügt. Laut SLA DR-001 dürfen wir bis zu 5 Sekunden für Authentifizierung und Autorisierung brauchen, insofern bleiben wir im Rahmen. Außerdem cachen wir Signaturen für 60 Sekunden, um wiederholte Checks zu vermeiden."}
{"ts": "145:24", "speaker": "I", "text": "Welche Audit-Artefakte entstehen dabei genau, und wie werden die in AUD-Logs integriert?"}
{"ts": "145:28", "speaker": "E", "text": "Jeder Access-Grant erzeugt ein JSON-Objekt nach Schema AUD-SEC-DR-v2. Das enthält Timestamp, UserID aus dem zentralen Directory, Rollen, angeforderte Region und den Decision-Code. Diese Objekte werden in Echtzeit in den AUD-Stream geschrieben und später ins immutables Archiv AUD-Vault repliziert. Wir hatten ein Ticket SEC-DR-445, in dem wir die Mapping-Funktionen zwischen Orchestrator-Logs und AUD-Logs gefixt haben."}
{"ts": "145:36", "speaker": "I", "text": "Gibt es Ausnahmen von diesem Prozess, wenn besonders schnelle manuelle Eingriffe nötig sind?"}
{"ts": "145:39", "speaker": "E", "text": "Extrem selten. Wir haben eine Break-Glass-Policy BGP-SEC-03, die nur von zwei Personen genutzt werden darf. In den letzten 12 Monaten kam das nur einmal vor, bei einem simulierten Totalausfall von Region West. Selbst dann wird der Zugriff lückenlos protokolliert, aber ohne vorherige Genehmigung. Die Nachgenehmigung muss innerhalb von 4 Stunden erfolgen."}
{"ts": "145:47", "speaker": "I", "text": "Wie ist in so einem Szenario die Schnittstelle zu Poseidon Networking gestaltet, gerade für mTLS-Policy-Anpassungen?"}
{"ts": "145:51", "speaker": "E", "text": "Poseidon Networking liefert eine API, über die wir mTLS-Policies je Region dynamisch anpassen können. Während eines Failovers ruft der DR-Orchestrator ein Poseidon-Endpoint \"/policy/switch-region\" auf. Das ist in Runbook RB-DR-001 Schritt 11 dokumentiert. Wir hatten einen Bugfix in NET-PO-102, bei dem wir die Reihenfolge der Policy-Updates optimiert haben, um Race Conditions mit der Security-Schicht zu vermeiden."}
{"ts": "145:59", "speaker": "I", "text": "Und wenn der Helios Datalake ausfällt, welche Auswirkungen hat das auf die DR-Strategie?"}
{"ts": "146:03", "speaker": "E", "text": "Der Helios Datalake speichert große Mengen an Transaktionshistorie. Bei seinem Ausfall fällt die Re-Analyse von Logdaten weg, wir müssen dann mit den letzten synchronisierten Snapshots auskommen. Das beeinflusst z.B. Fraud Detection Systeme, die dann in einen Degraded Mode schalten. Für geschäftskritische Systeme bedeutet das, dass manche Validierungen temporär ausgesetzt oder verzögert werden."}
{"ts": "146:11", "speaker": "I", "text": "Haben Sie schon Fälle, bei denen Observability-Daten aus Nimbus OBS genutzt wurden, um DR zu optimieren?"}
{"ts": "146:15", "speaker": "E", "text": "Ja, im Testlauf TEST-DR-2025-Q1 haben wir Metriken aus Nimbus OBS genutzt, um Engpässe beim Cross-Region Traffic zu identifizieren. Anhand der Traces konnten wir gezielt die Bandbreitenlimits im Poseidon Backbone von 8 Gbps auf 12 Gbps erhöhen. Das ist später als permanente Setting in Config RFC-DR-77 übernommen worden."}
{"ts": "146:23", "speaker": "I", "text": "Wie balancieren Sie das mit den Kosten, gerade wenn POL-FIN-007 bestimmte Budgets vorgibt?"}
{"ts": "146:27", "speaker": "E", "text": "Wir haben pro Region ein DR-Budget von 15% der regulären Betriebskosten, wie in POL-FIN-007 definiert. Die Bandbreitenerhöhung lag noch im Budget, weil wir parallel wenig genutzte Reserve-VMs abgebaut haben. Solche Umbuchungen werden in FIN-TKT-208 dokumentiert und quartalsweise vom Controlling geprüft."}
{"ts": "146:36", "speaker": "I", "text": "Lassen Sie uns jetzt noch mal konkret auf eine Entscheidung eingehen, die Sie im letzten Quartal getroffen haben – warum haben Sie sich für eine doppelte Verschlüsselung im Transit entschieden, obwohl das laut SLA nicht zwingend war?"}
{"ts": "146:41", "speaker": "E", "text": "Das war eine bewusste Maßnahme: Wir haben in RFC-DR-042 dokumentiert, dass wir TLS 1.3 plus eine zusätzliche mTLS-Schicht einsetzen, um bei einem Region-zu-Region-Failover, wie im Runbook RB-DR-001 Abschnitt 4.2 beschrieben, auch interne Traffic-Insider-Angriffe auszuschließen."}
{"ts": "146:47", "speaker": "I", "text": "Aber das erhöht ja auch die Latenz — wie haben Sie das in Kauf genommen?"}
{"ts": "146:52", "speaker": "E", "text": "Ja, wir haben in TEST-DR-2025-Q2 gemessen, dass sich die Latenz pro Request um ca. 7ms erhöht. Der Trade-off fiel zugunsten Sicherheit aus, weil ein Audit-Finding vom letzten Jahr (AUD-LOG Eintrag #AUD-556) genau an dieser Stelle eine Lücke gesehen hat."}
{"ts": "146:59", "speaker": "I", "text": "Wie wirkt sich das auf die Kosten aus, insbesondere im Kontext von POL-FIN-007?"}
{"ts": "147:04", "speaker": "E", "text": "Wir mussten unsere Budget-Quotas leicht anpassen — im Kapazitätsplan DR-2025-Rev3 sind etwa 4% Mehrkosten für CPU-Zeit in den Gateways eingeplant. Diese wurden mit dem Finance-Team gegen die SLA-Penalty-Risiken abgewogen."}
{"ts": "147:11", "speaker": "I", "text": "Gab es eine Alternative, die Sie ernsthaft erwogen haben?"}
{"ts": "147:15", "speaker": "E", "text": "Wir hatten kurz überlegt, nur auf TLS 1.3 zu setzen und statt mTLS einen Token-basierten Ansatz mit zeitlich limitierten JIT-Credentials aus POL-SEC-001 Annex B zu nutzen. Aber das hätte im Failover-Szenario höhere operative Risiken bedeutet."}
{"ts": "147:22", "speaker": "I", "text": "Wie dokumentieren Sie diese Trade-offs für spätere Lessons Learned?"}
{"ts": "147:27", "speaker": "E", "text": "Wir pflegen eine Decision Log Section im Confluence-Bereich 'Titan DR Architektur', und jede Entscheidung bekommt eine ID, hier DEC-DR-2025-07, mit Verweis auf die Testdaten, die Tickets (z.B. SEC-842) und die Risikobewertung aus dem Risk Register."}
{"ts": "147:34", "speaker": "I", "text": "Gibt es aktuell offene Risiken, die Sie noch nicht mitigieren konnten?"}
{"ts": "147:39", "speaker": "E", "text": "Ja, ein offener Punkt ist die Abhängigkeit von der Cross-Region-Replication im Helios Datalake. Wenn dort ein Lag von mehr als 2 Minuten entsteht, steigt unser RPO von 30 auf 120 Sekunden. Ticket RISK-DR-019 trackt das, Mitigation ist für Q3 geplant."}
{"ts": "147:46", "speaker": "I", "text": "Und was wäre Ihr Worst-Case-Szenario, falls diese Mitigation scheitert?"}
{"ts": "147:51", "speaker": "E", "text": "Worst Case: Wir verlieren im DR-Fall bis zu 90 Sekunden an Transaktionsdaten in der aktiven Region. Das würde zwar nicht die regulatorischen Grenzen reißen, aber die Kundenzufriedenheit beeinträchtigen, wie in SLA-DR-001 Abschnitt 5 definiert."}
{"ts": "147:58", "speaker": "I", "text": "Letzte Frage: Haben Sie einen konkreten Plan B, falls die Cross-Region-Replication ganz ausfällt?"}
{"ts": "148:03", "speaker": "E", "text": "Ja, Runbook RB-DR-004 beschreibt einen manuellen Export-Import-Prozess über gesicherte VPN-Tunnel via Poseidon Networking, kombiniert mit einer temporären Staging-Region. Das ist langsamer, aber sichert Datenintegrität, bis OBS-Metriken wieder im grünen Bereich sind."}
{"ts": "148:00", "speaker": "I", "text": "Lassen Sie uns jetzt noch einmal auf die Lessons Learned vom letzten GameDay TEST-DR-2025-Q1 zurückkommen. Welche Optimierungen haben Sie konkret umgesetzt?"}
{"ts": "148:06", "speaker": "E", "text": "Wir haben nach dem Drill zwei wesentliche Punkte angepasst: Erstens wurde im Runbook RB-DR-001 ein zusätzlicher Validierungsschritt eingefügt, um sicherzustellen, dass mTLS-Policies aus Poseidon Networking nach dem Failover sofort aktiv sind. Zweitens wurde die Latenzüberwachung in Nimbus OBS auf 500ms Intervall gesenkt, was uns in TCK-DR-4775 als Empfehlung dokumentiert wurde."}
{"ts": "148:15", "speaker": "I", "text": "500ms klingt ambitioniert. Hat das keinen Einfluss auf die Systemlast?"}
{"ts": "148:22", "speaker": "E", "text": "Doch, minimal. Wir mussten die OBS-Collector-Quotas gemäß POL-FIN-007 um 5% anheben. Das war ein bewusster Trade-off, weil wir im Drill gesehen haben, dass Latenzspitzen sonst zu spät erkannt werden. Die Mehrkosten pro Monat belaufen sich auf etwa 1200 €, was im DR-Budget abgedeckt ist."}
{"ts": "148:35", "speaker": "I", "text": "Gab es auch Anpassungen bei der Zugriffskontrolle während des Drills?"}
{"ts": "148:42", "speaker": "E", "text": "Ja, wir haben den JIT Access Mechanismus aus POL-SEC-001 verschärft. Im Drill wurde ein manuelles Override für einen privilegierten Account erforderlich. Das wurde jetzt so geändert, dass auch in DR-Events ein Vier-Augen-Prinzip gilt, protokolliert in AUD-Log-Cluster EU-CENTRAL-2."}
{"ts": "148:55", "speaker": "I", "text": "Und wie wird das im Runbook abgebildet?"}
{"ts": "149:02", "speaker": "E", "text": "Kapitel 4.3 von RB-DR-001 beschreibt jetzt explizit die Schritte für das Anfordern und Freigeben eines JIT Access Tokens im DR-Modus. Wir haben sogar Screenshots aus der Drill-Simulation eingefügt, um Missverständnisse zu vermeiden."}
{"ts": "149:15", "speaker": "I", "text": "Welche Risiken sehen Sie trotz dieser Optimierungen noch?"}
{"ts": "149:20", "speaker": "E", "text": "Eines der größten verbleibenden Risiken ist, dass bei gleichzeitigen Ausfällen in zwei Regionen unser Poseidon Networking Control Plane überlastet werden könnte. Ticket RSK-DR-221 deutet darauf hin, dass die Failover-Orchestrierung dann 30% länger dauern würde. Wir haben dafür ein RFC in Vorbereitung, um eine zusätzliche Standby-Control-Plane in AP-SOUTHEAST-1 einzurichten."}
{"ts": "149:35", "speaker": "I", "text": "Das klingt nach erheblichen Investitionen. Wurde das schon vom Management abgesegnet?"}
{"ts": "149:42", "speaker": "E", "text": "Vorläufig ja, unter der Bedingung, dass wir in Q3 einen Proof-of-Concept liefern. Falls der PoC zeigt, dass die Recovery Time mit der zusätzlichen Control Plane um mindestens 20% sinkt, wird es grünes Licht geben. Das ist in FIN-REQ-DR-872 dokumentiert."}
{"ts": "149:55", "speaker": "I", "text": "Wie würden Sie diesen PoC technisch aufsetzen?"}
{"ts": "150:02", "speaker": "E", "text": "Wir planen, eine isolierte Testumgebung mit simuliertem Traffic aus Helios Datalake aufzubauen. Dann wird ein orchestrierter Ausfall in zwei Regionen ausgelöst, um die Latenz und Stabilität der Standby-Control-Plane zu messen. Die Observability-Daten landen wieder in Nimbus OBS, und wir werten sie mit dem bestehenden DR-Dashboard aus."}
{"ts": "150:18", "speaker": "I", "text": "Letzte Frage: Würden Sie sagen, dass diese Maßnahmen die Balance zwischen Kosten, Leistung und Sicherheit halten?"}
{"ts": "150:24", "speaker": "E", "text": "Ja, im Rahmen der gegebenen SLAs. Wir priorisieren Sicherheit, weil ein Sicherheitsvorfall in einem DR-Event deutlich teurer wäre als zusätzliche Infrastrukturkosten. Gleichzeitig achten wir durch Quotas und Monitoring darauf, dass die Leistung nicht leidet und Budgets eingehalten werden."}
{"ts": "149:36", "speaker": "I", "text": "Kommen wir jetzt zu den Leistungs- und Kostenaspekten: Wie balancieren Sie die hohen SLA-Anforderungen mit den doch beträchtlichen Mehrkosten für redundante Ressourcen in zwei Regionen?"}
{"ts": "149:41", "speaker": "E", "text": "Wir fahren da einen hybriden Ansatz: Für kritische Services gemäß SLA-DR-99 halten wir Hot-Standby in beiden Regionen, für weniger kritische Workloads Cold-Standby. Das reduziert laufende Compute-Kosten um etwa 30%, ohne die RTO der Kernsysteme zu gefährden."}
{"ts": "149:47", "speaker": "I", "text": "Und inwiefern greifen dabei Quotas oder Budgets nach POL-FIN-007?"}
{"ts": "149:52", "speaker": "E", "text": "POL-FIN-007 schreibt vor, dass DR-Ressourcen nicht mehr als 15% des Gesamt-Cloud-Budgets ausmachen dürfen. Wir haben das im Monitoring-Dashboard integriert; sobald wir bei 13% liegen, gibt's ein automatisches Ticket (COST-ALERT-DR) an das FinOps-Team."}
{"ts": "149:59", "speaker": "I", "text": "Gab es nach dem letzten GameDay TEST-DR-2025-Q1 Optimierungen, die sowohl Kosten als auch Performance betreffen?"}
{"ts": "150:04", "speaker": "E", "text": "Ja, wir haben nach TEST-DR-2025-Q1 den Storage-Tiering-Plan angepasst. Warm-Daten liegen nun in Region-wechselbaren Buckets, die nur bei tatsächlichem Failover synchronisiert werden. Das spart rund 5 TB an replizierten Daten pro Monat."}
{"ts": "150:11", "speaker": "I", "text": "Wie wirkt sich das auf die Wiederanlaufzeiten aus?"}
{"ts": "150:16", "speaker": "E", "text": "Minimal, etwa +90 Sekunden auf den Recovery-Job laut Runbook RB-DR-001, Abschnitt 4.2. Die SLA-Grenze von 15 Minuten RTO für diese Datenkategorie bleibt eingehalten."}
{"ts": "150:22", "speaker": "I", "text": "Gut, dann lassen Sie uns in Richtung Entscheidungen und Trade-offs gehen: Welche Designentscheidungen haben Sie bewusst zugunsten höherer Sicherheit und contra Kostenoptimierung getroffen?"}
{"ts": "150:28", "speaker": "E", "text": "Wir haben z.B. auf dedizierte Interconnects zwischen den Regionen gesetzt statt VPN-over-Internet. Das ist teurer, aber reduziert laut internen Security PenTest-Reports (SEC-REP-DR-24Q4) die Angriffsfläche erheblich."}
{"ts": "150:36", "speaker": "I", "text": "Gab es auch Fälle, in denen Sie den BLAST_RADIUS bewusst vergrößert haben, um schneller recovern zu können?"}
{"ts": "150:41", "speaker": "E", "text": "Ja, beim Helios Datalake haben wir einige Service-Accounts mit Cross-Region-Rechten versehen. Das erhöht theoretisch den BLAST_RADIUS, beschleunigt aber den Datenzugriff im DR-Fall von 12 auf 5 Minuten."}
{"ts": "150:48", "speaker": "I", "text": "Das klingt nach einem Spannungsfeld. Welche offenen Risiken sehen Sie aktuell?"}
{"ts": "150:53", "speaker": "E", "text": "Eines ist der Single Vendor Lock-in beim Storage Layer. Wir haben zwar RFC-DR-ALTSTO in Prüfung, aber noch keine geprüfte Multi-Cloud-Strategie. Außerdem müssen wir JIT-Access-Prozesse für manuelle Eingriffe noch robuster gestalten."}
{"ts": "150:59", "speaker": "I", "text": "Wie planen Sie, diese Punkte zu mitigieren?"}
{"ts": "151:06", "speaker": "E", "text": "Für den Storage schauen wir auf Open-Format-Replikation, um einen schnelleren Anbieterwechsel zu ermöglichen. Beim JIT-Access wollen wir im nächsten Quartal eine Kombination aus zeitgesteuerten IAM-Policies und Break-Glass-Konten mit automatischer Audit-Log-Erstellung (AUD-LOG-DR) einführen."}
{"ts": "151:06", "speaker": "I", "text": "Lassen Sie uns jetzt auf die späten Entscheidungen eingehen – welche Designentscheidungen haben Sie zugunsten höherer Sicherheit getroffen, auch wenn dadurch die Kosten gestiegen sind?"}
{"ts": "151:13", "speaker": "E", "text": "Wir haben z.B. nach RFC-DR-2024-17 die asynchrone Replikation zwischen den Regionen um eine zusätzliche Verschlüsselungsschicht erweitert. Das erhöht die Latenz um etwa 80 ms, hat aber den Vorteil, dass selbst im Failover laut POL-SEC-001 kein Klartext über die Leitungen geht."}
{"ts": "151:25", "speaker": "I", "text": "Und wie wurde das gegenüber der Geschäftsführung begründet?"}
{"ts": "151:30", "speaker": "E", "text": "Wir haben ein Risikodossier erstellt, Ticket SEC-DR-5582, in dem wir Szenarien modelliert haben, bei denen unverschlüsselte Streams in Transit abgefangen werden könnten. Die Kostensteigerung war damit klar kommunizierbar."}
{"ts": "151:42", "speaker": "I", "text": "Gab es auch bewusst den Fall, dass Sie den BLAST_RADIUS vergrößert haben, um schneller recovern zu können?"}
{"ts": "151:47", "speaker": "E", "text": "Ja, im Drill TEST-DR-2025-Q2 haben wir im Runbook RB-DR-001A eine temporäre cross-region read-write Freigabe aktiviert. Das Risiko: mehr Systeme sind gleichzeitig exponiert. Vorteil: die Recovery-Zeit ist um 27 % gesunken."}
{"ts": "151:59", "speaker": "I", "text": "Wie haben Sie das Risiko mitigiert?"}
{"ts": "152:04", "speaker": "E", "text": "Durch JIT Access via unserem Identity Broker; nur drei Admins hatten während des 15-Minuten-Fensters Schreibrechte. Außerdem wurden alle Operationen in den AUD-Logs mit Flag `BLAST_EXC` markiert."}
{"ts": "152:16", "speaker": "I", "text": "Welche offenen Risiken sehen Sie aktuell?"}
{"ts": "152:21", "speaker": "E", "text": "Ein Risiko ist die Abhängigkeit von Poseidon Networking für die mTLS Policy Distribution. Wenn deren Control Plane verzögert ist, dauert das Policy-Rollout im DR-Fall länger. Wir haben dafür RFC-NET-882 in Arbeit."}
{"ts": "152:33", "speaker": "I", "text": "Und wie planen Sie, dieses zu mitigieren?"}
{"ts": "152:38", "speaker": "E", "text": "Wir testen gerade einen Sidecar-Ansatz, bei dem Titan DR selbständig die letzten gültigen mTLS-Policies aus Nimbus OBS Cache ziehen kann. Das reduziert die Abhängigkeit zur Poseidon Control Plane signifikant."}
{"ts": "152:50", "speaker": "I", "text": "Gab es in den Lessons Learned etwas, das Sie beim nächsten Drill sofort ändern würden?"}
{"ts": "152:55", "speaker": "E", "text": "Ja, wir würden die Thresholds im Auto-Failover gemäß SLA-DR-005 nachschärfen. Im letzten Drill waren sie zu konservativ, was zu unnötiger Verzögerung führte."}
{"ts": "153:07", "speaker": "I", "text": "Letzte Frage: Wird es dazu ein Update im Runbook geben?"}
{"ts": "153:11", "speaker": "E", "text": "Definitiv. Runbook RB-DR-001B wird in Version 3.2 die neuen Thresholds, die Policy-Fallback-Mechanismen und eine Checkliste für BLAST_RADIUS-Entscheidungen enthalten."}
{"ts": "153:06", "speaker": "I", "text": "Sie hatten vorhin angedeutet, dass Sie in einer Phase den BLAST_RADIUS absichtlich erhöht haben. Können Sie das bitte konkretisieren?"}
{"ts": "153:12", "speaker": "E", "text": "Ja, in der Simulation TEST-DR-2025-Q1 haben wir gemäß Runbook RB-DR-004 die Replikationsdomäne von drei auf fünf Subsysteme ausgeweitet, um die Recovery Time zu verkürzen. Das ging klar gegen unsere sonstige Isolationsstrategie, aber wir mussten innerhalb der RTO von 90 Minuten bleiben."}
{"ts": "153:19", "speaker": "I", "text": "Und wie wurde diese Entscheidung dokumentiert oder abgesichert?"}
{"ts": "153:23", "speaker": "E", "text": "Wir haben dazu das Change-Ticket CHG-DR-558 im internen Tracking-System eröffnet, mit Verweis auf POL-SEC-001 und eine temporäre Ausnahmegenehmigung durch den Security Council. Außerdem wurde in den AUD-Logs der Drill-Tag mit einem Flag versehen, um spätere Reviews zu erleichtern."}
{"ts": "153:31", "speaker": "I", "text": "Gab es dabei besondere Risiken, die Sie in Kauf genommen haben?"}
{"ts": "153:35", "speaker": "E", "text": "Natürlich – der größere BLAST_RADIUS bedeutete, dass ein potenzieller Konfigurationsfehler mehrere Regionen gleichzeitig hätte beeinträchtigen können. Das haben wir durch zusätzliche Checks aus RB-SEC-017 und manuelle Validierungsschritte mitigiert."}
{"ts": "153:44", "speaker": "I", "text": "Klingt nach einem erhöhten personellen Aufwand. Wie passt das zu Ihren Kostenzielen?"}
{"ts": "153:48", "speaker": "E", "text": "Kurzfristig haben wir die Mehrkosten akzeptiert, weil die Lessons Learned gezeigt haben, dass ein schnelleres Recovery bei geschäftskritischen Systemen wie Helios Datalake auch Folgekosten im Millionenbereich verhindern kann. Langfristig planen wir, diese Checks zu automatisieren, um die OPEX zu senken."}
{"ts": "153:57", "speaker": "I", "text": "Wie fließen solche Drill-Ergebnisse in Ihre künftigen Designentscheidungen ein?"}
{"ts": "154:02", "speaker": "E", "text": "Wir haben ein internes RFC-Format, RFC-DR-Template, in das wir pro Drill alle Abweichungen und Performance-Daten einpflegen. Diese werden im Architekturboard diskutiert, zusammen mit Vertretern von Poseidon Networking und Security Architecture."}
{"ts": "154:10", "speaker": "I", "text": "Gab es im Board auch Stimmen, die dagegen waren, den BLAST_RADIUS zu vergrößern?"}
{"ts": "154:14", "speaker": "E", "text": "Ja, vor allem aus der Compliance-Ecke. Bedenken bezogen sich auf regulatorische Anforderungen gemäß REG-FIN-022, die eine strikte Segmentierung vorschreiben. Wir haben das als temporäre Abweichung mit Genehmigung dokumentiert."}
{"ts": "154:22", "speaker": "I", "text": "Wenn Sie jetzt zurückblicken: War es die richtige Entscheidung?"}
{"ts": "154:26", "speaker": "E", "text": "Aus heutiger Sicht ja, weil wir die RTO gehalten haben und keine Sicherheitsvorfälle auftraten. Dennoch ist klar, dass wir diesen Modus nur im Drill oder Echtfall einsetzen und nicht dauerhaft."}
{"ts": "154:33", "speaker": "I", "text": "Welche offenen Risiken sehen Sie noch im Kontext dieser Anpassung?"}
{"ts": "154:38", "speaker": "E", "text": "Es gibt noch die Gefahr, dass bei gleichzeitigen Changes in mehreren Regionen ein Fehler unbemerkt repliziert wird. Dafür arbeiten wir an einem zusätzlichen Quarantine-Mechanismus, der in RB-DR-005 beschrieben ist."}
{"ts": "154:26", "speaker": "I", "text": "Lassen Sie uns mal genauer auf die Sicherheitsimplikationen dieser Schnittstellen eingehen – konkret: Wie wirkt sich die Anpassung der mTLS-Policy aus Poseidon Networking auf den DR-Prozess aus?"}
{"ts": "154:31", "speaker": "E", "text": "Das ist tatsächlich ein heikler Punkt. Die Policy-Änderungen triggern einen Refresh der Zertifikate über den Poseidon Control Plane. Dadurch wird im Failover-Moment ein kompletter TLS-Handshake mit allen Services forciert, was wenige Sekunden Latenz erzeugt, aber aus Sicht von POL-SEC-001 zwingend ist."}
{"ts": "154:45", "speaker": "I", "text": "Und diese paar Sekunden Verzögerung – haben die schon mal einen RTO-Verstoß provoziert?"}
{"ts": "154:49", "speaker": "E", "text": "Nein, bisher nicht. Laut den Drill-Daten aus TEST-DR-2025-Q1 lagen wir bei 37 Sekunden Gesamtdauer für das Umschalten der Region, der mTLS-Refresh hat im Schnitt 4 bis 5 Sekunden davon beansprucht."}
{"ts": "154:59", "speaker": "I", "text": "Und wie binden Sie da Nimbus OBS ein? Sie hatten vorhin angedeutet, dass deren Metriken helfen."}
{"ts": "155:04", "speaker": "E", "text": "Genau – wir haben in RB-DR-001 ein Step eingebaut, der vor dem eigentlichen Switch die OBS-Streams prüft. Wenn z.B. der Control Plane in Poseidon anormal hohe Latenzen meldet, passen wir die Reihenfolge der Failover-Schritte an, um Timeouts zu vermeiden."}
{"ts": "155:16", "speaker": "I", "text": "Heißt das, Sie modifizieren das Runbook dynamisch während eines Drills?"}
{"ts": "155:20", "speaker": "E", "text": "Ja, aber nur im Drill und mit Genehmigung durch den DR-Lead. Im Ernstfall wäre das ein RFC-Bypass nach POL-OPS-002. Die Änderungen werden als temporäre Variationen in AUD-Logs markiert, Ticket-Tag z.B. DR-ADJ-442."}
{"ts": "155:34", "speaker": "I", "text": "Wie gehen Sie sicher, dass solche dynamischen Anpassungen nicht zu Sicherheitslücken führen?"}
{"ts": "155:38", "speaker": "E", "text": "Wir haben einen Inline-Policy-Check eingebaut. Jede Änderung am Step-Flow wird gegen eine reduzierte Policy-Matrix validiert. Das ist in Script SEC-VAL-DR.ps1 implementiert und greift auf POL-SEC-001 und POL-SEC-004 zurück."}
{"ts": "155:50", "speaker": "I", "text": "Verstehe. Und wie ist die Abhängigkeit zum Helios Datalake in diesem Kontext?"}
{"ts": "155:54", "speaker": "E", "text": "Der Datalake wird für State-Replikation genutzt. Fällt Helios aus, greifen wir auf ein asynchrones Backup in der sekundären Region zu. Das verlängert das Recovery um ca. 15 Sekunden, und wir verlieren in der Regel die letzten 500ms an Daten – das ist noch innerhalb des RPO."}
{"ts": "156:08", "speaker": "I", "text": "Das heißt, ein Networking-Issue in Poseidon plus ein Datalake-Lag könnte theoretisch die RTO gefährden?"}
{"ts": "156:13", "speaker": "E", "text": "Ja, in der Theorie schon. Deshalb haben wir in RB-DR-003 eine Parallelisierungslogik eingeführt, die Netzwerk- und Datalake-Recovery gleichzeitig anstößt. Das war ein Ergebnis aus Drill-Analyse DAT-OBS-552."}
{"ts": "156:26", "speaker": "I", "text": "Und diese Optimierung hat sich in echten Tests bewährt?"}
{"ts": "156:30", "speaker": "E", "text": "Absolut. Bei TEST-DR-2025-Q1 konnten wir die kombinierte Recovery-Zeit um 11 Sekunden reduzieren und das Audit-Team hat die Umsetzung als Best Practice vermerkt."}
{"ts": "156:02", "speaker": "I", "text": "Kommen wir nochmal auf die Entscheidungen zurück, die Sie im Rahmen von Titan DR bewusst getroffen haben – speziell jene, die Sicherheit über Kosten gestellt haben."}
{"ts": "156:10", "speaker": "E", "text": "Ja, wir haben zum Beispiel entschieden, alle kritischen Datenreplikas gleichzeitig in drei Regionen aktiv zu halten. Das widerspricht POL-FIN-007, was Budgetoptimierung fordert, aber wir wollten die RTO von 15 Minuten sicher einhalten."}
{"ts": "156:25", "speaker": "I", "text": "Gab es intern Diskussionen oder Widerstand gegen diesen Ansatz?"}
{"ts": "156:29", "speaker": "E", "text": "Natürlich. Das FinOps-Team hat mehrfach nachgerechnet. Aber nach dem Drill TEST-DR-2025-Q1, wo die dritte Region unsere Recovery maßgeblich beschleunigt hat, gab es Akzeptanz."}
{"ts": "156:44", "speaker": "I", "text": "Wie dokumentieren Sie solche Abweichungen von Policies?"}
{"ts": "156:48", "speaker": "E", "text": "Wir erstellen ein Exception-Record im GRC-Tool, referenziert auf Ticket SEC-DR-452. Dort ist die Begründung, die Risikoanalyse und die Genehmigung vom CISO enthalten."}
{"ts": "157:02", "speaker": "I", "text": "Und wie stellen Sie sicher, dass Least Privilege trotzdem eingehalten wird?"}
{"ts": "157:07", "speaker": "E", "text": "Wir nutzen JIT Access-Module. Selbst im Failover wird nur für 30 Minuten temporär Admin-Zugriff gewährt, und alle Aktionen landen in den AUD-Logs, wie es POL-SEC-001 vorschreibt."}
{"ts": "157:21", "speaker": "I", "text": "Gab es Fälle, wo Sie den BLAST_RADIUS absichtlich vergrößert haben?"}
{"ts": "157:26", "speaker": "E", "text": "Einmal, ja. Beim Drill DR-TEST-2024-Q4 haben wir in Runbook RB-DR-001 eine Option genutzt, die Netzwerksegmente zusammenlegt, um schneller Traffic umzuleiten. Das Risiko war höher, aber der Recovery war unter 10 Minuten."}
{"ts": "157:44", "speaker": "I", "text": "Welche offenen Risiken sehen Sie aktuell?"}
{"ts": "157:48", "speaker": "E", "text": "Der Helios Datalake ist weiterhin ein Single Point of Delay. Fällt er im falschen Moment aus, verlängert sich unser Recovery um bis zu 20 Minuten. RFC-DR-119 ist in Arbeit, um eine asynchrone Replikation einzuführen."}
{"ts": "158:03", "speaker": "I", "text": "Gibt es noch andere geplante Maßnahmen?"}
{"ts": "158:07", "speaker": "E", "text": "Ja, wir wollen im nächsten GameDay die mTLS-Policy-Adjustments aus Poseidon Networking automatisieren, basierend auf den Observability-Metriken aus Nimbus OBS, um manuelle Eingriffe zu reduzieren."}
{"ts": "158:21", "speaker": "I", "text": "Letzte Frage: Was ist Ihre wichtigste Lesson Learned aus der Drill-Phase?"}
{"ts": "158:26", "speaker": "E", "text": "Dass technische Perfektion ohne geübte Abläufe wertlos ist. Das Zusammenspiel von klaren Runbooks, wie RB-DR-001, und trainierten Teams hat uns im Ernstfall gerettet – Kosten hin oder her."}
{"ts": "160:02", "speaker": "I", "text": "Sie hatten vorhin erwähnt, dass die Multi-Region-Architektur von Titan DR alle drei aktiven Zonen simultan synchron hält. Können Sie mir noch einmal konkret sagen, wie das im Drill-Phase-Szenario umgesetzt wird?"}
{"ts": "160:07", "speaker": "E", "text": "Ja, also im Drill nutzen wir die Runbooks RB-DR-001 und RB-DR-003 parallel. Die Images werden per asynchroner Replikation in die sekundären Regionen gepusht, und wir validieren die Checksums gegen die Referenzwerte aus Region-0. Dabei ist wichtig, dass die RPO-Vorgabe von maximal 15 Sekunden nicht überschritten wird."}
{"ts": "160:13", "speaker": "I", "text": "Und wie genau stellen Sie sicher, dass die RTO-Zeit eingehalten wird, wenn wir schon im Drill sehen, dass die Netzwerk-Latenz schwankt?"}
{"ts": "160:18", "speaker": "E", "text": "Wir haben in Poseidon Networking eine Policy-Erweiterung, die während des Failovers temporär bevorzugte Routen aktiviert. Das ist in RFC-NET-872 beschrieben. Zusätzlich setzen wir in diesem Modus auf Prefetching der kritischen Container Images, um Bootzeiten unter 90 Sekunden zu halten."}
{"ts": "160:25", "speaker": "I", "text": "Können Sie beschreiben, wie die Zuständigkeiten zwischen Ihnen als Cloud Architect und der Security Architecture in dieser Situation verteilt sind?"}
{"ts": "160:30", "speaker": "E", "text": "Klar, ich definiere die technische Umsetzung der Cross-Region-Repliken, also Netzwerk, Compute und Storage. Die Security Architecture prüft die Einhaltung von POL-SEC-001, insbesondere die Least-Privilege-Zugriffe auf DR-Storage-Buckets und JIT Access für Administrationskonten."}
{"ts": "160:37", "speaker": "I", "text": "Wie greifen die Authentifizierungsmechanismen beim Umschalten zwischen den Regionen?"}
{"ts": "160:42", "speaker": "E", "text": "Beim Umschalten wird eine mTLS-Verbindung zwischen Control Plane und Zielregion initiiert. Die Zertifikate sind kurzlebig, generiert via interner CA, und werden nach 5 Minuten automatisch revoked. Außerdem gibt es einen HMAC-Check auf alle API Calls, wie in SEC-GUIDE-DR-12 beschrieben."}
{"ts": "160:49", "speaker": "I", "text": "Und wie stellen Sie sicher, dass keine unverschlüsselten Datenströme entstehen, wenn beispielsweise Helios Datalake selbst betroffen ist?"}
{"ts": "160:54", "speaker": "E", "text": "In so einem Fall erzwingt unser Fallback-Pipeline-Skript aus RB-DR-005 die Nutzung von AES-256-GCM auf allen Transportebenen. Selbst wenn das Datalake-Cluster ausfällt, greifen wir auf eine verschlüsselte Kopie in Region-2 zu. Nimbus OBS liefert uns dafür Checksummen und Latenzmetriken."}
{"ts": "161:01", "speaker": "I", "text": "Gab es Testfälle, wo Sie Observability-Daten aus Nimbus OBS tatsächlich für Optimierungen im DR genutzt haben?"}
{"ts": "161:06", "speaker": "E", "text": "Ja, im GameDay TEST-DR-2025-Q1 haben wir anhand von OBS-Trace-ID 45af-9c die Bottlenecks beim Storage-Mount identifiziert. Daraufhin haben wir den Cache-Warmup-Prozess um 12 Sekunden verkürzt, was im Drill einen erheblichen Unterschied gemacht hat."}
{"ts": "161:13", "speaker": "I", "text": "Wie balancieren Sie in diesem Setup die Kosten für redundante Ressourcen mit den SLA-Anforderungen, gerade wenn drei Regionen aktiv sind?"}
{"ts": "161:18", "speaker": "E", "text": "Das ist tatsächlich ein Trade-off. Wir überschreiten das Budget laut POL-FIN-007 um etwa 8%, aber das wurde vom Steering Committee genehmigt, weil wir so die SLA von 99,95% Verfügbarkeit halten können. Wir monitoren Kostenabweichungen monatlich über das FinOps-Dashboard."}
{"ts": "161:25", "speaker": "I", "text": "Welche offenen Risiken sehen Sie aktuell noch, insbesondere im Bezug auf die Abhängigkeiten zum Datalake?"}
{"ts": "161:30", "speaker": "E", "text": "Das größte Risiko bleibt, dass ein simultaner Ausfall von Helios Datalake in allen Regionen die Wiederherstellungszeit verlängert. Unsere Mitigation ist eine teilweise Entkopplung: kritische Metadaten werden zusätzlich in einem separaten Key-Value-Store repliziert, was wir in RFC-DR-901 dokumentiert haben."}
{"ts": "161:38", "speaker": "I", "text": "Lassen Sie uns noch einmal konkret über den Runbook-Einsatz sprechen. Im Ernstfall – also wenn Titan DR wirklich auf Multi-Region-Failover schalten muss – wie greifen Sie auf RB-DR-001 zu und wer triggert den ersten Schritt?"}
{"ts": "161:43", "speaker": "E", "text": "RB-DR-001 ist im internen Confluence als PDF und als automatisiertes Playbook in unserem Orchestrator hinterlegt. Der Initial-Trigger liegt bei der Cloud Operations Lead, aber die Authentifizierung erfolgt über unser JIT-Access-Portal, das temporäre Adminrollen nach POL-SEC-001 vergibt. Wir haben das so gebaut, dass der Schritt 1–Region Health Check–innerhalb von 90 Sekunden abgerufen werden kann."}
{"ts": "161:51", "speaker": "I", "text": "Und wie stellen Sie sicher, dass bei diesem Umschalten keine unverschlüsselten Datenströme entstehen?"}
{"ts": "161:55", "speaker": "E", "text": "Wir haben in Poseidon Networking eine mTLS-Enforcement-Policy, die für alle Cross-Region-Verbindungen greift. Selbst wenn der Failover initiiert wird, erzwingt der API-Gateway-Cluster den TLS-Handshake, bevor irgendein Paket durchkommt. Zusätzlich wird in RB-DR-001 unter Abschnitt 4.2 gefordert, dass die Encryption-at-Rest-Keys aus KMS-DR synchronisiert werden, bevor Writes akzeptiert werden."}
{"ts": "162:03", "speaker": "I", "text": "Gut, und diese KMS-DR-Synchronisierung – hängt die am Helios Datalake oder ist das separat gelöst?"}
{"ts": "162:08", "speaker": "E", "text": "Das ist teilweise gekoppelt. KMS-DR selbst ist unabhängig, aber einige Services, die Helios als Storage Backend nutzen, ziehen ihre Keys indirekt über Helios-Metastore. Deshalb haben wir im Drill-Szenario P-TIT-DR-DRILL-23-04 einen separaten Key-Bootstrap für kritische Services gefahren, um nicht auf Datalake-Verfügbarkeit zu warten."}
{"ts": "162:16", "speaker": "I", "text": "Das heißt, ein Helios-Ausfall könnte zwar die Performance beeinflussen, aber nicht den initialen Failover blockieren?"}
{"ts": "162:20", "speaker": "E", "text": "Genau. Die Architektur wurde so angepasst, dass der Recovery Point Objective eingehalten wird, auch wenn Helios erst verzögert wieder online geht. Die Multi-Hop-Verbindung zwischen Nimbus OBS und Poseidon erlaubt uns, Observability-Daten unabhängig vom Datalake zu streamen. Das war eine der Lessons aus GameDay TEST-DR-2025-Q1."}
{"ts": "162:29", "speaker": "I", "text": "Apropos GameDay – welche Optimierungen haben Sie nach diesem Test konkret umgesetzt?"}
{"ts": "162:34", "speaker": "E", "text": "Wir haben drei große Punkte umgesetzt: Erstens wurden die Quotas für DR-Ressourcen gemäß POL-FIN-007 erhöht, da wir im Test in Region West an die CPU-Limits gestoßen sind. Zweitens haben wir das Audit-Log-Modul AUD-DR-ALPHA erweitert, sodass jetzt auch mTLS-Zertifikatswechsel geloggt werden. Drittens haben wir in SEC-DR-452 eine Präzisierung der Least-Privilege-Rollen vorgenommen."}
{"ts": "162:44", "speaker": "I", "text": "Welche Audit-Artefakte generieren Sie denn nach einem Failover, und wie werden die ins Gesamtsystem integriert?"}
{"ts": "162:49", "speaker": "E", "text": "Wir erzeugen einen vollständigen Event-Stream in AUD-Logs, der unter anderem die Change-IDs aus dem Orchestrator, die temporären Rollenvergaben und die Netzwerktopologie-Snapshots enthält. Diese Streams werden asynchron ins zentrale Compliance-Archiv im East-Region-Cluster repliziert, mit SHA-256 Signaturen versehen, um Tamper-Evidence zu gewährleisten."}
{"ts": "162:58", "speaker": "I", "text": "Wie gehen Sie mit privilegierten Zugriffen um, wenn schnelle manuelle Eingriffe nötig sind, und das alles unter Zeitdruck steht?"}
{"ts": "163:03", "speaker": "E", "text": "Wir haben ein Fast-Track-JIT, das über ein spezielles Approval-Flow in weniger als 30 Sekunden temporären Zugang gewährt. Allerdings ist das an vier Augen gekoppelt: Zwei voneinander unabhängige Leads müssen via SecureChat den Code bestätigen. Alle Schritte landen sofort im AUD-Log. In einem Drill am 12.03. haben wir so in 27 Sekunden ein Storage-Failover manuell übersteuert."}
{"ts": "163:13", "speaker": "I", "text": "Das klingt kontrolliert. Gab es schon Fälle, in denen Sie bewusst den Blast Radius vergrößert haben, um schneller zu recovern?"}
{"ts": "163:18", "speaker": "E", "text": "Ja, im Test-Run DR-SIM-0924 haben wir in Absprache mit Security den Scope der Netzwerksperren reduziert, um schneller Traffic auf den Secondary-Region-Cluster umzuleiten. Das hat zwar das Risiko minimal erhöht, aber die Recovery Time um 14 % verkürzt. Diese Entscheidung ist dokumentiert in RFC-DR-889, inklusive Risikobewertung und Mitigationsplan."}
{"ts": "163:38", "speaker": "I", "text": "Sie hatten vorhin die Abhängigkeit vom Helios Datalake erwähnt. Könnten Sie einmal konkret beschreiben, wie sich ein partieller Ausfall dort auf den Titan DR Failover-Plan auswirkt?"}
{"ts": "163:44", "speaker": "E", "text": "Ja, also wenn der Datalake nur teilweise verfügbar ist, greifen wir gemäß RB-DR-004 auf den read-only Snapshot vom Vortag in der sekundären Region zu. Das hat den Vorteil, dass kritische BI-Reports weiter laufen, allerdings mit leicht veralteten Daten. Wichtig ist, dass wir in dem Fall die mTLS-Policies aus Poseidon Networking temporär lockern, um den alten Endpoint zu erreichen."}
{"ts": "163:52", "speaker": "I", "text": "Das heißt, Sie müssen gleichzeitig Netzwerk- und Datenebene anpassen. Ist das im Runbook klar sequenziert?"}
{"ts": "163:56", "speaker": "E", "text": "Ja, in RB-DR-004 ist das als Schritt 7a und 7b hinterlegt – zuerst Policy-Update über Poseidon API, dann Mount des Snapshots. Wir haben das beim Drill im März getestet, Ticket NET-DR-212 dokumentiert den Ablauf und die Recovery Time von 14 Minuten für genau diesen Teilprozess."}
{"ts": "164:05", "speaker": "I", "text": "Und wie verhält sich das zu den SLA-Vorgaben? 14 Minuten klingt knapp unter der Schwelle."}
{"ts": "164:09", "speaker": "E", "text": "Genau, das SLA erlaubt bis zu 15 Minuten für Datenebenen-Recovery. Wir haben intern sogar eine Zielvorgabe von 12 Minuten, um Puffer für unerwartete Latenzen zu haben. Das ist eine ungeschriebene Regel im Team, weil wir wissen, dass Poseidon Policy-Propagation manchmal delayed ist."}
{"ts": "164:19", "speaker": "I", "text": "Stichwort Policy-Propagation: Gab es schon Fälle, in denen ein Delay den Failover verzögert hat?"}
{"ts": "164:23", "speaker": "E", "text": "Ja, im GameDay TEST-DR-2025-Q1 hatten wir eine 3-Minuten-Verzögerung, weil ein fehlerhaftes Zertifikat in der Staging-Region lag. Wir haben danach RFC-NET-DR-88 eingeführt, der einen Pre-Check für Zertifikatsgültigkeit vorschreibt."}
{"ts": "164:33", "speaker": "I", "text": "Lassen Sie uns kurz auf die Sicherheitsseite wechseln: Wie integrieren Sie POL-SEC-001 Enforcement in so einem Szenario, wenn Sie temporär lockern müssen?"}
{"ts": "164:38", "speaker": "E", "text": "Wir nutzen für diese temporären Lockerungen Just-in-Time Access Tokens, die über den SEC-GW Service ausgestellt werden. Das wird im Audit-Log als Ausnahmefall markiert und automatisch nach maximal 30 Minuten widerrufen. Das ist in SEC-DR-452 dokumentiert, der vom letzten Drill stammt."}
{"ts": "164:48", "speaker": "I", "text": "Könnte das nicht ein Risiko vergrößern, wenn gleichzeitig mehrere Systeme in JIT-Modus laufen?"}
{"ts": "164:52", "speaker": "E", "text": "Theoretisch ja, deshalb limitieren wir laut POL-SEC-001 maximal zwei parallele JIT-Sessions pro Region. Außerdem muss der Security Duty Officer jede Session explizit freigeben. Das reduziert den potenziellen BLAST_RADIUS, auch wenn es in manchen Recovery-Szenarien 1-2 Minuten mehr kostet."}
{"ts": "165:02", "speaker": "I", "text": "Das klingt nach einem bewussten Trade-off. War das eine schwierige Entscheidung?"}
{"ts": "165:06", "speaker": "E", "text": "Ja, definitiv. Wir hatten die Option, alles automatisiert zuzulassen, hätten dann aber das Least-Privilege-Prinzip verletzt. Mit dem manuellen Approval bleiben wir compliant und können im Audit klar zeigen, wer wann was freigegeben hat – siehe AUD-Log Einträge vom 14.03., IDs AUD-DR-550 bis -557."}
{"ts": "165:16", "speaker": "I", "text": "Wenn Sie auf Basis dieser Erfahrungen eine Änderung vorschlagen dürften – würden Sie am Prozess etwas anpassen?"}
{"ts": "165:20", "speaker": "E", "text": "Eventuell könnten wir einen hybriden Ansatz testen: automatisches Approval für Low-Risk-Subsysteme wie Test-Analytics, aber weiterhin manuelles für produktive Workloads. Das würde Recovery beschleunigen, ohne die kritischen Systeme zu kompromittieren. Das würde ich in RFC-DR-101 aufnehmen."}
{"ts": "165:14", "speaker": "I", "text": "Lassen Sie uns noch mal konkret auf das Runbook RB-DR-001 eingehen – wie genau würden Sie das im Ernstfall Schritt für Schritt anwenden?"}
{"ts": "165:29", "speaker": "E", "text": "RB-DR-001 ist in drei Phasen strukturiert: Erstens die automatische Traffic-Umleitung via Global Load Balancer, zweitens das Hochfahren der warm-standby Nodes in der Zielregion, drittens die Validierung der Datenintegrität über Checksummen. Wir haben das im Drill zwei Mal simuliert und im Ticket OPS-DR-778 dokumentiert."}
{"ts": "165:49", "speaker": "I", "text": "Und in dieser Validierungsphase – welche Authentifizierungsmechanismen greifen da?"}
{"ts": "166:02", "speaker": "E", "text": "Wir nutzen für den Cross-Region-API-Zugriff mTLS basierend auf Poseidon Networking-Policies. Zusätzlich erzwingen wir über POL-SEC-001, dass nur Service Accounts mit temporären JIT-Tokens Zugriff haben."}
{"ts": "166:20", "speaker": "I", "text": "Sie sagten mTLS über Poseidon – gibt es da Abhängigkeiten, die auch Helios Datalake tangieren?"}
{"ts": "166:36", "speaker": "E", "text": "Ja, genau. Wir beziehen gewisse Auth-Policies aus dem Helios Policy Store, der wiederum im Datalake gespiegelt wird. Wenn der Datalake ausfällt, haben wir eine lokale Policy-Cache-Mechanik, siehe RFC-DR-023. Das ist ein klassischer Multi-Hop-Link zwischen DR, Networking und Data Layer."}
{"ts": "166:59", "speaker": "I", "text": "Hm, also wenn dieser Cache veraltet ist, riskieren Sie falsche Policy-Anwendungen… haben Sie das getestet?"}
{"ts": "167:12", "speaker": "E", "text": "Ja, im Testfall TEST-DR-2025-Q1 haben wir den Cache absichtlich nicht aktualisiert und gesehen, dass der mTLS-Handschlag in einer Region fehlschlug. Das führte zu einer 90-Sekunden-Verzögerung, bis der Fallback-KeyStore griff."}
{"ts": "167:33", "speaker": "I", "text": "Wie stellen Sie sicher, dass in solchen Verzögerungen keine unverschlüsselten Datenströme entstehen?"}
{"ts": "167:46", "speaker": "E", "text": "Standardmäßig fällt der Traffic in einen Quarantine-Mode, in dem nur verschlüsselte Queues erlaubt sind. Unverschlüsselte Pakete werden verworfen und in den AUD-Logs markiert – siehe AUD-DR-LogSpec v2.1."}
{"ts": "168:05", "speaker": "I", "text": "Gab es Situationen, in denen Sie das bewusst lockern mussten, um RTO einzuhalten?"}
{"ts": "168:19", "speaker": "E", "text": "Einmal, ja – in Drill 2024-Q4 haben wir POL-SEC-001 temporär gelockert, um kritische Telemetrie zu synchronisieren. Das war abgewogen gegen das Risiko und im Ticket SEC-DR-463 festgehalten."}
{"ts": "168:38", "speaker": "I", "text": "Das klingt nach einem klaren Trade-off Security versus Availability. Welche offenen Risiken sehen Sie heute noch?"}
{"ts": "168:51", "speaker": "E", "text": "Eines ist die Abhängigkeit vom Helios Datalake trotz Cache. Ein zweites ist die begrenzte Testabdeckung für simultane Ausfälle von zwei Regionen. Wir planen dafür ein erweitertes GameDay-Szenario in Q3."}
{"ts": "169:11", "speaker": "I", "text": "Und wie planen Sie, diese Risiken zu mitigieren, ohne die Kosten aus dem Ruder laufen zu lassen?"}
{"ts": "169:34", "speaker": "E", "text": "Wir evaluieren, ob wir für kritische Subsysteme wie Auth-Policy einen zusätzlichen Light-Replica-Cluster in einer vierten Region hochziehen – nur mit minimalen Ressourcen, um sowohl SLA als auch Budgetgrenzen gemäß POL-FIN-007 einzuhalten."}
{"ts": "171:14", "speaker": "I", "text": "Wir waren eben bei den Kosten-Sicherheits-Trade-offs. Können Sie mir noch einmal konkret erläutern, wie Sie bei Titan DR das Budget aus POL-FIN-007 gegen die Laufenden Kosten der redundanten Instanzen in allen drei Regionen abwägen?"}
{"ts": "171:19", "speaker": "E", "text": "Ja, gerne. Wir haben ein monatliches DR-Budget festgelegt, das in POL-FIN-007 verankert ist. In der Planungsphase haben wir die RTO von 45 Minuten und das RPO von 5 Minuten als harte Constraints gesetzt. Das führte dazu, dass wir in Region West und Central jeweils 80 % der Produktionskapazität als Warm Standby halten, was natürlich teuer ist. Um innerhalb des Budgets zu bleiben, nutzen wir Spot-ähnliche Kapazitätsreservierungen für nicht-kritische, aber für den Failover notwendige Dienste."}
{"ts": "171:27", "speaker": "I", "text": "Und wenn, hypothetisch, eine Region komplett ausfällt, wie wird dann laut Runbook RB-DR-001 vorgegangen, um die Umschaltung so schnell wie möglich durchzuführen?"}
{"ts": "171:33", "speaker": "E", "text": "RB-DR-001 schreibt vor, dass nach der Ausfall-Detektion über unser Nimbus OBS Monitoring der Primary Region ein automatisches DNS-Failover via Poseidon Networking ausgelöst wird. Parallel starten wir in der Zielregion die letzten Datalake-Snapshots aus Helios, um die Datenkonsistenz sicherzustellen. Das Runbook enthält auch manuelle Prüfschritte, die von einem DR-Operator mit JIT-Privilege aus dem SEC-DR-452-Prozess freigegeben werden."}
{"ts": "171:42", "speaker": "I", "text": "Wie sieht es bei der Authentifizierung in diesem automatisierten Umschaltprozess aus? Greifen die gleichen mTLS-Policies wie im Normalbetrieb?"}
{"ts": "171:48", "speaker": "E", "text": "Ja, allerdings mit einer Erweiterung: Während des Failovers wird eine temporäre mTLS-Policy vom Poseidon Controller eingespielt, die im RFC-DR-2025-04 beschrieben ist. Diese Policy erlaubt es, dass sich auch die DR-Operator-Tools gegen die interne API authentifizieren können, ohne dass zusätzliche Zertifikatsausstellungen nötig sind. Das verringert die Umschaltzeit um ca. 3 Minuten."}
{"ts": "171:56", "speaker": "I", "text": "Und wie stellen Sie sicher, dass während dieser Übergangsphase keine unverschlüsselten Datenströme entstehen?"}
{"ts": "172:01", "speaker": "E", "text": "Das ist zweistufig abgesichert: Erstens erzwingen die Poseidon Firewalls auf Layer 4, dass nur TLS 1.3-Verbindungen mit PFS durchkommen. Zweitens wird jede Verbindung, die im DR-Modus neu aufgebaut wird, durch ein Inline-Inspection-Modul geprüft. Falls eine Verbindung unverschlüsselt ist, wird sie blockiert und im AUD-Log mit dem Tag DR-SEC-BLOCK vermerkt."}
{"ts": "172:09", "speaker": "I", "text": "Sie erwähnten vorhin Helios Datalake. Angenommen, der Datalake ist zeitgleich mit einer Region nicht verfügbar – wie wirkt sich das auf die DR-Strategie aus?"}
{"ts": "172:15", "speaker": "E", "text": "In diesem Doppel-Ausfall-Szenario greifen wir auf ein isoliertes Backup in Region East zurück, das im Rahmen von BACKUP-PLAN-003 wöchentlich synchronisiert wird. Der Performance-Impact ist signifikant – die Wiederanlaufzeit verlängert sich um bis zu 40 %. Das haben wir im Testfall DR-DL-FAULT-2024-Q4 simuliert und in der Lessons-Learned-Sitzung dokumentiert."}
{"ts": "172:24", "speaker": "I", "text": "Gab es bei diesen Tests auch schon die Nutzung von Observability-Daten aus Nimbus OBS zur Optimierung der DR-Prozesse?"}
{"ts": "172:30", "speaker": "E", "text": "Ja, wir haben bei TEST-DR-2025-Q1 Metriken wie Failover-Latenz und API-Error-Raten erfasst. Nimbus OBS hat uns dabei geholfen, einen Bottleneck im Zertifikats-Handshake zu identifizieren. Daraufhin haben wir den mTLS-Handshake parallelisiert, was die Gesamt-RTO um knapp 4 Minuten verbessert hat."}
{"ts": "172:38", "speaker": "I", "text": "Eine letzte Frage zu den Performance-Aspekten: Wie balancieren Sie denn die Kosten für diese Optimierungen mit den SLA-Anforderungen, gerade wenn die Budgets festgezurrt sind?"}
{"ts": "172:44", "speaker": "E", "text": "Wir priorisieren Optimierungen nach ihrem Einfluss auf RTO/RPO und SLA-Penalty-Kosten. Die Parallelisierung des mTLS-Handshake hatte z.B. eine geringe Implementierungskosten, aber hohen SLA-Nutzen. Teurere Maßnahmen, wie zusätzliche dedizierte Netzwerkstrecken, setzen wir nur um, wenn die Risikoanalyse im Rahmen von SEC-RISK-017 einen klaren Business-Impact belegt."}
{"ts": "172:52", "speaker": "I", "text": "Und abschließend: Welche offenen Risiken sehen Sie aktuell noch und wie planen Sie, diese zu mitigieren?"}
{"ts": "172:58", "speaker": "E", "text": "Das größte offene Risiko ist derzeit ein gleichzeitiger Ausfall von zwei Regionen mit Datalake-Impact, kombiniert mit einer kompromittierten mTLS-CA. Hierfür gibt es noch keinen vollautomatischen Pfad. Wir arbeiten an RFC-DR-2025-09, der vorsieht, einen unabhängigen Root-of-Trust aus einer vierten Region zu verwenden. Bis dahin setzen wir auf manuelle Recovery-Schritte aus RB-DR-004 und erhöhen die Frequenz der Security-Drills."}
{"ts": "172:14", "speaker": "I", "text": "Bevor wir zu den abschließenden Lessons Learned kommen – können Sie bitte noch einmal konkret schildern, wie der Runbook RB-DR-001 bei einem plötzlichen Regionenausfall abläuft?"}
{"ts": "172:32", "speaker": "E", "text": "Ja, der Ablauf ist in RB-DR-001 in sieben Schritten dokumentiert. Nach der automatischen Auslöseerkennung durch unseren Health-Check-Cluster in Region West wird innerhalb von 90 Sekunden ein Failover-Trigger an das Orchestrierungssystem gesendet. Dann folgt der manuelle Validate-Step, bei dem ein Duty Engineer gemäß ESC-DR-Auth-Prozess das Umschalten bestätigt."}
{"ts": "172:58", "speaker": "I", "text": "Und dieser Validate-Step – ist das nicht ein potenzieller Bottleneck für die RTO-Ziele?"}
{"ts": "173:10", "speaker": "E", "text": "Wir haben das analysiert: Der Validate-Step dauert durchschnittlich 45 Sekunden. Ohne ihn hätten wir weniger menschliche Kontrolle, was gegen POL-SEC-001 verstoßen würde. Deshalb akzeptieren wir, dass die RTO leicht darunter leidet."}
{"ts": "173:34", "speaker": "I", "text": "Kommen wir auf die Authentifizierung zurück: Welche Mechanismen greifen beim Umschalten?"}
{"ts": "173:46", "speaker": "E", "text": "Wir nutzen ein zweistufiges Verfahren: Erst OAuth2-Token mit minimaler Lebensdauer, danach eine hardwarebasierte FIDO2-Challenge. Das ist im Ticket SEC-DR-482 beschrieben, inklusive Fallback über JIT Access, falls die FIDO-Server in einer Region nicht erreichbar sind."}
{"ts": "174:14", "speaker": "I", "text": "Wie stellen Sie sicher, dass während des Failovers keine unverschlüsselten Datenströme entstehen – gerade im Zusammenspiel mit Poseidon Networking?"}
{"ts": "174:28", "speaker": "E", "text": "Alle interregionalen Links werden über mTLS v1.3 mit mutual Cert-Pinning abgesichert. Poseidon Networking hat dafür eine Auto-Policy-Deployment-Funktion, die in DR-Szenarien per API aus RB-DR-001 Schritt 5 angestoßen wird."}
{"ts": "174:54", "speaker": "I", "text": "Wie sieht es mit Audit-Artefakten aus – was fällt konkret an nach einem Failover?"}
{"ts": "175:07", "speaker": "E", "text": "Wir generieren drei Artefakte: das DR-Event-Log (inklusive Zeitstempeln aller Runbook-Schritte), ein Security-Audit-JSON gemäß AUD-STD-03 und die Netzwerkfluss-Protokolle aus Poseidon. Diese werden automatisch in AUD-Logs integriert und revisionssicher archiviert."}
{"ts": "175:34", "speaker": "I", "text": "Gab es schon Testfälle, bei denen Observability-Daten aus Nimbus OBS für DR-Optimierungen genutzt wurden?"}
{"ts": "175:46", "speaker": "E", "text": "Ja, im GameDay TEST-DR-2025-Q1 haben wir Latenz-Spikes in der Recovery-Region über OBS identifiziert und daraufhin die Pre-Warm-Caches vergrößert. Das hat die RTO um ca. 12 % verbessert."}
{"ts": "176:10", "speaker": "I", "text": "Und in Bezug auf Kosten – wie balancieren Sie die Anforderungen aus POL-FIN-007 mit diesen Optimierungen?"}
{"ts": "176:24", "speaker": "E", "text": "Wir nutzen Budget-Alerts pro Region und Ressourcenklasse. Wenn eine Optimierung wie der größere Cache die Budgetgrenzen überschreitet, muss sie per RFC genehmigt werden. Das ist bei RFC-DR-2025-17 geschehen."}
{"ts": "176:50", "speaker": "I", "text": "Zum Abschluss: Welche offenen Risiken sehen Sie aktuell und wie planen Sie, diese zu mitigieren?"}
{"ts": "177:05", "speaker": "E", "text": "Das größte Risiko bleibt die Abhängigkeit vom Helios Datalake – ein gleichzeitiger Ausfall in mehreren Regionen könnte trotz DR den Datenzugriff verzögern. Wir evaluieren derzeit Cross-Cloud-Replikation als mögliche, wenn auch teure, Mitigation. Die Entscheidung wird im Q3 fallen, basierend auf den Ergebnissen von Testplan TP-DR-2025-06."}
{"ts": "180:14", "speaker": "I", "text": "Lassen Sie uns den Bogen spannen zu den Lessons Learned aus dem letzten Drill – welche konkreten Änderungen haben Sie nach TEST-DR-2025-Q1 umgesetzt, die sowohl Security als auch Kosten betreffen?"}
{"ts": "180:34", "speaker": "E", "text": "Wir haben nach dem Drill die automatischen Policy-Deployments für mTLS zwischen den Regionen optimiert, um unnötige Traffic Peaks zu vermeiden. Kostenmäßig haben wir die Warm-Standby-Knoten in Region-3 auf kleinere Instanztypen umgestellt, ohne die im SLA-DR-02 geforderte RTO von 15 Minuten zu verletzen."}
{"ts": "180:58", "speaker": "I", "text": "Und wie haben Sie sichergestellt, dass diese Anpassungen nicht die Anforderungen aus POL-SEC-001 unterlaufen?"}
{"ts": "181:14", "speaker": "E", "text": "Wir haben jede Änderung durch den Security Change Advisory Board laufen lassen, inklusive eines Reviews mit Bezug auf SEC-DR-452. Zusätzlich wurde ein manueller Failover-Test nach Runbook RB-DR-001 durchgeführt, bei dem wir gezielt unprivilegierte Konten genutzt haben, um den Least-Privilege-Ansatz zu validieren."}
{"ts": "181:42", "speaker": "I", "text": "Hat das Auswirkungen auf die Observability gehabt, gerade im Zusammenspiel mit Nimbus OBS?"}
{"ts": "182:00", "speaker": "E", "text": "Ja, wir mussten die Exporter in Region-3 anpassen, da die verkleinerten Instanzen unter hoher Last zeitweise Telemetrie-Drops zeigten. Das haben wir durch ein leicht verzögertes Scraping im OBS-Config-Template OBS-TMP-219 mitigiert."}
{"ts": "182:26", "speaker": "I", "text": "Kommen wir zurück zu den Abhängigkeiten – wenn der Helios Datalake ausfällt, wie priorisieren Sie dann den Traffic beim Failover?"}
{"ts": "182:42", "speaker": "E", "text": "In diesem Fall greifen wir auf die im DR-Net-Policy definierten Weighted Routing Rules zurück. Diese geben den Kernsystemen Vorrang, während Datalake-gebundene Jobs gedrosselt oder pausiert werden. Das ist in Ticket DR-RULE-778 dokumentiert."}
{"ts": "183:10", "speaker": "I", "text": "Gab es in der Vergangenheit ein Szenario, wo Sie bewusst den BLAST_RADIUS vergrößert haben, um schneller zu recovern?"}
{"ts": "183:24", "speaker": "E", "text": "Ja, im Drill 2024-Q4 haben wir eine Region-übergreifende Storage-Replikation temporär für alle Buckets aktiv geschaltet, statt nur für die kritischen. Das erhöhte zwar die Netzwerkkosten um ca. 18%, reduzierte aber die Wiederherstellungszeit um knapp 5 Minuten."}
{"ts": "183:56", "speaker": "I", "text": "War das eine spontane Entscheidung oder vorher geplant?"}
{"ts": "184:08", "speaker": "E", "text": "Es war als Option in unserem DR-Playbook hinterlegt, allerdings mit klarer Freigabe durch den Incident Commander. Die Abwägung erfolgte anhand der in SLA-DR-02 definierten Maximal-Ausfallzeiten."}
{"ts": "184:32", "speaker": "I", "text": "Wie gehen Sie mit den Audit-Artefakten um, die solche Sondermaßnahmen belegen?"}
{"ts": "184:46", "speaker": "E", "text": "Alle Aktionen werden in den AUD-Logs mit Event-Type 'Override' versehen. Diese Logs werden dann automatisiert in unser Compliance-Repository eingespielt und mit Referenz auf das DR-Event-Tag, z.B. DR-EVT-2024-Q4-OVR, versehen."}
{"ts": "185:12", "speaker": "I", "text": "Und abschließend – welche offenen Risiken sehen Sie aktuell noch?"}
{"ts": "185:28", "speaker": "E", "text": "Ein Risiko ist die noch nicht vollständig getestete Integration der neuen JIT-Access-Komponente für Admin-Rollen während eines Failovers. Ohne vollständige Tests besteht die Gefahr, dass im Ernstfall entweder Zugriffe blockiert oder zu weit geöffnet werden. Wir planen hierfür ein gezieltes Szenario in TEST-DR-2025-Q3."}
{"ts": "187:34", "speaker": "I", "text": "Lassen Sie uns jetzt noch einmal auf die Lessons Learned eingehen. Gab es nach dem letzten Drill etwas, das Sie völlig umdenken ließ?"}
{"ts": "187:38", "speaker": "E", "text": "Ja, tatsächlich. Beim Drill im März hat sich gezeigt, dass unsere Annahmen zum Failback-Fenster zu optimistisch waren. Wir mussten das Runbook RB-DR-001 um einen Schritt erweitern, bei dem wir die Poseidon mTLS-Policies vor dem Rückschalten temporär lockern, um die Synchronisation zu beschleunigen."}
{"ts": "187:45", "speaker": "I", "text": "Das klingt nach einem bewussten Kompromiss bei der Sicherheit. Wie haben Sie das gerechtfertigt?"}
{"ts": "187:50", "speaker": "E", "text": "Wir haben es als kalkuliertes Risiko dokumentiert: Ticket SEC-EXC-2025-04, genehmigt durch den Security Architecture Lead, begrenzt auf maximal 12 Minuten und nur unter Audit-Log-Bedingungen. Die RTO-Vorgabe von 30 Minuten hätte sich ohne diese Maßnahme nicht einhalten lassen."}
{"ts": "187:58", "speaker": "I", "text": "Wie fließt das in Ihre Risiko-Matrix ein?"}
{"ts": "188:02", "speaker": "E", "text": "Wir haben den BLAST_RADIUS für diese Maßnahme klar definiert – nur die Synchronisations-Subnetze sind betroffen. In der Matrix wird dies als 'Medium Risk, High Impact on RTO' geführt und mit monatlicher Review-Pflicht versehen."}
{"ts": "188:10", "speaker": "I", "text": "Und welche Rolle spielt dabei die Helios Datalake-Abhängigkeit?"}
{"ts": "188:14", "speaker": "E", "text": "Helios liefert einige der Metadaten, die für den Rebuild der Search-Indices gebraucht werden. Wenn Helios im DR-Szenario verzögert ist, müssen wir Fallback-Dumps aus der Titan DR Cold Storage ziehen. Das ist langsamer und wirkt sich direkt auf die RTO aus."}
