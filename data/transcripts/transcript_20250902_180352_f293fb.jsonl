{"ts": "00:00", "speaker": "I", "text": "Können Sie bitte kurz umreißen, welche Hauptaufgaben Sie im Orion Edge Gateway Projekt haben?"}
{"ts": "02:15", "speaker": "E", "text": "Ja, gern. Ich bin im Build-Phase-Team als Senior DevSecOps Engineer tätig. Meine Hauptaufgaben sind die Integration der Authentifizierungsschicht, die mTLS-Absicherung aller Upstream-Connections sowie das Aufsetzen der Rate-Limiting-Policies. Zusätzlich übernehme ich die Deployment-Automatisierung und sorge für die Einhaltung der SLAs im laufenden Betrieb."}
{"ts": "05:00", "speaker": "I", "text": "Wie ist denn die Zusammenarbeit zwischen DevOps und SRE bei Ihnen konkret organisiert?"}
{"ts": "07:20", "speaker": "E", "text": "Wir arbeiten im sogenannten 'Ops Pod'-Modell. DevOps kümmert sich primär um die Build-Pipelines, Infrastructure-as-Code und CI/CD. SRE übernimmt danach Monitoring, Incident Response und Reliability-Engineering. Es gibt einen wöchentlichen Sync, in dem wir Übergabepunkte durchgehen – vor allem bei sicherheitskritischen Deployments wie im Gateway."}
{"ts": "10:05", "speaker": "I", "text": "Welche sicherheitsrelevanten Anforderungen sind Ihnen zu Beginn des Projekts kommuniziert worden?"}
{"ts": "13:10", "speaker": "E", "text": "Ganz am Anfang, in RFC-ORI-SEC-01, wurde klar festgelegt: Alle externen und internen APIs müssen über mTLS abgesichert werden, Auth muss via Aegis IAM laufen, und wir dürfen keine statischen API-Keys persistieren. Außerdem gab es Vorgaben zum Rate Limiting: Maximal 500 Requests pro Sekunde pro Client, mit Burst bis 800, um DoS zu verhindern."}
{"ts": "16:45", "speaker": "I", "text": "Wie haben Sie die mTLS-Handshake-Implementierung geplant, um Risiken wie in Ticket GW-4821 zu vermeiden?"}
{"ts": "20:00", "speaker": "E", "text": "In GW-4821 hatten wir ja das Problem mit veralteten Cipher Suites. Für Orion haben wir deshalb in der Planungsphase eine zentrale TLS-Konfiguration in unserem Policy-as-Code-Repo definiert, die automatisch über die Gateways ausgerollt wird. Wir erzwingen TLS 1.3 und mutual Auth mit Client-Zertifikatsprüfung gegen eine interne CA. Die Handshake-Timeouts sind auf 2s gesetzt, um Hängesituationen früh zu beenden."}
{"ts": "23:40", "speaker": "I", "text": "Welche Policy-as-Code Ansätze setzen Sie für die Auth-Integration ein?"}
{"ts": "27:05", "speaker": "E", "text": "Wir verwenden Open Policy Agent (OPA) mit Rego-Policies, die in Git versioniert sind. Jede Änderung durchläuft den gleichen CI/CD-Prozess wie der Code. Bei Merge Requests laufen Policy-Tests gegen simulierten Traffic, um sicherzustellen, dass wir keine legitimen Requests blockieren."}
{"ts": "30:50", "speaker": "I", "text": "Wie stellen Sie sicher, dass Rate Limiting nicht versehentlich legitimen Traffic blockiert?"}
{"ts": "34:00", "speaker": "E", "text": "Wir monitoren die 429-Response-Rate pro Client-ID. In RB-GW-004 haben wir definiert, dass bei Anstieg über 0,5% in 10 Minuten ein Alert getriggert wird. Dann prüfen wir, ob es legitime Lastspitzen sind, etwa durch geplante Kampagnen, und passen temporär die Limits an."}
{"ts": "38:30", "speaker": "I", "text": "Welche Metriken nutzen Sie, um das SLA-ORI-02 p95 Latency < 120ms einzuhalten?"}
{"ts": "42:10", "speaker": "E", "text": "Wir erfassen p95 und p99 Latenzen pro Endpoint via Prometheus, getrennt nach Auth- und Non-Auth-Routen. Zusätzlich tracken wir TLS-Handshake-Dauer separat, weil die stark schwanken kann. Das SLA-ORI-02 betrachten wir verletzt, wenn p95 über 120ms für länger als 5 Minuten liegt."}
{"ts": "46:25", "speaker": "I", "text": "Können Sie mir den Ablauf aus RB-GW-011 für Blue/Green Deployments schildern?"}
{"ts": "50:00", "speaker": "E", "text": "RB-GW-011 beschreibt, dass wir zuerst den Green-Cluster mit neuer Version hochfahren, dann über Canary-Routing 10% des Traffics umleiten. Wenn Error-Rate und Latenz stabil bleiben, schalten wir auf 100% um und fahren den Blue-Cluster kontrolliert herunter. Rollback erfolgt über simples Rerouting des Loadbalancers."}
{"ts": "90:00", "speaker": "I", "text": "Sie haben vorhin schon kurz mTLS erwähnt. Mich würde jetzt interessieren: wie konkret haben Sie in der Planungsphase verhindert, dass das Problem aus GW-4821 – Sie erinnern sich, dieser Handshake-Timeout unter Last – nicht erneut auftritt?"}
{"ts": "90:07", "speaker": "E", "text": "Ja, genau, GW-4821 war ein schmerzhafter Lerneffekt. Wir haben deshalb im Design-Doc ORI-Sec-07 festgelegt, dass die Handshake-Puffer in der Gateway-Implementierung um 25% über dem Peak-Wert aus dem Lasttest liegen. Zusätzlich haben wir in Runbook RB-GW-009 eine Canary-Phase für mTLS-Änderungen definiert, die immer mit synthetischem Traffic gegen unsere Staging-IAM läuft."}
{"ts": "90:21", "speaker": "I", "text": "Und wie hängt das mit Ihrer Auth-Integration und Aegis IAM zusammen?"}
{"ts": "90:27", "speaker": "E", "text": "Das ist mehrstufig: Der mTLS-Handshake prüft zunächst das Client-Zertifikat gegen eine interne CRL, die wir per gRPC vom Aegis IAM abrufen. Falls der IAM-Service verzögert antwortet, greift ein 200 ms Fallback-Cache. Diese Werte haben wir mit Nimbus Observability korreliert, um zu sehen, ob es zeitgleich Spike-Warnungen im Auth-Service gab."}
{"ts": "90:44", "speaker": "I", "text": "Sie sprechen von Korrelation – welche Tools setzen Sie dafür ein?"}
{"ts": "90:50", "speaker": "E", "text": "Wir haben in der Nimbus Observability Plattform ein Trace-Tagging eingeführt: jeder Gateway-Request bekommt eine Trace-ID, die sowohl in den API-Gateway-Logs als auch im Auth-Integration-Service auftaucht. In Runbook RB-OBS-004 ist beschrieben, wie man mit einem einzigen Query in der Trace-UI den Pfad vom mTLS-Handshake bis zur Tokenvalidierung verfolgen kann."}
{"ts": "91:08", "speaker": "I", "text": "Kommen wir kurz zum Rate Limiting: wie verhindern Sie, dass legitimer Traffic blockiert wird, vor allem wenn der Auth-Service hakt?"}
{"ts": "91:14", "speaker": "E", "text": "Wir haben eine adaptive Rate-Limiting-Policy als Code (auf Basis von ORI-Policy-Lib v2). Die berücksichtigt nicht nur Requests pro Sekunde, sondern auch den Auth-Status. Wenn der Auth-Service temporär langsamer ist, hebt das Gateway die Burst-Toleranz an, um legitime Benutzer nicht auszuschließen. Das Ganze ist in RFC-ORI-14 dokumentiert."}
{"ts": "91:31", "speaker": "I", "text": "Und wie überprüfen Sie, ob das SLA-ORI-02 mit p95 Latency < 120 ms eingehalten wird?"}
{"ts": "91:37", "speaker": "E", "text": "Wir sammeln Latenzen aus Envoy-Metriken direkt im Gateway und forwarden die an Nimbus. Dort laufen Alerts auf Basis von PromQL, die bei Überschreitung des 100 ms p95 Vorwarnungen geben. Außerdem gibt es im Runbook RB-SLA-ORI einen wöchentlichen Review-Prozess der Metriken."}
{"ts": "91:54", "speaker": "I", "text": "Sie haben Cross-Projekt-Abhängigkeiten erwähnt. Können Sie ein Beispiel geben, wie Lessons Learned aus Helios Datalake hier eingeflossen sind?"}
{"ts": "92:00", "speaker": "E", "text": "Ja, in Helios hatten wir ein Problem mit verzögerter Log-Indizierung. Daraus haben wir hier abgeleitet, dass wir Gateway-Logs erst in einen lokalen Puffer schreiben und batchweise an Nimbus senden. Das reduziert sowohl Latenz im Request-Path als auch Risiko von Datenverlust bei Netzwerkproblemen."}
{"ts": "92:15", "speaker": "I", "text": "Interessant. Und gab es auch Übertragungen aus Poseidon Networking?"}
{"ts": "92:20", "speaker": "E", "text": "Definitiv. Poseidon hat uns gelehrt, mTLS-Session-Reuse aggressiver zu nutzen. Wir haben das in ORI übernommen, indem wir Session-Tickets für 5 Minuten zulassen, was die Handshake-Last halbiert. Das war ein Abwägen zwischen Sicherheit und Performance."}
{"ts": "92:34", "speaker": "I", "text": "Das klingt schon nach einem Trade-off. Welche Risiken sehen Sie da noch?"}
{"ts": "92:40", "speaker": "E", "text": "Das Hauptrisiko ist, dass ein kompromittiertes Zertifikat länger gültig bleibt, solange das Session-Ticket aktiv ist. Wir mitigieren das, indem wir bei kritischen CRL-Updates die Sessions sofort invalidieren. Dieser Prozess ist in RB-GW-015 beschrieben und wurde zuletzt im Drill TCK-ORI-221 getestet."}
{"ts": "98:00", "speaker": "I", "text": "Lassen Sie uns jetzt konkret auf die Entscheidungen eingehen, wo Sie zwischen Sicherheit und Performance abwägen mussten. Gab es da einen markanten Fall im Orion Edge Gateway?"}
{"ts": "98:10", "speaker": "E", "text": "Ja, das prominenteste Beispiel war beim Rollout der erweiterten Cipher-Suite für mTLS. Wir hatten in RFC-ORI-017 dokumentiert, dass wir von TLS 1.2 auf 1.3 wechseln und nur noch elliptische Kurven mit mindestens 256 Bit zulassen. Das brachte im Staging einen ca. 8% höheren Handshake-Overhead pro Verbindung."}
{"ts": "98:28", "speaker": "I", "text": "Und wie sind Sie damit umgegangen? Haben Sie das akzeptiert oder mitigiert?"}
{"ts": "98:37", "speaker": "E", "text": "Wir haben in Runbook RB-GW-014 eine Ausnahme für interne Service-zu-Service-Calls definiert, die weiterhin eine optimierte Curve verwenden, um die Latenz unter dem SLA-ORI-02 p95 < 120ms zu halten. Für externe Clients blieb die strenge Suite aktiv."}
{"ts": "98:54", "speaker": "I", "text": "Das klingt nach einem bewussten Trade-off. Wie haben Sie die Risiken bewertet?"}
{"ts": "99:02", "speaker": "E", "text": "Wir haben ein Security-Risk-Assessment gemäß SEC-Template-05 durchgeführt. Das interne Netz ist durch Poseidon Networking segmentiert, und Traffic zwischen den Subnetzen wird zusätzlich über Aegis IAM Tokens abgesichert. Das reduzierte die Angriffsfläche deutlich."}
{"ts": "99:20", "speaker": "I", "text": "Gab es dennoch Bedenken aus dem Security-Team?"}
{"ts": "99:26", "speaker": "E", "text": "Ja, ein Kollege hat in Ticket SEC-921 angemerkt, dass wir die Ausnahme jährlich revalidieren müssen, um sicherzustellen, dass keine neuen Exploits gegen die gewählte Curve auftauchen."}
{"ts": "99:39", "speaker": "I", "text": "Wie dokumentieren Sie solche jährlichen Prüfungen?"}
{"ts": "99:45", "speaker": "E", "text": "Wir führen das im Compliance-Abschnitt des Runbooks nach, mit Verweis auf die entsprechenden Security-Bulletins. Außerdem setzen wir in Nimbus Observability einen jährlichen Alert-Reminder, der ein Jira-Subtask erstellt."}
{"ts": "100:02", "speaker": "I", "text": "Interessant. Gab es ähnliche Performance-Sicherheits-Abwägungen beim Rate Limiting?"}
{"ts": "100:09", "speaker": "E", "text": "Ja, beim globalen Burst-Rate-Limit. Ursprünglich war in RFC-ORI-021 ein sehr restriktiver Wert vorgesehen. In Lasttests blockierte das aber legitime Traffic-Spitzen aus dem Helios Datalake Export-Service. Wir haben dann pro-Service-Limits eingeführt, basierend auf historischen Traffic-Profilen."}
{"ts": "100:27", "speaker": "I", "text": "Wie haben Sie sichergestellt, dass diese Anpassung nicht zu Missbrauch führt?"}
{"ts": "100:33", "speaker": "E", "text": "Durch Kombination aus Policy-as-Code in unserem API Gateway Config Repo und Echtzeit-Monitoring via Nimbus. Jeder ungewöhnliche Spike triggert einen Incident-Flow nach RB-GW-009, der innerhalb von 15 Minuten eine manuelle Review vorsieht."}
{"ts": "100:50", "speaker": "I", "text": "Abschließend: Welche offenen Risiken sehen Sie aktuell noch im mTLS-Setup?"}
{"ts": "101:00", "speaker": "E", "text": "Das größte Restrisiko liegt in der Verwaltung der Client-Zertifikate. Ticket GW-517 trackt die Automatisierung des Zertifikats-Rollovers. Bis das abgeschlossen ist, besteht ein manuelles Handling, das fehleranfällig sein kann. Wir mitigieren das durch doppelte Review und wöchentliche Checks im Deployment-Board."}
{"ts": "114:00", "speaker": "I", "text": "Sie hatten vorhin angedeutet, dass es eine Spannung zwischen Latenz und Sicherheit gab. Können Sie das bitte noch einmal konkret machen und auf welche Daten Sie sich dabei gestützt haben?"}
{"ts": "114:05", "speaker": "E", "text": "Ja, also wir standen vor der Wahl, ob wir die mTLS-Handshake-Timouts von 500ms auf 300ms reduzieren, um p95-Latenzen besser einzuhalten. Das hätte aber bei schwankender Netzwerkqualität zu mehr Handshake-Abbrüchen geführt. Die Metriken aus dem SLA-ORI-02 Monitoring – speziell der Vergleich aus den letzten drei Sprints – haben gezeigt, dass 500ms zwar knapp, aber stabil waren."}
{"ts": "114:15", "speaker": "I", "text": "Und warum haben Sie dann nicht einfach auf 300ms gesetzt und mit Retries gearbeitet?"}
{"ts": "114:20", "speaker": "E", "text": "Weil Retries in diesem Fall den Auth-Service stärker belasten würden. In RFC-ORI-17 habe ich dokumentiert, dass zusätzliche Retries bei mTLS den CPU-Load am Gateway-Knoten um bis zu 18% erhöhen – gemessen in einem Lasttest mit Szenario LT-GW-07."}
{"ts": "114:32", "speaker": "I", "text": "Gab es intern Diskussionen dazu, wie viel CPU-Load akzeptabel ist?"}
{"ts": "114:36", "speaker": "E", "text": "Ja, wir haben als informelle Daumenregel im DevOps/SRE-Standup: niemals dauerhaft über 65% Auslastung pro Core für Gateway-Prozesse. Das ist nicht offiziell in einem SLA, steht aber in Runbook RB-GW-011 unter 'Operational Heuristics'."}
{"ts": "114:46", "speaker": "I", "text": "Welche Risiken sehen Sie trotz der jetzigen Timeout-Einstellung noch?"}
{"ts": "114:50", "speaker": "E", "text": "Das Haupt-Risiko ist, dass bei plötzlichen Latenzspitzen im Aegis IAM Cluster – z.B. wenn ein Node im Maintenance Mode ist – die Handshakes trotzdem ablaufen. Wir haben Alert-Regeln in Nimbus Observability, die bei >5% Handshake-Failures in 5 Minuten Alarm schlagen, aber es bleibt eine Reaktionszeit."}
{"ts": "115:02", "speaker": "I", "text": "Haben Sie über Fallback-Mechanismen nachgedacht?"}
{"ts": "115:06", "speaker": "E", "text": "Ja, in RFC-ORI-23 diskutieren wir ein optionales Failover auf Token-basiertes Auth, falls mTLS mehrfach scheitert. Aber das birgt Sicherheitsrisiken, da Tokens länger gültig sind. Deshalb haben wir das bisher nur als 'break glass' Option im Runbook RB-GW-015 vermerkt."}
{"ts": "115:18", "speaker": "I", "text": "Wie stellen Sie sicher, dass solche 'break glass'-Aktionen nicht missbraucht werden?"}
{"ts": "115:22", "speaker": "E", "text": "Nur SRE Leads haben die Berechtigung, und jeder Einsatz muss in Ticket-System ORI-JIRA mit Incident-Typ 'SEC-FAL' protokolliert werden. Zusätzlich gibt es einen Audit-Job, der wöchentlich prüft, ob Tokens außerhalb der Norm erstellt wurden."}
{"ts": "115:34", "speaker": "I", "text": "Wie dokumentieren Sie all diese Entscheidungen, damit neue Teammitglieder den Kontext verstehen?"}
{"ts": "115:38", "speaker": "E", "text": "Wir führen ein zentrales Confluence-Space 'ORI-DEC', in dem alle RFCs, Runbooks und Lessons Learned verlinkt sind. Jede kritische Änderung wie die Timeout-Diskussion hat einen eigenen Abschnitt mit Datum, Entscheidung, Begründung und Messdaten."}
{"ts": "115:48", "speaker": "I", "text": "Gab es schon einen Fall, wo diese Dokumentation eine falsche Entscheidung verhindert hat?"}
{"ts": "115:52", "speaker": "E", "text": "Ja, ein neuer Kollege wollte testweise die Cipher-Suite ändern, um angeblich die Performance zu steigern. Durch die Dokumentation in RFC-ORI-11 konnte er sehen, dass wir das schon untersucht hatten und es zu Inkompatibilitäten mit Poseidon Networking kam – er hat den Änderungsversuch sofort verworfen."}
{"ts": "116:00", "speaker": "I", "text": "Wir hatten eben schon die Trade-offs angesprochen. Mich würde jetzt interessieren: können Sie noch einmal beschreiben, wie genau Sie im Orion Edge Gateway das Blue/Green Deployment aus RB-GW-011 operationalisieren?"}
{"ts": "116:15", "speaker": "E", "text": "Ja, gern. RB-GW-011 beschreibt bei uns Schritt für Schritt den Ablauf – beginnend mit dem Pre-Deployment Health Check in der Green-Umgebung. Wir fahren dort alle mTLS-Handshake Tests mit simulierten Client-Zertifikaten, bevor wir den Load Balancer switchen. Wichtig ist, dass wir die Canary Requests aus dem Nimbus Observability Test-Cluster nutzen, um Latenz und Fehlerraten vorab zu messen."}
{"ts": "116:40", "speaker": "I", "text": "Und wie stellen Sie sicher, dass die Metriken den SLA-ORI-02 Vorgaben entsprechen, bevor der Switch erfolgt?"}
{"ts": "116:52", "speaker": "E", "text": "Wir haben in Prometheus einen speziellen SLA-Dashboard-Job, der das p95 Latenzfenster über 10 Minuten vor dem Cutover prüft. Liegen wir über 120 ms, wird laut Runbook automatisch ein Rollback auf Blue ausgelöst. Diese Prüfung ist im Jenkins-Pipeline-Step `verify_sla()` implementiert."}
{"ts": "117:15", "speaker": "I", "text": "Das klingt sauber. Gab es dabei schon mal False Positives, die Sie ausbremsen?"}
{"ts": "117:25", "speaker": "E", "text": "Einmal, ja. Da hatten wir durch eine verspätete Log-Flush-Operation im Helios Datalake eine Verzögerung in der Metrikaggregation. Wir haben daraus gelernt und im Runbook notiert, dass bei Verdacht auf Metrik-Lags manuell mit Live-Traffic-Tests gegengeprüft wird."}
{"ts": "117:47", "speaker": "I", "text": "Okay. Apropos Helios: sehen Sie aktuell weitere Abhängigkeiten, die kritisch sind?"}
{"ts": "117:56", "speaker": "E", "text": "Ja, besonders die Schnittstellen zur Aegis IAM Plattform. Die Tokenvalidierung ist für jede Anfrage zwingend. Wir haben hier eine gegenseitige mTLS-Verbindung plus signierte JWTs. Wenn Aegis verzögert antwortet, müssen wir auf den Caching Layer im Gateway ausweichen, der im RFC-ORI-SEC-09 beschrieben ist."}
{"ts": "118:20", "speaker": "I", "text": "Wie koordinieren Sie Änderungen an diesen Schnittstellen?"}
{"ts": "118:29", "speaker": "E", "text": "Über wöchentliche Cross-Project-Standups mit Aegis und Nimbus Teams. Zusätzlich führen wir vor jedem Release einen Contract-Test-Lauf durch, der die OpenAPI-Spezifikationen abgleicht. Änderungen werden in unserem internen Changelog-Dienst 'OrionOps' erfasst."}
{"ts": "118:52", "speaker": "I", "text": "Hatten Sie schon Situationen, wo Auth-Integration-Service komplett fehlschlug?"}
{"ts": "119:02", "speaker": "E", "text": "Ja, im Ticket GW-5173. Da war das Zertifikat im Auth-Service abgelaufen. Laut Incident-Runbook RB-AUTH-004 wechselten wir auf den Backup-Service in einer anderen Region und setzten temporär die TTL im Cache hoch, um die Ausfälle für Clients zu minimieren."}
{"ts": "119:25", "speaker": "I", "text": "War das ein automatischer Failover oder manuell?"}
{"ts": "119:33", "speaker": "E", "text": "Teil-automatisch. Das Monitoring in Nimbus hat den Ausfall bemerkt und ein Alert ausgelöst. Den Failover-Schritt mussten wir aber manuell in der Orchestrierung anstoßen, um sicherzustellen, dass keine halbfertigen Handshakes in den neuen Service gezogen werden."}
{"ts": "119:53", "speaker": "I", "text": "Letzte Frage: Welche Lessons Learned aus Poseidon Networking konnten Sie im Orion-Projekt umsetzen?"}
{"ts": "120:00", "speaker": "E", "text": "Die größte war, dass wir bei TLS-Parametern nicht nur die Cipher Suites, sondern auch die Session Resumption Strategien dokumentieren. Poseidon hatte damals Latenzspitzen wegen fehlender Session Tickets – deshalb haben wir in Orion von Anfang an das in RFC-ORI-NET-05 festgehalten und in allen Runbooks verlinkt."}
{"ts": "132:00", "speaker": "I", "text": "Bevor wir zum Abschluss kommen, würde mich noch interessieren, wie Sie im Build-Phase-Kontext die Abhängigkeiten zu Nimbus Observability täglich managen."}
{"ts": "132:15", "speaker": "E", "text": "Wir haben da einen festen Sync-Slot um 09:15 Uhr, in dem wir Log- und Trace-IDs aus dem Orion Edge Gateway mit den Nimbus-Streams verheiraten. Das ist im Runbook RB-OBS-014 dokumentiert, inklusive der Fallback-Strategie, falls der Trace-Injektor ausfällt."}
{"ts": "132:37", "speaker": "I", "text": "Fallback-Strategie – können Sie das konkretisieren? Ich frage, weil gerade im Incident IR-7891 da wohl ein Gap war."}
{"ts": "132:55", "speaker": "E", "text": "Ja, im IR-7891 hatten wir den Fall, dass der Trace-Injektor 12 Minuten nicht verfügbar war. Laut Runbook leiten wir dann die Logs temporär in den lokalen Elastic-Buffer um und taggen sie mit 'deferred-trace'. Später werden sie über ein Reconcile-Skript in Nimbus importiert."}
{"ts": "133:20", "speaker": "I", "text": "Gut, und wie stellen Sie sicher, dass beim Reconcile keine Duplikate entstehen?"}
{"ts": "133:33", "speaker": "E", "text": "Wir nutzen da einen dedizierten UUID-Generator pro Request, der auch bei Retry konstant bleibt. Nimbus verwirft dann beim Merge anhand dieser UUIDs Duplikate. Das ist in der RFC-ORI-024 festgehalten."}
{"ts": "133:56", "speaker": "I", "text": "Wechseln wir kurz: In SLA-ORI-02, p95 Latenz < 120ms, haben Sie erwähnt, dass Sie bestimmte Metriken überwachen. Ist das rein Gateway-intern oder auch cross-service?"}
{"ts": "134:15", "speaker": "E", "text": "Cross-service. Wir messen nicht nur im Gateway, sondern auch die Auth-Response-Zeiten aus der Aegis IAM Plattform. Das ist wichtig, weil mTLS-Handshake und Token-Validation direkt ins Latenzbudget schlagen."}
{"ts": "134:36", "speaker": "I", "text": "Und wenn die Auth-Response über 60ms liegt?"}
{"ts": "134:49", "speaker": "E", "text": "Dann greift ein Pre-Warning im Prometheus-Alert, wir drosseln optional per Rate-Limit-Regel SL-Temp-05 nur nicht-kritische APIs, um kritische Pfade zu schützen."}
{"ts": "135:10", "speaker": "I", "text": "Dieses SL-Temp-05, ist das codiert oder manuell?"}
{"ts": "135:22", "speaker": "E", "text": "Codiert als Policy-as-Code in unserem Gatekeeper-Repo. Wir können sie on-the-fly aktivieren, weil der Deploy-Pfad für Policies getrennt von der Applikation ist."}
{"ts": "135:43", "speaker": "I", "text": "Okay. Letzte Frage: Was ist aus Ihrer Sicht die dringendste Lesson Learned aus der Build-Phase?"}
{"ts": "135:56", "speaker": "E", "text": "Dass wir Auth und Observability nicht als Add-ons betrachten dürfen. Die frühe Integration in CI/CD hätte uns bei GW-4821 viel Debug-Zeit gespart."}
{"ts": "136:15", "speaker": "I", "text": "Würden Sie das, äh, als festen Standard in künftigen Projekten verankern?"}
{"ts": "136:30", "speaker": "E", "text": "Ja, absolut. Wir haben dazu bereits einen Draft für RFC-ORI-030 erstellt, der genau dieses Vorgehen als Pflichtschritt vor dem Build-Gate definiert."}
{"ts": "143:00", "speaker": "I", "text": "Sie hatten vorhin erwähnt, dass die Rate Limiting Regeln in einer eigenen Policy-Definition gepflegt werden. Können Sie mir bitte ein Beispiel geben, wie Sie dort Ausnahmen für interne Services definieren?"}
{"ts": "143:07", "speaker": "E", "text": "Ja, klar. In unserer Policy-as-Code Implementierung im Repo `gw-policies.git` nutzen wir YAML-Files, die mit einem Service-Tag versehen werden. Für interne Services setzen wir etwa `bypass: true`, aber nur wenn sie in der Whitelist aus `RB-GW-007` auftauchen. So verhindern wir, dass versehentlich neue interne Endpunkte ohne Review durchkommen."}
{"ts": "143:21", "speaker": "I", "text": "Und wie stellen Sie sicher, dass diese Whitelist aktuell bleibt?"}
{"ts": "143:26", "speaker": "E", "text": "Wir haben einen nightly Job in Jenkins, der die Whitelist gegen den Service-Registry-Export aus dem Aegis IAM vergleicht. Änderungen erzeugen automatisch ein Ticket, z.B. `GW-MAINT-245`, und müssen von DevOps und Security gemeinsam genehmigt werden."}
{"ts": "143:39", "speaker": "I", "text": "Interessant. Kommen wir zum Monitoring: Welche Metriken sehen Sie sich täglich an, um das SLA-ORI-02 einzuhalten?"}
{"ts": "143:45", "speaker": "E", "text": "Primär p95 Latency und Error Rate pro Endpoint. Wir haben im Nimbus Observability ein Dashboard `ORI-Gateway-Latency` mit Alerts bei >110 ms p95 für mehr als 3 Minuten. Zusätzlich checken wir mTLS-Handshake-Dauer, weil schleppende Handshakes oft Vorboten von Zertifikat-Problemen sind."}
{"ts": "143:59", "speaker": "I", "text": "Hatten Sie schon mal einen Fall, wo der Handshake die Latenz massiv beeinflusst hat?"}
{"ts": "144:04", "speaker": "E", "text": "Ja, im Ticket `INC-GW-531` vor zwei Wochen. Ein ablaufendes Zwischenzertifikat führte zu Retries. Wir haben daraufhin Runbook `RB-MTLS-004` ergänzt: Schritt 3 sieht nun vor, die CRL-Checks im Staging zu simulieren, um solche Verlangsamungen früher zu erkennen."}
{"ts": "144:18", "speaker": "I", "text": "Wie kommunizieren Sie solche Runbook-Änderungen intern?"}
{"ts": "144:23", "speaker": "E", "text": "Über den wöchentlichen SRE-Standup und einen Eintrag im internen Confluence-Bereich \"Gateway Ops\". Außerdem pusht unser Runbook-Repo einen Changelog in den `#gw-ops` Slack-Channel."}
{"ts": "144:34", "speaker": "I", "text": "Gab es beim Blue/Green Deployment nach RB-GW-011 schon mal unvorhergesehene Probleme?"}
{"ts": "144:39", "speaker": "E", "text": "Einmal, ja. Beim Umschalten hat der Green-Cluster aufgrund einer fehlerhaften ConfigMap die Auth-Integration nicht geladen. Das führte zu sofortigen 401-Fehlern. Seitdem haben wir in RB-GW-011 eine Pre-Switch-Checklist eingebaut, die auch den Auth-Healthcheck prüft."}
{"ts": "144:53", "speaker": "I", "text": "Wenn Sie Cross-Project Abhängigkeiten betrachten – wie lief die Abstimmung mit Nimbus Observability beim Einführen neuer Trace-IDs?"}
{"ts": "144:59", "speaker": "E", "text": "Wir haben mit dem Nimbus-Team im Ticket `OBS-REQ-118` definiert, dass das Gateway den `x-orion-trace` Header injiziert. Das erforderte Anpassungen im Helios Datalake, damit Logs und Traces korrekt gematcht werden. War ein schönes Beispiel für Multi-Team-Koordination."}
{"ts": "145:13", "speaker": "I", "text": "Sehen Sie in diesem Trace-Setup noch Risiken?"}
{"ts": "145:18", "speaker": "E", "text": "Ja, wenn ein Downstream-Service den Header überschreibt, verlieren wir die Kette. Wir planen deshalb laut RFC-ORI-TRC-05 eine Sig-Verifikation des Headers am Gateway selbst, um Manipulationen zu erkennen, bevor wir die Daten nach Nimbus schicken."}
{"ts": "145:00", "speaker": "I", "text": "Lassen Sie uns noch einmal konkret auf den Betrieb eingehen: Wie nutzen Sie aktuell RB-GW-011 für die Blue/Green Deployments, und gab es Anpassungen seit der letzten Iteration?"}
{"ts": "145:05", "speaker": "E", "text": "Ja, wir haben den Ablauf aus RB-GW-011 leicht erweitert. Ursprünglich hatten wir einen starren Umschaltplan mit festen Health-Check Intervallen. Jetzt prüfen wir zusätzlich die mTLS-Session-Establish-Zeit als Kriterium, weil wir in einem Incident laut Ticket GW-5093 gesehen haben, dass eine schnelle Umschaltung Traffic mit ungültigen Zertifikaten erzeugt hat."}
{"ts": "145:15", "speaker": "I", "text": "Das heißt, Sie haben den Runbook-Trigger erweitert? Wie wirkt sich das auf die Dauer des Deployments aus?"}
{"ts": "145:21", "speaker": "E", "text": "Genau, der Trigger ist jetzt zweistufig. Wir warten nicht nur auf erfolgreiche HTTP 200 Responses, sondern auch auf einen mTLS Handshake < 80ms p95. Das verlängert den Umschaltvorgang um durchschnittlich 45 Sekunden, aber reduziert die Fehlerrate beim ersten Traffic-Peak signifikant."}
{"ts": "145:33", "speaker": "I", "text": "Wie messen Sie diese Handshake-Zeiten? Nutzen Sie Nimbus Observability oder interne Tools?"}
{"ts": "145:39", "speaker": "E", "text": "Wir haben die Metrik über Nimbus angebunden, aber die Messpunkte kommen direkt aus dem Envoy-Filter im API Gateway. Nimbus aggregiert sie für unsere Dashboards, sodass wir im Deployment-Dashboard sowohl Latenzmetriken als auch Auth-Fehlerquoten sehen."}
{"ts": "145:50", "speaker": "I", "text": "Gab es dabei Abhängigkeiten zur Aegis IAM Plattform, die Sie beachten mussten?"}
{"ts": "145:56", "speaker": "E", "text": "Ja, die mTLS-Handshake-Zeit hängt auch vom OCSP-Responder der Aegis IAM Plattform ab. Wir mussten in Abstimmung mit dem IAM-Team den Cache-TTL für Zertifikatsprüfungen erhöhen, um unnötige Roundtrips zu vermeiden. Diese Änderung wurde in RFC-ORI-014 festgehalten."}
{"ts": "146:08", "speaker": "I", "text": "Können Sie mir ein Beispiel geben, wie sich diese Anpassung in einem Staging-Test ausgewirkt hat?"}
{"ts": "146:15", "speaker": "E", "text": "Im Staging haben wir mit simulierten 500 gleichzeitigen Verbindungen getestet. Vor der Anpassung lag die p95 Handshake-Zeit bei 120ms, danach bei 78ms. Die Fehlerquote bei der Auth-Initialisierung sank von 2,3% auf 0,4%."}
{"ts": "146:27", "speaker": "I", "text": "Das klingt solide. Gab es dabei Risiken, die Sie akzeptieren mussten?"}
{"ts": "146:33", "speaker": "E", "text": "Wir akzeptieren ein leicht erhöhtes Risiko, dass ein widerrufenes Zertifikat erst nach Ablauf des Caches erkannt wird. Das Risiko haben wir als 'medium' klassifiziert und im Security-Risk-Register ORI-SRR-07 dokumentiert, mit einem Hinweis, dass bei kritischen Kunden ein manueller Cache-Flush möglich ist."}
{"ts": "146:45", "speaker": "I", "text": "Wie wird dieser manuelle Flush im Betrieb ausgelöst?"}
{"ts": "146:50", "speaker": "E", "text": "Über einen gesicherten Admin-Endpoint, der nur via Bastion-Host erreichbar ist. Das Verfahren ist in RB-GW-019 beschrieben. Wir haben einen 4-Augen-Check im ChatOps-Channel, bevor der Befehl abgesetzt wird."}
{"ts": "147:02", "speaker": "I", "text": "Und wie stellen Sie sicher, dass dieser Endpoint nicht missbraucht wird?"}
{"ts": "147:08", "speaker": "E", "text": "Der Endpoint erfordert eine mTLS-Authentifizierung mit einem separaten Admin-Zertifikat, das in einem HSM hinterlegt ist. Zusätzlich wird jeder Zugriff in Nimbus geloggt und mit einer Incident-ID versehen, sodass wir im Audit-Fall komplette Nachvollziehbarkeit haben."}
{"ts": "147:00", "speaker": "I", "text": "Sie hatten vorhin die RFC-Dokumentation erwähnt – konkret, wie wurde die Umsetzung im Runbook RB-GW-015 berücksichtigt, wenn wir von Incident Response sprechen?"}
{"ts": "147:04", "speaker": "E", "text": "Im RB-GW-015 haben wir die Szenarien aus RFC-ORI-07 direkt abgebildet. Das heißt, wir haben die mTLS-Handshake-Failures aus Ticket GW-4821 als eigenes Kapitel dokumentiert und klare Steps definiert: erst mTLS-Session reset, dann Fallback auf sekundären Auth-Endpunkt, bevor ein globaler Failover initiiert wird."}
{"ts": "147:09", "speaker": "I", "text": "Und diese sekundären Auth-Endpunkte – sind die physisch getrennt oder nur logisch partitioniert?"}
{"ts": "147:13", "speaker": "E", "text": "Physisch getrennt, zwei verschiedene Availability Zones. Wir haben das so gelöst, um die Latenzspitzen im Falle einer Zonenstörung zu minimieren und gleichzeitig die Sicherheitsdomänen strikt zu trennen."}
{"ts": "147:18", "speaker": "I", "text": "Wie messen Sie dann, ob das SLA-ORI-02 mit p95 < 120ms eingehalten wird, auch im Failover?"}
{"ts": "147:22", "speaker": "E", "text": "Wir kombinieren Prometheus-Histogramme und die Trace-Daten aus Nimbus Observability. Im Failover-Runbook ist genau festgelegt: fünf Minuten nach Event-Start prüfen wir die p95-Latenz und schließen das Incident nur, wenn es wieder unter 120ms liegt."}
{"ts": "147:27", "speaker": "I", "text": "Gab es mal den Fall, dass diese Schwelle länger überschritten wurde?"}
{"ts": "147:31", "speaker": "E", "text": "Ja, im Integrationstest gegen Aegis IAM, Build 4.2. Dort hat ein fehlerhaftes Policy-as-Code Deployment die JWT-Validierung verlangsamt. Wir haben das als Lesson Learned in RFC-ORI-11 aufgenommen."}
{"ts": "147:36", "speaker": "I", "text": "Interessant – und wie genau haben Sie das Policy-as-Code Problem gelöst?"}
{"ts": "147:40", "speaker": "E", "text": "Wir haben ein Pre-Deployment Linter-Tool eingebaut, das OPA-Regeln gegen eine Performance-Whitelist prüft. Zusätzlich wird jede Regeländerung in einer Staging-Umgebung mit synthetischen Lasttests verifiziert."}
{"ts": "147:45", "speaker": "I", "text": "Gab es dabei Trade-offs in Bezug auf die Deploy-Geschwindigkeit?"}
{"ts": "147:49", "speaker": "E", "text": "Natürlich, der Pre-Deployment-Test verlängert den Pipeline-Run um etwa 7 Minuten. Wir haben bewusst entschieden, dass diese Verzögerung den Gewinn an Stabilität und Sicherheit rechtfertigt."}
{"ts": "147:54", "speaker": "I", "text": "Wie wurde diese Entscheidung intern dokumentiert?"}
{"ts": "147:58", "speaker": "E", "text": "In RFC-ORI-11, Abschnitt 5.3, mit einem Verweis auf die Runbook-Änderung RB-GW-015-v2. Außerdem gibt es im internen Wiki eine Entscheidungs-Matrix, die die Gewichtung Sicherheit vs. Deployment-Tempo transparent macht."}
{"ts": "148:03", "speaker": "I", "text": "Sehen Sie noch offene Risiken im aktuellen mTLS-Setup?"}
{"ts": "148:08", "speaker": "E", "text": "Ja, wir haben noch keinen automatisierten Zertifikats-Rollout über alle Edge-Nodes hinweg. Das steht als PBI-ORI-143 im Backlog. Bis dahin bleibt ein Restrisiko bei manuellen Updates, was wir durch enges Monitoring und Audit-Logs abfedern."}
{"ts": "148:36", "speaker": "I", "text": "Sie hatten vorhin erwähnt, dass die mTLS-Risiken noch nicht vollständig ausgeräumt sind. Können Sie mir bitte genauer sagen, welche Punkte in der letzten Sicherheitsüberprüfung offengeblieben sind?"}
{"ts": "148:41", "speaker": "E", "text": "Ja, also konkret geht es um zwei Dinge: Erstens, in der Testumgebung haben wir festgestellt, dass unser Gateway bei einem abgelaufenen Client-Zertifikat nicht sofort den TLS-Handshake abbricht, sondern noch einen Retry versucht. Das ist im Prinzip durch Ticket GW-5213 dokumentiert. Zweitens, fehlt uns noch eine vollständige CRL-Distribution in allen Edge-Nodes, weil wir bei hoher Last nicht jede CRL-Abfrage synchron ziehen wollten."}
{"ts": "148:50", "speaker": "I", "text": "Verstehe. Und wie planen Sie, diese CRL-Problematik zu lösen, ohne die Latenz zu sprengen?"}
{"ts": "148:54", "speaker": "E", "text": "Wir haben in RFC-ORI-28 eine asynchrone CRL-Aktualisierung vorgeschlagen. Dabei cached jeder Node die CRL lokal und aktualisiert sie über einen dedizierten Sidecar-Prozess, der über das interne Poseidon-Netzwerk läuft. So vermeiden wir, dass der Handshake-Thread blockiert, und halten gleichzeitig die p95-Latenz unter 120ms, wie es SLA-ORI-02 fordert."}
{"ts": "149:03", "speaker": "I", "text": "Wie wirkt sich das auf Ihre Blue/Green Deployments aus, die Sie ja nach RB-GW-011 fahren?"}
{"ts": "149:07", "speaker": "E", "text": "Da müssen wir sicherstellen, dass der Sidecar vor dem Traffic-Switch auf die neue Green-Umgebung läuft und die aktuellste CRL geladen hat. Wir haben in RB-GW-011 einen zusätzlichen Pre-Deployment-Check ergänzt, der genau das prüft und bei Fehlermeldung den Switch blockiert."}
{"ts": "149:15", "speaker": "I", "text": "Gab es da schon mal eine Situation, wo dieser Check einen Switch verhindert hat?"}
{"ts": "149:19", "speaker": "E", "text": "Ja, im Build-Test vor zwei Wochen. Wir hatten einen Netzwerk-Glitch im Nimbus Observability Cluster, wodurch der Sidecar die CRL nicht ziehen konnte. Der Switch wurde gestoppt, was uns zwar 15 Minuten Delay kostete, aber eben verhinderte, dass Clients mit widerrufenen Zertifikaten durchkamen."}
{"ts": "149:28", "speaker": "I", "text": "Das bringt mich zu einer anderen Frage: Wie eng ist die Integration mit Nimbus Observability in Bezug auf Security-Events?"}
{"ts": "149:32", "speaker": "E", "text": "Sehr eng. Wir pushen nicht nur Latenz- und Throughput-Metriken, sondern auch mTLS-Handshake-Fehler mit einer speziellen Event-ID-Struktur. Zum Beispiel 'MTLS_FAIL:GW:<NodeID>'. Diese Events werden dann via Aegis IAM mit korrelierten Auth-Fehlern angereichert, sodass wir im Incident-Fall sofort sehen, ob es ein Zertifikatsproblem oder ein Policy-Deny war."}
{"ts": "149:42", "speaker": "I", "text": "Und diese Policy-Denies, laufen die komplett über Ihren Policy-as-Code Ansatz?"}
{"ts": "149:46", "speaker": "E", "text": "Genau. Wir nutzen ein OPA-basiertes Setup, das in Repo 'gw-policy' versioniert ist. Jede Änderung durchläuft einen Security Review, dokumentiert in den RFCs, typischerweise mit ID 'RFC-SEC-xx'. Das hilft uns, Änderungen nachvollziehbar zu halten und im Zweifel schnell zu rollbacken."}
{"ts": "149:55", "speaker": "I", "text": "Wenn Sie auf die Gesamtabhängigkeiten blicken – Orion Edge, Aegis IAM, Nimbus Observability – wo sehen Sie die kritischste Schnittstelle?"}
{"ts": "149:59", "speaker": "E", "text": "Die kritischste ist die direkte Auth-Token-Prüfung am Gateway gegen Aegis. Fällt Aegis aus oder liefert verzögert, sind wir gezwungen, entweder Traffic zu blocken oder in einen statischen Allow-List-Fallback zu gehen, wie in Runbook RB-GW-019 beschrieben. Letzteres ist ein enormer Trade-off zwischen Security und Availability."}
{"ts": "150:08", "speaker": "I", "text": "Und wie dokumentieren Sie diese Trade-offs für das Team?"}
{"ts": "150:12", "speaker": "E", "text": "Wir führen ein zentrales Decision-Log im Confluence-Bereich 'Orion Arch Decisions'. Jede Entscheidung hat dort Referenzen zu Tickets, RFCs und Runbooks. Beispielsweise die Allow-List-Entscheidung ist unter DEC-ORI-14 erfasst, mit Risikoanalyse, Metrik-Impact und Genehmigung vom Security Board."}
{"ts": "150:08", "speaker": "I", "text": "Sie hatten eben die Performance-Thematik im Zusammenhang mit mTLS angesprochen. Mich würde interessieren, wie Sie in RB-GW-024 den Failover-Prozess dokumentiert haben, um bei einem Handshake-Timeout nicht gleich einen Serviceabbruch zu riskieren."}
{"ts": "150:14", "speaker": "E", "text": "Ja, das ist in RB-GW-024 so geregelt, dass wir beim ersten Timeout einen Non-blocking Retry im selben TCP-Context versuchen und erst nach zwei aufeinanderfolgenden Fehlschlägen in den passiven Node umschwenken. Damit reduzieren wir die Downtime, ohne dass der Security Layer umgangen wird."}
{"ts": "150:21", "speaker": "I", "text": "Und wie messen Sie, ob dieser Retry-Mechanismus nicht selbst zu einer Latenzerhöhung führt, die dann das SLA-ORI-02 verletzt?"}
{"ts": "150:26", "speaker": "E", "text": "Wir haben dafür in Nimbus Observability Custom Metrics hinterlegt, die die Zeitspanne zwischen initialem Request und erfolgreichem mTLS-Handshake messen. Alerts triggern bei >100 ms, sodass wir noch unter der 120 ms p95 Grenze bleiben."}
{"ts": "150:33", "speaker": "I", "text": "Gab es schon reale Incidents, bei denen dieser Mechanismus gegriffen hat?"}
{"ts": "150:38", "speaker": "E", "text": "Ja, Ticket INC-GW-773 vom letzten Monat: Ein fehlerhaftes Zertifikat auf einem Blue-Deployment-Node verursachte Handshake-Fehler. Der Retry hat den Traffic sauber auf Green umgeleitet, und Endkunden haben nichts bemerkt."}
{"ts": "150:45", "speaker": "I", "text": "Interessant. Wie spielen dabei Ihre Schnittstellen zur Aegis IAM Plattform hinein?"}
{"ts": "150:50", "speaker": "E", "text": "Die Aegis IAM Plattform liefert die OCSP-Responder-URLs, die wir für Certificate Revocation Checks nutzen. Fällt Aegis aus, greifen wir auf gecachte CRLs zurück, was im Runbook RB-IAM-007 beschrieben ist."}
{"ts": "150:57", "speaker": "I", "text": "Das heißt, Sie haben eine Art Grace-Period für Revocation-Checks?"}
{"ts": "151:02", "speaker": "E", "text": "Genau, maximal 12 Stunden laut Policy. Danach zwingt das Gateway einen Hard-Fail, um keine kompromittierten Zertifikate durchzulassen. Das ist ein Balanceakt zwischen Verfügbarkeit und Security."}
{"ts": "151:08", "speaker": "I", "text": "Und wie dokumentieren Sie solche Balanceakte, damit das Team bei späteren Änderungen die ursprüngliche Entscheidung nachvollziehen kann?"}
{"ts": "151:13", "speaker": "E", "text": "Wir nutzen dafür RFC-Dokumente im internen Confluence, z.B. RFC-ORI-SEC-14 für genau diese OCSP/CRL-Strategie. Jedes RFC enthält auch einen Risk-Acceptance-Abschnitt und Verweise auf die zugehörigen Runbooks."}
{"ts": "151:20", "speaker": "I", "text": "Gab es bei der Erstellung von RFC-ORI-SEC-14 Meinungsverschiedenheiten im Team?"}
{"ts": "151:25", "speaker": "E", "text": "Ja, die SRE-Seite wollte zunächst eine längere Grace-Period von 24 Stunden, um weniger Failover-Events zu haben. Die Security-Architekten haben aber anhand von Bedrohungsmodellierung aus dem Poseidon Networking Projekt gezeigt, dass 12 Stunden das Maximum sein sollten."}
{"ts": "151:33", "speaker": "I", "text": "Das klingt nach einer Entscheidung mit deutlichen Auswirkungen. Wie stellen Sie sicher, dass solche Lessons Learned auch in anderen Projekten ankommen?"}
{"ts": "151:38", "speaker": "E", "text": "Über das interne Guild-Meeting-System; da präsentieren wir monatlich Cross-Project Learnings. RFC-ORI-SEC-14 ist inzwischen als Referenz in den Projekten Helios Datalake und Aegis IAM verlinkt."}
{"ts": "151:44", "speaker": "I", "text": "Sie hatten vorhin kurz den Blue/Green-Ansatz aus RB-GW-011 erwähnt. Können Sie bitte einmal durchgehen, wie Sie den im Orion Edge Gateway konkret umsetzen?"}
{"ts": "151:49", "speaker": "E", "text": "Klar. Wir haben in RB-GW-011 eine Schritt-für-Schritt-Anleitung: erst wird die Green-Umgebung mit identischer Config wie Blue hochgezogen, dann führen wir mTLS-Handshake-Tests gegen beide durch. Erst wenn die Smoke-Tests von Green den p95 Latency-Check unter 120 ms bestehen, schalten wir im Load Balancer um."}
{"ts": "151:57", "speaker": "I", "text": "Und wie minimieren Sie in der Umschaltphase das Risiko von Auth-Fehlern?"}
{"ts": "152:02", "speaker": "E", "text": "Wir synchronisieren mit dem Aegis IAM Team, sodass Token-Cache und CRLs gleichzeitig aktualisiert werden. Das ist ein Punkt, der in Ticket GW-4821 auch explizit als Stolperstein markiert wurde."}
{"ts": "152:09", "speaker": "I", "text": "Sie sprachen eben vom Token-Cache – überwachen Sie diesen aktiv?"}
{"ts": "152:14", "speaker": "E", "text": "Ja, wir haben eine Metrik 'auth_cache_hit_ratio'. Wenn die unter 92% fällt, triggert Nimbus Observability einen Alert. Dieser Alert ist mit dem Runbook RB-AUTH-007 verknüpft."}
{"ts": "152:21", "speaker": "I", "text": "Das klingt sehr integriert. Wie stellen Sie sicher, dass die Log- und Trace-Korrelation zwischen Orion und Nimbus funktioniert?"}
{"ts": "152:28", "speaker": "E", "text": "Wir injizieren in jede API Gateway-Response eine Trace-ID, die aus dem W3C TraceContext stammt. Nimbus Observability zieht sich diese IDs, um Spans über Orion, Aegis und auch Helios Datalake hinweg zu verbinden."}
{"ts": "152:36", "speaker": "I", "text": "Gab es da Probleme, z.B. durch unterschiedliche Zeitquellen?"}
{"ts": "152:41", "speaker": "E", "text": "Ja, genau. Anfangs hatten wir bei Poseidon Networking ein NTP-Driftproblem, wodurch Logs um Sekunden versetzt waren. Wir haben dann in RFC-ORI-013 festgelegt, dass alle Subsysteme chrony mit denselben Zeitservern nutzen."}
{"ts": "152:49", "speaker": "I", "text": "Wenn wir auf die SLAs schauen – wie überprüfen Sie, dass SLA-ORI-02 auch während hoher Last eingehalten wird?"}
{"ts": "152:55", "speaker": "E", "text": "Wir fahren Synthetic Load Tests über einen Canary-Client alle 5 Minuten. Die Ergebnisse fließen in ein Prometheus-Dashboard. Fällt p95 über 120 ms, geht automatisch ein Incident in unser Ticket-System mit Prio P1 auf."}
{"ts": "153:03", "speaker": "I", "text": "Gab es eine konkrete Situation, in der Sie einen solchen P1-Incident hatten?"}
{"ts": "153:07", "speaker": "E", "text": "Ja, am 14. Mai, Ticket ORI-INC-223. Da war die Rate Limiting-Policy zu streng. Wir mussten kurzfristig in der Policy-as-Code-Definition einen Threshold anheben, um legitimen Traffic nicht zu blocken."}
{"ts": "153:15", "speaker": "I", "text": "War das dann nicht ein sicherheitskritischer Kompromiss?"}
{"ts": "153:20", "speaker": "E", "text": "Ja, durchaus. Wir haben das als temporären Workaround dokumentiert und in RFC-ORI-019 vermerkt, mit dem Plan, einen adaptiven Rate Limiter einzuführen, der zwischen Angriffsmustern und legitimen Lastspitzen unterscheiden kann."}
{"ts": "153:16", "speaker": "I", "text": "Sie hatten vorhin die RFC-Dokumente erwähnt – können Sie bitte konkret sagen, welche RFC-Nummern für das mTLS-Setup im Orion Edge Gateway maßgeblich waren?"}
{"ts": "153:20", "speaker": "E", "text": "Ja, maßgeblich waren bei uns RFC 5246 als Basis für TLS 1.2, ergänzt durch RFC 8446 für TLS 1.3. Die Entscheidung, im internen Cluster schon jetzt auf TLS 1.3 zu setzen, entstammt RFC-ORI-09 aus unserem internen Architekturboard."}
{"ts": "153:27", "speaker": "I", "text": "Und wie haben Sie das in Ihrem Runbook abgebildet? Also speziell, falls ein Handshake fehlschlägt?"}
{"ts": "153:33", "speaker": "E", "text": "Das ist in RB-GW-017 festgehalten. Dort haben wir einen Abschnitt 'Handshake Recovery', der beschreibt, wie wir über unseren Auth-Fallback-Service temporär auf einen Ersatz-Zertifikats-Provider ausweichen, um den Traffic nicht zu unterbrechen."}
{"ts": "153:40", "speaker": "I", "text": "Können Sie das bitte mit einem konkreten Vorfall belegen, vielleicht ein Ticket?"}
{"ts": "153:45", "speaker": "E", "text": "Ja, Ticket GW-5112 vom 14. März. Da ist nach einem fehlerhaften Zertifikats-Rollover der Handshake zu 32% der Clients fehlgeschlagen. Wir haben laut RB-GW-017 reagiert und binnen 4 Minuten wieder 99,8% Erfolgsrate erreicht."}
{"ts": "153:53", "speaker": "I", "text": "Sie sprachen vorhin von der Koordination mit Nimbus Observability. Wie genau fließen deren Logs und Traces in Ihre Latenzanalysen ein?"}
{"ts": "153:59", "speaker": "E", "text": "Wir haben über das OpenTelemetry-Protokoll eine direkte Anbindung, die per gRPC secure channel läuft. Die Traces werden mit Gateway-IDs getaggt, sodass wir sie mit den p95-Latenzen aus SLA-ORI-02 korrelieren können."}
{"ts": "154:06", "speaker": "I", "text": "Gab es Situationen, wo diese Korrelation nicht funktioniert hat?"}
{"ts": "154:10", "speaker": "E", "text": "Ja, im Build 1.3.4 hatten wir einen Bug im Trace-Injector, der das Tagging bei mTLS-Fehlerpfaden unterdrückte. Das wurde in Patch GW-PAT-44 gefixt."}
{"ts": "154:16", "speaker": "I", "text": "Noch mal zu den Rate-Limits – wie sichern Sie ab, dass legitime Requests nicht geblockt werden, gerade bei kurzfristigen Traffic-Spitzen?"}
{"ts": "154:22", "speaker": "E", "text": "Wir nutzen adaptive Burst-Tokens. Das heißt, die Rate-Limiter-Regeln werden in Echtzeit über Traffic-Patterns aus den letzten 30 Sekunden angepasst. In RB-GW-023 ist beschrieben, wie wir bei Events > 500req/s kurzzeitig die Limits um 20% anheben."}
{"ts": "154:30", "speaker": "I", "text": "Und welche Risiken entstehen dadurch?"}
{"ts": "154:34", "speaker": "E", "text": "Das Risiko ist, dass bei einem DDoS-Versuch die adaptive Logik zunächst zu permissiv reagiert. Deshalb koppeln wir sie mit unserem Threat-Intel-Feed, der IP-Ranges mit schlechtem Ruf hart blockt, egal wie die Burst-Analyse ausfällt."}
{"ts": "154:41", "speaker": "I", "text": "Würden Sie sagen, dass diese Kopplung ein bewusster Trade-off ist – also Performance gegen Sicherheit?"}
{"ts": "154:46", "speaker": "E", "text": "Ganz klar, ja. Wir akzeptieren kurze Latenzspitzen, wenn der Threat-Intel-Feed greift, weil uns der Schutz der Backend-Services wichtiger ist. Das ist auch so in RFC-ORI-SEC-12 dokumentiert."}
{"ts": "154:46", "speaker": "I", "text": "Sie hatten vorhin den mTLS-Handshake erwähnt… können Sie mir nochmal konkret sagen, wie Sie in der Praxis verhindern, dass wir in das Problem aus Ticket GW‑4821 laufen?"}
{"ts": "154:53", "speaker": "E", "text": "Ja, klar. Also wir prüfen im Handshake jetzt explizit das Zertifikat gegen die CRL, bevor wir überhaupt den Session Key aushandeln. In GW‑4821 war der Fehler, dass der Fallback-Mechanismus zu früh griff. Laut Runbook RB‑SEC‑007 haben wir daher die OpenSSL-Config so geändert, dass kein Fallback ohne vollständige Validierung erlaubt ist."}
{"ts": "155:06", "speaker": "I", "text": "Und wie testen Sie diese Änderung? Haben Sie da automatisierte Checks im Build?"}
{"ts": "155:12", "speaker": "E", "text": "Ja, wir haben im CI-Pipeline‑Schritt eine Test-Suite, die simulierte abgelaufene Zertifikate und Revocations einspielt. Der Test schlägt fehl, wenn der Handshake trotzdem durchgeht. Außerdem laufen diese Tests auch in der Staging‑Umgebung gegen unseren Aegis IAM‑Mock."}
{"ts": "155:25", "speaker": "I", "text": "Sie sprechen Aegis IAM an – wie genau sichern Sie die Schnittstellen zwischen Orion Edge Gateway und dieser Plattform ab?"}
{"ts": "155:31", "speaker": "E", "text": "Alle API-Calls zum Aegis IAM laufen über einen dedizierten mTLS‑Kanal mit Client‑Zertifikaten, die nur für den Gateway‑Service ausgestellt sind. Zudem enforce’n wir per Policy‑as‑Code (OPA‑Regeln), dass bestimmte Claims im JWT vorhanden sein müssen, bevor ein Request überhaupt weitergereicht wird."}
{"ts": "155:45", "speaker": "I", "text": "Können Sie das mit Nimbus Observability korrelieren? Also Logs und Traces über die ganze Kette?"}
{"ts": "155:51", "speaker": "E", "text": "Ja, wir propagieren eine Trace‑ID aus dem Gateway in alle Downstream‑Calls, auch ins Aegis IAM. Nimbus Observability hat dafür einen speziellen Decoder, der sowohl die Gateway‑Access‑Logs als auch die IAM‑Audit‑Logs zusammenführt. So sehen wir im TraceView genau, wo Latenzen entstehen."}
{"ts": "156:05", "speaker": "I", "text": "Sie hatten letztes Mal Lessons Learned aus Helios Datalake erwähnt – wie passt das hier rein?"}
{"ts": "156:11", "speaker": "E", "text": "Bei Helios haben wir gelernt, dass zentrale Auth‑Services ein Single Point of Failure sein können. Deshalb bauen wir im Orion‑Gateway jetzt einen Graceful‑Degradation‑Modus ein, der bei kurzfristigem Ausfall vom IAM für bestimmte Endpunkte Caching zulässt, um das SLA‑ORI‑02 p95 Latency < 120 ms einzuhalten."}
{"ts": "156:27", "speaker": "I", "text": "Dieser Modus – ist der nicht ein Sicherheitsrisiko?"}
{"ts": "156:32", "speaker": "E", "text": "Ja, potenziell. Wir haben deshalb ein sehr enges Zeitfenster – maximal 90 Sekunden – in dem gecachte Tokens akzeptiert werden. Das ist in RFC‑ORI‑023 dokumentiert. Danach geht der Endpunkt in einen Fail‑Closed‑Zustand."}
{"ts": "156:45", "speaker": "I", "text": "Und wie überwachen Sie, dass dieser Fail‑Closed auch wirklich greift?"}
{"ts": "156:50", "speaker": "E", "text": "Wir haben in Nimbus einen Alert konfiguriert, der auf den Metric‑Stream `gateway.auth.cache_mode_active` hört. Wenn der Wert > 90 Sekunden bleibt, feuert ein PagerDuty‑Alert an das SRE‑Team. Das ist auch Teil von Runbook RB‑GW‑011."}
{"ts": "157:04", "speaker": "I", "text": "Letzte Frage dazu: gab es Diskussionen im Team, ob man diese 90 Sekunden nicht länger machen sollte, um Availability zu verbessern?"}
{"ts": "157:10", "speaker": "E", "text": "Ja, gab es. Aber nach Analyse der Security‑Risiken im mTLS‑Setup, insbesondere Reuse‑Gefahr von kompromittierten Tokens, haben wir uns bewusst für kürzer entschieden. Die Entscheidung ist dokumentiert in Decision‑Log DL‑042 mit Verweis auf PenTest‑Report PT‑ORI‑07."}
{"ts": "157:26", "speaker": "I", "text": "Lassen Sie uns noch einmal konkret auf die Schnittstellen zur Aegis IAM Plattform eingehen – wie stellen Sie sicher, dass hier keine unautorisierten Zugriffe durchrutschen?"}
{"ts": "157:33", "speaker": "E", "text": "Wir setzen auf eine Kombination aus mTLS zwischen Gateway und IAM, plus signierte JWT Tokens, die von Aegis ausgegeben werden. In Runbook RB-IAM-004 ist beschrieben, wie wir im Fehlerfall Tokens invalidieren."}
{"ts": "157:44", "speaker": "I", "text": "Gab es da schon mal eine Situation, in der die Tokenvalidierung zu Latenzproblemen geführt hat?"}
{"ts": "157:50", "speaker": "E", "text": "Ja, im Loadtest Q3/Build-Phase haben wir gesehen, dass bei hoher Anfragefrequenz die Signaturprüfung knapp 15 ms pro Request kostete. Wir haben daraufhin einen in-memory Cache mit 30 Sekunden TTL eingeführt."}
{"ts": "158:02", "speaker": "I", "text": "Wie wird dieser Cache invalidiert, falls ein Token kompromittiert ist?"}
{"ts": "158:07", "speaker": "E", "text": "Über einen Push-Mechanismus aus Aegis – wenn ein Token als kompromittiert markiert wird, sendet Aegis ein Invalidate-Event an alle Gateways, das sofort den Cacheeintrag löscht."}
{"ts": "158:17", "speaker": "I", "text": "Und wie korrelieren Sie das im Monitoring?"}
{"ts": "158:21", "speaker": "E", "text": "Wir leiten sowohl Cache Hits/Misses als auch Invalidate-Events an Nimbus Observability weiter, taggen sie mit Trace-IDs aus dem Orion Request Flow, so dass wir im Trace Viewer sehen, ob ein Request ein frisches oder gecachtes Token hatte."}
{"ts": "158:35", "speaker": "I", "text": "Das klingt nach einer engen Integration. Welche Lessons Learned aus Helios Datalake haben Sie hier genutzt?"}
{"ts": "158:42", "speaker": "E", "text": "Aus Helios haben wir übernommen, dass Event-Schemas versioniert und strikt validiert werden müssen. Bei einem Validierungsfehler drohte dort mal ein Data Loss – bei Orion verwerfen wir fehlerhafte Invalidate-Events sofort und loggen sie als Critical."}
{"ts": "158:56", "speaker": "I", "text": "Gab es einen Trade-off zwischen schneller Eventverarbeitung und Validierungstiefe?"}
{"ts": "159:01", "speaker": "E", "text": "Ja, wir haben bewusst eine tiefe JSON-Schema-Validierung eingebaut, obwohl sie 2–3 ms kostet, weil ein falsches Event potenziell alle Token-Caches leeren könnte – das Risiko wollten wir nicht eingehen."}
{"ts": "159:13", "speaker": "I", "text": "Wie dokumentieren Sie solche Entscheidungen?"}
{"ts": "159:16", "speaker": "E", "text": "In RFC-ORI-018 ist der gesamte Entscheidungsprozess inkl. Benchmarks und Risikobewertung enthalten, verlinkt im internen Confluence unter 'Security/TokenCache'."}
{"ts": "159:27", "speaker": "I", "text": "Letzte Frage: Sehen Sie noch offene Risiken in dieser Integration?"}
{"ts": "159:32", "speaker": "E", "text": "Ja, ein offenes Risiko ist der Single Point of Failure bei der Event-Queue von Aegis. Wir haben zwar einen Fallback auf Polling implementiert, aber der kann bis zu 60 Sekunden Verzögerung bringen, was im Incident-Fall kritisch wäre."}
{"ts": "159:26", "speaker": "I", "text": "Sie hatten vorhin die Integration mit Nimbus Observability nur kurz angerissen. Können Sie mir bitte noch einmal genau schildern, wie die Log-Korrelation zwischen Orion Edge Gateway und Nimbus umgesetzt ist?"}
{"ts": "159:31", "speaker": "E", "text": "Ja, gern. Wir haben auf beiden Seiten einen gemeinsamen Trace-Header-Standard eingeführt, basierend auf W3C Trace Context. Das Gateway annotiert jeden eingehenden Request mit einer Trace-ID, die dann durch alle internen Dienste wandert. Nimbus Observability zieht sich diese ID, um Logs, Metriken und Traces zusammenzuführen. Wir haben das in Runbook RB-OBS-027 dokumentiert."}
{"ts": "159:39", "speaker": "I", "text": "Und wie stellen Sie sicher, dass bei hoher Last diese Korrelation nicht ins Stocken gerät?"}
{"ts": "159:43", "speaker": "E", "text": "Dazu haben wir ein Sampling von 100% für Fehlerfälle und 10% für Success-Fälle implementiert. Außerdem buffern wir Trace-Events lokal bis zu 500 ms, bevor sie asynchron an Nimbus gehen. Damit vermeiden wir Blocking Calls im Gateway-Prozess."}
{"ts": "159:50", "speaker": "I", "text": "Klingt robust. Gab es da Schnittstellenprobleme mit Aegis IAM?"}
{"ts": "159:54", "speaker": "E", "text": "Anfangs ja. Aegis hat eigene Audit-Logs mit einer anderen ID-Struktur. Wir haben in Ticket INT-AG-104 eine Mapping-Logik beschrieben, die beim AuthN/A Handshake die Aegis-Session-ID zusätzlich in den Trace-Context einbettet. Das war wichtig, um sicherheitsrelevante Vorgänge lückenlos nachvollziehen zu können."}
{"ts": "160:02", "speaker": "I", "text": "Wie wirkt sich das auf die Latenz im SLA-ORI-02 aus?"}
{"ts": "160:06", "speaker": "E", "text": "Messbar kaum. Wir haben durch die asynchrone Architektur im Mittel <3 ms Overhead bei p95. Das liegt deutlich unter den 120 ms, die SLA-ORI-02 fordert."}
{"ts": "160:12", "speaker": "I", "text": "Sie hatten die Lessons Learned aus Poseidon Networking erwähnt. Können Sie ein Beispiel nennen, das hier eingeflossen ist?"}
{"ts": "160:16", "speaker": "E", "text": "Ja, aus Poseidon haben wir die Erkenntnis übernommen, dass TLS Session Resumption aggressiv genutzt werden sollte. Wir haben beim mTLS-Setup OCSP-Stapling und Session Tickets eingebaut, um Handshake-Latenz zu reduzieren. Das verringert den CPU-Load auf beiden Seiten, ohne die Security zu schwächen."}
{"ts": "160:24", "speaker": "I", "text": "Gab es dafür ein spezielles RFC-Dokument?"}
{"ts": "160:27", "speaker": "E", "text": "Ja, RFC-ORI-008, wo wir die Trade-offs zwischen unterschiedlichen Session Resumption Modi analysiert haben. Wir haben uns für Session Tickets mit kurzer Lebensdauer entschieden, dokumentiert samt Key-Rollover-Prozess."}
{"ts": "160:34", "speaker": "I", "text": "Wenn der Auth-Integration-Service spontan ausfällt, wie reagieren Sie laut Runbook?"}
{"ts": "160:38", "speaker": "E", "text": "Das ist in RB-GW-019 beschrieben. Schritt eins: Circuit Breaker schließen, damit fehlerhafte Calls nicht propagieren. Schritt zwei: Failover auf den Standby-Service in Zone B. Schritt drei: Alert an das On-Call-Team via PagerDuty. Und dann manuelle Validierung der zuletzt ausgestellten Tokens."}
{"ts": "160:46", "speaker": "I", "text": "Sie hatten vorhin schon mTLS-Risiken genannt. Gibt es aktuell noch offene Punkte?"}
{"ts": "160:50", "speaker": "E", "text": "Ja, wir beobachten noch das Risiko von Client-Zertifikats-Expiry in Multi-Cluster-Setups. Wenn ein Cluster die CRL schneller aktualisiert als ein anderer, kann es zu inkonsistenten Verbindungen kommen. Das steht als offenes Risiko in unserem Risk Log RSK-ORI-12. Wir planen, das mit einem zentralisierten CRL-Distributor zu entschärfen."}
{"ts": "161:02", "speaker": "I", "text": "Bevor wir zum Abschluss kommen, möchte ich noch verstehen, wie Sie den Lastausgleich im Zusammenhang mit der Authentifizierung gestalten. Können Sie das kurz skizzieren?"}
{"ts": "161:07", "speaker": "E", "text": "Ja, wir haben im Orion Edge Gateway ein duales Load-Balancing-Konzept – L4 für die reine Verteilung, L7 für die Auth-Entscheidung. Das ist in RB-GW-009 dokumentiert. Der L7-Balancer prüft mTLS-Certs und JWTs in einer Pre-Check-Phase, bevor Requests an die eigentlichen API-Backends gehen."}
{"ts": "161:14", "speaker": "I", "text": "Und wie stellen Sie sicher, dass dieser Pre-Check bei Lastspitzen nicht zum Bottleneck wird?"}
{"ts": "161:20", "speaker": "E", "text": "Wir haben ein Warm-Cache-Konzept für Zertifikatsketten implementiert und parallelisieren die JWT-Validierung über Worker-Threads. Zusätzlich laufen synthetische Lasttests, die in Ticket GW-4973 dokumentiert sind, um ab 85 % CPU-Auslastung Alerts zu triggern."}
{"ts": "161:28", "speaker": "I", "text": "Das heißt, Sie haben Metriken, die Sie kontinuierlich überprüfen?"}
{"ts": "161:31", "speaker": "E", "text": "Genau, wir messen p95 Latenz, Error-Rates pro Auth-Stage und mTLS-Handshake-Dauer. Diese fließen direkt in das SLA-ORI-02 Monitoring ein."}
{"ts": "161:37", "speaker": "I", "text": "Gab es dabei schon Fälle, in denen Sie das Deployment zurückrollen mussten?"}
{"ts": "161:42", "speaker": "E", "text": "Einmal, ja. Beim Blue/Green nach RB-GW-011 hatten wir in der Green-Umgebung eine fehlerhafte CRL-Distribution. Das führte zu 12 % Reject-Rate im mTLS-Handshake. Wir haben sofort auf Blue zurückgeschwenkt und das Problem in RFC-ORI-27 festgehalten."}
{"ts": "161:51", "speaker": "I", "text": "Wie koordinieren Sie solche Erkenntnisse mit Nimbus Observability?"}
{"ts": "161:56", "speaker": "E", "text": "Wir exportieren Gateway-Traces via OpenTelemetry in Nimbus und taggen sie mit Deployment-ID und Auth-Stage. So können die Kollegen dort sofort korrelieren, wenn etwa erhöhte Latenzwerte mit einem bestimmten Cert-Bundle zusammenfallen."}
{"ts": "162:04", "speaker": "I", "text": "Sie hatten vorhin Helios Datalake erwähnt – konnten Sie da etwas wiederverwenden?"}
{"ts": "162:09", "speaker": "E", "text": "Ja, das zentrale Schema für Event-Logs. Wir loggen Auth-Fehler im gleichen JSON-Format wie Helios, damit Analyse-Tools ohne Anpassung funktionieren. Das spart uns echt Zeit bei Root-Cause-Analysen."}
{"ts": "162:16", "speaker": "I", "text": "Welche Risiken sehen Sie im Bereich Rate Limiting aktuell noch?"}
{"ts": "162:21", "speaker": "E", "text": "Das größte Risiko ist ein falsch konfiguriertes Burst-Limit, das legitime Batch-Requests blockt. Wir haben dafür in RB-GW-014 eine Heuristik hinterlegt: MaxBurst = 3×AvgRPS innerhalb von 500 ms, um False Positives zu minimieren."}
{"ts": "162:28", "speaker": "I", "text": "Und wie dokumentieren Sie diese Heuristiken?"}
{"ts": "162:32", "speaker": "E", "text": "Immer als Ergänzung zu den Runbooks und mit Verweis auf das zugehörige Ticket im internen Confluence. So ist nachvollziehbar, warum wir einen bestimmten Wert gewählt haben, und welche Messdaten dahinter stehen."}
{"ts": "162:02", "speaker": "I", "text": "Sie hatten vorhin erwähnt, dass im Ticket GW-4821 ein spezieller mTLS-Handshake-Fehler diskutiert wurde. Können Sie mir schildern, wie Sie den Plan konkret angepasst haben, um diesen Fehler künftig auszuschließen?"}
{"ts": "162:09", "speaker": "E", "text": "Ja, wir haben die TLS-Konfiguration in der Stage-Umgebung verschärft, indem wir die Allowed Cipher Suites auf eine Whitelist gesetzt haben und zusätzlich im Runbook RB-GW-015 eine Validierungsroutine vor dem Deployment eingeführt haben. Das verhindert, dass schwache Ciphers oder falsche Zertifikatsketten in Produktion gelangen."}
{"ts": "162:18", "speaker": "I", "text": "Und wie stellen Sie sicher, dass diese Validierung nicht zu lange dauert und das SLA-ORI-02 nicht beeinflusst?"}
{"ts": "162:24", "speaker": "E", "text": "Wir führen die Checks im CI/CD-Pipeline-Schritt vor dem Blue/Green Deployment aus. Das dauert unter zwei Minuten und findet außerhalb des Live-Traffic-Pfads statt. So bleibt p95 Latency < 120ms unberührt."}
{"ts": "162:32", "speaker": "I", "text": "Sie sprechen Blue/Green an – können Sie den Ablauf laut RB-GW-011 einmal durchgehen?"}
{"ts": "162:39", "speaker": "E", "text": "Klar: Zuerst deployen wir auf die Green-Umgebung, führen dort Canary-Tests mit simuliertem Auth-Traffic durch, dann schwenken wir via Load Balancer um. Falls Fehler auftreten, gibt's im Runbook einen definierten Rollback-Plan, der in unter 90 Sekunden live geht."}
{"ts": "162:49", "speaker": "I", "text": "Wie koordinieren Sie sich dabei mit Nimbus Observability, um Logs und Traces zu korrelieren?"}
{"ts": "162:56", "speaker": "E", "text": "Wir pushen Gateway-Logs über eine gRPC-Schnittstelle direkt an Nimbus. Dort gibt's eine Rule, die TraceIDs aus dem Aegis IAM Kontext mit den Gateway TraceIDs verknüpft. So sehen wir End-to-End, ob Auth-Latenzen aus IAM oder Gateway stammen."}
{"ts": "163:05", "speaker": "I", "text": "Das bringt mich zu Aegis IAM. Welche Absicherungen setzen Sie dort an den Schnittstellen um?"}
{"ts": "163:11", "speaker": "E", "text": "Wir nutzen signierte JWTs, die von Aegis IAM ausgestellt werden, und validieren sie serverseitig im Gateway. Zusätzlich haben wir eine mTLS-Verbindung zwischen Gateway und IAM, um Man-in-the-Middle zu verhindern."}
{"ts": "163:20", "speaker": "I", "text": "Gab es schon mal den Fall, dass der Auth-Integration-Service plötzlich ausfiel?"}
{"ts": "163:25", "speaker": "E", "text": "Ja, in Incident INC-ORI-072. Da hat ein Zertifikats-Renewal im IAM den Dienst für 4 Minuten blockiert. Laut Runbook RB-GW-020 sind wir dann auf einen Fallback-Cache für bereits validierte Tokens umgestiegen, um bestehende Sessions nicht zu unterbrechen."}
{"ts": "163:36", "speaker": "I", "text": "Wie dokumentieren Sie solche Vorfälle und die daraus resultierenden Anpassungen?"}
{"ts": "163:41", "speaker": "E", "text": "Wir erstellen ein Post-Mortem-Dokument im internen Confluence mit Verweisen auf das Incident-Ticket, die betroffenen RFCs und die geänderten Runbooks. Zum Beispiel wurde RFC-ORI-19 nach INC-ORI-072 angepasst, um den Fallback-Mechanismus obligatorisch zu machen."}
{"ts": "163:51", "speaker": "I", "text": "Wenn Sie auf die bisherigen Entscheidungen zu mTLS und Rate Limiting schauen – wo sehen Sie noch Risiken?"}
{"ts": "163:58", "speaker": "E", "text": "Beim mTLS-Setup ist der größte Risikofaktor menschliches Versäumnis beim Zertifikatsmanagement – ein abgelaufenes Zertifikat kann in Sekunden den Traffic stoppen. Beim Rate Limiting besteht die Gefahr, dass fehlerhafte Konfiguration legitimen Traffic großer Kunden blockt. Deswegen haben wir Pre-Deploy-Tests mit repräsentativen Traffic-Mustern etabliert."}
{"ts": "163:38", "speaker": "I", "text": "Sie hatten vorhin die Entscheidung zwischen Security und Performance erwähnt – können Sie mir jetzt noch genauer schildern, wie Sie das in der letzten Deployment-Woche abgewogen haben?"}
{"ts": "163:42", "speaker": "E", "text": "Ja, wir haben im Build-Stand von vor zwei Wochen gemerkt, dass die durchgehende mTLS-Validierung auf jeder internen Hop-Layer die Latenz im p95 um etwa 18 ms erhöhte. Laut SLA-ORI-02 hatten wir noch Luft, aber nur knapp. Wir haben daher, wie in RFC-ORI-SEC-14 dokumentiert, für interne, vertrauenswürdige Subnetze auf Session Resumption gesetzt, um den Handshake-Overhead zu reduzieren."}
{"ts": "163:47", "speaker": "I", "text": "Und wie wurde das abgesichert, dass hier kein Sicherheitsloch entsteht?"}
{"ts": "163:50", "speaker": "E", "text": "Wir haben einen zusätzlichen Check in den Sidecar-Proxies implementiert, der das Zertifikat-Expiry und den CN prüft, auch wenn der Handshake übersprungen wird. Das ist in Runbook RB-GW-015 Schritt 4 beschrieben. So war der Trade-off transparent und jederzeit rücknehmbar."}
{"ts": "163:56", "speaker": "I", "text": "Gab es Gegenstimmen im Team zu diesem Ansatz?"}
{"ts": "163:59", "speaker": "E", "text": "Ja, der Security Lead wollte zunächst keinerlei Ausnahmen, hat aber nach dem Proof-of-Concept und einem Pen-Test unter Ticket SEC-VAL-221 zugestimmt, weil die Angriffsfläche faktisch nicht größer wurde."}
{"ts": "164:04", "speaker": "I", "text": "Wie haben Sie das Monitoring angepasst, um solche Änderungen zu überwachen?"}
{"ts": "164:08", "speaker": "E", "text": "Wir haben in Nimbus Observability einen neuen mTLS-Session-Resumption-Counter eingebaut und Alerts auf Anomalien gesetzt. Wenn z. B. die Resumption-Rate unter 70 % fällt, schlägt das System Alarm und wir sehen im Trace sofort, wo ein voller Handshake erzwungen wurde."}
{"ts": "164:14", "speaker": "I", "text": "Interessant. Und gibt es hier Schnittstellen zu anderen Projekten, die sich auf diese Änderung verlassen?"}
{"ts": "164:18", "speaker": "E", "text": "Ja, Aegis IAM nutzt denselben Gateway-Pfad für Token-Ausstellung. Wir mussten im IAM-Connector-Modul sicherstellen, dass die Session Resumption keine Token-Leak-Risiken erzeugt. Das wurde in Absprache mit dem Aegis-Team in einem gemeinsamen RFC-AG-SEC-09 festgehalten."}
{"ts": "164:24", "speaker": "I", "text": "Gab es Lessons Learned aus anderen Projekten, die hier eingeflossen sind?"}
{"ts": "164:27", "speaker": "E", "text": "Definitiv. Aus Poseidon Networking wissen wir, dass Session Resumption schlecht mit ungleichmäßigen Load-Balancer-Hashes funktioniert. Deshalb haben wir die LB-Config in GW-LB-004 schon vorab angepasst, um Session Stickiness zu sichern."}
{"ts": "164:33", "speaker": "I", "text": "Und was wäre Ihr Plan, falls trotz allem ein mTLS-Bypass-Szenario entdeckt wird?"}
{"ts": "164:37", "speaker": "E", "text": "Runbook RB-SEC-EMERG-02 sieht vor, sofort alle Resumption-Settings auf 0 zu setzen und einen Full-Redeploy des Gateways zu triggern. Das dauert etwa 4 Minuten im Blue/Green-Modus, wie wir es aus RB-GW-011 kennen."}
{"ts": "164:43", "speaker": "I", "text": "Okay, dann abschließend: Wie dokumentieren Sie diese Abwägungen für spätere Audits?"}
{"ts": "164:46", "speaker": "E", "text": "Wir pflegen alle Entscheidungen in unserem internen Decision Log, das mit den RFC-Nummern verlinkt ist. Zusätzlich hängen wir relevante Runbooks an die Tickets in JIRA, z. B. GW-SEC-TRD-07, sodass Auditoren die technische Begründung und Umsetzung nachvollziehen können."}
{"ts": "165:02", "speaker": "I", "text": "Lassen Sie uns mal konkret auf die Abhängigkeiten zur Aegis IAM Plattform eingehen. Wie haben Sie die Schnittstelle abgesichert, gerade im Kontext der Orion Edge Gateway Auth-Flows?"}
{"ts": "165:08", "speaker": "E", "text": "Wir haben dafür in der Build-Phase ein dediziertes mTLS zwischen Gateway und Aegis-Service aufgesetzt, zusätzlich zu signierten JWTs für jede Anfrage. Das ist im Runbook RB-SEC-044 beschrieben, zusammen mit den Health-Check-Endpoints, die wir vor jedem Deployment verifizieren."}
{"ts": "165:16", "speaker": "I", "text": "Und wie stellen Sie sicher, dass Nimbus Observability die relevanten Logs und Traces bekommt?"}
{"ts": "165:21", "speaker": "E", "text": "Wir pushen strukturierte JSON-Logs über gRPC an Nimbus, mit Trace-IDs aus dem OpenTelemetry-Collector. Das Mapping der Trace-IDs zu Gateway-Requests ist Teil des Deployment-Skripts, Ticket LOG-672 beschreibt die Implementierung."}
{"ts": "165:30", "speaker": "I", "text": "Gab es da jemals Probleme bei der Korrelation von Traces, vielleicht durch asynchrone Verarbeitung?"}
{"ts": "165:36", "speaker": "E", "text": "Ja, wir hatten in Sprint 14 einen Fall, wo der Trace-Context im mTLS-Handshake verloren ging. Das haben wir gelöst, indem wir den Context im Gateway selbst persistent halten, bis der komplette Auth-Zyklus abgeschlossen ist."}
{"ts": "165:44", "speaker": "I", "text": "Was konnten Sie aus Helios Datalake dafür lernen?"}
{"ts": "165:49", "speaker": "E", "text": "Helios hatte ein ähnliches Problem mit Batch-Processing und Trace-Verlust. Wir haben deren Retry-Mechanismus für Context-Propagation übernommen, dokumentiert in RFC-HEL-23, und für Streaming-Requests angepasst."}
{"ts": "165:58", "speaker": "I", "text": "Und aus Poseidon Networking?"}
{"ts": "166:02", "speaker": "E", "text": "Von Poseidon kam der Tipp, TLS-Session-Resumption nur selektiv zu aktivieren, um den Overhead zu reduzieren, ohne die Sicherheit zu kompromittieren. Das war wichtig für unser SLA-bezogenes Latenzbudget."}
{"ts": "166:11", "speaker": "I", "text": "Apropos Latenzbudget: mussten Sie dabei Abstriche bei der Sicherheitsstufe machen?"}
{"ts": "166:15", "speaker": "E", "text": "Teilweise, ja. Wir haben beispielsweise OCSP-Stapling eingeführt, um die Zertifikatsprüfung zu beschleunigen, statt jedes Mal eine externe Abfrage zu machen. Das reduziert zwar die Echtzeit-Prüfung, ist aber durch kürzere Stapling-Intervalle abgesichert."}
{"ts": "166:24", "speaker": "I", "text": "Wie dokumentieren Sie solche Entscheidungen?"}
{"ts": "166:28", "speaker": "E", "text": "Wir nutzen RFC-Dokumente im internen Confluence und verlinken diese direkt in die entsprechenden Runbooks, z.B. RB-GW-015 für TLS-Optimierungen. Außerdem gibt es in Jira die Decision-Tickets wie DEC-082."}
{"ts": "166:36", "speaker": "I", "text": "Sehen Sie aktuell noch Risiken im mTLS-Setup, trotz dieser Optimierungen?"}
{"ts": "166:41", "speaker": "E", "text": "Ja, vor allem das Risiko, dass bei hoher Last der Handshake-Cache ausfällt und dann alle Sessions neu aufgebaut werden müssen. Das könnte kurzfristig unser p95-Limit gefährden. Wir haben dafür in RB-GW-022 einen Notfallplan definiert, der den Cache dynamisch skaliert."}
{"ts": "167:02", "speaker": "I", "text": "Lassen Sie uns noch kurz auf die Schnittstelle zur Aegis IAM Plattform eingehen – können Sie erläutern, wie Sie dort die Authentifizierungs-Token absichern?"}
{"ts": "167:15", "speaker": "E", "text": "Ja, wir verwenden für den Token-Austausch ausschließlich mTLS über einen dedizierten Port 8443 und zusätzlich JWT-Signaturen mit einem RS512-Algorithmus. Das steht so auch in unserem Runbook RB-GW-019 unter Abschnitt 4.2 festgelegt."}
{"ts": "167:34", "speaker": "I", "text": "Und wie überprüfen Sie, dass diese Signaturen im Gateway selbst gültig sind?"}
{"ts": "167:42", "speaker": "E", "text": "Der Gateway-Service hat eine integrierte Validierungs-Pipeline, die beim Eintreffen eines Requests den Public Key aus dem Aegis JWKS-Endpunkt cached und gegen das eingehende JWT prüft. Das reduziert Roundtrips und schützt gleichzeitig gegen Replay-Angriffe, wie wir sie in Ticket SEC-712 protokolliert hatten."}
{"ts": "168:05", "speaker": "I", "text": "Interessant. Und was passiert, wenn Nimbus Observability für einige Minuten keine Logs empfangen kann?"}
{"ts": "168:13", "speaker": "E", "text": "Wir haben einen Fallback-Mechanismus: Die Logs werden lokal im Gateway-Pod in einer Persistent Volume Claim gespeichert, maximal 500 MB, und dann asynchron nachgesendet. Dieses Verhalten ist in RB-GW-014 dokumentiert. So vermeiden wir Datenverlust, selbst wenn der Log-Collector down ist."}
{"ts": "168:36", "speaker": "I", "text": "Und wie korrelieren Sie dann später diese nachgesendeten Logs mit Traces?"}
{"ts": "168:44", "speaker": "E", "text": "Wir fügen jedem Log-Eintrag Trace- und Span-IDs hinzu, die auch im OpenTelemetry-Trace vorhanden sind. Nimbus kann beim Import die IDs matchen und die zeitliche Abweichung mit einem Offset korrigieren. Das ist ein Lesson Learned aus Helios Datalake, wo wir ohne IDs viel Kontext verloren haben."}
{"ts": "169:09", "speaker": "I", "text": "Gab es bei der Übernahme dieser Lessons Learned technische Hürden?"}
{"ts": "169:16", "speaker": "E", "text": "Ja, vor allem beim Propagieren der Trace-IDs durch interne gRPC-Calls. Einige internen Services mussten wir umbauen, um die Metadata-Header korrekt weiterzugeben. Das stand so nicht im ursprünglichen Scope von P-ORI, wir haben aber in RFC-ORI-27 eine Ausnahme dokumentiert."}
{"ts": "169:38", "speaker": "I", "text": "Wie stellen Sie sicher, dass diese zusätzlichen Header nicht zu einer Erhöhung der Latenz führen, die das SLA-ORI-02 verletzt?"}
{"ts": "169:46", "speaker": "E", "text": "Wir haben in unserem Performance-Test-Cluster mit synthetischem Traffic gemessen, dass die Header-Propagation maximal 2 ms pro Hop kostet. Bei unserem Budget von p95 < 120 ms bleibt ausreichend Spielraum. Die Testläufe dokumentieren wir in unserem QA-Report QAR-082."}
{"ts": "170:10", "speaker": "I", "text": "Stichwort Spielraum – gab es bei den Observability-Anpassungen einen Trade-off, den Sie eingehen mussten?"}
{"ts": "170:18", "speaker": "E", "text": "Ja, wir mussten entscheiden, ob wir die Trace-Sampling-Rate erhöhen, um mehr Detail für Debugging zu haben, oder sie konstant bei 20 % belassen, um Performance und Speicher zu schonen. Wir haben uns für 20 % entschieden, weil höhere Raten in Lasttests zu erhöhter CPU-Auslastung geführt haben, siehe Messreihe in PERF-GW-05."}
{"ts": "170:42", "speaker": "I", "text": "Gab es dazu eine formale Risikoabschätzung?"}
{"ts": "170:49", "speaker": "E", "text": "Ja, in unserem Risk Log RISK-ORI-11. Dort haben wir aufgeführt, dass ein geringeres Sampling das Risiko birgt, seltene Fehler zu übersehen. Als Gegenmaßnahme haben wir ein manuelles Re-Sampling-Feature eingebaut, das bei Incident-Response aktiviert werden kann – dokumentiert in RB-OBS-003."}
{"ts": "176:02", "speaker": "I", "text": "Lassen Sie uns noch kurz beim Incident Response bleiben – wie haben Sie im letzten Auth-Service-Ausfall reagiert, konkret an welchem Punkt sind Sie in RB-GW-022 eingestiegen?"}
{"ts": "176:21", "speaker": "E", "text": "Wir sind nach dem Playbook aus RB-GW-022 bei Step 3 eingestiegen, weil der Healthcheck-Alarm vom Nimbus Observability bereits bestätigt hatte, dass nur der Auth-Integration-Service betroffen war. Sofortige Aktion war das Umschalten auf den Fallback-JWT-Validator, wie in Schritt 3.2 beschrieben."}
{"ts": "176:48", "speaker": "I", "text": "Und wie schnell konnten Sie damit das SLA-ORI-05, also Error Rate unter 0,5%, wieder herstellen?"}
{"ts": "177:03", "speaker": "E", "text": "Innerhalb von 4 Minuten. Die Error Rate ist, laut Metrik aus dem Prometheus-Export, von 3,2% auf 0,3% gefallen, und wir haben das in unserem Incident-Channel dokumentiert, Ticket OPS-774."}
{"ts": "177:26", "speaker": "I", "text": "Gab es dabei Koordination mit dem Aegis IAM Team oder haben Sie komplett isoliert gearbeitet?"}
{"ts": "177:38", "speaker": "E", "text": "Wir haben parallel eine Bridge mit dem Aegis IAM On-Call aufgemacht, weil wir prüfen mussten, ob das Problem upstream lag. In diesem Fall war es aber ein Timeout im Gateway selbst, verursacht durch eine falsch konfigurierte mTLS-Session-Renewal-Policy – siehe GW-4923."}
{"ts": "178:04", "speaker": "I", "text": "Interessant… warum wurde diese Policy nicht vorher im Staging erkannt?"}
{"ts": "178:15", "speaker": "E", "text": "Weil unser Staging-Cluster noch mit einer älteren Version der Policy-as-Code-Module lief. Wir haben inzwischen in RFC-ORI-19 festgelegt, dass Staging und Prod synchron bleiben müssen, inklusive aller mTLS-Parameter."}
{"ts": "178:37", "speaker": "I", "text": "Gut, beim Thema Synchronität – wie handhaben Sie das bei Cross-Project Abhängigkeiten, z.B. zu Nimbus Observability?"}
{"ts": "178:50", "speaker": "E", "text": "Wir haben ein wöchentliches Sync-Meeting und zusätzlich eine automatisierte Contract-Test-Pipeline, die bei jedem Build prüft, ob die Log- und Trace-Schemas noch kompatibel sind. Das reduziert Überraschungen, wenn plötzlich ein Feld fehlt und unsere Latency-Dashboards leer bleiben."}
{"ts": "179:17", "speaker": "I", "text": "Das klingt robust. Gab es dennoch einmal einen Fall, wo diese Pipeline versagt hat?"}
{"ts": "179:28", "speaker": "E", "text": "Ja, im März, als ein optionales Feld in der Trace-Payload aus dem Helios Datalake entfernt wurde. Der Contract-Test war auf Pflichtfelder fokussiert, also fiel es nicht auf. Wir haben daraus gelernt und die Tests auf optionale Felder erweitert."}
{"ts": "179:54", "speaker": "I", "text": "Zurück zur Sicherheit: Welche Risiken sehen Sie aktuell noch im mTLS-Setup, rein technisch?"}
{"ts": "180:07", "speaker": "E", "text": "Das größte Risiko ist momentan die kurze Gültigkeit der Client-Zertifikate – wir rotieren alle 48h. Wenn das Renewal-Skript fehlschlägt, wie in GW-5012, kann es binnen Minuten zu Verbindungsabbrüchen kommen. Wir evaluieren gerade ein Grace-Period-Feature aus RFC-ORI-22."}
{"ts": "180:33", "speaker": "I", "text": "Und welchen Trade-off sehen Sie da zwischen Sicherheit und Verfügbarkeit?"}
{"ts": "180:44", "speaker": "E", "text": "Mit längeren Zertifikatslaufzeiten würden wir die Verfügbarkeit natürlich erhöhen, aber das vergrößert die Angriffsfläche bei kompromittierten Keys. Unser heuristischer Ansatz: nicht über 72h gehen, und jede Verlängerung nur mit zusätzlichem Monitoring- und Revocation-Mechanismus kombinieren."}
{"ts": "185:02", "speaker": "I", "text": "Lassen Sie uns nochmal auf die Lessons Learned eingehen. Gab es aus Ihrer Sicht ein Szenario, wo ein Security-Fix die Performance stärker beeinträchtigt hat als erwartet?"}
{"ts": "185:15", "speaker": "E", "text": "Ja, beim Fix für Ticket GW-5172. Wir haben damals die Cipher-Suite auf TLS_ECDHE_RSA mit 4096-Bit Keys hochgezogen, um eine potenzielle Schwachstelle zu schließen. Laut unserem internen RFC-071 war das zwar sicherer, aber die p95-Latenz ist kurzfristig um 18 ms gestiegen."}
{"ts": "185:34", "speaker": "I", "text": "Wie haben Sie das kompensiert, um SLA-ORI-02 nicht zu verletzen?"}
{"ts": "185:39", "speaker": "E", "text": "Wir haben parallel auf Kernel-Level den Socket-Tuning-Parameter net.core.somaxconn angepasst und im Gateway-Threadpool das concurrency limit von 64 auf 80 Threads angehoben. Das war in Runbook RB-GW-019 dokumentiert."}
{"ts": "185:56", "speaker": "I", "text": "Und gab es dabei Koordination mit Nimbus Observability?"}
{"ts": "186:02", "speaker": "E", "text": "Genau. Wir haben einen gemeinsamen Trace-View eingerichtet, um die Latenzspitzen gegen das Log der Auth-Integration zu korrelieren. Dadurch konnten wir nachweisen, dass der Latenzanstieg nicht aus dem Downstream IAM-Service kam."}
{"ts": "186:18", "speaker": "I", "text": "Sie erwähnten vorhin Runbook RB-GW-019 – wie aktuell halten Sie diese Dokumente?"}
{"ts": "186:24", "speaker": "E", "text": "Wir versionieren jedes Runbook im internen Git und versehen es mit einem Changelog. RB-GW-019 hatte zuletzt ein Update vor drei Wochen nach einem Blue/Green Deployment laut RB-GW-011."}
