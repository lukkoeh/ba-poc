{"ts": "00:00", "speaker": "I", "text": "Können Sie mir kurz Ihre Rolle im Titan DR Projekt beschreiben?"}
{"ts": "01:15", "speaker": "E", "text": "Ja, klar. Ich bin Site Reliability Engineer bei Novereon Systems und im Projekt Titan DR hauptsächlich für die Umsetzung und Validierung der Disaster-Recovery-Prozeduren zuständig. In der aktuellen Drill-Phase koordiniere ich das Auslösen des Multi-Region-Failovers gemäß unserem Runbook RB-DR-001 und überwache die SLA-Konformität während der Übung."}
{"ts": "05:42", "speaker": "I", "text": "Wie sieht Ihr typischer Oncall-Tag während einer Drill-Phase aus?"}
{"ts": "07:03", "speaker": "E", "text": "Während eines Drills beginnt der Tag oft mit einer kurzen Lagebesprechung im Incident Command Channel. Danach überprüfe ich die Health-Dashboards in Nimbus Observability, löse Test-Alerts aus und simuliere teilweise Netzwerkausfälle zusammen mit dem Poseidon Networking Team. Wenn der Failover startet, folge ich strikt den Schritten aus RB-DR-001, halte aber immer mein persönliches Notizbuch bereit, um Abweichungen sofort festzuhalten."}
{"ts": "11:20", "speaker": "I", "text": "Welche Runbooks sind für Sie in dieser Phase am relevantesten?"}
{"ts": "13:05", "speaker": "E", "text": "Primär RB-DR-001, das die Regional Failover Procedure beschreibt. Daneben RB-DR-004 für das Wiederanheben der Primärregion und RB-NET-002 für Netzwerksegmentumschaltungen mit Poseidon. Bei komplexeren Szenarien greife ich auch auf RB-MON-005 zurück, das beschreibt, wie wir False Positives im Alerting während eines Drills filtern."}
{"ts": "17:50", "speaker": "I", "text": "Wie ist die Multi-Region-Architektur konkret aufgebaut?"}
{"ts": "20:15", "speaker": "E", "text": "Wir betreiben zwei aktive Regionen, West-Europa und US-Ost, mit synchroner Datenreplikation über unser internes Stream-Replication-Protokoll. Die Core Services wie Auth, Billing und Data-Ingest laufen in beiden Regionen, wobei wir im Normalbetrieb 70/30 Traffic-Split fahren. Für das RTO von 15 Minuten ist entscheidend, dass die Messaging-Queues und das Objektstorage konsistent rüberkommen – das orchestrieren wir mit den Playbooks aus RB-DR-001."}
{"ts": "25:05", "speaker": "I", "text": "Welche Services sind kritisch für das Erreichen unseres RTO/RPO?"}
{"ts": "27:12", "speaker": "E", "text": "Die Auth-API, weil ohne sie kein User Login möglich ist, und der Data-Ingest-Service für eingehende Kundenstreams. Das Objektstorage muss maximal 5 Minuten RPO haben, sonst verlieren wir Datenpunkte. Auch der interne Config-Service ist kritisch, da er Feature Flags ausliefert, die für das Routing im Failover gebraucht werden."}
{"ts": "31:40", "speaker": "I", "text": "Welche Schnittstellen zu anderen Projekten bestehen, z. B. zu Nimbus Observability oder Poseidon Networking?"}
{"ts": "34:20", "speaker": "E", "text": "Nimbus Observability liefert uns Metriken, Logs und Traces in Echtzeit, inklusive spezieller DR-Dashboards. Poseidon Networking stellt die BGP- und DNS-Umschaltungen bereit, ohne die ein Regionswechsel nicht sauber funktioniert. Außerdem haben wir eine lose Kopplung zu Projekt Helios Security, das im Failover Fall zusätzliche Auth-Checks einführt – das erfordert enge Abstimmung, damit wir nicht durch zu strikte Policies blockiert werden."}
{"ts": "39:55", "speaker": "I", "text": "Wie oft wurde RB-DR-001 in der Praxis getestet?"}
{"ts": "41:20", "speaker": "E", "text": "In den letzten 12 Monaten haben wir es sechs Mal vollständig durchgespielt – vier geplante Drills, zwei ungeplante Tests nach kleineren Incidents. Jeder Drill wird in unserem Confluence-DR-Space dokumentiert, mit Verweis auf das entsprechende Ticket, z. B. DR-DRILL-2023-04."}
{"ts": "46:05", "speaker": "I", "text": "Gab es Abweichungen zwischen Runbook und tatsächlicher Ausführung?"}
{"ts": "48:30", "speaker": "E", "text": "Ja, insbesondere bei Schritt 7 von RB-DR-001. Dort steht, dass die DNS-Propagation 120 Sekunden dauert, wir haben aber in Drill DR-DRILL-2023-04 teilweise über 5 Minuten gemessen. Wir mussten ad hoc einen Workaround aktivieren, indem wir den TTL-Wert auf 30 Sekunden gesetzt haben. Das haben wir dann als Verbesserungsvorschlag ins Runbook aufgenommen."}
{"ts": "90:00", "speaker": "I", "text": "Könnten Sie bitte genauer beschreiben, wie RB-DR-001 praktisch im Drill angewendet wird, insbesondere wenn mehrere Regionen gleichzeitig betroffen sind?"}
{"ts": "90:15", "speaker": "E", "text": "Ja, klar. RB-DR-001 enthält einen Abschnitt für 'Concurrent Region Impact'. In unserem letzten Drill mussten wir Step 4.3 und 4.4 parallel ausführen – also das Failover der Datenbank und den DNS-Switch – und das erfordert laut Runbook zwei separate Command Leads. Wir haben dazu intern Ticket T-DR-172 angelegt, um die parallele Arbeit zu koordinieren."}
{"ts": "90:48", "speaker": "I", "text": "Gab es bei dieser parallelen Ausführung Abweichungen von der Dokumentation?"}
{"ts": "91:02", "speaker": "E", "text": "Ein kleines Detail: Im Runbook steht, dass das TTL-Update für DNS-Einträge vor der DB-Replikation passieren soll. In der Praxis haben wir es umgedreht, weil wir bei den Tests mit Poseidon Networking festgestellt haben, dass sonst kurzzeitige Connection Drops entstehen."}
{"ts": "91:32", "speaker": "I", "text": "Interessant. Und wie haben Sie diese Erkenntnis dokumentiert?"}
{"ts": "91:46", "speaker": "E", "text": "Wir haben im Confluence-Bereich 'Titan DR / Runbook Updates' eine Revision angelegt, RB-DR-001 v2.1, und den Change auch im Incident-Postmortem des Drills unter Lessons Learned verlinkt."}
{"ts": "92:10", "speaker": "I", "text": "Welche Rolle spielt hier das Monitoring über Nimbus Observability beim Erkennen solcher Nebeneffekte?"}
{"ts": "92:28", "speaker": "E", "text": "Nimbus liefert uns in quasi Echtzeit die TCP-Reset-Rate pro Region. Während der Drill-Phase hatten wir einen Spike von 0,5 % in Region West-2 direkt nach dem DNS-Switch – genau das hat uns auf die Reihenfolge-Problematik aufmerksam gemacht."}
{"ts": "92:54", "speaker": "I", "text": "Gab es auch Abhängigkeiten zu anderen Projekten, die hier relevant waren?"}
{"ts": "93:09", "speaker": "E", "text": "Ja, der Alert-Dispatcher im Projekt Nimbus musste angepasst werden, weil die Metrik 'DBReplicationLag' aus Poseidon Networking nur jede Minute aktualisiert wird. Für den Drill haben wir einen temporären Webhook implementiert, um diese Verzögerung zu umgehen."}
{"ts": "93:38", "speaker": "I", "text": "Und wie wird in solchen Fällen die Eskalation gehandhabt, wenn die RTO in Gefahr ist?"}
{"ts": "93:53", "speaker": "E", "text": "Wir folgen dem internen ICS-Playbook ICS-DR-002. Bei drohendem RTO-Bruch geht automatisch eine Stufe-2-Eskalation an den Duty Manager und den Produktverantwortlichen. Das ist in Drill-Szenarien schon zweimal passiert, zuletzt bei Drill D-23-05."}
{"ts": "94:20", "speaker": "I", "text": "Welche Verbesserungen wurden nach diesem letzten Drill beschlossen?"}
{"ts": "94:35", "speaker": "E", "text": "Wir haben entschieden, ein Pre-Failover Validation Script zu entwickeln, das innerhalb von 90 Sekunden alle kritischen Services prüft. Damit wollen wir die Entscheidungszeit im Failover-Window um ca. 25 % reduzieren."}
{"ts": "94:58", "speaker": "I", "text": "Gibt es noch offene Risiken, die hierbei relevant sind?"}
{"ts": "95:15", "speaker": "E", "text": "Ja, das größte offene Risiko ist aktuell die Kosten-Nutzen-Balance beim permanenten Warm-Standby in drei Regionen. Wir haben dazu RFC-DR-019 vorbereitet, der diese Tradeoffs detailliert beschreibt – das ist einer der Punkte, die wir in der nächsten Steering-Runde entscheiden müssen."}
{"ts": "106:00", "speaker": "I", "text": "Wir haben jetzt die Architektur und die Schnittstellen im Detail, inklusive Nimbus und Poseidon, besprochen. Ich würde gern in Richtung Optimierungen und Entscheidungen nach den Drills gehen. Welche größten Verbesserungen wurden nach dem letzten Drill tatsächlich umgesetzt?"}
{"ts": "106:15", "speaker": "E", "text": "Nach dem letzten Drill im März haben wir RB-DR-001 angepasst, um die Reihenfolge der DNS-Failover-Tasks zu optimieren. Wir haben festgestellt, dass in Ticket DR-872 die Latenz bei der Umstellung um 40 Sekunden höher war als geplant, weil ein Healthcheck-Timeout zu konservativ gesetzt war. Das wurde im Runbook auf 15s heruntergesetzt."}
{"ts": "106:40", "speaker": "I", "text": "Gab es dabei auch Diskussionen mit dem Networking-Team von Poseidon zu diesen Timeouts?"}
{"ts": "106:48", "speaker": "E", "text": "Ja, klar. Poseidon hatte Bedenken, dass zu kurze Timeouts bei transienten Netzwerkproblemen zu voreiligen Reroutes führen. Wir haben kompromissweise einen zweistufigen Check eingebaut: initial 15s, dann ein zweiter Confirm-Check mit 10 Paketen ICMP über 5s. Das ist auch im Anhang B des Runbooks dokumentiert."}
{"ts": "107:15", "speaker": "I", "text": "Und wie beeinflusst das unsere RTO/RPO-Ziele?"}
{"ts": "107:22", "speaker": "E", "text": "Positiv. Der RTO konnte im Drill um ca. 1:20 min verbessert werden, der RPO blieb unverändert bei 5 Minuten, da die Datenreplikation nicht tangiert war. Wir haben das in SLA-Report DR-SLA-2024-03 vermerkt."}
{"ts": "107:45", "speaker": "I", "text": "Gibt es aktuell noch offene Risiken, die wir vor dem nächsten Drill adressieren müssen?"}
{"ts": "107:52", "speaker": "E", "text": "Ja, zwei Punkte. Erstens: die Abhängigkeit von einem einzigen Cloud-Provider für den East-Region Object Store. Zweitens: unser Alert Suppression Window ist fix auf 10 Minuten, was bei längeren Failover-Ketten zu ‚schwarzen Löchern‘ in der Beobachtbarkeit führen könnte. Beides ist in Risk-Register DR-RISK-14 und -15 erfasst."}
{"ts": "108:20", "speaker": "I", "text": "Wie diskutieren Sie intern den Tradeoff zwischen zusätzlichen Kosten für Multi-Cloud-Redundanz und der höheren Resilienz?"}
{"ts": "108:28", "speaker": "E", "text": "Das ist eine laufende Debatte im Steering Committee. Multi-Cloud würde laut Kostenschätzung FC-DR-2024-07 etwa 35% höhere Betriebskosten verursachen, könnte aber das East-Region-Risiko fast eliminieren. Momentan tendieren wir zu einer gestaffelten Einführung, beginnend mit kritischen Storage-Buckets."}
{"ts": "108:55", "speaker": "I", "text": "Wurden Lessons Learned aus vergangenen Drills systematisch dokumentiert und priorisiert?"}
{"ts": "109:02", "speaker": "E", "text": "Ja, wir pflegen ein Confluence-Board 'DR-Lessons', das jede Abweichung zwischen Runbook und Realität listet, verlinkt mit den zugehörigen Tickets. Priorisierung erfolgt nach Auswirkung auf RTO/RPO und Compliance-Anforderungen."}
{"ts": "109:20", "speaker": "I", "text": "Gab es konkrete Beispiele, wo eine scheinbar kleine Abweichung große Wirkung hatte?"}
{"ts": "109:27", "speaker": "E", "text": "Beim Drill im letzten Jahr hatten wir einen Befehl im Runbook, der auf eine veraltete CLI-Option verwies. Das führte zu 5 Minuten Verzögerung, bis der Oncall die richtige Syntax fand. Seitdem haben wir ein halbjährliches Runbook-CLI-Review eingeführt."}
{"ts": "109:50", "speaker": "I", "text": "Zum Abschluss: Welche kurzfristigen Maßnahmen wollen Sie bis zum nächsten Drill umsetzen, um diese offenen Risiken zu minimieren?"}
{"ts": "109:58", "speaker": "E", "text": "Wir wollen bis Ende des Quartals einen Proof-of-Concept für Cross-Cloud-Replikation in einer Testregion fahren und das Alert-Suppression-Window dynamisch an die Länge der Failover-Kette anpassen. Das wird in RFC-DR-2024-11 spezifiziert und soll im Juli getestet werden."}
{"ts": "114:00", "speaker": "I", "text": "Sie haben eben die Kosten- versus Resilienzfrage angesprochen – können Sie das im Kontext unseres letzten Drill noch einmal konkret machen?"}
{"ts": "114:05", "speaker": "E", "text": "Ja, gern. Wir hatten im Drill-Szenario einen Failover von Region West zu Region Nord simuliert. Die vollständige Redundanz auf Storage-Ebene hätte monatlich knapp 18 % höhere Betriebskosten verursacht. Wir haben uns daher entschieden, nur kritische Datenbanken mit synchronem Replication Mode zu fahren und weniger kritische auf asynchron zu setzen. Das ist im Übrigen in RFC-DR-2023-11 dokumentiert."}
{"ts": "114:20", "speaker": "I", "text": "Gab es dazu auch Diskussionen mit Finance oder war das eine rein technische Entscheidung?"}
{"ts": "114:27", "speaker": "E", "text": "Das war eine Mischentscheidung. Finance hat uns die Budget-Obergrenzen aus Ticket FIN-482 bereitgestellt, und wir mussten im SRE-Architekturboard abwägen, welche Systeme unter die harten RTO 15 min fallen. Das Board-Protokoll vom 15.03. ist dazu recht eindeutig."}
{"ts": "114:41", "speaker": "I", "text": "Und welche Risiken bleiben nach dieser Anpassung offen?"}
{"ts": "114:46", "speaker": "E", "text": "Ein Restrisiko ist, dass bei einem gleichzeitigen Ausfall beider Regionen die asynchron replizierten Daten einen Verlust von bis zu 90 Sekunden aufweisen könnten. Das liegt unter unserem definierten RPO von 2 Minuten, aber im Krisenfall sind diese 90 Sekunden Datenverlust trotzdem relevant."}
{"ts": "114:59", "speaker": "I", "text": "Haben Sie dafür ein spezielles Runbook oder Workaround vorgesehen?"}
{"ts": "115:04", "speaker": "E", "text": "Ja, wir haben RB-DR-014 \"Dual-Region Data Loss Mitigation\" angelegt. Darin steht, dass wir im Ernstfall sofort eine Point-in-Time-Recovery auf die letzte synchrone Sicherung fahren und danach inkrementell aus den Write-Ahead-Logs rekonstruieren. Der Ablauf wurde im Drill zwei Mal durchgespielt."}
{"ts": "115:18", "speaker": "I", "text": "Kommen wir kurz zur Netzwerkseite: Gab es dort ebenfalls Tradeoffs?"}
{"ts": "115:23", "speaker": "E", "text": "Ja, Poseidon Networking hatte vorgeschlagen, in beiden Regionen identische Transit-Gateways vorzuhalten. Aus Kostengründen haben wir in der Passiv-Region nur einen skalierbaren Gateway-Cluster im Warm-Standby. Im Drill haben wir gesehen, dass das Hochfahren etwa 90 Sekunden dauert, was noch im Rahmen der SLA-Nebenziele liegt."}
{"ts": "115:37", "speaker": "I", "text": "Wie dokumentieren Sie solche Erkenntnisse für künftige Drills?"}
{"ts": "115:42", "speaker": "E", "text": "Wir führen nach jedem Drill eine Postmortem-Analyse in Confluence durch, verlinken die relevanten Tickets, z. B. DR-INC-771, und aktualisieren betroffene Runbooks. Zusätzlich gibt es ein internes Brown-Bag-Meeting, in dem wir Lessons Learned teamübergreifend teilen."}
{"ts": "115:56", "speaker": "I", "text": "Gab es bei diesem Drill ein Beispiel, wo die Dokumentation direkt zu einer Prozessänderung geführt hat?"}
{"ts": "116:01", "speaker": "E", "text": "Ja, beim Failover der Authentifizierungsdienste. Früher haben wir nach dem Region-Switch den Token-Cache neu aufgebaut, was zu 2–3 Minuten längeren Login-Zeiten führte. Durch die Dokumentation im Drill fiel das auf, und wir haben im Runbook RB-AUTH-005 jetzt ein Verfahren zur Cache-Replikation zwischen Regionen ergänzt."}
{"ts": "116:15", "speaker": "I", "text": "Wie sieht denn der nächste Schritt aus, um diese Tradeoff-Entscheidungen weiter zu validieren?"}
{"ts": "116:20", "speaker": "E", "text": "Wir planen im Q3 einen Chaos-Test mit partiellen Ausfällen in beiden Regionen gleichzeitig. Damit wollen wir evaluieren, ob unsere Mischstrategie aus synchroner und asynchroner Replikation tragfähig ist. Ticket DR-TEST-892 ist dafür schon angelegt, und die Vorbereitungen laufen."}
{"ts": "116:00", "speaker": "I", "text": "Sie hatten vorhin kurz die Anpassung von RB-DR-001 erwähnt. Können Sie das bitte etwas ausführen?"}
{"ts": "116:06", "speaker": "E", "text": "Ja, also wir haben im letzten Drill festgestellt, dass der Abschnitt zur DNS-Umschaltung zu allgemein war. Wir haben deshalb konkretisiert, welche CNAME-Records bei einem Failover zuerst angepasst werden, und haben dafür im Runbook eine Tabelle mit Prioritäten ergänzt."}
{"ts": "116:16", "speaker": "I", "text": "Gab es dafür eine formale Änderung, also ein Change-Ticket?"}
{"ts": "116:20", "speaker": "E", "text": "Genau, das lief unter TKT-DR-4821. Darin haben wir auch gleich die aktualisierte Sequenz der Health Checks dokumentiert, damit das Monitoring-Team in Nimbus Observability die Validierung automatisiert starten kann."}
{"ts": "116:33", "speaker": "I", "text": "Wie haben Sie diese Änderungen getestet, bevor sie ins Runbook übernommen wurden?"}
{"ts": "116:38", "speaker": "E", "text": "Wir haben einen isolierten Sandbox-Cluster hochgefahren, der die Multi-Region-Verkabelung simuliert. Da haben wir im Prinzip einen Mini-Failover gefahren, mit allen Schritten, und die Ergebnisse mit den erwarteten SLOs aus dem DR-SLA-2024 abgeglichen."}
{"ts": "116:51", "speaker": "I", "text": "Und die SLOs konnten alle erfüllt werden?"}
{"ts": "116:54", "speaker": "E", "text": "Ja, zumindest im Testlauf. Wir lagen beim RTO bei 17 Minuten, was unter dem Zielwert von 20 liegt, und der Datenverlust, also RPO, war unter 30 Sekunden. In der echten Drill-Situation ist das aber immer noch knapper, weil Netzwerkpfade variabler sind."}
{"ts": "117:09", "speaker": "I", "text": "Stichwort Netzwerkpfade – wie stellen Sie sicher, dass bei einem Engpass die kritischen Services zuerst durchkommen?"}
{"ts": "117:14", "speaker": "E", "text": "Wir nutzen im Poseidon Networking Projekt definierte QoS-Klassen. Für Titan DR haben wir mit NetEng abgesprochen, dass Klasse-0-Traffic, also Control Plane und DB-Replikation, Vorrang hat. Das basiert auf Config-Template NET-QOS-07."}
{"ts": "117:27", "speaker": "I", "text": "Gab es mal Situationen, in denen diese Priorisierung nicht gegriffen hat?"}
{"ts": "117:31", "speaker": "E", "text": "Einmal, im Drill vom August, hat ein fehlerhaftes Routing-Policy-Update die Tags für den Traffic nicht gesetzt. Das war Incident INC-DR-771. Seitdem haben wir einen Validations-Webhook zwischen Poseidon und dem DR-Orchestrator eingeführt."}
{"ts": "117:46", "speaker": "I", "text": "Das klingt nach einer engen Verzahnung zwischen mehreren Subsystemen. Wie halten Sie da die Dokumentation aktuell?"}
{"ts": "117:51", "speaker": "E", "text": "Wir haben ein Confluence-Space 'Titan DR Crosslinks', da pflegen wir nicht nur Runbooks, sondern auch Abhängigkeitsdiagramme, die direkt aus dem Nimbus CMDB generiert werden. Änderungen aus Tickets wie TKT-DR-4821 werden dort automatisch verlinkt."}
{"ts": "118:04", "speaker": "I", "text": "Zum Abschluss, wenn Sie an die nächste Drill-Iteration denken – was ist Ihre größte Sorge?"}
{"ts": "118:10", "speaker": "E", "text": "Dass wir durch Kostendruck bei den Warm-Standby-Ressourcen zu knapp kalkulieren und dann im Ernstfall das RTO reißen. Wir haben das als Risk RSK-DR-305 dokumentiert und werden das im Steering Committee nächste Woche nochmal adressieren."}
{"ts": "124:00", "speaker": "I", "text": "Lassen Sie uns noch einmal auf die Beobachtungen während des letzten Drill-Schritts eingehen – gab es im Failover-Pfad unerwartete Latenzspitzen?"}
{"ts": "124:05", "speaker": "E", "text": "Ja, im Segment EU-West zu AP-South haben wir einen Spike von 350ms gesehen, dokumentiert im Drill-Log #DR-744. Ursache war ein temporärer Engpass im Poseidon Networking Layer 3 Routing."}
{"ts": "124:15", "speaker": "I", "text": "Und wie haben Sie darauf reagiert, gab es eine improvisierte Maßnahme oder sind Sie strikt nach Runbook RB-DR-001 vorgegangen?"}
{"ts": "124:21", "speaker": "E", "text": "Wir sind initial strikt nach RB-DR-001 vorgegangen, Step 4.2 ‚Validate Inter-Region Latency‘. Als klar war, dass es persistent ist, haben wir ein temporäres BGP-Pref-Adjust eingefügt, was nur in unserem internen Zusatzprotokoll RZ-DR-Notes steht."}
{"ts": "124:36", "speaker": "I", "text": "Das klingt nach einer Abweichung, wurde die schon ins offizielle Runbook übernommen?"}
{"ts": "124:40", "speaker": "E", "text": "Noch nicht, sie ist in Ticket OPS-982 eingetragen und wird beim nächsten Runbook-Review mit Platform und Networking evaluiert, um sicherzustellen, dass es keine unbeabsichtigten Route-Flaps erzeugt."}
{"ts": "124:52", "speaker": "I", "text": "Wie hat sich diese Latenz auf das RTO ausgewirkt in der Simulation?"}
{"ts": "124:56", "speaker": "E", "text": "Minimal, wir lagen immer noch bei 7 Minuten 45 Sekunden, also unter dem SLA von 10 Minuten. Aber es hat unsere SLO für Response Time in der Transition-Phase um 8% überschritten."}
{"ts": "125:08", "speaker": "I", "text": "Gab es vom Monitoring her genügend Frühwarnungen oder kam der Alert erst bei Überschreiten des Thresholds?"}
{"ts": "125:13", "speaker": "E", "text": "Der Alert kam bei 250ms, das ist unser konfigurierter Warnschwellwert in Nimbus Observability. Frühwarnung war da, aber wir hatten sie intern als 'watch' markiert, nicht als 'page', um Alert Fatigue zu vermeiden."}
{"ts": "125:26", "speaker": "I", "text": "Sehen Sie da Optimierungspotential bei den Alert-Policies?"}
{"ts": "125:29", "speaker": "E", "text": "Ja, wir überlegen einen dynamischen Threshold basierend auf historischen Drill-Daten – das steht als Proposal in RFC-2139. Das würde adaptive Warnlevel setzen, ohne die Oncall-Last zu erhöhen."}
{"ts": "125:42", "speaker": "I", "text": "Abschließend, welche Lessons Learned aus dieser spezifischen Situation nehmen Sie mit?"}
{"ts": "125:46", "speaker": "E", "text": "Dass wir Cross-Team-Knowledge im Drill stärker priorisieren müssen. Die schnelle BGP-Anpassung war möglich, weil ein Networking-Kollege zufällig im War Room war. Wir wollen das als festen IC-Role-Assignment im ICS verankern."}
{"ts": "125:59", "speaker": "I", "text": "Klingt nach einer strukturellen Änderung – wer entscheidet das final?"}
{"ts": "126:00", "speaker": "E", "text": "Das geht über den DR Steering Committee, mit Input aus allen beteiligten Squads. Wir fügen die Empfehlung in das Drill After Action Report AAR-2024-05 ein, Deadline ist Ende des Quartals."}
{"ts": "128:00", "speaker": "I", "text": "Lassen Sie uns noch einmal konkret auf die Netzwerk-Engpässe eingehen, die Sie im Drill beobachtet haben. Welche Knotenpunkte waren am kritischsten?"}
{"ts": "128:15", "speaker": "E", "text": "Am auffälligsten war der Traffic-Backbone zwischen Region Nord und der Failover-Region West. Auf dem Interface gw-west-03 hatten wir laut Nimbus Observability zeitweise 92 % Auslastung. Das ist im Runbook RB-DR-001 nicht explizit adressiert, wir haben dazu jetzt ein Ergänzungsblatt erstellt."}
{"ts": "128:40", "speaker": "I", "text": "Und wie fließt diese Beobachtung in Ihre künftigen Prozeduren ein?"}
{"ts": "128:52", "speaker": "E", "text": "Wir haben ein RFC-Dokument, RFC-DR-2024-05, aufgesetzt, das beschreibt, wie wir für die kritischen Interfaces Pre-Warming und zusätzliche Bandbreiten-Reserven vorsehen. Außerdem wird im Runbook ein neuer Schritt eingefügt: Vor Failover Start einen Live-Check der Netzwerkpfade."}
{"ts": "129:20", "speaker": "I", "text": "Gab es dazu schon Abstimmungen mit dem Poseidon Networking Team?"}
{"ts": "129:33", "speaker": "E", "text": "Ja, wir hatten letzte Woche ein gemeinsames Review. Poseidon wird ihre automatisierten QoS-Regeln so anpassen, dass DR-Traffic priorisiert wird. Das ist über Ticket NET-7423 im Tracking-System hinterlegt."}
{"ts": "129:55", "speaker": "I", "text": "Können Sie beschreiben, wie diese QoS-Anpassung mit den SLOs im DR-Szenario zusammenhängt?"}
{"ts": "130:08", "speaker": "E", "text": "Klar. Unser DR-SLO für Recoverability lautet: 95 % aller kritischen Services müssen innerhalb von 45 Minuten nach Start des Failovers wieder verfügbar sein. Ohne angepasste QoS-Regeln riskieren wir Packet Loss auf den Service-Mesh-Kanälen und verfehlen das Ziel."}
{"ts": "130:34", "speaker": "I", "text": "Wie messen Sie den Effekt solcher Änderungen während eines Drills?"}
{"ts": "130:46", "speaker": "E", "text": "Wir setzen auf einen kombinierten Ansatz: synthetische Transaktionen aus allen Regionen, plus Real-User-Monitoring-Daten aus Nimbus. In der letzten Simulation haben wir z. B. mit dem Tool 'latcheck' Latenzen auf gw-west-03 vor und nach QoS-Änderung verglichen."}
{"ts": "131:12", "speaker": "I", "text": "Gab es signifikante Unterschiede?"}
{"ts": "131:22", "speaker": "E", "text": "Ja, die durchschnittliche Latenz fiel von 240 ms auf 160 ms bei Peak-Load. Das hat direkt dazu geführt, dass der kritische Service 'OrderProc' 8 Minuten schneller wieder online war."}
{"ts": "131:45", "speaker": "I", "text": "Beeinflusst diese Optimierung auch die Kosten, über die wir vorhin gesprochen haben?"}
{"ts": "131:57", "speaker": "E", "text": "Natürlich. Mehr reservierte Bandbreite heißt höhere laufende Kosten, wir sprechen hier von ca. +4 % auf das Netzwerkbudget. Im Entscheidungsprotokoll DEC-DR-12 haben wir das als akzeptablen Trade-off dokumentiert, weil der Gewinn an Resilienz messbar ist."}
{"ts": "132:25", "speaker": "I", "text": "Wie gehen Sie mit der Dokumentation solcher Trade-offs um, um sie für zukünftige Drills verfügbar zu machen?"}
{"ts": "132:38", "speaker": "E", "text": "Wir pflegen alle Entscheidungen und ihre Messwerte in unserem internen Confluence-Bereich 'Titan DR Knowledge Base'. Jeder Eintrag hat Verlinkungen zu den relevanten Tickets, Runbooks und Monitoring-Dashboards, sodass das Incident Command Team im Ernstfall schnell darauf zugreifen kann."}
{"ts": "136:00", "speaker": "I", "text": "Sie hatten vorhin die Netzwerkengpässe als offenes Risiko erwähnt. Können Sie genauer ausführen, welche Regionen davon im Drill am stärksten betroffen waren?"}
{"ts": "136:05", "speaker": "E", "text": "Ja, hauptsächlich Region EU-Central2, weil dort die Crosslink-Bandbreite zum US-East Knoten unter Last auf unter 60% des SLA-Werts gefallen ist. Das haben wir im Drill-Report DR-REP-2024-05 festgehalten."}
{"ts": "136:18", "speaker": "I", "text": "Gab es in der Runbook-Version RB-DR-001 eine Passage, die diesen Engpass adressiert?"}
{"ts": "136:23", "speaker": "E", "text": "Nicht explizit. Wir haben jetzt ein Addendum vorgeschlagen, RB-DR-001-Annex-NW, siehe Ticket NOC-4821, um vorab Bandbreitentests in die Failover-Checkliste einzubauen."}
{"ts": "136:38", "speaker": "I", "text": "Und wie würden Sie die Priorität dieser Änderung im Verhältnis zu anderen Optimierungen einschätzen?"}
{"ts": "136:43", "speaker": "E", "text": "Sehr hoch, weil das RTO direkt beeinflusst wird. Wenn der Crosslink limitiert, verlängert sich der Datenabgleich zwischen den Clustern um bis zu 8 Minuten, was nicht tolerierbar ist."}
{"ts": "136:57", "speaker": "I", "text": "Gab es dafür schon eine Abstimmung mit dem Poseidon Networking Team?"}
{"ts": "137:02", "speaker": "E", "text": "Ja, wir hatten letzte Woche ein Sync-Meeting. Dort kam heraus, dass eine temporäre Routing-Optimierung über die Nordic Transit Nodes möglich ist, was wir im nächsten Drill testen wollen."}
{"ts": "137:16", "speaker": "I", "text": "Könnte diese Routing-Optimierung auch Nebeneffekte haben?"}
{"ts": "137:20", "speaker": "E", "text": "Eventuell höhere Latenzen für nicht-DR-Traffic, ja. Deshalb wollen wir im Staging mit synthetischen Lastprofilen messen, bevor wir es ins Runbook übernehmen."}
{"ts": "137:34", "speaker": "I", "text": "Sie sprachen vom Staging-Test – welche Metriken sind da ausschlaggebend?"}
{"ts": "137:38", "speaker": "E", "text": "Wir schauen auf Throughput in Gbps, Paketverlust unter 0,1% und Latenz unter 150 ms im Transatlantik-Link. Zusätzlich Alert-Rate aus Nimbus Observability, um false positives zu erkennen."}
{"ts": "137:54", "speaker": "I", "text": "Falls die Tests negativ ausfallen – gibt es einen Fallback-Plan?"}
{"ts": "137:59", "speaker": "E", "text": "Ja, Beibehaltung der aktuellen Routen, aber parallele Erhöhung der Replikationsintervalle, um das Bandbreitenfenster zu entlasten. Das steht als Option C im internen RFC-DR-23-07."}
{"ts": "138:15", "speaker": "I", "text": "Wie wird die Entscheidung zwischen Option C und der Routing-Optimierung letztlich getroffen?"}
{"ts": "138:20", "speaker": "E", "text": "Basierend auf den Staging-Ergebnissen, Kostenabschätzung vom Finance-Team und einer Risikoanalyse aus dem ICS-Board. Alle drei Inputs müssen laut unserem DR-Governance-Dokument erfüllt sein, bevor die Änderung ins Runbook geht."}
{"ts": "145:00", "speaker": "I", "text": "Könnten Sie bitte noch einmal genauer erläutern, wie Sie die Netzwerkengpässe identifiziert haben, die Sie vorhin angesprochen haben?"}
{"ts": "145:05", "speaker": "E", "text": "Ja, das war während des letzten Drill-Laufs, Stage 3. Wir haben im Monitoring von Nimbus Observability einen plötzlichen Spike in den Latenzen zwischen Region West und Region Ost gesehen. Die Alert-Rule 'NET-LINK-LAT-CRIT' ist mehrfach hochgegangen, und das deckte sich mit unseren Logs aus dem Poseidon Networking Layer. Wir haben dann via Runbook RB-NET-004 eine temporäre Bandbreitenlimitierung umgangen."}
{"ts": "145:15", "speaker": "I", "text": "Und diese Engpässe, waren die durch externe Faktoren oder interne Konfigurationen verursacht?"}
{"ts": "145:20", "speaker": "E", "text": "Eine Mischung. Es gab eine externe BGP-Routenänderung durch unseren Provider, aber intern hatten wir in zwei Edge-Gateways noch die alten QoS-Profile aktiv. Das hat das Problem verschärft. Ticket NET-DR-221 hat die Details dazu, inklusive der Config-Diffs."}
{"ts": "145:32", "speaker": "I", "text": "In Bezug auf das Runbook RB-DR-001, gab es dadurch Anpassungen für künftige Drills?"}
{"ts": "145:38", "speaker": "E", "text": "Genau, wir haben einen neuen Schritt eingeführt: Vor dem Failover wird jetzt ein automatischer Check der aktuellen QoS-Profile in allen beteiligten Gateways durchgeführt. Das ist als Step 4.2 aufgenommen worden, mit Verweis auf NET-DR-221."}
{"ts": "145:48", "speaker": "I", "text": "Haben Sie diesen neuen Schritt schon in einer Testumgebung verprobt?"}
{"ts": "145:54", "speaker": "E", "text": "Ja, in unserer Staging-Umgebung Titan-Stg-02. Wir haben dort die Simulation mit künstlichen Latenzen gefahren und konnten sehen, dass die Pre-Checks den Engpass proaktiv gemeldet haben. Das war in Testlauf DR-STG-12 dokumentiert."}
{"ts": "146:05", "speaker": "I", "text": "Interessant. Und wie wirkt sich das auf die RTO-Werte aus?"}
{"ts": "146:10", "speaker": "E", "text": "Durch die Prävention sparen wir im Ernstfall schätzungsweise 3 bis 5 Minuten. Das ist signifikant, da unser Ziel-RTO laut SLA-DR-01 bei maximal 15 Minuten liegt. Mit der Anpassung bleiben wir also auch bei komplexeren Incidents im grünen Bereich."}
{"ts": "146:20", "speaker": "I", "text": "Gab es intern Diskussionen, ob diese zusätzlichen Checks nicht zu Verzögerungen führen könnten, wenn sie falsch positive Ergebnisse liefern?"}
{"ts": "146:27", "speaker": "E", "text": "Ja, das war Thema im letzten SRE-Review. Wir haben deshalb ein Threshold-Tuning vorgenommen, sodass kleinere Latenz-Spikes nicht sofort blockieren. Außerdem gibt es im Runbook einen Override-Pfad, der vom Incident Commander freigegeben werden kann."}
{"ts": "146:38", "speaker": "I", "text": "Wie dokumentieren Sie diese Änderungen, damit sie auch in zukünftigen Drill-Phasen konsistent angewendet werden?"}
{"ts": "146:43", "speaker": "E", "text": "Alle Änderungen gehen ins zentrale DR-Knowledge-Repo, Confluence-ähnlich. Dort gibt es den Bereich 'Runbook Change Log', und wir hängen auch Screenshots und Config-Diffs an. Für RB-DR-001 ist das Revision 7.3 mit Link auf das Testprotokoll."}
{"ts": "146:54", "speaker": "I", "text": "Abschließend: Sehen Sie noch weitere Risiken, die sich aus den letzten Beobachtungen ergeben, und wie priorisieren Sie diese?"}
{"ts": "147:00", "speaker": "E", "text": "Neben den Netzwerkengpässen haben wir noch ein offenes Thema bei der asynchronen Datenreplikation, speziell zwischen den Storage-Clustern West und Süd. Das ist in Ticket STOR-DR-309 erfasst. Wir priorisieren nach Auswirkung auf RPO/RTO und Eintrittswahrscheinlichkeit, was in unserem Risk-Matrix-Dokument RM-DR-2024-05 abgebildet ist."}
{"ts": "147:00", "speaker": "I", "text": "Lassen Sie uns an der Stelle noch einmal konkret auf die Umsetzung im letzten Drill eingehen. Wie sind Sie mit den identifizierten Netzwerkengpässen umgegangen?"}
{"ts": "147:05", "speaker": "E", "text": "Wir haben temporär ein Throttling auf den weniger kritischen Replikationsjobs aktiviert, um die Bandbreite für die vorrangigen DR-Streams frei zu machen. Das ist in Runbook RB-NET-014 dokumentiert, allerdings haben wir es im Drill leicht angepasst, indem wir zusätzlich im Poseidon Networking Layer eine QoS-Policy gesetzt haben."}
{"ts": "147:14", "speaker": "I", "text": "Gab es dafür eine formale Freigabe oder lief das ad hoc über das Incident Command System?"}
{"ts": "147:18", "speaker": "E", "text": "Das lief ad hoc, aber mit sofortiger Dokumentation im ICS-Channel #dr-drill. Wir haben um 10:42 einen ICS-Form-B3 ausgefüllt und die Änderung in Ticket DR-OPS-2211 referenziert."}
{"ts": "147:27", "speaker": "I", "text": "Wie hat Nimbus Observability hierbei geholfen, die Wirkung der QoS-Policy zu messen?"}
{"ts": "147:31", "speaker": "E", "text": "Wir haben die Bandbreitenmetriken aus dem Modul net.bandwidth.regionA und regionB in Echtzeit betrachtet. Über ein speziell für den Drill eingerichtetes Dashboard, das auf RB-OBS-009 basiert, konnten wir sehen, dass die kritischen Streams ihre SLA von <200ms Latenz wieder erreicht haben."}
{"ts": "147:42", "speaker": "I", "text": "Gab es dabei False Positives im Alerting, oder lief es relativ sauber?"}
{"ts": "147:46", "speaker": "E", "text": "Ein paar False Positives gab es, vor allem aus älteren Alert-Regeln, die noch nicht an den DR-Kontext angepasst waren. Wir haben diese in DR-ALRT-315 markiert und im Nachgang zur Optimierung in den Backlog aufgenommen."}
{"ts": "147:55", "speaker": "I", "text": "Wie fließen solche Anpassungen dann ins Runbook zurück? Wird RB-DR-001 direkt geändert?"}
{"ts": "148:00", "speaker": "E", "text": "Zunächst geht es ins Draft-Runbook RB-DR-001-Draft, das im internen Git-Repo liegt. Dort werden Änderungen aus Lessons Learned Sessions eingepflegt, bevor sie nach einem RFC-Prozess – siehe RFC-DR-77 – in die produktive Version übernommen werden."}
{"ts": "148:10", "speaker": "I", "text": "Und wie lange dauert dieser RFC-Prozess in der Regel?"}
{"ts": "148:14", "speaker": "E", "text": "Normalerweise zwei Wochen, es sei denn, es handelt sich um eine kritische Lücke. Beim Thema QoS-Policy haben wir den Fast-Track genutzt, weil das direkt Auswirkungen auf unser RTO hatte."}
{"ts": "148:23", "speaker": "I", "text": "Wenn Sie auf den Drill zurückblicken: War die Entscheidung, Bandbreite zugunsten kritischer Streams zu priorisieren, auch im Sinne unserer Kosten-Resilienz-Balance?"}
{"ts": "148:28", "speaker": "E", "text": "Ja, weil wir dadurch teure Ausfallzeiten vermieden haben. Die temporäre Drosselung hat keine nachhaltigen Kosten verursacht, aber die Resilienz deutlich erhöht. Das steht auch so in der Kosten-Nutzen-Analyse im Post-Mortem-Dokument DR-PM-2023-04."}
{"ts": "148:38", "speaker": "I", "text": "Sehen Sie hier noch Risiken für den nächsten Drill?"}
{"ts": "148:42", "speaker": "E", "text": "Ein Restrisiko bleibt: Wenn der Netzwerkengpass gleichzeitig mit einer Storage-Degradation auftritt, könnte unsere Priorisierung nicht ausreichen. Dafür ist ein neues Runbook in Planung, RB-DR-021, das cross-layer Engpässe behandelt."}
{"ts": "148:36", "speaker": "I", "text": "Gut, Sie hatten eben von den offenen Risiken gesprochen, u. a. Netzwerkengpässe. Können Sie mir bitte genauer erklären, wie diese Engpässe sich im letzten Drill bemerkbar gemacht haben?"}
{"ts": "148:41", "speaker": "E", "text": "Ja, im Drill vor drei Wochen haben wir beim Umschalten von Region West nach Region Nord einen unerwarteten Flaschenhals im internen Transitlink festgestellt. Laut Runbook RB-DR-001 hätten wir unter 90 Sekunden auf die neue Route umschalten müssen, tatsächlich waren es 142 Sekunden."}
{"ts": "148:45", "speaker": "E", "text": "Das lag daran, dass das Poseidon Networking Team in RFC-NET-2024-17 eine neue Paketfilterregel eingeführt hatte, die nicht in der Drill-Config whitelisted war."}
{"ts": "148:50", "speaker": "I", "text": "Verstehe, und wurde diese Abweichung dann sofort dokumentiert?"}
{"ts": "148:54", "speaker": "E", "text": "Ja, wir haben sofort ein Incident-Ticket INC-DR-7845 aufgemacht und im Lessons-Learned-Dokument vermerkt. Zusätzlich haben wir RB-DR-001 um einen Check erweitert, der vor dem Drill die aktuellen Firewall-Regeln gegen die DR-Whitelist prüft."}
{"ts": "148:59", "speaker": "I", "text": "Wenn wir nochmal auf das Thema Alerting im DR-Fall schauen: gab es in dieser Situation unnötige Alarme?"}
{"ts": "149:03", "speaker": "E", "text": "Ja, leider. Nimbus Observability hat während des Umschaltens noch Health-Checks gegen die alte Region gesendet, was mehrere kritische Alerts ausgelöst hat. Das hat unsere Oncall-Queue für etwa vier Minuten überflutet."}
{"ts": "149:08", "speaker": "E", "text": "Wir haben daraufhin in Alert-Policy AP-DR-Filter-02 eine temporäre Unterdrückung von Altregion-Checks für 180 Sekunden nach Failover eingebaut."}
{"ts": "149:13", "speaker": "I", "text": "Und diese Policy greift nun automatisch im Drill?"}
{"ts": "149:17", "speaker": "E", "text": "Genau, das war Teil des Mid-Phase-Deployments von Titan DR. Wir mussten allerdings dafür in Nimbus' Config-Service auch das neue Tagging-Schema übernehmen, sonst hätten die Filter nicht gegriffen."}
{"ts": "149:21", "speaker": "I", "text": "Sie erwähnen Tagging – betrifft das auch die Schnittstellen zu anderen Projekten?"}
{"ts": "149:26", "speaker": "E", "text": "Ja, das ist der Multi-Hop-Aspekt: Das Tagging-Update kam ursprünglich aus dem Capella Asset Management Projekt. Wir mussten es in Poseidon Networking übernehmen, damit unsere Alert-Filter in Nimbus Observability korrekt auf die DR-Ressourcen angewandt werden."}
{"ts": "149:31", "speaker": "I", "text": "Das heißt, eine Änderung in Capella hat direkten Einfluss auf Titan DR während eines Drills?"}
{"ts": "149:35", "speaker": "E", "text": "Richtig. Ohne die Anpassungen hätten wir im Failover mehr als doppelt so viele False Positives gehabt. Deswegen haben wir jetzt in der Pre-Drill-Checklist den Punkt 'Cross-Project Tagging Alignment' aufgenommen."}
{"ts": "149:40", "speaker": "I", "text": "Wenn wir zum Abschluss auf die Trade-offs schauen: Gibt es durch diese zusätzlichen Checks und Policies auch Performance-Nachteile?"}
{"ts": "149:44", "speaker": "E", "text": "Ein wenig, ja. Die Pre-Check-Phase verlängert sich um ca. 25 Sekunden, was im Drill verschmerzbar ist, aber im echten Incident könnte das knapp werden. Wir erwägen daher laut RFC-DR-OPT-09, die Checks parallel statt sequentiell laufen zu lassen, was aber höhere CPU-Last in den DR-Coordinator-Knoten verursachen würde."}
{"ts": "150:06", "speaker": "I", "text": "Sie hatten eben die Netzwerkengpässe erwähnt – können Sie da noch ein bisschen tiefer reingehen, wie die im Drill sichtbar wurden?"}
{"ts": "150:13", "speaker": "E", "text": "Ja, klar. Bei der simulierten Umschaltung auf Region West kam es zu einem saturierten Link zwischen Core-Switch R6 und dem DR-Storage-Cluster. Das stand so nicht explizit im RB-DR-001, wurde aber in Ticket DRNET-482 dokumentiert."}
{"ts": "150:25", "speaker": "I", "text": "Und wie haben Sie das ad hoc gelöst?"}
{"ts": "150:29", "speaker": "E", "text": "Wir haben kurzfristig einen Traffic-Shaping-Policy aus dem Poseidon-Netzwerk-Repo angewendet, um die Replikations-Queues zu drosseln. Das ist natürlich ein Tradeoff – RPO ging kurzzeitig auf 17 Minuten hoch, SLA erlaubt 15."}
{"ts": "150:43", "speaker": "I", "text": "Gab es dafür ein vorbereitetes Runbook oder war das eher improvisiert?"}
{"ts": "150:48", "speaker": "E", "text": "Teilweise vorbereitet. Wir haben ein generisches Netzwerk-Throttling-Runbook RB-NET-014, aber die Parametrisierung für den DR-Fall mussten wir on the fly anpassen."}
{"ts": "150:59", "speaker": "I", "text": "Sie hatten im Vorfeld auch Nimbus Observability erwähnt – konnten die Dashboards die Situation gut widerspiegeln?"}
{"ts": "151:05", "speaker": "E", "text": "Teils, teils. Die Latenz-Metriken waren sofort rot, aber das Kapazitäts-Widget für die Interconnect-Links wird nur alle 60 Sekunden aktualisiert. Wir haben in DRMON-212 als Verbesserungsvorschlag aufgenommen, das Intervall auf 15 Sekunden zu senken."}
{"ts": "151:19", "speaker": "I", "text": "Wie haben Sie dabei mit den anderen Teams kommuniziert?"}
{"ts": "151:23", "speaker": "E", "text": "Über den Incident Command Channel in Matterwave. Platform-Team bekam Level-2-Alert, Security nur FYI, weil keine Anzeichen für Angriffe vorlagen."}
{"ts": "151:33", "speaker": "I", "text": "Gab es in dieser Phase auch externe Stakeholder, die informiert wurden?"}
{"ts": "151:38", "speaker": "E", "text": "Nein, da wir im Drill-Modus waren. Aber das Communications-Runbook CR-DR-003 wurde im Paralleltest durch das Comms-Team geübt."}
{"ts": "151:47", "speaker": "I", "text": "Welche Lessons Learned ziehen Sie persönlich aus diesem Engpass?"}
{"ts": "151:52", "speaker": "E", "text": "Dass wir die Netzwerkkapazitäten proaktiv in der DR-Planung berücksichtigen müssen. Vor allem, wenn gleichzeitig Backlog-Replikation und User-Traffic auf denselben Pfaden laufen."}
{"ts": "152:04", "speaker": "I", "text": "Und wie fließt das jetzt in die Roadmap ein?"}
{"ts": "152:09", "speaker": "E", "text": "Wir haben für Q3 ein RFC-Update geplant, RFC-DR-202, das eine getrennte Trassenführung für Replikations- und Live-Traffic vorsieht. Dazu müssen natürlich auch Budgetfreigaben erfolgen – das ist der nächste Kostendiskussionspunkt."}
{"ts": "152:06", "speaker": "I", "text": "Sie hatten vorhin die Netzwerkengpässe als offenes Risiko angesprochen – können Sie ausführen, wie sich das konkret auf das aktuelle Drill-Szenario auswirkte?"}
{"ts": "152:10", "speaker": "E", "text": "Ja, im Drill vom letzten Freitag hat sich gezeigt, dass die Transitrouten zwischen Region West und Region Nord nur 60 % der erwarteten Bandbreite lieferten. Das führte dazu, dass der Sync-Prozess aus Runbook RB-DR-001, Schritt 5, langsamer war und wir unser RPO nur knapp einhalten konnten."}
{"ts": "152:16", "speaker": "I", "text": "Gab es dafür eine unmittelbare Eskalation im Incident Command System?"}
{"ts": "152:20", "speaker": "E", "text": "Ja, ICS-Stufe 2 wurde ausgelöst, und wir haben Ticket NET-3421 im internen Tracker erstellt. Parallel haben wir im War Room den Poseidon Networking Lead eingebunden, um alternative Routen via Region Süd zu prüfen."}
{"ts": "152:28", "speaker": "I", "text": "Wie hat Nimbus Observability in dieser Situation geholfen, die Engpässe sichtbar zu machen?"}
{"ts": "152:32", "speaker": "E", "text": "Nimbus hat uns mit dem Modul 'FlowLens' in Echtzeit die Latenz und Paketverluste angezeigt. Wir haben dann einen Snapshot exportiert und an das Networking-Team geschickt. Ohne diese Visualisierung wäre die Ursachenanalyse deutlich länger gewesen."}
{"ts": "152:40", "speaker": "I", "text": "Gab es Abweichungen von RB-DR-001, die Sie im Nachgang dokumentiert haben?"}
{"ts": "152:44", "speaker": "E", "text": "Ja, wir mussten Schritt 7 vor Schritt 6 ausführen, um den Traffic-Shift schneller zu initiieren. Das haben wir in der Drill-Dokumentation unter Ref. LL-DR-2024-05 vermerkt."}
{"ts": "152:50", "speaker": "I", "text": "Lessons Learned – haben Sie diese schon in ein Update des Runbooks übertragen?"}
{"ts": "152:54", "speaker": "E", "text": "Teilweise. Wir haben ein Draft-Update in Confluence erstellt, aber es hängt noch an der Freigabe durch den Compliance Officer, da die Reihenfolgeänderung Auswirkungen auf SLAs haben könnte."}
{"ts": "153:00", "speaker": "I", "text": "Wie balancieren Sie in diesem Fall zwischen schneller Fehlerbehebung und der Einhaltung formaler Freigaben?"}
{"ts": "153:04", "speaker": "E", "text": "Wir setzen auf ein zweistufiges Verfahren: Hotfix-Anpassungen dürfen wir ad hoc während eines Drills anwenden, dokumentieren aber alles sofort im Incident Report. Für permanente Änderungen gilt RFC-Prozess RF-DR-2024-11, der auch Kostenabschätzungen beinhaltet."}
{"ts": "153:12", "speaker": "I", "text": "Und wie fließen die Kostenabschätzungen in die Entscheidung ein?"}
{"ts": "153:16", "speaker": "E", "text": "Wenn eine Anpassung, wie z. B. zusätzliche Transitkapazitäten, mehr als 15 % unseres DR-Budgets beansprucht, muss der CTO entscheiden. In unserem Fall lag die Prognose für NET-3421 bei ca. 12 %, also knapp unter der Schwelle – trotzdem haben wir eine formale Freigabe eingeholt."}
{"ts": "153:24", "speaker": "I", "text": "Sehen Sie hier langfristig einen Tradeoff zwischen Kosteneffizienz und Resilienz?"}
{"ts": "153:28", "speaker": "E", "text": "Ja, absolut. Mehr Bandbreite und redundante Routen erhöhen die Resilienz, aber sie treiben die OPEX hoch. Die Risikoanalyse aus Ticket RISK-558 zeigt, dass wir bei 2 % Wahrscheinlichkeit pro Jahr mit einem Total-Ausfall dieser Route rechnen müssen – das rechtfertigt in meinen Augen die Investition."}
{"ts": "153:36", "speaker": "I", "text": "Wir waren eben bei den Trade-offs zwischen Kosten und Resilienz, könnten Sie jetzt noch einmal ausführen, wie sich diese Diskussion konkret auf die Netzwerk-Architektur im Titan DR Drill auswirkt?"}
{"ts": "153:41", "speaker": "E", "text": "Ja, klar. Wir hatten in den letzten zwei Drills immer wieder das Problem, dass der Cross-Region-Link an seine Kapazitätsgrenzen kam. Das war mit einer der Treiber für Ticket NET-DR-472, in dem wir geprüft haben, ob wir temporär Bandbreitenreserven einkaufen oder alternative Routen via Poseidon Networking aktivieren."}
{"ts": "153:49", "speaker": "I", "text": "Wie haben Sie das dann im Runbook RB-DR-001 abgebildet, oder gab es dafür ein separates Dokument?"}
{"ts": "153:54", "speaker": "E", "text": "Wir haben ein Addendum erstellt, RB-DR-001-A, in dem genau beschrieben wird, wie das Failover auf die sekundäre Leitung auszulösen ist. Das ist Schritt 5b im Abschnitt 'Network Path Switch'. Vorher war das nur implizit unter 'Fallback Procedures' erwähnt."}
{"ts": "154:02", "speaker": "I", "text": "Gab es bei der Umsetzung dieser neuen Prozedur technische Stolpersteine?"}
{"ts": "154:07", "speaker": "E", "text": "Ja, beim ersten Test hat das Routing nicht automatisch umgeschwenkt, weil der BGP-Advertise-Delay in Poseidon Networking noch auf 120 Sekunden stand. Wir haben das in Ticket NET-CONF-195 dokumentiert und den Delay auf 30 Sekunden reduziert."}
{"ts": "154:15", "speaker": "I", "text": "Interessant, und wie wirkt sich diese Verkürzung auf unsere RTO aus?"}
{"ts": "154:20", "speaker": "E", "text": "Das gibt uns im Schnitt 90 Sekunden Vorsprung bei der Wiederherstellung. In unseren Drill-Metriken bedeutet das, dass wir im letzten Szenario 8:45 Minuten RTO hatten statt 10:15, also deutlich unter dem SLA von 12 Minuten."}
{"ts": "154:28", "speaker": "I", "text": "Das klingt nach einem klaren Gewinn. Gab es dazu Rückmeldungen vom Security-Team, gerade im Hinblick auf die temporären Routenänderungen?"}
{"ts": "154:33", "speaker": "E", "text": "Ja, SecOps wollte sicherstellen, dass die ACLs auf der sekundären Route identisch zur Primärverbindung sind. Wir haben dafür einen Pre-Check in unser Drill-Runbook aufgenommen, der mit dem Tool 'acl-verify' aus Nimbus Observability läuft."}
{"ts": "154:41", "speaker": "I", "text": "Wie koordinieren Sie solche Anpassungen über die verschiedenen Teams hinweg, ohne dass im Drill Zeit verloren geht?"}
{"ts": "154:46", "speaker": "E", "text": "Wir nutzen im Incident Command System eine dedizierte Rolle 'Network Liaison'. Die Person übernimmt in der Drill-Phase die Brücke zwischen SRE, Poseidon-Netzwerk-Team und SecOps. So vermeiden wir Mehrfachabfragen und halten die Kommunikationskette schlank."}
{"ts": "154:54", "speaker": "I", "text": "Gab es im letzten Drill dazu ein konkretes Beispiel?"}
{"ts": "154:59", "speaker": "E", "text": "Ja, während Drill T-DR-09 hat der Network Liaison noch während Step 5b einen Konflikt in der Routing-Tabelle erkannt und direkt über unseren ICS-Channel im ChatOps-Tool gelöst, ohne dass der Incident Commander eingreifen musste. Das hat uns bestimmt zwei Minuten gespart."}
{"ts": "155:07", "speaker": "I", "text": "Zum Abschluss: Welche offenen Risiken sehen Sie noch, die wir vor der nächsten Drill-Runde adressieren sollten?"}
{"ts": "155:12", "speaker": "E", "text": "Das größte Risiko bleibt die Abhängigkeit von einem einzigen Cloud-Provider für die sekundäre Region. Wir haben in RFC-DR-220 einen Vorschlag hinterlegt, wie wir eine dritte Region bei einem anderen Provider als 'cold standby' aufbauen könnten. Das erhöht die Kosten, würde aber unsere Resilienz gegen Provider-Ausfälle massiv steigern."}
{"ts": "155:06", "speaker": "I", "text": "Könnten Sie bitte genauer erläutern, wie die Netzwerk-Engpässe im aktuellen Drill sichtbar wurden?"}
{"ts": "155:11", "speaker": "E", "text": "Ja, wir haben im Failover-Szenario bemerkt, dass der Transfer von Replikationsdaten zwischen Region Süd und Region West auf unter 60% der erwarteten Bandbreite fiel. Das war in den Metriken aus Nimbus Observability klar zu sehen, besonders im Panel 'Inter-Region Latency'."}
{"ts": "155:20", "speaker": "I", "text": "Gab es dafür einen spezifischen Auslöser oder war es ein generelles Kapazitätsproblem?"}
{"ts": "155:26", "speaker": "E", "text": "In diesem Fall war es ein Zusammenspiel: Wir hatten parallel eine geplante Poseidon Networking-Konfigurationsänderung (RFC-NET-482), die einige Routen neu berechnet hat, und gleichzeitig erhöhte Last durch den Drill. Dadurch traten Queue-Bildungen auf."}
{"ts": "155:38", "speaker": "I", "text": "Wie haben Sie in der Command-Struktur darauf reagiert?"}
{"ts": "155:43", "speaker": "E", "text": "Der Incident Commander hat schnell einen Netzwerk-SME eingebunden, wir haben gemäß RB-DR-001 Abschnitt 4.3 den 'Network Throughput Degradation'-Pfad aktiviert. Innerhalb von 12 Minuten war ein Workaround implementiert, um Traffic über die Nord-Region zu rerouten."}
{"ts": "155:56", "speaker": "I", "text": "Hat sich dieser Workaround auf das RTO ausgewirkt?"}
{"ts": "156:00", "speaker": "E", "text": "Minimal, wir lagen am Ende 90 Sekunden über dem Zielwert, also 16:30 statt 15:00. In den Lessons Learned haben wir das als akzeptabel eingestuft, aber mit Vermerk in Ticket DR-OPT-312 zur Optimierung der Routing-Pfade."}
{"ts": "156:12", "speaker": "I", "text": "Welche Monitoring-Anpassungen leiten Sie daraus ab?"}
{"ts": "156:17", "speaker": "E", "text": "Wir planen, im Nimbus Observability zwei neue Alert-Regeln einzubauen: 'Inter-Region Bandwidth Drop >20% for 3min' und 'Route Recalc During DR'. Das soll uns früher warnen, bevor kritische Pfade beeinträchtigt werden."}
{"ts": "156:29", "speaker": "I", "text": "Und wie sieht es mit der Dokumentation dieser Änderungen aus?"}
{"ts": "156:33", "speaker": "E", "text": "Alle Anpassungen werden in Confluence unter 'Titan DR > Runbooks > RB-DR-001 Changelog' dokumentiert, und wir verlinken direkt auf die Jira-Tickets. Zusätzlich führen wir einen internen Review-Call mit dem Platform- und Security-Team durch."}
{"ts": "156:45", "speaker": "I", "text": "Gibt es noch offene Risiken, die Sie für den nächsten Drill adressieren möchten?"}
{"ts": "156:49", "speaker": "E", "text": "Ja, neben den Netzwerk-Engpässen haben wir gesehen, dass die automatisierte Datenbank-Failover-Latenz bei hohen Transaktionslasten steigt. Das ist in DB-ISSUE-77 erfasst, wir diskutieren gerade, ob wir dafür Read-Replica-Promotions vorhalten."}
{"ts": "157:02", "speaker": "I", "text": "Das klingt nach einem Tradeoff zwischen Kosten und Resilienz, oder?"}
{"ts": "157:06", "speaker": "E", "text": "Genau, Read-Replicas in drei Regionen bedeuten laufende Mehrkosten von ca. 15%, aber im Ernstfall könnten wir das RPO halbieren. Die Entscheidung liegt aktuell beim Steering Committee, basierend auf den Daten aus diesem Drill und den verknüpften Tickets."}
{"ts": "156:30", "speaker": "I", "text": "Könnten Sie noch einmal genauer auf die in der letzten Drill-Iteration identifizierten Netzwerk-Engpässe eingehen?"}
{"ts": "156:35", "speaker": "E", "text": "Ja, gern. Wir hatten beim Failover von Region West auf Region Ost in zwei Subnetzen eine Latenzspitze von bis zu 650 ms. Laut RB-DR-001 soll der Traffic innerhalb von 300 ms umgeleitet werden. Das Problem trat bei den internen Service-Mesh-Gateways auf, wie in Ticket #DR-482 dokumentiert."}
{"ts": "156:44", "speaker": "I", "text": "Und wie haben Sie diese Diskrepanz im Runbook adressiert?"}
{"ts": "156:49", "speaker": "E", "text": "Wir haben eine Ergänzung im Abschnitt 4.3 von RB-DR-001 vorgenommen: ein zusätzlicher Health-Check-Endpoint pro Gateway, um frühzeitige Warnungen zu generieren. Der Change ist in RFC-DR-77 vermerkt und wird im nächsten Drill getestet."}
{"ts": "156:58", "speaker": "I", "text": "Gab es währenddessen Auswirkungen auf die SLOs, besonders im Bereich Availability?"}
{"ts": "157:03", "speaker": "E", "text": "Kurzzeitig ja, wir sind für den Payment-Service auf 99,1 % Availability gefallen, unser Ziel-SLO ist 99,5 %. Das war innerhalb des Drill akzeptabel, aber hat gezeigt, dass der Engpass kritisch ist."}
{"ts": "157:12", "speaker": "I", "text": "Welche Rolle spielte dabei Nimbus Observability?"}
{"ts": "157:17", "speaker": "E", "text": "Nimbus hat durch den Multi-Region-Dashboard-View sehr schnell Alarme ausgelöst. Die Metrik 'Gateway RTT' hatte einen roten Status, und das Alert-Playbook AP-DR-GW-02 wurde automatisch in unserem Incident Tool verlinkt."}
{"ts": "157:26", "speaker": "I", "text": "Wie wurde diese Information an das Incident Command weitergegeben?"}
{"ts": "157:31", "speaker": "E", "text": "Über den internen Slack-Bridge-Channel #ic-drill, der bei jedem Drill automatisch erstellt wird. Der IC konnte so alle relevanten Stakeholder, inklusive Networking-Team, sofort ins Boot holen."}
{"ts": "157:39", "speaker": "I", "text": "Gab es Verzögerungen bei der Eskalation?"}
{"ts": "157:43", "speaker": "E", "text": "Minimal, etwa zwei Minuten. Laut unserem Eskalationspfad wären maximal fünf Minuten erlaubt, also waren wir noch im Rahmen. Ticket #DR-490 enthält das komplette Timeline-Log."}
{"ts": "157:51", "speaker": "I", "text": "Welche Trade-offs zwischen Kosten und Resilienz wurden in diesem Kontext diskutiert?"}
{"ts": "157:56", "speaker": "E", "text": "Wir haben diskutiert, ob wir die Gateway-Kapazität um 30 % überprovisionieren. Das würde laut Kostenschätzung in DOC-DR-COST-12 etwa 120 000 € jährlich mehr kosten, könnte aber die Latenzspitzen verhindern. Entscheidung steht noch aus."}
{"ts": "158:05", "speaker": "I", "text": "Und wie sehen Sie das persönliche Risiko, wenn diese Entscheidung verzögert wird?"}
{"ts": "158:10", "speaker": "E", "text": "Das Risiko ist, dass im Ernstfall unser RTO von 15 Minuten nicht gehalten werden kann, wenn die Gateways saturieren. Das ist in der Risk-Matrix RM-DR-04 als 'hoch' eingestuft. Daher dränge ich auf eine schnelle Entscheidung."}
{"ts": "158:06", "speaker": "I", "text": "Könnten Sie bitte noch mal konkret beschreiben, wie Sie die Netzwerk-Engpässe im Rahmen des Titan DR Drills erkannt haben?"}
{"ts": "158:12", "speaker": "E", "text": "Ja, klar. Während der simulierten Umschaltung auf die West-Europa-Region haben wir im Nimbus Observability Dashboard einen Anstieg der Latenz auf der internen Poseidon-Linkstrecke gesehen – das war knapp 35 % über unserem internen SLA von 250 ms."}
{"ts": "158:25", "speaker": "I", "text": "War das ein einmaliger Peak oder hat sich das über die gesamte Failover-Phase gezogen?"}
{"ts": "158:31", "speaker": "E", "text": "Es war leider persistent für etwa 17 Minuten, was laut RB-DR-001 eigentlich schon die Eskalationsstufe 2 triggern müsste. Wir haben das in Ticket NET-842 dokumentiert."}
{"ts": "158:46", "speaker": "I", "text": "Wie haben Sie in dieser Situation die Kommunikation organisiert, gerade zwischen SRE und Networking-Team?"}
{"ts": "158:52", "speaker": "E", "text": "Wir sind direkt über den Incident Command Channel gegangen, der im Confluence-Runbook RB-IC-005 definiert ist. Dort gibt es einen festen Slot für Network SMEs, die dann parallel zu den Failover-Tasks die Ursachenanalyse starten."}
{"ts": "159:06", "speaker": "I", "text": "Gab es Abweichungen vom Runbook oder war alles nach Plan?"}
{"ts": "159:12", "speaker": "E", "text": "Kleine Abweichung: laut Runbook sollten wir nach 10 Minuten Latency-Issue schon auf die Reserve-Leitung umschalten, aber wir haben bewusst noch weitere 5 Minuten gewartet, um zu sehen, ob es sich stabilisiert – das war eine spontane Entscheidung des Incident Commanders."}
{"ts": "159:28", "speaker": "I", "text": "Haben Sie diese Entscheidung im Lessons-Learned-Prozess hinterlegt?"}
{"ts": "159:33", "speaker": "E", "text": "Ja, in der Drill-Retrospektive als LL-DR-2024-04-17 festgehalten, mit der Empfehlung, im Runbook eine Flexibilitätsklausel einzufügen, damit der Commander situativ agieren darf."}
{"ts": "159:48", "speaker": "I", "text": "Können Sie den Zusammenhang zwischen diesem Netzwerkproblem und der Kosten-Resilienz-Balance, die Sie vorhin erwähnt haben, noch mal darstellen?"}
{"ts": "159:54", "speaker": "E", "text": "Natürlich: Die Reserve-Leitung ist ein Premium-Link mit hohen Betriebskosten. Jede Umschaltung bedeutet zusätzliche Gebühren. Deshalb steht im Raum, ob wir die sofortige Umschaltung wirklich als Default behalten oder einen Threshold-basierten Ansatz fahren."}
{"ts": "160:10", "speaker": "I", "text": "Das ist ja ein klarer Trade-off zwischen Resilienz und Kosten. Welche Entscheidungstendenz gibt es aktuell?"}
{"ts": "160:15", "speaker": "E", "text": "Tendenz geht dahin, dass wir für kritische Services mit RTO < 15 Minuten beim sofortigen Switch bleiben, für weniger kritische Lasten aber den Threshold nutzen. Das ist gerade in RFC DR-OPT-019 in Review."}
{"ts": "160:31", "speaker": "I", "text": "Gibt es Risiken, wenn man diesen Threshold-Ansatz implementiert?"}
{"ts": "160:36", "speaker": "E", "text": "Ja, klar. Das Hauptrisiko ist, dass eine Latenzdegradation schneller eskaliert als erwartet und dann RPO-Verluste drohen. Deshalb wollen wir parallel ein predictive Alerting-Modul aus Nimbus einsetzen, um solche Trends frühzeitig zu erkennen."}
{"ts": "160:06", "speaker": "I", "text": "Sie hatten vorhin Ticket-IDs erwähnt, um die Runbook-Updates zu verfolgen. Können Sie konkret sagen, wie diese Nachverfolgung bei Novereon organisiert ist?"}
{"ts": "160:12", "speaker": "E", "text": "Ja, wir nutzen intern das Change Tracking Tool CTR-Board. Für das Update von RB-DR-001 ist z. B. Ticket DR-347 offen. Das ist mit Subtasks für Netzwerk- und Storage-Anpassungen verknüpft, jeweils eigene IDs wie NET-882 und STO-914."}
{"ts": "160:21", "speaker": "I", "text": "Und wie stellen Sie sicher, dass diese Subtasks im Drill-Umfeld rechtzeitig umgesetzt werden?"}
{"ts": "160:28", "speaker": "E", "text": "Wir koppeln die CTR-Board-Tasks an unseren Drill-Kalender. Bei jedem Dry Run wird automatisch ein Status-Pull gemacht. Wenn z. B. NET-882 noch 'In Progress' ist, blockiert das den Go/No-Go-Checkpoint."}
{"ts": "160:36", "speaker": "I", "text": "Gab es zuletzt Fälle, wo so ein Blocker wirklich den Drill gestoppt hat?"}
{"ts": "160:43", "speaker": "E", "text": "Ja, im März-Drill. Da war die L2-L3-Übergabe in Region West noch nicht mit den neuen BGP-Peers getestet. NET-882 war noch offen, und der Incident Commander hat laut Protokoll IC-DR-2023-03-14 entschieden, den Failover auf Stufe 2 zu beschränken."}
{"ts": "160:55", "speaker": "I", "text": "Das klingt nach einer bewussten Risikominimierung. Wie wurde das dokumentiert?"}
{"ts": "161:01", "speaker": "E", "text": "Im Drill-Report DRR-2023-03 haben wir das festgehalten, inklusive Lessons Learned: 'No-Go bei offenen Netzknotenänderungen'. Das ist jetzt eine Art implizite Policy geworden, auch wenn sie noch nicht im offiziellen Runbook steht."}
{"ts": "161:10", "speaker": "I", "text": "Das bringt mich zu einer anderen Frage: Wie fließen solche impliziten Policies in die formale Dokumentation ein?"}
{"ts": "161:16", "speaker": "E", "text": "Meistens über das Quarterly Runbook Review. Da gehen wir alle offenen Lessons Learned durch und entscheiden, ob sie in RB-DR-001 oder ergänzende RB-DR-00x aufgenommen werden. Das passiert in Abstimmung mit Platform und Security."}
{"ts": "161:27", "speaker": "I", "text": "Sie hatten vorhin Netzwerk-Engpässe angesprochen. Gibt es technische Pläne, diese vor dem nächsten Drill zu adressieren?"}
{"ts": "161:34", "speaker": "E", "text": "Ja, wir haben in STO-914 eine Storage-Replication-Optimierung und in NET-901 einen zusätzlichen 100 Gbps Link zwischen Region Ost und Süd geplant. Beides soll die Failover-Zeit unter unser RTO von 45 Minuten bringen."}
{"ts": "161:45", "speaker": "I", "text": "Wie sieht die Kosten-Nutzen-Rechnung für so einen Link aus?"}
{"ts": "161:50", "speaker": "E", "text": "Wir kalkulieren ca. 18 % höhere Betriebskosten für die zusätzliche Bandbreite, aber eine erwartete Reduktion der Recovery Time um 12 Minuten. Das bringt uns näher an die SLA-Garantie und reduziert potenziell Pönalen bei SLA-Verletzungen."}
{"ts": "161:59", "speaker": "I", "text": "Wird diese Entscheidung noch intern diskutiert oder ist sie fix?"}
{"ts": "162:05", "speaker": "E", "text": "Sie ist im Steering Committee noch auf der Agenda. Wir prüfen parallel, ob wir durch Optimierung der Replikations-Queues in der Storage-Schicht ähnlichen Gewinn erzielen können – das wäre günstiger, birgt aber das Risiko, die Replikationskonsistenz zu verschlechtern."}
{"ts": "161:42", "speaker": "I", "text": "Lassen Sie uns noch mal genauer auf die Netzwerk-Engpässe eingehen, die Sie eben angesprochen haben – wie konkret wirken die sich im Titan DR Drill aus?"}
{"ts": "161:47", "speaker": "E", "text": "Ja, also in der Drill-Phase haben wir festgestellt, dass der Cross-Region-Throughput zwischen FRA-Cluster und DUB-Cluster bei Lastspitzen unter 8 Gbit/s fällt, was laut SLA-DR-2023 eigentlich nicht passieren dürfte. Das beeinflusst direkt die Replikationslatenz und damit unser RPO."}
{"ts": "161:55", "speaker": "I", "text": "Gab es dazu schon Maßnahmen oder Workarounds während des Drills?"}
{"ts": "162:00", "speaker": "E", "text": "Wir haben kurzfristig gemäß RB-NET-014 den Traffic auf dedizierte VLANs umgeleitet, um Noise von Batch-Jobs zu reduzieren. Außerdem wurde ein temporäres QoS-Profil aktiviert, was wir im Ticket NET-3461 dokumentiert haben."}
{"ts": "162:08", "speaker": "I", "text": "Wie lief die Abstimmung dazu mit dem Poseidon Networking Team?"}
{"ts": "162:12", "speaker": "E", "text": "Die lief erstaunlich schnell – wir nutzen im Drill-Fall den ICS-Kanal #dr-net-bridge, und der Incident Commander hat direkt um 09:14 UTC den Eskalationslevel 2 ausgelöst. Poseidon hat dann innerhalb von 12 Minuten geantwortet."}
{"ts": "162:21", "speaker": "I", "text": "Wurde das auch im Laufzeit-Monitoring sichtbar?"}
{"ts": "162:25", "speaker": "E", "text": "Ja, in Nimbus Observability haben wir im Dashboard 'DR Cross-Region' den Throughput-Drop als roten Spike gesehen. Der Alert-Name war CR_LINK_DEGRADE, Severity 'high', ausgelöst um 09:13 UTC."}
{"ts": "162:32", "speaker": "I", "text": "Sie hatten vorhin Runbook-Updates erwähnt – betrifft das auch RB-DR-001?"}
{"ts": "162:37", "speaker": "E", "text": "Genau, wir haben festgestellt, dass RB-DR-001 Schritt 4.3 zur Netzwerkvalidierung nicht genug Detail hat. Wir wollen dort künftig einen Pre-Failover-Linktest einbauen, wie wir ihn jetzt ad hoc gefahren haben."}
{"ts": "162:45", "speaker": "I", "text": "Wird das als Change Request erfasst?"}
{"ts": "162:48", "speaker": "E", "text": "Ja, ich habe bereits RFC-DR-77 angelegt, Change Window geplant für Ende des Quartals. Review durch SRE-Lead und Poseidon steht noch aus."}
{"ts": "162:54", "speaker": "I", "text": "Wenn wir jetzt auf die Kosten-Resilienz-Balance zurückkommen – wie wiegt man den zusätzlichen Testaufwand gegen die Kosten ab?"}
{"ts": "162:59", "speaker": "E", "text": "Wir rechnen intern mit KPI-DR-Cost, der pro Drill die Personentage und den Ressourcenverbrauch monetär bewertet. Der Pre-Failover-Test würde ca. +3% Drillkosten verursachen, aber könnte im Ernstfall RPO von 15 auf 5 Minuten verbessern."}
{"ts": "163:07", "speaker": "I", "text": "Gibt es Risiken, dass dieser Test die Failover-Zeit verlängert?"}
{"ts": "163:11", "speaker": "E", "text": "Ja, minimal – der Linktest dauert ca. 45 Sekunden. Aber gemäß Lessons Learned aus Drill Q2/2023 (siehe LL-Doc-DR-12) ist dieser Overhead akzeptabel im Vergleich zum potenziellen Datenverlust."}
{"ts": "162:18", "speaker": "I", "text": "Sie hatten eben die Netzwerk-Engpässe angesprochen. Können Sie noch einmal genauer beschreiben, wie sich diese im letzten Drill auf den Failover-Prozess ausgewirkt haben?"}
{"ts": "162:23", "speaker": "E", "text": "Ja, klar. Wir haben im Drill am 14.03. festgestellt, dass zwischen Region Nord und Ost während der Umschaltung ein temporärer Paketverlust von bis zu 8 % auftrat. Das hat laut Log in Nimbus Observability den Sync-Job von RB-DR-001 Schritt 6 verzögert, wodurch unser RTO fast um 4 Minuten überschritten worden wäre."}
{"ts": "162:34", "speaker": "I", "text": "Gab es dazu schon eine Analyse, ob das Problem in der Poseidon Networking Schicht oder eher im Applikations-Layer lag?"}
{"ts": "162:39", "speaker": "E", "text": "Wir haben laut Ticket NET-482 die Route-Table-Updates in Poseidon geprüft. Die waren korrekt. Das Problem trat eher bei der TCP-Session-Reassembly im App-Layer auf, weil ein Fallback-Mechanismus nicht sauber griff. Deshalb gibt es jetzt ein RFC zur Anpassung der Keepalive-Parameter."}
{"ts": "162:50", "speaker": "I", "text": "Verstehe. Welche Maßnahmen planen Sie kurzfristig, um das Risiko zu minimieren, falls es im Ernstfall wieder auftritt?"}
{"ts": "162:55", "speaker": "E", "text": "Kurzfristig setzen wir auf zusätzliche Monitoring-Checks in Nimbus mit einem aggressiveren Sampling-Intervall von 5 Sekunden während des Failovers. Außerdem wollen wir, wie in DR-Improvement-Plan 23/07 dokumentiert, das Runbook RB-DR-001 um einen manuellen Retry-Pfad ergänzen."}
{"ts": "163:06", "speaker": "I", "text": "Okay, und wie fließen diese Änderungen in die nächste Drill-Iteration ein?"}
{"ts": "163:10", "speaker": "E", "text": "Wir haben im Sprint-Backlog für KW15 die Stories DR-217 und DR-218 aufgenommen. Die enthalten sowohl die Runbook-Änderung als auch die Konfigurationsanpassung für Nimbus. Diese werden wir im April-Drill validieren."}
{"ts": "163:20", "speaker": "I", "text": "Sie hatten vorhin auch die Kosten-Resilienz-Balance erwähnt. Wie genau beeinflussen diese geplanten Maßnahmen das Budget?"}
{"ts": "163:25", "speaker": "E", "text": "Es sind moderate Mehrkosten zu erwarten, etwa 500 € monatlich für die zusätzlichen Monitoring-Ressourcen. Allerdings ist der potenzielle Schaden bei einem RTO-Verlust um ein Vielfaches höher, daher wurde im Risk Committee laut Protokoll RC-58 die Freigabe erteilt."}
{"ts": "163:36", "speaker": "I", "text": "Gibt es von Security-Seite Einwände gegen die engeren Sampling-Intervalle, z. B. wegen zusätzlicher Datenlast?"}
{"ts": "163:41", "speaker": "E", "text": "Security hat in SEC-Review-Note 09 bestätigt, dass die erhöhte Sampling-Frequenz keine Compliance-Risiken bringt. Wir mussten lediglich die Data-Retention-Parameter von 14 auf 7 Tage reduzieren, um Speicher zu sparen."}
{"ts": "163:52", "speaker": "I", "text": "Das heißt, Lessons Learned aus dem Drill finden jetzt unmittelbaren Eingang in die operative Praxis?"}
{"ts": "163:56", "speaker": "E", "text": "Genau. Wir haben im Confluence-Space 'Titan DR' die Drill-Review-Matrix, die jede Abweichung aus RB-DR-001 gegen Maßnahmen mappt. Diese Matrix wird vom Incident Commander freigegeben, bevor Änderungen live gehen."}
{"ts": "164:07", "speaker": "I", "text": "Und wie behalten Sie die langfristigen Optimierungen im Blick, gerade wenn mehrere Bereiche wie Networking und Application betroffen sind?"}
{"ts": "164:12", "speaker": "E", "text": "Dafür nutzen wir den Quarterly Resilience Report. Dort verknüpfen wir Metriken aus Nimbus, Change-Logs aus Poseidon und Release-Notes der Applikation. So sehen wir im Trend, ob die Maßnahmen nachhaltig wirken oder ob neue Bottlenecks entstehen."}
{"ts": "165:18", "speaker": "I", "text": "Sie hatten vorhin erwähnt, dass die Balance zwischen Kosten und Resilienz ein ständiges Thema ist. Können Sie das bitte noch einmal im Kontext des Titan DR Drills ausführen?"}
{"ts": "165:23", "speaker": "E", "text": "Ja, im Drill sehen wir deutlich, dass mehr Redundanz in den Netzwerkpfaden die RTO verkürzt – aber das treibt die OPEX hoch. Wir haben intern in RFC-221-P-TIT diskutiert, ob wir zusätzliche MPLS-Links aktivieren oder rein auf SD-WAN-Fallback setzen."}
{"ts": "165:32", "speaker": "I", "text": "Und wie binden Sie die Netzwerk-Teams in solche Entscheidungen ein?"}
{"ts": "165:36", "speaker": "E", "text": "Über das Incident Command Protokoll ICS-DR-03. Während Drills gibt es einen dedizierten Liaison-Officer zwischen SRE und Poseidon Networking. Der meldet Engpässe direkt per ChatOps-Kanal #dr-net-escalation."}
{"ts": "165:44", "speaker": "I", "text": "Gab es in diesem Drill einen konkreten Engpassfall?"}
{"ts": "165:48", "speaker": "E", "text": "Ja, im Segment EU-West wurde der Failover-Traffic zunächst über einen suboptimalen Pfad geroutet. Wir haben das innerhalb von 3 Minuten identifiziert und per Runbook RB-NW-017 korrigiert."}
{"ts": "165:55", "speaker": "I", "text": "Wie haben Sie das so schnell erkannt?"}
{"ts": "165:59", "speaker": "E", "text": "Nimbus Observability hat ein Custom-Dashboard für DR-Latenz. Wir haben Alert-Thresholds bei 120 ms gesetzt; als wir plötzlich 210 ms sahen, war klar, dass der Traffic nicht optimal läuft."}
{"ts": "166:07", "speaker": "I", "text": "Und das wird dann auch dokumentiert?"}
{"ts": "166:10", "speaker": "E", "text": "Ja, jeder Drill-Vorfall bekommt ein Ticket im DR-Board. Hier war es DR-TKT-5842. Dort verlinken wir Logs, MTR-Traces und die genutzten Runbook-Schritte."}
{"ts": "166:17", "speaker": "I", "text": "Sie sagten vorhin, es gäbe noch ausstehende Runbook-Updates. Betrifft das auch RB-DR-001?"}
{"ts": "166:21", "speaker": "E", "text": "Teilweise. RB-DR-001 beschreibt den Regional Failover noch ohne die neuen SD-WAN-Policies. Das Update ist in Arbeit unter Change-Request CR-DR-77."}
{"ts": "166:28", "speaker": "I", "text": "Wie priorisieren Sie solche Updates gegenüber anderen Aufgaben?"}
{"ts": "166:32", "speaker": "E", "text": "Wir bewerten Impact und Wahrscheinlichkeit. Ein Outdated-Runbook in einem kritischen Bereich wie DR hat Priorität 1. Daher verschieben wir weniger kritische CI/CD-Pipeline-Optimierungen, um die DR-Dokumentation aktuell zu halten."}
{"ts": "166:40", "speaker": "I", "text": "Also gehen Sie bewusst gewisse Tradeoffs ein, um Resilienz zu sichern."}
{"ts": "166:43", "speaker": "E", "text": "Genau. Wir akzeptieren kurzfristig höhere Wartungskosten oder Verzögerungen in Feature-Releases, wenn das die Wiederanlaufzeit im Ernstfall nachweislich senkt – das ist auch durch die Lessons Learned aus DR-TKT-5730 untermauert."}
{"ts": "167:18", "speaker": "I", "text": "Wenn wir auf die Lessons Learned vom letzten Drill schauen, gab es ja insbesondere dieses Thema mit den Netzwerkengpässen. Können Sie mir den aktuellen Stand dazu geben?"}
{"ts": "167:26", "speaker": "E", "text": "Ja, klar. Wir haben nach dem Drill die Engpässe in der Ost-West-Kommunikation genauer analysiert. Die Ursache lag an einer veralteten BGP-Policy zwischen Region Süd und Nord. Das ist jetzt in RFC-NET-042 dokumentiert und wir haben ein Change-Fenster für die nächste Woche gebucht."}
{"ts": "167:40", "speaker": "I", "text": "Heißt das, die Änderungen werden noch vor dem nächsten geplanten Drill live gehen?"}
{"ts": "167:45", "speaker": "E", "text": "Genau, das Ziel ist, dass wir vor dem Q4-Drill die Policy-Updates im Routing implementiert haben. Das minimiert das Risiko, dass wir beim Failover wieder in den 300ms-Latenzbereich kommen."}
{"ts": "167:56", "speaker": "I", "text": "Gibt es hierzu auch Anpassungen in den Runbooks?"}
{"ts": "168:00", "speaker": "E", "text": "Ja, RB-DR-001 bekommt einen neuen Abschnitt 'Network Health Check pre-Failover'. Das ist in Ticket DR-2154 dokumentiert. Wir fügen dort konkrete BGP-Statusprüfungen hinzu."}
{"ts": "168:11", "speaker": "I", "text": "Sie hatten zuvor die Balance zwischen Kosten und Resilienz angesprochen. Wird diese Netzwerkoptimierung in dieses Spannungsfeld fallen?"}
{"ts": "168:18", "speaker": "E", "text": "Teilweise. Die Policy-Änderung selbst kostet uns nichts zusätzlich, aber wir evaluieren parallel eine zusätzliche redundante MPLS-Strecke. Das wäre ein signifikanter OPEX-Posten und muss gegen den tatsächlichen Nutzen abgewogen werden."}
{"ts": "168:32", "speaker": "I", "text": "Wie fließt da das Feedback aus den Drills in die Entscheidung ein?"}
{"ts": "168:37", "speaker": "E", "text": "Wir haben ja die Metriken aus Nimbus Observability, speziell die Drill-Session vom März. Da sieht man klar die Paketverluste in der Failover-Phase. Solche Daten sind die Basis für unsere Business-Impact-Analyse, die dann in das Entscheidungsboard geht."}
{"ts": "168:51", "speaker": "I", "text": "Gibt es Risiken, dass die Anpassungen an den Policies selbst neue Probleme verursachen?"}
{"ts": "168:56", "speaker": "E", "text": "Natürlich, jede Netzwerkänderung birgt Risiken. Wir mitigieren das, indem wir sie zuerst in der Staging-Region West testen. Da haben wir ein isoliertes BGP-Lab, das die Produktionspfade simuliert. Erst nach bestandenen Tests und Peer-Review im NetOps-Team rollen wir das in Produktion."}
{"ts": "169:12", "speaker": "I", "text": "Und wie kommunizieren Sie diese Änderungen an die anderen Projektteams, z. B. Poseidon Networking?"}
{"ts": "169:18", "speaker": "E", "text": "Wir nutzen das interne ICS-Channel-Setup. Es gibt vor dem Change ein Briefing im 'Platform & Network' War Room, sowie eine Info-Mail mit Change-ID CN-2024-09 an alle Stakeholder. Poseidon Networking ist da als Owner für interregionale Links direkt im Verteiler."}
{"ts": "169:32", "speaker": "I", "text": "Abschließend: Wird die OPEX-Entscheidung für die MPLS-Strecke noch vor Jahresende fallen?"}
{"ts": "169:38", "speaker": "E", "text": "Das ist der Plan. Wir haben das Thema auf der Agenda für das Steering Committee im November. Bis dahin wollen wir alle technischen und finanziellen Argumente, inklusive der Drill-Daten aus Oktober, aufbereitet haben."}
{"ts": "176:38", "speaker": "I", "text": "Sie hatten ja vorhin schon die Kosten-Resilienz-Balance angesprochen. Können Sie noch einmal erläutern, wie diese Überlegung konkret in die aktuelle Drill-Phase einfließt?"}
{"ts": "176:46", "speaker": "E", "text": "Ja, klar. In der Drill-Phase gewichten wir die Resilienz etwas höher als die Kosten, weil wir realistische Ausfallszenarien simulieren. Das heißt, wir akzeptieren z. B. temporär höhere Compute-Kosten in der Zweit-Region, solange das Runbook RB-DR-001 Step 14 erfüllt wird und wir unser RTO von 30 Minuten halten."}
{"ts": "176:58", "speaker": "I", "text": "Und wie dokumentieren Sie diese temporären Kostenanpassungen? Gibt es da ein spezielles Verfahren?"}
{"ts": "177:05", "speaker": "E", "text": "Wir tragen das in unserem DR-Cost-Log innerhalb des Confluence-Bereichs \"Titan DR Ops\" ein, mit Verweis auf das Drill-Datum und die zugehörige Jira-ID, z. B. TIT-DR-4421. So können Finance und SRE später nachvollziehen, warum ein Peak auftrat."}
{"ts": "177:19", "speaker": "I", "text": "Verstehe. Wegen der Netzwerke hatten Sie eben Engpässe als Risiko genannt. Haben Sie inzwischen Messdaten aus diesem Drill?"}
{"ts": "177:27", "speaker": "E", "text": "Ja, wir haben im Drill an zwei Standorten Latenzspitzen über 250 ms gemessen, was über unserem SLA für interregionale Replica-Syncs liegt. Das wurde als Incident INC-DR-339 geloggt und in der Poseidon Networking-Queue priorisiert."}
{"ts": "177:41", "speaker": "I", "text": "Gab es dabei Auswirkungen auf das Einhalten des RPO?"}
{"ts": "177:47", "speaker": "E", "text": "Kurzzeitig ja – der RPO ist von 5 auf 7 Minuten gerutscht. Das fiel in den Metriken von Nimbus Observability auf, genauer im Dashboard 'DR-Replication Lag', und wir mussten RB-DR-001 Step 9 manuell triggern."}
