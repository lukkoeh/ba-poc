{"ts": "00:00", "speaker": "I", "text": "Können Sie kurz Ihre Hauptverantwortlichkeiten im Orion Edge Gateway Build-Phase schildern?"}
{"ts": "00:42", "speaker": "E", "text": "Ja, also… ich bin Lead DevOps Engineer für das Projekt, äh, verantwortlich für die komplette Provisionierung des API Gateways, die Implementierung der Rate-Limiting Policies und die Integration der Auth-Mechanismen aus Aegis IAM. Und, also, wir müssen dabei immer SLA-ORI-02 im Blick behalten – p95 Latenz unter 120 Millisekunden."}
{"ts": "05:30", "speaker": "I", "text": "Wie genau haben Sie diese SLA-Anforderung in Ihre Arbeitspakete integriert?"}
{"ts": "06:05", "speaker": "E", "text": "We divided the backlog by performance impact. Zum Beispiel, jedes Terraform Modul für das Gateway wurde mit Benchmarks versehen. Any pull request that caused p95 latency regression beyond 10ms triggered a performance review before merge."}
{"ts": "12:15", "speaker": "I", "text": "Welche Schnittstellen zu anderen Teams sind in Ihrem Alltag besonders relevant?"}
{"ts": "13:00", "speaker": "E", "text": "Hauptsächlich Security für mTLS und Token-Validation, und SRE für Observability. Wir haben wöchentliche Check-ins, plus Ad-hoc-Slack Channels bei Incidents. Also, for example, last month we synced with Security to update the cipher suites in line with Poseidon Networking's recommendations."}
{"ts": "18:45", "speaker": "I", "text": "Welche IaC-Tools nutzen Sie für die Provisionierung der Gateway-Infrastruktur?"}
{"ts": "19:20", "speaker": "E", "text": "Terraform als Primary, kombiniert mit Ansible für Post-Provision Konfiguration. We also have custom Go templates für die NGINX config, die direkt aus GitOps-Repos deployed werden."}
{"ts": "25:50", "speaker": "I", "text": "Wie haben Sie Rolling Deployments gemäß RB-GW-011 umgesetzt?"}
{"ts": "26:25", "speaker": "E", "text": "Wir nutzen Canary Deployments mit 10% Traffic Shift alle 5 Minuten. The pipeline calls a health-check script; falls Error-Rate > 0.5% ist, wird automatisch ein Rollback per Runbook RB-GW-011-rollback.yaml getriggert."}
{"ts": "32:40", "speaker": "I", "text": "Können Sie ein Beispiel für eine Pipeline-Optimierung nennen, die die Deployment-Zeit reduziert hat?"}
{"ts": "33:15", "speaker": "E", "text": "Wir haben den Container-Build Schritt parallelisiert. Instead of sequential image builds, we leverage BuildKit with cache mounts. Dadurch haben wir die Zeit von 18 auf etwa 10 Minuten reduziert."}
{"ts": "40:00", "speaker": "I", "text": "Wie sind Sie beim Incident GW-4821 vorgegangen, um den MTLS Handshake Bug zu isolieren?"}
{"ts": "40:50", "speaker": "E", "text": "Zuerst haben wir im Gateway-Cluster Debug-Logging aktiviert. Then, mit tcpdump auf einem Canary-Knoten konnten wir sehen, dass der TLS handshake bei bestimmten cipher suites hängen blieb. Wir haben diese in der Config disabled und einen Patch über Ticket GW-4821-p1 ausgerollt."}
{"ts": "45:30", "speaker": "I", "text": "Welche Monitoring-Maßnahmen haben Sie implementiert, um ähnliche Probleme früh zu erkennen?"}
{"ts": "46:10", "speaker": "E", "text": "Prometheus Metriken für handshake_duration_seconds, plus ein Alert in Grafana, wenn Median > 50ms. Zusätzlich haben wir ein Synthetic mTLS Check Script im CI, das nightly läuft."}
{"ts": "91:00", "speaker": "I", "text": "Können Sie nochmal genauer auf die IaC-Toolchain eingehen – speziell, wie Sie Terraform mit den internen Modulen für das Gateway verknüpfen?"}
{"ts": "91:08", "speaker": "E", "text": "Ja, also wir nutzen Terraform 1.4 mit einem Custom Provider, den wir intern als 'tf-orion-gw' pflegen. Der mappt direkt auf unser API provisioning via Poseidon Control Plane. Dadurch können wir in einem Plan sowohl Gateway Nodes als auch mTLS-Peer-Konfigurationen ausrollen."}
{"ts": "91:23", "speaker": "I", "text": "Und diese mTLS-Peers kommen direkt aus den Poseidon Networking Policies?"}
{"ts": "91:26", "speaker": "E", "text": "Genau, wir ziehen die Policy YAMLs nightly aus dem Poseidon Git-Repo, validieren sie mit einem internen Linter – das ist wichtig, weil Aegis IAM Changes manchmal implizit neue CNs erzwingen, und das schlägt sonst beim Gateway TLS Handshake fehl."}
{"ts": "91:41", "speaker": "I", "text": "Das klingt nach einer engen Verzahnung zwischen drei Systemen. Gab es mal einen Vorfall wegen eines nicht abgestimmten Changes?"}
{"ts": "91:46", "speaker": "E", "text": "Ja, Ticket GW-5297 – da hat das IAM Team ein neues ServicePrincipal-Feld eingeführt, ohne die CN-Mapping-Table in Poseidon zu aktualisieren. Folge: Zwei Knoten sind in Quarantine Mode gegangen, weil der mTLS-Handshake gebrochen war."}
{"ts": "91:59", "speaker": "I", "text": "Wie haben Sie das gelöst?"}
{"ts": "92:02", "speaker": "E", "text": "Wir haben einen Hotfix-Branch in tf-orion-gw erstellt, der temporär die CN-Validation gelockert hat – gemäß Runbook RB-GW-REC-07. Danach Abstimmung per RFC-POSEI-43, um die Schemaänderung sauber in alle Systeme zu propagieren."}
{"ts": "92:19", "speaker": "I", "text": "Interessant. Sie sagten vorhin, Sie nutzen Rolling Deployments. Haben Sie dabei Anpassungen für die Latenz-SLA vorgenommen?"}
{"ts": "92:24", "speaker": "E", "text": "Ja, wir haben im ArgoCD Deployment-Template die maxUnavailable auf 1 gesetzt und preStop Hooks eingebaut, die erst abklinken, wenn die active connection count unter 10 fällt. Das half, p95 stabil unter 120 ms zu halten, auch während Deployments."}
{"ts": "92:41", "speaker": "I", "text": "Wie testen Sie das vor dem Go-Live?"}
{"ts": "92:44", "speaker": "E", "text": "Wir fahren eine Canary-Phase mit 5 % Traffic, messen mit unserem Prometheus Setup die Latenz pro Node und triggern ein automatisches Rollback, falls drei Messintervalle nacheinander > 115 ms liegen. Das ist so im Runbook RB-GW-VAL-03 dokumentiert."}
{"ts": "93:00", "speaker": "I", "text": "Haben Sie in diesem Prozess auch Rate Limiting im Blick?"}
{"ts": "93:03", "speaker": "E", "text": "Ja, wir haben ein duales Ziel: Maintain UX und BLAST_RADIUS minimieren. Wir setzen soft limits pro API-Key, die nur bei Traffic-Spikes greifen. Das erfordert Feintuning – zu aggressiv, und die User Experience leidet; zu lasch, und einzelne Keys können den Cluster stressen."}
{"ts": "93:20", "speaker": "I", "text": "Und wie priorisieren Sie Performance- versus Sicherheitsaspekte, wenn beides gleichzeitig betroffen ist?"}
{"ts": "93:24", "speaker": "E", "text": "Da greifen wir auf unseren internen Decision Guide DG-SEC-PERF zurück: Safety First hat Vorrang, aber wir suchen Low-Impact-Optimierungen zuerst. Beispiel: Wir aktivierten TLS1.3 early data nur für interne Services, um Latenz zu senken, ohne externe Angriffsfläche zu vergrößern."}
{"ts": "97:00", "speaker": "I", "text": "Lassen Sie uns jetzt auf das Thema Risikomanagement eingehen. Wie stellen Sie sicher, dass Änderungen im Gateway den 'Safety First'-Wert von Novereon Systems erfüllen?"}
{"ts": "97:07", "speaker": "E", "text": "Also, wir haben intern eine Change Control Checklist, die auf Policy SAF-001 basiert. Ich prüfe vor jedem Merge Request die potenziellen Auswirkungen auf mTLS-Flows, IAM-Scopes und Rate Limiting. Zusätzlich laufen Canary Deployments in einer isolierten Staging-Zone, bevor wir die Änderungen in prod pushen. Safety First heißt für uns: No blind releases."}
{"ts": "97:28", "speaker": "I", "text": "Gab es zuletzt eine Situation, wo Sie bei einer neuen Auth-Mechanik ein Risiko erkannt und adressiert haben?"}
{"ts": "97:33", "speaker": "E", "text": "Ja, beim Ticket GW-5192. Da ging es um die Einführung von client-bound access tokens. Wir haben im Security Review festgestellt, dass ein bestimmter Token-Refresh-Flow mit älteren Poseidon TLS Libraries nicht kompatibel war. Also haben wir einen Feature Toggle implementiert, um das neue Verhalten nur bei kompatiblen Clients zu aktivieren."}
{"ts": "97:54", "speaker": "I", "text": "Das klingt nach einem klaren Trade-off zwischen Feature-Rollout und Stabilität. How did you quantify the SLA impact?"}
{"ts": "98:00", "speaker": "E", "text": "Wir haben synthetic load tests gefahren mit Gatling-Skripten aus unserem PerfLab. Dabei haben wir die p95 Latency gemessen, wie in SLA-ORI-02 definiert. Mit aktiviertem Feature lag p95 bei 122ms, leicht über Threshold – deshalb haben wir es erstmal nur für 5% des Traffics enabled."}
{"ts": "98:20", "speaker": "I", "text": "Interessant. Können Sie beschreiben, wie Sie Runbooks in solche Entscheidungen einbinden?"}
{"ts": "98:25", "speaker": "E", "text": "Klar, wir haben das Runbook RB-INC-042 für Auth-Mechanismen. Darin sind Entscheidungspfade hinterlegt – etwa: wenn Latenz > SLA + 5%, dann Rollback. Außerdem stehen dort die Kontaktketten für Security und Networking, sodass wir im Incidentfall sofort die richtigen Leute erreichen."}
{"ts": "98:42", "speaker": "I", "text": "Gab es beim Incident GW-4821, den wir vorhin kurz angesprochen hatten, eine Nutzung solcher Pfade?"}
{"ts": "98:47", "speaker": "E", "text": "Ja, damals haben wir genau diesen Pfad befolgt. After we isolated the MTLS handshake bug, step 3 im Runbook war 'switch to fallback certificate profile'. Das hat in weniger als 15 Minuten die Verbindung zu 80% der Clients wiederhergestellt."}
{"ts": "99:05", "speaker": "I", "text": "Wie binden Sie bei komplexen Abhängigkeiten – etwa zu Aegis IAM – Risiken in Ihre Planung ein?"}
{"ts": "99:11", "speaker": "E", "text": "Wir führen vor jedem Orion Release ein Dependency Risk Assessment durch. That means we check last-minute changes in Aegis IAM policies und testen sie gegen unsere Auth-Flows im PreProd. Bei Policy-Mismatches dokumentieren wir im RISK-LOG-ORI und verschieben ggf. die Gateway-Rollouts."}
{"ts": "99:29", "speaker": "I", "text": "Und bei Poseidon Networking? Insbesondere bei mTLS Policy Updates?"}
{"ts": "99:34", "speaker": "E", "text": "Da gibt es ein wöchentliches Cross-Team Standup. Wenn Poseidon ein TLS Cipher Update plant, evaluieren wir im Lab-Cluster. Wir haben ein internes Script, tls_compat_check.py, das gegen die geplanten Cipher Suites testet. Nur wenn alle Orion Endpunkte 'green' sind, geben wir Go."}
{"ts": "99:54", "speaker": "I", "text": "Zum Abschluss: Können Sie eine Entscheidung nennen, bei der Sie bewusst eine SLA-Risikoabwägung vorgenommen haben, auch wenn es interne Pressure gab?"}
{"ts": "100:00", "speaker": "E", "text": "Ja, bei der Einführung des Adaptive Rate Limiting (GW-5055). Das hätte unsere p95 Latency auf 110ms verbessert, aber die User Experience für Low-Bandwidth Clients verschlechtert. Wir haben nach Diskussion im ArchBoard entschieden, das Feature zu verschieben, um die BLAST_RADIUS-Reduktion nicht auf Kosten der Access Equity zu erkaufen."}
{"ts": "113:00", "speaker": "I", "text": "Können Sie bitte ein Beispiel nennen, wo Sie bewusst eine SLA-Risikoabwägung vorgenommen haben – insbesondere im Kontext von SLA-ORI-02 und dem p95 Latency Ziel?"}
{"ts": "113:05", "speaker": "E", "text": "Ja, klar. Wir hatten im Ticket GW-5290 den Fall, dass ein neues Auth-Modul für Partner-APIs eingeführt werden sollte. Das Modul erhöhte initial die p95 Latency um ca. 15 ms. Wir haben in einem Risk-Review mit dem SRE-Team abgewogen, ob wir den Rollout verzögern oder unter Feature-Flag mit limitierter Audience starten – wir haben uns für letzteres entschieden, um Feedback zu sammeln, ohne SLA-Verletzung im Gesamtschnitt."}
{"ts": "113:15", "speaker": "I", "text": "Und wie haben Sie das technisch abgesichert, dass nur eine limitierte Audience betroffen war?"}
{"ts": "113:20", "speaker": "E", "text": "Wir nutzten im API Gateway eine Header-basierte Routing-Regel, gesteuert über unsere IaC-Parameter in Terraform. Durch den Toggle in der variables.tf konnten wir gezielt nur Requests mit einem X-Beta-Partner-Header auf die neue Auth-Chain schicken. That way we kept the blast radius small."}
{"ts": "113:33", "speaker": "I", "text": "Gab es bei diesem Rollout besondere Kommunikationsmaßnahmen mit den Stakeholdern?"}
{"ts": "113:38", "speaker": "E", "text": "Ja, wir haben ein Change Advisory Meeting einberufen und im internen Confluence die RFC-ORI-2023-17 dokumentiert. Plus, ein Runbook-Abschnitt 'Beta Auth Rollback' wurde ergänzt, damit On-Call schnell reagieren kann – auch wenn das Szenario unwahrscheinlich war."}
{"ts": "113:52", "speaker": "I", "text": "Sie erwähnten Runbooks – wie binden Sie diese im Incident Response Prozess konkret ein?"}
{"ts": "113:57", "speaker": "E", "text": "Wir haben in PagerDuty bei jedem Service einen Link zum relevanten Runbook in der Alert-Payload. For example, for TLS handshake issues like GW-4821, the on-call gets directly to 'Runbook-GW-mTLS-v2', Abschnitt 3.2, der erklärt, wie man mit OpenSSL den Handshake simuliert und Logs verifiziert."}
{"ts": "114:10", "speaker": "I", "text": "Wie stellen Sie sicher, dass die Lessons Learned aus Incidents auch in Präventivmaßnahmen übergehen?"}
{"ts": "114:15", "speaker": "E", "text": "Nach jedem Major Incident führen wir eine Post-Mortem-Review durch. Die Action Items werden als Jira-Tasks mit Label 'PREVENTIVE' versehen. Zum Beispiel haben wir nach GW-4821 mTLS-Handshake Checks in die CI-Pipeline integriert, so that breakages are caught before prod deployment."}
{"ts": "114:28", "speaker": "I", "text": "Gibt es dabei Konflikte zwischen schneller Auslieferung und vollständiger Prävention?"}
{"ts": "114:33", "speaker": "E", "text": "Ja, definitiv. Wir haben manchmal den Druck, Features schnell rauszubringen. In solchen Fällen nutzen wir 'canary deploys' und setzen striktere Monitoring-Thresholds für die Canary Nodes, um früh abzubrechen, falls Latency oder Error-Rate abweichen."}
{"ts": "114:45", "speaker": "I", "text": "Wie gehen Sie bei Performance-Optimierungen mit dem Thema BLAST_RADIUS um?"}
{"ts": "114:50", "speaker": "E", "text": "Wir segmentieren Traffic nach Region und Use-Case. Optimierungen wie neue Rate-Limiter-Algorithmen werden zuerst nur in einer Region aktiv. Außerdem setzen wir Traffic Shadowing ein – that lets us test new code paths without impacting real users."}
{"ts": "115:05", "speaker": "I", "text": "Zum Abschluss: welche Risiken sehen Sie aktuell bei Einführung neuer Auth-Mechanismen im Gateway?"}
{"ts": "115:10", "speaker": "E", "text": "Hauptsächlich Kompatibilitätsrisiken mit bestehenden Clients, vor allem wenn diese TLS-Parameter oder Tokenformate hart codiert haben. Wir mitigieren das durch umfangreiche Staging-Tests mit simulierten Legacy-Clients und ein Extended Deprecation Notice. Plus, wir legen Wert auf klare Kommunikation im Partner-Portal."}
{"ts": "115:00", "speaker": "I", "text": "Zum Thema SLA-Compliance: Wie haben Sie konkret bei der letzten Änderung an der Auth-Komponente sichergestellt, dass SLA-ORI-02 (p95 < 120ms) eingehalten wird?"}
{"ts": "115:05", "speaker": "E", "text": "Wir haben vor dem Merge ein synthetic traffic replay gemacht, basierend auf den Top 50 API calls aus Prod, gemessen in der Staging-Umgebung. Dadurch konnten wir schon vorab sehen, dass wir im Median bei 98ms lagen. Zusätzlich haben wir einen Canary Release mit nur 5% des Traffics gefahren und die Latency-Metriken in Grafana mit den SLA-Grenzwerten im Overlay beobachtet."}
{"ts": "115:18", "speaker": "I", "text": "Gab es dabei ein Risiko, dass der Canary-Test nicht alle Edge Cases abdeckt?"}
{"ts": "115:22", "speaker": "E", "text": "Ja, definitely. Deshalb hatten wir im Runbook RB-ORI-LAT-004 festgelegt, dass in solchen Fällen zusätzlich ein Burst-Test mit simulierten TLS Rehandshakes durchgeführt wird. Das haben wir hier auch angewendet, um mögliche Spikes wie im Incident GW-4821 früh zu sehen."}
{"ts": "115:35", "speaker": "I", "text": "Stichwort GW-4821 – haben Sie nach diesem Bug den Incident Response Prozess angepasst?"}
{"ts": "115:39", "speaker": "E", "text": "Ja, wir haben den Detection-Teil erweitert: Ein Alert in Prometheus auf ungewöhnliche Handshake-Dauern > 50ms, plus ein automatischer Dump der Session-Logs via Sidecar. Das ist jetzt in der SOP IR-ORI-08 verankert."}
{"ts": "115:51", "speaker": "I", "text": "Und wie binden Sie Security in solche SOP-Änderungen ein?"}
{"ts": "115:55", "speaker": "E", "text": "Wir haben einen festen Review-Slot mit dem Security-Team aus Aegis IAM. Changes an IR-SOPs gehen erst durch deren Policy Check, vor allem wenn mTLS oder Auth-Flows betroffen sind. Das ist Teil unseres cross-project governance models."}
{"ts": "116:08", "speaker": "I", "text": "Gibt es da manchmal Zielkonflikte zwischen schnellen Deployments und Safety First?"}
{"ts": "116:12", "speaker": "E", "text": "Klar, zum Beispiel bei der Einführung des neuen JWT-Validator-Moduls. Fast Deploy hätte Risiko für Token-Bypass bedeutet. Wir haben uns entschieden, lieber zwei zusätzliche QA-Zyklen einzubauen und dadurch eine Woche Delay in Kauf zu nehmen. Das war eine bewusste SLA-Risikoabwägung, dokumentiert im Change Request CR-ORI-221."}
{"ts": "116:27", "speaker": "I", "text": "Wie messen Sie den Erfolg solcher vorsichtigen Entscheidungen?"}
{"ts": "116:31", "speaker": "E", "text": "Wir schauen auf Post-Deployment Incidents – in diesem Fall gab es null Auth-related Alerts in den ersten 30 Tagen. Außerdem haben wir im Lessons Learned festgehalten, dass die zusätzliche Testtiefe 15 potenzielle Fehlkonfigurationen früh aufgedeckt hat."}
{"ts": "116:44", "speaker": "I", "text": "Hatten diese Tests auch Einfluss auf die Skalierungsstrategien?"}
{"ts": "116:48", "speaker": "E", "text": "Indirectly, ja. Wir haben gesehen, dass unter höherer TLS-Last bestimmte Nodes schneller an CPU-Limits kamen. Das hat uns veranlasst, den Horizontal Pod Autoscaler mit einer zusätzlichen TLS_CPU_Metric zu füttern, damit Scale-Out nicht nur bei Request Count triggert."}
{"ts": "117:02", "speaker": "I", "text": "Letzte Frage: Welche offenen Risiken sehen Sie aktuell im Orion Gateway in Bezug auf Auth-Mechanismen?"}
{"ts": "117:06", "speaker": "E", "text": "Hauptsächlich das Zusammenspiel von neuen OAuth2 Device Flows mit Legacy Clients. Da könnten Race Conditions bei Token Refresh entstehen. Wir haben bereits einen Proof-of-Concept Monitor dafür in Dev, aber bis es produktionsreif ist, bleibt das ein gelbes Risiko in unserem Risk Register ID RR-ORI-15."}
{"ts": "123:00", "speaker": "I", "text": "Lassen Sie uns jetzt noch genauer auf den 'Safety First'-Wert eingehen — wie stellen Sie in der Praxis sicher, dass jede Änderung am Orion Gateway diesen erfüllt, insbesondere unter SLA-ORI-02?"}
{"ts": "123:05", "speaker": "E", "text": "Also, wir haben intern die Policy PF-GW-009, die besagt, dass kein Merge ohne einen vollständigen Canary-Test in Stage erfolgt. Safety First heißt bei uns: wir nutzen die Stage-Cluster im vollen Traffic-Mirroring Mode, before anything hits prod."}
{"ts": "123:15", "speaker": "I", "text": "Und wie gehen Sie vor, wenn Canary-Tests zwar SLA-konform sind, aber Incident-Daten aus der Vergangenheit ein anderes Bild zeigen?"}
{"ts": "123:20", "speaker": "E", "text": "Dann greifen wir auf unser Incident-Archiv, z.B. GW-4821 und GW-4907, zurück und simulieren die damaligen Lastmuster. Das ist so’n bisschen retroactive Chaos Testing, kombiniert mit den aktuellen Build-Artefacts."}
{"ts": "123:32", "speaker": "I", "text": "Interessant, und dokumentieren Sie diese zusätzlichen Tests formal?"}
{"ts": "123:36", "speaker": "E", "text": "Ja, wir hängen sie als Appendix an das Change Request im RFC-System (z.B. RFC-GW-221) an, mit Link zu den Runbooks. Das ist nicht nur Best Practice, sondern auch ein Audit Requirement laut IS-QA-07."}
{"ts": "123:48", "speaker": "I", "text": "Wie fließen Risiken neuer Auth-Mechanismen, sagen wir mal Token Binding, in diese Freigabeprozesse ein?"}
{"ts": "123:54", "speaker": "E", "text": "We do a dual-track review. Security Team prüft cryptographic soundness, während wir SRE-seitig die Latenz- und Failure-Modes analysieren. For Token Binding haben wir z.B. gefunden, dass der Handshake in 5% der Fälle > 200ms dauerte — nicht acceptable under SLA-ORI-02."}
{"ts": "124:08", "speaker": "I", "text": "Und was war dann der Trade-off, um das Problem zu entschärfen?"}
{"ts": "124:12", "speaker": "E", "text": "Wir haben im Build eine Fallback-Mechanik eingebaut: wenn Token Binding handshake > 150ms, switchen wir auf Standard mTLS. Das reduziert zwar den Security-Gewinn in seltenen Fällen, aber wir bleiben SLA-konform."}
{"ts": "124:24", "speaker": "I", "text": "Gab es dabei Bedenken bezüglich der BLAST_RADIUS-Reduktion?"}
{"ts": "124:28", "speaker": "E", "text": "Ja, klar. Fallback kann den blast radius vergrößern, weil mehr Sessions nicht token-gebunden sind. Deshalb haben wir laut Risk Register RSK-GW-14 eine wöchentliche Audit-Policy, um zu checken, ob das Fallback überproportional greift."}
{"ts": "124:40", "speaker": "I", "text": "Wie binden Sie Runbooks konkret im Incident Response ein, wenn so ein Fallback fehlschlägt?"}
{"ts": "124:45", "speaker": "E", "text": "Im Runbook RB-INC-032 steht: Step 1 – isolieren des betroffenen Gateway-Nodes via Poseidon Networking isolation API, Step 2 – rotate certificates über Aegis IAM API, Step 3 – traffic drain. Wir trainieren das quartalsweise im GameDay."}
{"ts": "124:58", "speaker": "I", "text": "Können Sie ein Beispiel nennen, wo Sie bewusst eine SLA-Risikoabwägung vorgenommen haben, die nicht nur technisch motiviert war?"}
{"ts": "125:04", "speaker": "E", "text": "Ja, beim Feature 'Adaptive Rate Limiting'. Technisch konnten wir es so konfigurieren, dass Power-User bursts allowed werden, aber das hätte p95 latency um 15ms erhöht. Wir haben uns entschieden, konservativ zu bleiben, um Business-Kunden nicht mit unvorhersehbaren Latenzen zu verärgern. Dokumentiert in DEC-GW-78 mit Business Impact Analysis."}
{"ts": "128:00", "speaker": "I", "text": "Lassen Sie uns jetzt ein bisschen tiefer einsteigen in die Performance-Metriken, die Sie für das Orion Edge Gateway verwenden. Welche KPIs sind für Sie ausschlaggebend, um zu entscheiden, ob ein Gateway-Knoten skaliert werden muss?"}
{"ts": "128:15", "speaker": "E", "text": "Also, primär schauen wir auf CPU Utilization und p95 Latency, klar, wegen SLA-ORI-02. Aber wir haben auch custom Metriken wie concurrent mTLS sessions und connection churn rate. Wenn die Churn rate über 15% in 5 Minuten geht, planen wir pre-emptive scale-out."}
{"ts": "128:38", "speaker": "I", "text": "Und wie binden Sie das in Ihre Automatisierung ein? Nutzen Sie da ein festes IaC-Pattern?"}
{"ts": "128:48", "speaker": "E", "text": "Ja, wir haben in Terraform ein Modul 'gw_autoscale.tf' mit Threshold-Variablen. Das Modul triggert via EventBridge einen Pipeline-Run, der wiederum einen Rolling Deployment nach RB-GW-011 anstößt, aber mit Canary Weighting von 10% für 5 Minuten."}
{"ts": "129:12", "speaker": "I", "text": "In Bezug auf BLAST_RADIUS-Reduktion – wie priorisieren Sie Performance-Verbesserungen, wenn sie im Widerspruch dazu stehen könnten?"}
{"ts": "129:25", "speaker": "E", "text": "Das ist tricky. Wir haben z.B. im Ticket GW-5023 eine Optimierung im Connection Pooling, die theoretisch 8% weniger Latency bringt, aber auch das Risiko von Cross-Tenant Leakage erhöhen könnte. Da haben wir staged rollout mit nur einem AZ gemacht, um Blast Radius zu minimieren."}
{"ts": "129:49", "speaker": "I", "text": "Gab es Fälle, wo das Rate Limiting die User Experience merklich beeinträchtigt hat?"}
{"ts": "130:00", "speaker": "E", "text": "Ja, im April hatten wir eine Policy-Änderung aus Aegis IAM, die zu aggressiv war. Users sahen 429 Errors bei nur 60% des vorherigen Limits. Wir mussten via Runbook RB-GW-Rate-07 temporär den Burst-Wert erhöhen, um Support-Tickets abzufangen."}
{"ts": "130:24", "speaker": "I", "text": "Wie stellen Sie in so einem Fall sicher, dass der 'Safety First'-Wert trotzdem erfüllt bleibt?"}
{"ts": "130:35", "speaker": "E", "text": "Wir dokumentieren jede Abweichung in unserem Change Log mit Risk Assessment. Für den Burst-Wert hatten wir eine parallel laufende Observability Query, die Missbrauchsversuche filtert. Erst wenn die Query <1% False Positives zeigt, lassen wir die Änderung live."}
{"ts": "130:58", "speaker": "I", "text": "Gab es auch Risiken bei der Einführung neuer Auth-Mechanismen, die Sie abwägen mussten?"}
{"ts": "131:08", "speaker": "E", "text": "Definitiv. Beim Switch zu mutual OAuth2 aus Aegis gab es Overhead im Token Exchange. Wir mussten entscheiden, ob wir SLA-ORI-02 riskieren. Lösung war ein Caching-Layer mit 90s TTL, basierend auf Poseidon Networking Shared Cache Nodes."}
{"ts": "131:30", "speaker": "I", "text": "Können Sie ein Beispiel nennen, wo Sie bewusst eine SLA-Risikoabwägung getroffen haben?"}
{"ts": "131:40", "speaker": "E", "text": "Ja, bei GW-4977. Wir mussten ein Security Patch einspielen, der den Handshake verlangsamt. Wir haben bewusst SLA-ORI-02 für 48h ausgesetzt, dokumentiert im RFC-ORI-SEC-29, um die Exploit-Fenster zu schließen."}
{"ts": "132:05", "speaker": "I", "text": "Und wie wurde das intern kommuniziert?"}
{"ts": "132:15", "speaker": "E", "text": "Über unser Incident Channel #gw-incidents und ein spezielles Safety Bulletin. Alle Stakeholder, inkl. SRE und Security, mussten den Waiver elektronisch signieren, bevor wir deployed haben."}
{"ts": "136:00", "speaker": "I", "text": "Zum Abschluss unseres Gesprächs möchte ich noch auf Ihre Entscheidungskompetenz eingehen. Können Sie ein konkretes Beispiel nennen, wo Sie zwischen einer SLA-Konformität und einer neuen Feature-Einführung abwägen mussten?"}
{"ts": "136:12", "speaker": "E", "text": "Ja, im Ticket GW-5179 stand die Entscheidung an, ein neues Auth-Plugin zu deployen, das aus Aegis IAM v4 kam. Die p95 Latenzprognose war +15ms, und SLA-ORI-02 erlaubt nur 120ms. Wir mussten abwägen: Feature Go-Live für Partner-API versus Risiko eines SLA-Verstoßes."}
{"ts": "136:30", "speaker": "I", "text": "Und wie sind Sie da methodisch vorgegangen, um den 'Safety First'-Wert zu berücksichtigen?"}
{"ts": "136:43", "speaker": "E", "text": "Wir haben einen Canary Deployment Plan nach RB-GW-015 gefahren, mit nur 5% Traffic auf dem neuen Plugin. In parallel haben wir Latency-Monitoring mit Prometheus Query `histogram_quantile(0.95, ...)` aktiviert. Erst als wir nach 48h keine Degradation sahen, haben wir sukzessive hochskaliert."}
{"ts": "137:05", "speaker": "I", "text": "Gab es in dieser Phase Abstimmungen mit anderen Projekten?"}
{"ts": "137:15", "speaker": "E", "text": "Ja, Poseidon Networking musste die mTLS Cipher Suites validieren, damit die Auth-Handshake-Zeit nicht explodiert. Wir hatten einen Cross-Project Call, wo wir gemeinsam die OpenSSL Configs verglichen und unnötige Ciphers abgeschaltet haben."}
{"ts": "137:34", "speaker": "I", "text": "Interesting. Hat das irgendwelche Nebeneffekte gehabt?"}
{"ts": "137:44", "speaker": "E", "text": "Minimal. Ein paar ältere Clients konnten nicht mehr verbinden, aber die waren ohnehin EOL laut Policy DOC-END-031. Wir haben das in den Release Notes transparent kommuniziert."}
{"ts": "138:00", "speaker": "I", "text": "Wie dokumentieren Sie solche Entscheidungen intern, um spätere Audits zu erleichtern?"}
{"ts": "138:12", "speaker": "E", "text": "Wir nutzen das Decision Log im Confluence Space ORI-BUILD, Eintrag DLOG-2024-07. Da steht: Problem, Optionen, Risiko-Bewertung (inkl. SLA-Impact), finaler Beschluss. Zusätzlich verlinken wir auf das Runbook und die zugehörigen Tickets."}
{"ts": "138:35", "speaker": "I", "text": "Okay. Und in Sachen Monitoring – haben Sie nach dem Go-Live noch Anpassungen vorgenommen?"}
{"ts": "138:45", "speaker": "E", "text": "Ja, wir haben die Alert-Thresholds für `gateway_request_duration_seconds` enger gestellt: Warning bei 105ms, Critical bei 115ms. Damit bekommen wir Headroom, bevor das SLA gebrochen wird."}
{"ts": "139:02", "speaker": "I", "text": "Gab es Lessons Learned aus diesem Fall für zukünftige Feature-Rollouts?"}
{"ts": "139:12", "speaker": "E", "text": "Definitiv: Erstens, frühzeitige Einbindung der Netzwerkteams, um Latenzfallen zu vermeiden. Zweitens, Canary-Traffic nicht unter 48h evaluieren. Drittens, User Communication Plan vorbereiten, falls Clients inkompatibel werden."}
{"ts": "139:32", "speaker": "I", "text": "Würden Sie sagen, dass hier ein Trade-off zugunsten von Feature Velocity oder Stability erfolgt ist?"}
{"ts": "139:43", "speaker": "E", "text": "Eigentlich ein Balance-Act – wir haben Feature Velocity leicht gedrosselt, um Stability nicht zu gefährden. Das war im Sinne der Unternehmenswerte, und SLA-ORI-02 blieb in allen Messpunkten compliant."}
{"ts": "144:00", "speaker": "I", "text": "Lassen Sie uns noch einmal konkret auf eine Entscheidung eingehen, where you had to balance performance with compliance requirements."}
{"ts": "144:05", "speaker": "E", "text": "Ja, das war beim Feature-Flag für den neuen JWT-Verifier. Wir mussten entscheiden, ob wir die Library-Version sofort hochziehen, um p95-Latenz von 110ms zu halten, obwohl das Audit-Policy-Check noch nicht final war."}
{"ts": "144:14", "speaker": "I", "text": "Und wie haben Sie das vor dem Hintergrund der SLA-ORI-02 bewertet?"}
{"ts": "144:18", "speaker": "E", "text": "We ran a shadow deployment in staging, mit aktiviertem Verifier, aber nur 10% Traffic. Parallel haben wir mit Security den Audit-Check beschleunigt. Unser Runbook RB-GW-019 hat genau diese Canary-Strategie beschrieben."}
{"ts": "144:29", "speaker": "I", "text": "Gab es dabei irgendwelche signifikanten Risiken für Safety First?"}
{"ts": "144:34", "speaker": "E", "text": "Das Hauptrisiko war, dass im Canary ein Auth-Bypass passieren könnte. Deswegen hatten wir mTLS forced und zusätzliche Logs im Poseidon-Netzwerkpfad aktiviert, um jede Abweichung zu sehen."}
{"ts": "144:45", "speaker": "I", "text": "Interessant. How did you decide the roll-out speed after the canary phase?"}
{"ts": "144:50", "speaker": "E", "text": "Wir haben Metriken wie error_rate_auth_fail und handshake_time_mtls ausgewertet. Als beide stabil unter den Thresholds im SLA-ORI-02 und im internen SLO-DOC-7 lagen, sind wir in 25%-Schritten weitergegangen."}
{"ts": "145:02", "speaker": "I", "text": "Gab es dafür eine formale Freigabe?"}
{"ts": "145:06", "speaker": "E", "text": "Ja, Change Request CR-ORI-884, approved by the CAB. Enthielt alle Canary-Logs, Vergleich zu Baseline und Verweis auf GW-4821 Lessons Learned, um ähnliche mTLS-Bugs zu vermeiden."}
{"ts": "145:16", "speaker": "I", "text": "Wie haben Sie die Lessons Learned dokumentiert?"}
{"ts": "145:21", "speaker": "E", "text": "In Confluence im Bereich Orion/Incidents, Eintrag INC-LL-21. Dort steht die Schritt-für-Schritt-Analyse, plus Hinweise wie 'immer Poseidon mTLS-Policy gegen Aegis IAM cert chain testen'."}
{"ts": "145:33", "speaker": "I", "text": "Hat diese Dokumentation später noch eine Rolle gespielt?"}
{"ts": "145:38", "speaker": "E", "text": "Absolutely. Beim späteren Ticket GW-5099 konnten wir durch dieselbe Canary- und Audit-Check-Kombi den Rollout ohne SLA-Verstoß durchführen. Das hat uns intern Kudos gebracht."}
{"ts": "145:48", "speaker": "I", "text": "Würden Sie sagen, dass diese Art von Entscheidungsmatrix jetzt Standard ist?"}
{"ts": "145:52", "speaker": "E", "text": "Ja, sie ist jetzt Teil von RB-GW-011 als Appendix C. Jede Performance-gegen-Compliance-Entscheidung muss diese Matrix durchlaufen, bevor wir live gehen."}
{"ts": "145:35", "speaker": "I", "text": "Lassen Sie uns jetzt noch mal auf diese finale Abwägung eingehen: Wie haben Sie konkret entschieden, bei GW‑5023 die temporäre Degradation der p95 Latency in Kauf zu nehmen?"}
{"ts": "145:40", "speaker": "E", "text": "Das war tricky. We had SLA-ORI-02 in mind, klar, aber die Safety-First Policy hat Vorrang. Wir haben anhand des Runbook RB-GW-019 den Hotfix so gestaltet, dass er zwar +15 ms latency brachte, aber den MTLS handshake sicher gemacht hat."}
{"ts": "145:47", "speaker": "I", "text": "Gab es da formale Freigabeprozesse oder war das eine Ad-hoc Entscheidung im Incident Bridge Call?"}
{"ts": "145:52", "speaker": "E", "text": "Es war eine Mischung. Im Bridge Call haben wir die Risikoanalyse mit dem SRE Lead aus Poseidon Networking gemacht; danach ein expedited RFC-Formular (RFC‑GW‑144) durchgeschleust."}
{"ts": "145:59", "speaker": "I", "text": "Und wie haben Sie intern kommuniziert, dass SLA‑Verletzungen kurzfristig akzeptiert werden?"}
{"ts": "146:04", "speaker": "E", "text": "Wir nutzen den Orion Status Channel. Message war zweisprachig: 'SLA impact expected, safety mitigation active'. Damit konnten Business Stakeholder sofort einschätzen, dass es bewusst ist."}
{"ts": "146:10", "speaker": "I", "text": "Gab es danach eine Postmortem‑Analyse?"}
{"ts": "146:14", "speaker": "E", "text": "Ja, ein blameless Postmortem nach Template PM‑GW‑01. Wir haben dokumentiert, dass die BLAST_RADIUS-Reduktion durch gestaffelte Rollouts geholfen hat, nur 20 % der Knoten initial zu treffen."}
{"ts": "146:21", "speaker": "I", "text": "Was waren dabei die wichtigsten Lessons Learned?"}
{"ts": "146:25", "speaker": "E", "text": "One key lesson: frühzeitige Integration der Aegis IAM mTLS Policies in Staging Pipelines. Und, dass wir das Observability-Dashboard schneller an neue Policy IDs anpassen müssen."}
{"ts": "146:32", "speaker": "I", "text": "Haben Sie daraufhin die Runbooks angepasst?"}
{"ts": "146:36", "speaker": "E", "text": "Genau, RB-GW-019 wurde erweitert mit einem Decision Tree: 'Wenn SLA <-> Safety Konflikt, dann Priorität Safety, mit dokumentierter Approval Chain'."}
{"ts": "146:42", "speaker": "I", "text": "Wie stellen Sie sicher, dass alle Engineers diese Updates auch kennen?"}
{"ts": "146:46", "speaker": "E", "text": "Wir haben ein verpflichtendes Brown-Bag Training eingeführt; plus, the CI pipeline checks for outdated runbook references, fails the merge if found."}
{"ts": "146:52", "speaker": "I", "text": "Letzte Frage: Würden Sie bei einem ähnlichen Incident wieder gleich entscheiden, oder gäbe es Änderungen im Prozess?"}
{"ts": "146:57", "speaker": "E", "text": "Ich würde again Safety first setzen, aber wir würden vorher ein Canary-Test mit synthetischen mTLS Clients fahren, um die Latenz-Auswirkung präziser zu quantifizieren, bevor wir global ausrollen."}
{"ts": "147:05", "speaker": "I", "text": "Lassen Sie uns noch einmal zur Pipeline-Optimierung zurückkommen – gab es einen spezifischen Schritt, den Sie in der letzten Sprint-Iteration beschleunigen konnten?"}
{"ts": "147:14", "speaker": "E", "text": "Ja, wir haben den Container Security Scan aus der Build-Pipeline in einen parallelen Branch verschoben, um die mainline deploys nicht zu blockieren. The scan still runs before prod promotion, aber das verkürzt die Standard-Builds um knapp 40 Sekunden."}
{"ts": "147:24", "speaker": "I", "text": "Wie haben Sie sichergestellt, dass dadurch keine Sicherheitslücken durchrutschen?"}
{"ts": "147:32", "speaker": "E", "text": "Wir haben im Runbook RB-GW-SEC-07 festgelegt, dass jeder Merge-Request vor der Freigabe in den Release-Branch den vollen Scan durchlaufen muss. Zusätzlich gibt es ein Nightly Job, der alle Images im Artifact Repo gegen die aktuelle CVE-Liste prüft."}
{"ts": "147:45", "speaker": "I", "text": "Interessant. Im Zusammenhang mit dem MTLS Bug GW-4821 – haben Sie das Monitoring dafür noch erweitert?"}
{"ts": "147:54", "speaker": "E", "text": "Absolut. Wir haben ein Custom Prometheus Exporter Modul geschrieben, das den TLS Handshake Dauer und Fehlercodes pro Upstream-Cluster exposed. Zusätzlich gibt es jetzt eine Alert Rule AL-GW-021, die bei mehr als 2% handshake_failures in 5 Minuten feuert."}
{"ts": "148:08", "speaker": "I", "text": "Wie schnell wird das in der Praxis ausgelöst? Gab es schon False Positives?"}
{"ts": "148:17", "speaker": "E", "text": "Ja, während eines Poseidon Netzwerk-Upgrades hatten wir einen kurzen Spike. The runbook says wir sollen erst den Poseidon Statuschannel checken, bevor wir in den Gateway Nodes eingreifen – hat unnötigen Aktionismus verhindert."}
{"ts": "148:31", "speaker": "I", "text": "Sie haben ja vorhin Aegis IAM erwähnt – gab es zuletzt Änderungen, die den Gateway-Auth-Flow beeinflusst haben?"}
{"ts": "148:40", "speaker": "E", "text": "Ja, Aegis hat die JWT Signing Keys rotiert, RFC-AGI-KEYROT-04. Wir mussten die JWKS Fetch Interval in der Gateway Config von 30 auf 10 Sekunden reduzieren, um innerhalb des SLA-ORI-07 für Auth-Token Validation zu bleiben."}
{"ts": "148:55", "speaker": "I", "text": "Gab es Auswirkungen auf die Latenz?"}
{"ts": "149:02", "speaker": "E", "text": "Minimal – wir cachen die Keys lokal für 5 Minuten, so der Impact ist negligible. Aber wir haben die Cache Invalidation Logik ausgiebig im Staging getestet, um keinen Auth-Drop zu riskieren."}
{"ts": "149:14", "speaker": "I", "text": "Wie beurteilen Sie generell den Trade-off zwischen kürzeren Fetch Intervals und zusätzlicher Load auf Aegis?"}
{"ts": "149:22", "speaker": "E", "text": "Das ist so ein klassischer Balance-Act – Security freshness versus system load. Wir haben mit Aegis ein Rate-Limit von 60 req/min ausgehandelt, das in der Config enforce wird. Anything beyond wird geblockt, um deren SLAs nicht zu verletzen."}
{"ts": "149:37", "speaker": "I", "text": "Könnten Sie zum Schluss noch ein Beispiel nennen, wo eine Änderung in Poseidon Networking direkt zu einem Incident im Orion Gateway führte?"}
{"ts": "149:46", "speaker": "E", "text": "Klar – beim Upgrade auf PN-Release 3.4 wurde die Default Cipher Suite geändert. Das führte zu inkompatiblen Handshakes bei zwei Legacy-Clients. Wir haben das in Ticket GW-5187 dokumentiert und einen Hotfix-Config-Patch via RB-GW-TLS-03 ausgerollt."}
{"ts": "149:03", "speaker": "I", "text": "Lassen Sie uns noch einmal auf die Cross-Project dependencies eingehen. How did the Poseidon Networking changes in Q2 affect Orion's mTLS policy enforcement?"}
{"ts": "149:08", "speaker": "E", "text": "Ja, im April hat Poseidon ein Update auf PN-Policy-1.4 ausgerollt, das die Cipher Suite-Prioritäten änderte. That meant, unser Gateway konnte initial einige Handshakes nicht abschließen, weil wir laut RB-GW-015 noch die alte ECDHE-Reihenfolge hatten."}
{"ts": "149:15", "speaker": "E", "text": "Wir mussten in der Staging-Umgebung kurzfristig ein Hotfix-Deployment fahren, IaC-Änderungen durch Terraform-Modul `gw_tls_profiles` pushen und gleichzeitig mit dem Aegis IAM-Team abstimmen, dass die Auth-Header-Validierung nicht beeinflusst wird."}
{"ts": "149:25", "speaker": "I", "text": "Das klingt nach einem klassischen Multi-Hop-Problem. Did you document that in a runbook for future reference?"}
{"ts": "149:29", "speaker": "E", "text": "Ja, wir haben Runbook RB-DEP-042 ergänzt: Ein Flussdiagramm eingefügt, das zeigt, wie Änderungen in Poseidon's TLS-Stack zuerst auf unsere Staging-Knoten getestet werden. Plus: ein automatischer Canary-Build mit synthetischen MTLS-Requests."}
{"ts": "149:39", "speaker": "I", "text": "Und wie haben Sie sichergestellt, dass SLA-ORI-02 in dieser Übergangsphase nicht verletzt wurde?"}
{"ts": "149:43", "speaker": "E", "text": "Wir haben temporär die Rate-Limits etwas gesenkt, um den P95 Latency-Anstieg durch Retries zu kompensieren. Gleichzeitig haben wir im Prometheus-Alert `ORI_LATENCY_P95` den Schwellenwert um 5ms verschärft, so dass wir schneller reagieren konnten."}
{"ts": "149:53", "speaker": "I", "text": "Interesting. Gab es dabei Konflikte mit dem User Experience Team?"}
{"ts": "149:57", "speaker": "E", "text": "Ja, UX war nicht begeistert, weil einige Clients throttling bemerkten. Aber wir haben transparent die Gründe kommuniziert und auf den Safety-First-Wert verwiesen. Der Kompromiss war, den Burst-Parameter im Rate Limiter leicht zu erhöhen."}
{"ts": "150:06", "speaker": "I", "text": "Können Sie ein weiteres Beispiel für so eine schnelle Reaktion nennen, vielleicht aus einem anderen Subsystem?"}
{"ts": "150:10", "speaker": "E", "text": "Beim Incident GW-4978, a serialization bug in the Auth token parser, mussten wir ähnlich schnell agieren. Wir haben den Parser-Modul-Branch 'hotfix/token-npe' in die CI-Pipeline injected, Tests aus RB-AUTH-009 laufen lassen und innerhalb von 45 Minuten in Produktion gebracht."}
{"ts": "150:21", "speaker": "I", "text": "Wie konnten Sie dabei sicherstellen, dass keine Regressionen in anderen Projekten auftreten?"}
{"ts": "150:25", "speaker": "E", "text": "Durch den Cross-Project Integration Job im Jenkinsfile. Der lädt automatisch die neuesten Aegis IAM und Poseidon Networking Mocks, sodass unsere Regressionstests gegen reale API-Contracts fahren. Zusätzlich gibt es einen manuellen Review-Schritt mit beiden Teams."}
{"ts": "150:36", "speaker": "I", "text": "Das klingt nach guter Governance. Haben Sie diese Prozesse in den SOPs verankert?"}
{"ts": "150:40", "speaker": "E", "text": "Ja, SOP-ORI-DEP-03 wurde aktualisiert. Enthält jetzt die Pflicht zu Canary-Deployments bei allen netzwerkrelevanten Änderungen und eine Checkliste für Auth-Mechanismen mit BLAST_RADIUS-Bewertung."}
{"ts": "150:48", "speaker": "I", "text": "Danke, das hilft uns sehr, die Stabilität der Cross-Project Integration zu verstehen."}
{"ts": "151:03", "speaker": "I", "text": "Bevor wir ganz zum Abschluss kommen, würde ich noch gern hören, wie Sie mit den Cross-Project Abhängigkeiten zum Poseidon Networking umgehen, gerade wenn es um mTLS Policies geht."}
{"ts": "151:10", "speaker": "E", "text": "Ja, also wir nutzen hier ein kombiniertes Review-Board. Die mTLS Policies werden initial vom Poseidon-Team in deren RFC-Templates beschrieben, wir importieren dann diese Definitionen über einen CI-Job, der nightly läuft. Dabei prüfen wir per Script `verify-mtls-compat` ob die Cipher Suites mit unserem Gateway-Handshake kompatibel sind."}
{"ts": "151:22", "speaker": "E", "text": "Wenn wir Abweichungen finden – zum Beispiel Poseidon hat neulich auf TLS 1.3 only umgestellt – dann triggert der Job automatisch ein internes Ticket, das bei uns im ORI-Board landet, z.B. ORI-NET-274."}
{"ts": "151:35", "speaker": "I", "text": "Das klingt sehr automatisiert. Gab es trotzdem mal Ausnahmen, wo Sie manuell eingreifen mussten?"}
{"ts": "151:39", "speaker": "E", "text": "Ja, in einem Fall war die neue Poseidon Policy so restriktiv, dass unser Legacy-Client vom Test-Team nicht mehr verbinden konnte. Da mussten wir im Gateway kurzfristig einen Fallback aktivieren, wie in Runbook RB-MTLS-042 beschrieben, um die Regressionstests nicht zu blockieren."}
{"ts": "151:52", "speaker": "I", "text": "Haben Sie dabei mit dem Security-Team koordiniert?"}
{"ts": "151:55", "speaker": "E", "text": "Genau, Security musste das temporär absegnen. Wir haben einen Change Control Request erstellt, CCR-551, der explizit begrenzt war auf die Test-Subnetze. So konnten wir Safety First und Test-Continuity balancen."}
{"ts": "152:07", "speaker": "I", "text": "Wie fließen solche Lessons Learned dann zurück in die Automatisierung?"}
{"ts": "152:11", "speaker": "E", "text": "We record them im Knowledge Base Modul des CI-Systems. Dadurch können wir im verify-mtls-compat Script Exceptions hinterlegen, die nur in Staging greifen. Beim nächsten Policy-Update wird dann ein Dry-Run gestartet, der diese Ausnahmefälle simuliert."}
{"ts": "152:25", "speaker": "I", "text": "Das hilft sicher bei der Reduktion des BLAST_RADIUS in Zukunft?"}
{"ts": "152:28", "speaker": "E", "text": "Absolut. By limiting the scope of risky changes to known safe zones, we can uphold SLA-ORI-02 und trotzdem neue Security-Policies testen. Die Kombination aus Staging-Dry-Runs und Runbook-basierten Fallbacks ist da zentral."}
{"ts": "152:40", "speaker": "I", "text": "Wie sieht es mit Aegis IAM Policies aus – gibt es da ähnliche Stolpersteine?"}
{"ts": "152:44", "speaker": "E", "text": "Ja, besonders beim Token-Refresh-Flow. Aegis hat vor zwei Monaten die TTL-Logik geändert. Unser Gateway musste daraufhin das Cache-Invalidation-Intervall anpassen, sonst hätten wir eine erhöhte Auth-Latenz gehabt. Ticket ORI-IAM-107 deckt den Fixprozess ab."}
{"ts": "152:57", "speaker": "I", "text": "Und war das wieder ein Fall für Ihre Runbooks?"}
{"ts": "153:00", "speaker": "E", "text": "Ja, Runbook RB-AUTH-015 beschreibt genau, wie wir die Cache-Parameter ändern und gleichzeitig das Monitoring so anpassen, dass wir p95 Latency Metriken in Echtzeit sehen. Gerade weil Safety First heißt: keine Experimente im Blindflug."}
{"ts": "153:10", "speaker": "I", "text": "Vielen Dank, das rundet das Bild gut ab."}
{"ts": "153:03", "speaker": "I", "text": "Lassen Sie uns nochmal kurz auf das Thema Cross-Project Abhängigkeiten zurückkommen. Wie haben Sie konkret den Sync der Aegis IAM Policies ins Orion Gateway in dieser Build-Phase orchestriert?"}
{"ts": "153:08", "speaker": "E", "text": "Wir haben dafür einen wöchentlichen Pull-Job in der CI/CD Pipeline konfiguriert, der via Terraform Module die neuesten IAM-Policies zieht. About 60% der Anpassungen konnten wir automatisch mappen, der Rest ging in ein manuelles Review mit dem Security-Team."}
{"ts": "153:17", "speaker": "I", "text": "Gab es dabei auch Konflikte mit dem Poseidon Networking, speziell bei der mTLS Policy?"}
{"ts": "153:21", "speaker": "E", "text": "Ja, es gab einen Fall, Ticket NET-2317, wo Poseidon die Cipher Suites geändert hat. Das hat sofort den Handshake im Orion Gateway beeinflusst, weil unsere default mTLS Config auf eine andere Suite gesetzt war. Wir mussten einen Hotfix deployen."}
{"ts": "153:30", "speaker": "I", "text": "Wie haben Sie den Hotfix in die bestehenden Rolling Deployments integriert, ohne RB-GW-011 zu verletzen?"}
{"ts": "153:35", "speaker": "E", "text": "Wir haben ein Blue-Green Switch eingesetzt — zwei Gateway-Pools, einer mit der alten Config, einer mit der gefixten. Per Feature Flag konnten wir Traffic graduell shiften, fully compliant mit RB-GW-011."}
{"ts": "153:44", "speaker": "I", "text": "Interessant. Und wie wurde sichergestellt, dass SLA-ORI-02 während des Switchovers eingehalten wurde?"}
{"ts": "153:49", "speaker": "E", "text": "In Grafana hatten wir ein p95 Latency Panel mit 10s Intervallen. Die Runbook-GW-Deploy-04 schreibt vor, dass wir bei >110ms sofort den Rollback-Mechanismus triggern. Das mussten wir zum Glück nicht auslösen."}
{"ts": "153:59", "speaker": "I", "text": "Können Sie ein Beispiel geben, wie Sie bei solchen Abhängigkeiten proaktiv Risiken minimieren?"}
{"ts": "154:03", "speaker": "E", "text": "Wir pflegen eine Dependencies-Matrix im Confluence, with version pins und Review-Dates. Außerdem setzen wir Canary Tests auf Staging, die explizit gegen Aegis und Poseidon APIs laufen."}
{"ts": "154:12", "speaker": "I", "text": "Das klingt strukturiert. Gibt es auch ungeschriebene Regeln im Team, wie man bei Änderungen aus anderen Projekten vorgeht?"}
{"ts": "154:16", "speaker": "E", "text": "Ja, eine informelle Regel ist: 'No Friday merges from external repos'. Wir vermeiden späte Woche Integrationen, because if something breaks, weekend on-call load goes up."}
{"ts": "154:24", "speaker": "I", "text": "Wie haben Sie diese Regel im Incident GW-4821 berücksichtigt?"}
{"ts": "154:28", "speaker": "E", "text": "Der Bug fiel an einem Donnerstag auf. Wir haben den Fix vorbereitet, aber erst Montagmorgen deployt, um volle Teamverfügbarkeit zu haben. Das war auch im Einklang mit Runbook-GW-Incident-02."}
{"ts": "154:37", "speaker": "I", "text": "Hat das gewartete Deployment irgendwelche negativen Auswirkungen gehabt?"}
{"ts": "154:41", "speaker": "E", "text": "Minimal, ein paar Partner-APIs hatten degraded performance, aber wir konnten via Rate Limiting und Cache Extension den Impact unter 5% halten, all documented in Postmortem GW-4821."}
{"ts": "155:03", "speaker": "I", "text": "Lassen Sie uns noch etwas tiefer auf die Automatisierung eingehen. Wie haben Sie die IaC-Templates für Orion Edge Gateway strukturiert, um Änderungen aus Aegis IAM effizient einzubinden?"}
{"ts": "155:09", "speaker": "E", "text": "Wir haben die Terraform-Module so geschnitten, dass IAM-spezifische Ressourcen in einem separaten Modul 'iam_integration' liegen. Das erlaubt es uns, Änderungen aus Aegis IAM über ein einziges Variables-File zu übernehmen, ohne den Gateway-Core zu berühren."}
{"ts": "155:16", "speaker": "I", "text": "Gab es dafür spezielle Pipeline-Steps?"}
{"ts": "155:20", "speaker": "E", "text": "Ja, in der GitLab-CI hatten wir einen Stage 'sync-iam', der vor 'deploy-gateway' lief. Dort wird ein kleiner Go-Script ausgeführt, der die Policies aus dem Aegis-Repo clont und gegen unser JSON-Schema validiert."}
{"ts": "155:29", "speaker": "I", "text": "Interessant. Und wie haben Sie Rolling Deployments gemäß RB-GW-011 umgesetzt, wenn sich so ein Policy-Update ergeben hat?"}
{"ts": "155:36", "speaker": "E", "text": "Wir nutzen Canary-Slices, jeweils 10% der Knoten. Der Rollout-Step ist parametrisiert, sodass er bei sicherheitsrelevanten Änderungen maximal zwei Slices parallel zieht. Das ist zwar langsamer, erfüllt aber Safety First und minimiert Blast Radius."}
{"ts": "155:45", "speaker": "I", "text": "Gab es Fälle, wo diese Vorsicht zu SLA-Verletzungen geführt hat?"}
{"ts": "155:50", "speaker": "E", "text": "Einmal, bei GW-5177, einem Auth-Decoder-Bug. Wir haben den Fix so vorsichtig ausgerollt, dass p95 Latency für ein paar Minuten auf 138ms hochging. Wir haben das akzeptiert, dokumentiert im Post-Mortem PM-5177."}
{"ts": "155:59", "speaker": "I", "text": "Wie binden Sie Monitoring in diese Deployments ein, um so etwas früh zu sehen?"}
{"ts": "156:04", "speaker": "E", "text": "Wir haben Prometheus-Alerts auf 'p95_latency' und 'error_rate' pro Canary-Slice. Außerdem sendet ein kleiner Python-Daemon in der Pipeline die Metriken an Slack, damit das DevSecOps-Team live reagieren kann."}
{"ts": "156:14", "speaker": "I", "text": "Hat das auch beim MTLS Handshake Bug GW-4821 gegriffen?"}
{"ts": "156:19", "speaker": "E", "text": "Teilweise. Der Bug war eher sporadisch, und trat nur bei Cross-Region-Calls auf. Aber die Slice-basierte Latenz-Metrik hat uns gezeigt, dass AP-Region-Knoten auffällig waren. Das war der erste Hinweis, bevor wir ins Wireshark-Trace gingen."}
{"ts": "156:30", "speaker": "I", "text": "Können Sie kurz erklären, wie die Zusammenarbeit mit Poseidon Networking in diesem Fall lief?"}
{"ts": "156:36", "speaker": "E", "text": "Wir haben ein gemeinsames Incident-Bridge-Call mit deren TLS-Spezialisten gehabt. Poseidon hat uns bestätigt, dass ein Edge-Router-Firmware-Update in AP-Region mTLS-Handshake-Timings verändert hat. Wir mussten daraufhin unsere Gateway-TLS-Timeouts erhöhen."}
{"ts": "156:47", "speaker": "I", "text": "Das klingt nach einer engen Abstimmung. Wie dokumentieren Sie solche Cross-Project Learnings?"}
{"ts": "156:53", "speaker": "E", "text": "Wir pflegen ein internes Confluence-Space 'Gateway-Knowledge', dort gibt es eine Sektion 'Dependencies'. Jeder Eintrag hat den Ticket-Link, betroffene Services, Lessons Learned und Impact auf SLAs. Für GW-4821 ist das DOK-4821."}
{"ts": "158:03", "speaker": "I", "text": "Lassen Sie uns noch einmal auf die Cross-Project Abhängigkeiten eingehen. Wie haben Sie konkret die mTLS Policy mit Poseidon Networking abgestimmt, ohne die Orion-Build-Timeline zu gefährden?"}
{"ts": "158:09", "speaker": "E", "text": "Das war tricky, weil Poseidon just in Sprint 14 ein eigenes Cipher-Suite-Upgrade geplant hatte. Wir haben daher in einem Joint RFC-Meeting, RFC-POS-41, die TLSv1.3-Only Policy abgestimmt und in einem separaten Canary-Knoten getestet, bevor wir den Merge in die Haupt-Pipeline gemacht haben."}
{"ts": "158:17", "speaker": "I", "text": "Und wie haben Sie diesen Canary-Knoten in Ihre IaC-Definition integriert?"}
{"ts": "158:22", "speaker": "E", "text": "Wir haben im Terraform-Code ein optionales Modul `poseidon_canary` definiert, mit Feature-Flag aus der CI/CD-Var-Datei. Das ermöglichte uns, den Knoten temporär in die Staging-Umgebung einzufügen, ohne die produktiven Gateways zu beeinflussen."}
{"ts": "158:30", "speaker": "I", "text": "Gab es währenddessen Auswirkungen auf SLA-ORI-02?"}
{"ts": "158:35", "speaker": "E", "text": "Minimal. In den Canary-Runs hatten wir p95 Latency bei 128 ms, was über dem Ziel lag, aber wir haben dies explizit in der Ausnahme-Liste SLA-Exempt-2024-07 dokumentiert und mit SRE abgestimmt."}
{"ts": "158:44", "speaker": "I", "text": "Sie hatten vorhin Runbooks erwähnt. Gab es eines, das hier hilfreich war?"}
{"ts": "158:48", "speaker": "E", "text": "Ja, Runbook RB-GW-011 sah explizit vor, wie Rolling Deployments bei TLS-Änderungen ablaufen. Wir haben die Steps 4 bis 9 exakt befolgt, inklusive Post-Deploy Smoke Tests und Metrics Check in Grafana."}
{"ts": "158:57", "speaker": "I", "text": "How did you ensure that these Grafana checks were actually tied to early warning alerts?"}
{"ts": "159:02", "speaker": "E", "text": "We had Prometheus alert rules bound to the same metrics—latency, handshake errors—feeding into Alertmanager. We also configured a lower severity 'pre-warning' that goes to the #gw-observe Slack channel, so devs can react before SLOs are breached."}
{"ts": "159:12", "speaker": "I", "text": "Interessant. Können Sie ein Beispiel geben, wo eine Änderung aus Aegis IAM direkte Auswirkungen auf Orion hatte?"}
{"ts": "159:17", "speaker": "E", "text": "Klar, Ticket IAM-7754: Aegis hat die JWT-Lifetime von 60 auf 30 Minuten reduziert. Unsere Gateway-Session-Cache-Logik musste angepasst werden, sonst hätten wir erhöhte Re-Auth-Requests und damit Latenzspitzen gesehen."}
{"ts": "159:26", "speaker": "I", "text": "Wie sind Sie das angegangen?"}
{"ts": "159:30", "speaker": "E", "text": "Wir haben im Go-basierten Auth-Middleware Modul den Cache-Invalidierungs-Job von 55 auf 25 Minuten umgestellt und parallel ein Load-Test-Szenario gefahren, um sicherzugehen, dass p95 stabil bleibt."}
{"ts": "159:39", "speaker": "I", "text": "Looking back, what was the main trade-off between quickly adapting to IAM changes and maintaining Gateway stability?"}
{"ts": "159:44", "speaker": "E", "text": "The main trade-off was speed vs. regression risk. Deploying fast reduced the window of inconsistency, but increased the chance of auth regressions. We mitigated by dark-launching the new cache policy under a 10% traffic slice before full rollout, in line with Safety First and BLAST_RADIUS reduction."}
{"ts": "159:39", "speaker": "I", "text": "Lassen Sie uns kurz auf die Cross-Project Abhängigkeiten zurückkommen. Wie haben Sie konkret die IAM-Policies aus dem Aegis IAM in den Orion Edge Gateway Build integriert?"}
{"ts": "159:46", "speaker": "E", "text": "Also, wir haben da einen dedizierten Sync-Prozess via Terraform Module etabliert, der die Policy-Definitionen aus dem Aegis-Repo zieht. The tricky part was mapping the role-based claims into our JWT validation layer without breaking backward compatibility."}
{"ts": "159:57", "speaker": "I", "text": "Gab es da Konflikte mit bestehenden Auth-Flows des Gateways?"}
{"ts": "160:00", "speaker": "E", "text": "Ja, besonders bei den legacy clients, die noch HMAC-based tokens nutzen. We had to implement a dual validation path, documented in RFC-ORI-112, um sowohl die neuen mTLS/JWT Flows als auch die alten zu bedienen."}
{"ts": "160:14", "speaker": "I", "text": "Und wie haben Sie die mTLS Policy Abstimmung mit Poseidon Networking gelöst?"}
{"ts": "160:18", "speaker": "E", "text": "Da hatten wir ein Joint-Change-Board Meeting, und wir haben den Poseidon mTLS handshake timeout von 3s auf 5s erhöht, based on the incident GW-4821 post-mortem learnings. Zusätzlich haben wir eine Canary-Teststrecke im Stage-Netz eingeführt."}
{"ts": "160:34", "speaker": "I", "text": "Können Sie ein Beispiel nennen, wo eine Änderung in einem anderen Projekt direkte Auswirkungen auf Orion hatte?"}
{"ts": "160:38", "speaker": "E", "text": "Klar, als Poseidon die Cipher Suite Liste in PN-RFC-77 geändert hat, sind uns in Orion plötzlich die handshake error rates von 0.2% auf 3% hochgeschnellt. We had to hotfix our Envoy proxy configs to align."}
{"ts": "160:53", "speaker": "I", "text": "Wenn wir auf Optimierung schauen: Welche Metriken nutzen Sie, um Skalierungsentscheidungen zu treffen?"}
{"ts": "160:58", "speaker": "E", "text": "Wir monitoren p95 Latency, concurrent connections und CPU saturation > 75% als Trigger. Additionally, we correlate error rates with upstream dependencies to avoid false positives."}
{"ts": "161:09", "speaker": "I", "text": "Wie priorisieren Sie Performance-Verbesserungen im Hinblick auf BLAST_RADIUS Reduktion?"}
{"ts": "161:14", "speaker": "E", "text": "Unser Heuristik: erst isolieren in einzelne Gateway-Zonen, dann Verbesserungen in low-traffic zones testen. This way, any regression has minimal impact, documented in Runbook RB-GW-015."}
{"ts": "161:26", "speaker": "I", "text": "Gab es einen Trade-off zwischen Rate Limiting und User Experience?"}
{"ts": "161:30", "speaker": "E", "text": "Ja, wir mussten das Burst-Limit von 200 auf 150 req/s senken, um Upstream-APIs zu schützen. That caused slight delays for premium users, but SLA-ORI-02 remained green."}
{"ts": "161:42", "speaker": "I", "text": "Wie stellen Sie sicher, dass Änderungen den 'Safety First'-Wert erfüllen, auch unter Zeitdruck?"}
{"ts": "161:47", "speaker": "E", "text": "Wir haben ein Safety Checklist Template in jeder Merge-Request Pipeline, plus mandatory approval von SRE Leads. Even in hotfixes, at least one peer review is enforced, per Policy POL-SAFE-07."}
{"ts": "161:39", "speaker": "I", "text": "Lassen Sie uns nochmal auf die Cross-Project Dependencies eingehen. Wie genau haben Sie damals die Policies aus Aegis IAM in das Orion Gateway integriert?"}
{"ts": "161:46", "speaker": "E", "text": "Das war ein mehrstufiger Prozess. Zuerst haben wir die JSON-basierten Policy-Templates aus dem Aegis Repo via einem internen IaC Modul geladen, dann in Terraform als data source gemappt. Danach mussten wir die Claims-Mappings anpassen, um sie kompatibel mit dem Orion Auth Filter zu machen."}
{"ts": "161:58", "speaker": "I", "text": "Gab es dabei spezielle Challenges mit Poseidon Networking, insbesondere bei mTLS Policies?"}
{"ts": "162:05", "speaker": "E", "text": "Ja, Poseidon hatte zum Zeitpunkt der Integration ein anderes Cipher Suite Default-Set. Wir mussten ein RFC-Change-Doc, RFC-GW-221, einreichen, um das Alignment zu erreichen. Das bedeutete, dass wir temporär einen Dual-Handshake-Mode implementierten."}
{"ts": "162:19", "speaker": "I", "text": "Dual-Handshake klingt aufwändig. Wie haben Sie das in den Deployment Pipelines abgebildet?"}
{"ts": "162:25", "speaker": "E", "text": "Wir haben in der GitLab-CI Pipeline ein zusätzliches Stage 'mtls-validation' eingeführt. Dort liefen Integration Tests gegen beide Cipher Sets. Das war nur für drei Wochen aktiv, bis die Poseidon Nodes das neue Set ausgerollt hatten."}
{"ts": "162:38", "speaker": "I", "text": "Wenn wir auf die Optimierung schauen: Welche Metriken nutzen Sie, um Scaling-Entscheidungen zu treffen?"}
{"ts": "162:44", "speaker": "E", "text": "Primär p95 Latency und Error Rate > 0.2% über 5 Minuten, kombiniert mit dem Queue Depth im Ingress Buffer. Zusätzlich haben wir ein internes KPI 'Hops per Request', das anzeigt, ob Routing-Optimierungen wirken."}
{"ts": "162:57", "speaker": "I", "text": "Interesting. And how do you balance rate limiting with user experience when you see those metrics spike?"}
{"ts": "163:04", "speaker": "E", "text": "We use a staged degradation. Zuerst erhöhen wir die bucket size für trusted clients leicht, um deren UX zu schützen, while applying stricter limits to anonymous traffic. That way we keep SLA-ORI-02 in range without cutting off key partners."}
{"ts": "163:18", "speaker": "I", "text": "Gab es einen Fall, wo eine Änderung in Aegis oder Poseidon direkte Auswirkungen auf Orion hatte, die Sie nicht vorhergesehen hatten?"}
{"ts": "163:25", "speaker": "E", "text": "Ja, als Aegis die JWT-Lifetime von 15 auf 5 Minuten reduzierte (Ticket IAM-1342), brachen unsere long-running WebSocket Sessions alle 5 Minuten. Das mussten wir durch ein Heartbeat-Refresh im Gateway-Reverse-Proxy fixen."}
{"ts": "163:39", "speaker": "I", "text": "Wie haben Sie diesen Fix koordiniert?"}
{"ts": "163:44", "speaker": "E", "text": "Wir haben einen Hotfix-Branch erstellt, basierend auf dem letzten Green Build, und ein Runbook RB-GW-015 angewendet, das den Zero-Downtime Patch beschreibt. Parallel lief ein Canary-Test in einer Staging-Zone."}
{"ts": "163:57", "speaker": "I", "text": "Sounds like a multi-hop dependency resolution. Did you also update the monitoring for it?"}
{"ts": "164:03", "speaker": "E", "text": "Ja, wir haben einen Prometheus Alert 'ws_auth_refresh_error' hinzugefügt. That alert ties into our Incident Response ChatOps bot, so we can act within 2 minutes if the refresh fails again."}
{"ts": "162:15", "speaker": "I", "text": "Lassen Sie uns noch mal auf die Cross-Project Dependencies eingehen: Wie haben Sie konkret die Aegis IAM-Policies auf das Orion Edge Gateway gemappt?"}
{"ts": "162:20", "speaker": "E", "text": "Ja, also… wir haben die IAM-Policies aus Aegis über einen Policy-Export im JSON-Format gezogen, dann via Terraform Data Sources in unser IaC übernommen. Das war nötig, damit wir die mTLS Regeln von Poseidon Networking in einer konsistenten Chain auswerten konnten."}
{"ts": "162:29", "speaker": "I", "text": "Okay, und gab es da Konflikte zwischen den Projekten?"}
{"ts": "162:33", "speaker": "E", "text": "Absolut. Zum Beispiel hat Poseidon ein strengeres Cipher-Suite-Set gefordert als Aegis. Wir mussten im Gateway eine Conditional Policy implementieren, die anhand der Service-ID entscheidet, welches Set aktiv ist. That required changes in our RB-GW-014 runbook."}
{"ts": "162:42", "speaker": "I", "text": "Wie haben Sie diese Conditional Policy getestet?"}
{"ts": "162:46", "speaker": "E", "text": "Wir haben das in einer Staging-Umgebung mit Traffic Replay getestet. Dabei haben wir die p95-Latency gemessen, um sicherzustellen, dass SLA-ORI-02 nicht gebrochen wird. Zusätzlich haben wir Synthetic mTLS Clients genutzt, die wir aus Ticket LAB-3272 gebaut hatten."}
{"ts": "162:57", "speaker": "I", "text": "Können Sie ein Beispiel nennen, wo Änderung in Poseidon direkten Einfluss hatte?"}
{"ts": "163:01", "speaker": "E", "text": "Ja, im Februar kam RFC-POS-221 rein, das den Default Port für mTLS von 8443 auf 9443 verlegte. Das hat unseren Gateway-Healthcheck lahmgelegt, weil der Hardcoded war. Wir mussten hotfixen, siehe Incident GW-5119."}
{"ts": "163:12", "speaker": "I", "text": "Wie sind Sie beim Hotfix vorgegangen?"}
{"ts": "163:16", "speaker": "E", "text": "Erst haben wir den Port als Environment Variable konfigurierbar gemacht, dann in der CI-Pipeline einen Step ergänzt, der Poseidon’s Config API abfragt. So konnten wir bei Deployments automatisch den aktuell gültigen Port setzen. That reduced downtime by 70%."}
{"ts": "163:27", "speaker": "I", "text": "Gab es Auswirkungen auf die Rate Limiting Konfiguration?"}
{"ts": "163:31", "speaker": "E", "text": "Nur indirekt. Weil die Healthchecks wieder funktionierten, wurde der Auto-Scaler nicht mehr falsch getriggert, was vorher zu aggressivem Rate Limiting geführt hatte. Wir haben das später in unserem Scaling Runbook RB-SC-009 dokumentiert."}
{"ts": "163:40", "speaker": "I", "text": "Wie fließen solche Lessons Learned in Ihre Risikoabwägungen ein?"}
{"ts": "163:44", "speaker": "E", "text": "Wir pflegen ein internes Confluence-Board mit 'Risk Patterns'. Da steht z.B. drin: 'External Port Change → Validate Healthchecks'. Bei neuen Auth-Mechanismen schauen wir gezielt, ob ähnliche Patterns greifen, um BLAST_RADIUS zu minimieren."}
{"ts": "163:53", "speaker": "I", "text": "Und wenn eine Änderung das SLA gefährden könnte?"}
{"ts": "163:57", "speaker": "E", "text": "Dann machen wir einen Pre-Deployment SLA-Check. Wir haben ein Skript 'sla_guard.sh', das die letzten 24h Metriken zieht und simuliert, ob die Änderung p95-Latency > 120ms treiben könnte. Falls ja, müssen wir ein Approval nach Policy SAF-CHK-002 einholen."}
{"ts": "164:45", "speaker": "I", "text": "Lassen Sie uns nochmal auf die Cross-Project Dependencies eingehen: how exactly did you coordinate mit dem Poseidon Networking Team bei der Anpassung der mTLS handshake parameters?"}
{"ts": "164:52", "speaker": "E", "text": "Ja, also wir hatten ein gemeinsames RFC-Dokument, RFC-POS-ORI-07, in dem wir die Cipher Suites und TLS Versionen abgestimmt haben. Die wöchentliche Sync-Session war bilingual, um Missverständnisse zu vermeiden. Ich habe dabei explizit Mapping-Tests in unserer Staging-Umgebung gefahren, before pushing changes to prod."}
{"ts": "164:59", "speaker": "I", "text": "Und diese Mapping-Tests, waren die Teil Ihrer CI/CD Pipeline oder eher ad-hoc?"}
{"ts": "165:06", "speaker": "E", "text": "Teil der Pipeline. Wir haben ein Stage namens `mtls-verify` eingeführt, scripted via Terraform und Ansible. Das war wichtig, weil Änderungen aus Poseidon manchmal breaking waren, vor allem wenn sie ihre Root CAs rotierten."}
{"ts": "165:14", "speaker": "I", "text": "Können Sie ein konkretes Beispiel nennen, where that stage actually prevented a fault?"}
{"ts": "165:21", "speaker": "E", "text": "Ja, Build 2024.05-rc2. Poseidon hatte kurzfristig von ECDSA zu RSA Zertifikaten gewechselt, und unser Gateway akzeptierte nur die ECDSA Chain. Der `mtls-verify` Stage schlug fehl, wir haben Ticket NET-OR-145 aufgemacht, und so einen Outage in prod vermieden."}
{"ts": "165:29", "speaker": "I", "text": "Interessant. Switching gears: how did this dependency influence your rate limiting strategy?"}
{"ts": "165:36", "speaker": "E", "text": "Durch die Latenzschwankungen bei TLS Handshakes mussten wir die initial burst allowance im Rate Limiter höher ansetzen, sonst hätten legitime Clients 429 Errors bekommen. Das haben wir in ConfigMap `gw-rl-prod` dokumentiert."}
{"ts": "165:43", "speaker": "I", "text": "Gab es dafür ein spezielles Monitoring?"}
{"ts": "165:50", "speaker": "E", "text": "Ja, wir tracken `handshake_duration_p95` und korrelieren das mit `rate_limit_hits`. Ein Prometheus Alert GW-HS95-ALERT feuert, wenn die p95 über 80ms liegt und gleichzeitig die 429-Rate um mehr als 5% steigt."}
{"ts": "165:58", "speaker": "I", "text": "Let’s talk scaling decisions. Was sind für Sie die Trigger, einen Gateway-Knoten hochzuskalieren?"}
{"ts": "166:05", "speaker": "E", "text": "Primär CPU > 70% über 5 Minuten plus concurrent connections > 15k. Aber wir berücksichtigen auch BLAST_RADIUS: lieber horizontal skalieren mit kleinen Instanzen, um die Ausfallfolgen zu minimieren."}
{"ts": "166:12", "speaker": "I", "text": "Und wenn scaling und Rate Limiting in Konflikt geraten? Trade-offs?"}
{"ts": "166:19", "speaker": "E", "text": "Da wägen wir ab: scaling kostet Ressourcen, aber strengeres Rate Limiting kann UX verschlechtern. In GW-DEC-019 habe ich dokumentiert, dass wir bei SLA-ORI-02 Violations zuerst skalieren und erst bei dauerhaftem Traffic-Spike die Limits anpassen."}
{"ts": "166:27", "speaker": "I", "text": "Wie stellen Sie sicher, dass diese Entscheidungen mit 'Safety First' aligned sind?"}
{"ts": "166:34", "speaker": "E", "text": "Wir nutzen das Safety-Checklist-Template aus Runbook RB-SAFE-004. Jede Änderung im Scaling- oder Rate-Limit-Config muss ein Risiko-Assessment beinhalten, inkl. Fallback-Plan. Ohne das keine Freigabe im Change Advisory Board."}
{"ts": "166:21", "speaker": "I", "text": "Könnten wir jetzt noch etwas tiefer auf die Optimierungsstrategien eingehen, speziell wie Sie die Rate Limiting Parameter anpassen, ohne dass die User Experience leidet?"}
{"ts": "166:27", "speaker": "E", "text": "Ja, klar. Wir haben eine zweistufige Logik eingebaut: zunächst ein soft limit mit Warnungen ab 80% Auslastung, und erst danach das harte limit. That way, wir geben Clients Zeit, sich anzupassen. Monitoring greift via Prometheus-Alert GW-RATE-AL-07."}
{"ts": "166:35", "speaker": "I", "text": "Verstehe. Und wie evaluieren Sie, ob diese Anpassung den BLAST_RADIUS tatsächlich reduziert?"}
{"ts": "166:40", "speaker": "E", "text": "Wir fahren kontrollierte Load-Tests in einer Staging-Zone, simulieren Traffic-Spitzen, und messen p95 Latenz. If latency stays under SLA-ORI-02 even during throttle, dann gilt die Maßnahme als erfolgreich."}
{"ts": "166:49", "speaker": "I", "text": "Gab es in letzter Zeit einen Vorfall, der diese Tests besonders validiert hat?"}
{"ts": "166:54", "speaker": "E", "text": "Ja, Ticket INC-GW-5632 vor drei Wochen. Ein unerwarteter Traffic-Burst von einem fehlerhaften Client. Die Limits griffen wie geplant; dadurch blieb der Ausfall auf einen Node beschränkt."}
{"ts": "167:02", "speaker": "I", "text": "Wie war da die Zusammenarbeit mit dem SRE-Team?"}
{"ts": "167:07", "speaker": "E", "text": "Wir haben im Runbook RB-GW-011 gleich eine Eskalationsmatrix. SRE sprang in Minute 3 ein, hat Logs korreliert mit Poseidon Netflow Daten, und wir konnten den fehlerhaften Client isolieren."}
{"ts": "167:15", "speaker": "I", "text": "Nutzen Sie für solche Analysen eher automatisierte Dashboards oder manuelle Logins?"}
{"ts": "167:20", "speaker": "E", "text": "Automated first. Grafana Dashboards sind mit Loki-Logstreams verknüpft. Bei Abweichung >15% vom Normwert gibt's einen Slack-Webhook an den Incident-Channel. Manual login nur für tiefergehende Trace-Analysen."}
{"ts": "167:29", "speaker": "I", "text": "Wie fließen diese Erkenntnisse zurück ins Build-Team?"}
{"ts": "167:34", "speaker": "E", "text": "Wir haben einen wöchentlichen DevOps-Sync, wo wir solche Incidents als Lessons Learned einbringen. Outcome sind häufig kleine IaC-Änderungen, z.B. Terraform module tweaks for better autoscaling."}
{"ts": "167:42", "speaker": "I", "text": "Gab es mal einen Fall, wo eine solche Änderung ein neues Risiko geschaffen hat?"}
{"ts": "167:47", "speaker": "E", "text": "Ja, bei TF-Modul 3.2 haben wir versehentlich die mTLS handshake timeout zu niedrig gesetzt. That caused premature disconnects. Wir haben das binnen 2 Stunden revertet, aber es war eine klare Safety First Entscheidung."}
{"ts": "167:56", "speaker": "I", "text": "Und welche Gegenmaßnahmen haben Sie danach implementiert?"}
{"ts": "168:01", "speaker": "E", "text": "Wir haben eine zusätzliche Staging-Prüfung eingeführt, die handshake timeouts unter diversen Latenzprofilen testet. Plus, ein Review-Step im CI-Pipeline YAML, bevor TF-Änderungen ins Main-Branch gehen."}
{"ts": "167:21", "speaker": "I", "text": "Lassen Sie uns nochmal auf das Thema Kapazitätsplanung zurückkommen – welche Metriken nutzen Sie konkret, um zu entscheiden, wann ein Gateway-Knoten skaliert werden muss?"}
{"ts": "167:29", "speaker": "E", "text": "Primär schauen wir auf p95 Latenz, CPU-Auslastung über 70 % über mehr als 5 Minuten und Queue-Tiefen. Zusätzlich haben wir im Runbook RB-SCALE-07 definiert, dass bei Memory-Leak-Indikatoren > 120 MB/h Wachstum ein manueller Review startet. That way we avoid blind auto-scaling that might mask deeper issues."}
{"ts": "167:46", "speaker": "I", "text": "Haben Sie ein Beispiel, wo Sie dadurch unnötiges Scaling vermeiden konnten?"}
{"ts": "167:51", "speaker": "E", "text": "Ja, im Ticket GW-5123 trat p95 Latenz-Spikes auf, aber es lag an einer fehlerhaften Rate-Limiting-Regel, die wir in zwei Stunden gefixt haben. Scaling hätte die SLA-ORI-02 kurzfristig gerettet, aber den Root Cause verschleiert."}
{"ts": "168:05", "speaker": "I", "text": "Interessant. Apropos Rate Limiting – gab es da Trade-offs zur User Experience?"}
{"ts": "168:11", "speaker": "E", "text": "Ja, wir mussten zwischen aggressiven Limits (zum Schutz der Backend-Services) und einer smooth UX abwägen. In einem AB-Test mit 5 % Traffic haben wir die Limits leicht gelockert, wodurch die Fehlerrate um 0,8 % sank, ohne die BLAST_RADIUS Policy zu verletzen."}
{"ts": "168:27", "speaker": "I", "text": "Und wie binden Sie solche Erkenntnisse in Ihre Deployment-Pipelines ein?"}
{"ts": "168:32", "speaker": "E", "text": "Wir haben einen Canary-Stage in der GitOps-Pipeline ergänzt, der parallel neue Rate-Limit-Configs deployt und Metriken wie Error Budget Consumption live tracked. This is aligned with RB-GW-011 on rolling deployments."}
{"ts": "168:47", "speaker": "I", "text": "Gab es bei diesen Canary-Deployments besondere Herausforderungen?"}
{"ts": "168:52", "speaker": "E", "text": "Ja, vor allem die Synchronisation mit Poseidon Networking wegen mTLS Session Resets. Wir mussten ein Pre-Deployment-Hook schreiben, der mit PN-API v2 prüft, ob alle Edge-Nodes die neue Policy schon cachen."}
{"ts": "169:08", "speaker": "I", "text": "Wie stellen Sie sicher, dass solche Hooks nicht selbst zum Risiko werden?"}
{"ts": "169:13", "speaker": "E", "text": "Wir haben eine Safety-First Checklist eingeführt: Code Review durch zwei Maintainer, Unit-Tests mit Mock-Poseidon-Endpoints und ein Dry-Run-Mode, der Logs statt Changes sendet. Außerdem gibt's einen Fallback in Form von RB-HOOK-02."}
{"ts": "169:29", "speaker": "I", "text": "Wir haben vorhin über SLA-Risikoabwägungen gesprochen – können Sie ein weiteres Beispiel geben?"}
{"ts": "169:35", "speaker": "E", "text": "Im Incident GW-5299 gab es eine Auth-Mechanismus-Änderung aus Aegis IAM. Wir hätten in 4 Std. patchen können, aber ohne vollständige Security-Review. Wir entschieden uns für 12 Std. Downtime auf einem Teilsegment, um Safety First und SLA-ORI-02 möglichst beide zu erfüllen."}
{"ts": "169:53", "speaker": "I", "text": "Und wie haben Sie das intern kommuniziert?"}
{"ts": "169:57", "speaker": "E", "text": "Über den Incident-Kanal und ein Post-Mortem im Confluence, wo wir die Entscheidungsschritte, beteiligte SLAs und die Bezugnahme auf RFC-SEC-014 dokumentiert haben. Transparency is key to maintain trust across teams."}
{"ts": "172:21", "speaker": "I", "text": "Könnten wir jetzt noch etwas detaillierter auf die Fehlerbehandlung eingehen, speziell wie Sie Runbooks im Incident Response Prozess nutzen?"}
{"ts": "172:29", "speaker": "E", "text": "Ja klar, also im Orion Kontext haben wir für jeden kritischen Pfad ein dediziertes Runbook in Confluence. The MTLS handshake section ist mit Steps für packet capture, log correlation und TLS config diff versehen. Das hilft dem On-Call-Team, in unter 15 Minuten die Root Cause zumindest einzugrenzen."}
{"ts": "172:46", "speaker": "I", "text": "Wie binden Sie diese Runbooks in Ihre Monitoring-Tools ein?"}
{"ts": "172:54", "speaker": "E", "text": "Wir haben in Grafana Alert Rules, die direkt einen Link zum relevanten Runbook triggern. Also wenn der Alert ORI-MTLS-ERR > 5/min schlägt, öffnet sich in PagerDuty automatisch der Runbook-Abschnitt. That reduces cognitive load during high-pressure incidents."}
{"ts": "173:12", "speaker": "I", "text": "Gab es einen Fall, wo das besonders geholfen hat?"}
{"ts": "173:18", "speaker": "E", "text": "Ja, beim Incident GW-5178, da war ein Rate Limiting Modul in Kombination mit mTLS fehlerhaft. Ohne Runbook hätten wir sicher eine Stunde länger gebraucht. Der Schritt 'Temporär RL-Filter deaktivieren' stand da drin, hat sofort den Traffic stabilisiert."}
{"ts": "173:36", "speaker": "I", "text": "Switching gears – welche Metriken sind für Sie entscheidend, um Skalierungsentscheidungen zu treffen?"}
{"ts": "173:43", "speaker": "E", "text": "Primär schauen wir auf p95 Latency und CPU Utilization pro Gateway-Knoten. Wenn p95 > 100 ms bei >70% CPU über 5 Minuten, triggert unsere Auto-Scaler Policy. We also consider connection churn rates, especially für WebSocket clients."}
