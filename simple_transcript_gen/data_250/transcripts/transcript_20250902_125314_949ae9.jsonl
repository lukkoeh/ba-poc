{"ts": "00:00", "speaker": "I", "text": "Können Sie bitte den aktuellen Stand des Titan DR Projekts schildern, damit wir alle auf demselben Level starten?"}
{"ts": "02:15", "speaker": "E", "text": "Ja, gern. Wir befinden uns aktuell in der Drill-Phase, also kein echter Failover, sondern ein geplanter Simulationstest. Die letzten zwei Wochen haben wir alle Runbooks und Playbooks gegen die aktuellen Cloud-Region-Setups validiert. Der Kernstatus: 85 % der kritischen Systeme sind für automatisierten Failover vorbereitet."}
{"ts": "05:10", "speaker": "I", "text": "Und welche Hauptziele verfolgen wir mit diesem Drill, und an welchen KPIs orientieren Sie sich?"}
{"ts": "07:30", "speaker": "E", "text": "Primär wollen wir die SLA-Vorgaben testen – also Recovery Time Objective (RTO) von 45 Minuten und Recovery Point Objective (RPO) von 15 Minuten. Wir messen zusätzlich den Mean Time to Detect (MTTD) und den Mean Time to Recover (MTTR) pro Service."}
{"ts": "10:00", "speaker": "I", "text": "Wie sieht denn aktuell die Multi-Region-Architektur aus, mit der Sie diese Ziele erreichen wollen?"}
{"ts": "13:05", "speaker": "E", "text": "Wir nutzen zwei Hauptregionen, West-Europa als Primär und Nord-Europa als Sekundär. Die kritischen Datenbanken replizieren wir synchron, Applikations-VMs asynchron über Messaging-Queues. Load Balancer und API-Gateways sind per DNS-Failover gekoppelt."}
{"ts": "17:15", "speaker": "I", "text": "Welche kritischen Systeme sind konkret im Failover-Scope enthalten?"}
{"ts": "20:00", "speaker": "E", "text": "Im Scope sind alle Customer-Facing APIs, das zentrale IAM-System, und die Billing-Engine. Interne Tools wie das DevOps-Portal lassen wir bewusst außen vor, um den BLAST_RADIUS zu minimieren."}
{"ts": "23:45", "speaker": "I", "text": "Stichwort BLAST_RADIUS – wie haben Sie den im Entwurf minimiert?"}
{"ts": "27:10", "speaker": "E", "text": "Durch Segmentierung der Netzwerke und Service-Mesh-Isolation. Wir haben in RB-DR-001 festgelegt, dass nicht betroffene Microservices im Primär-Cluster verbleiben, um unnötige Replikationskosten zu vermeiden."}
{"ts": "30:45", "speaker": "I", "text": "Können Sie etwas zu den Runbooks sagen, die im Ernstfall zum Einsatz kommen?"}
{"ts": "34:20", "speaker": "E", "text": "Klar, das wichtigste ist RB-DR-001 für den initialen Region-Switch, dann RB-DR-004 für Datenbank-Failover. Diese haben wir im letzten Drill durchgespielt, inklusive manueller Checkliste für IAM-Token-Refresh."}
{"ts": "38:00", "speaker": "I", "text": "Wie hoch ist der Automatisierungsgrad bei der Failover-Auslösung derzeit?"}
{"ts": "42:15", "speaker": "E", "text": "Etwa 70 %. Der Triggervorgang für den DNS-Switch ist automatisiert, genauso wie die Datenbank-Replikationsumschaltung. Manuell bleibt die Validierung der Service-Integrität via Observability-Dashboards."}
{"ts": "47:25", "speaker": "I", "text": "Gibt es manuelle Schritte, die noch nicht automatisiert sind, und warum?"}
{"ts": "54:00", "speaker": "E", "text": "Ja, vor allem Security- und Compliance-Checks. Diese erfordern menschliche Beurteilung, z. B. Ticketprüfung in SEC-DR-2025-07, weil automatisierte Systeme die regulatorischen Nuancen nicht sicher abdecken können."}
{"ts": "90:00", "speaker": "I", "text": "Könnten Sie bitte genauer auf die eingesetzten Runbooks eingehen, speziell RB-DR-001, und welche Rolle es im Failover-Prozess spielt?"}
{"ts": "90:18", "speaker": "E", "text": "Ja, RB-DR-001 ist unser primäres Disaster-Recovery-Runbook. Es beschreibt Schritt für Schritt, wie wir von der Failover-Auslösung bis zur Validierung der Dienste vorgehen. Es enthält z.B. automatisierte Terraform-Skripte für das Hochfahren der Ersatzinfrastruktur und die Umschaltung der DNS-Zonen."}
{"ts": "90:45", "speaker": "I", "text": "Und wie hoch ist aktuell der Automatisierungsgrad bei dieser Auslösung?"}
{"ts": "91:00", "speaker": "E", "text": "Wir liegen bei etwa 85 %. Die Auslösung selbst ist vollautomatisch über unseren Orchestrator 'NovaOps'. Allerdings gibt es noch manuelle Freigaben für kritische Schritte, z.B. das Aktivieren der Payment-API in der Ersatzregion, um Fehlbuchungen zu verhindern."}
{"ts": "91:27", "speaker": "I", "text": "Welche Schritte sind das genau, die noch nicht automatisiert sind, und warum?"}
{"ts": "91:42", "speaker": "E", "text": "Das betrifft vor allem IAM-Rollenzuweisungen. Wir haben festgestellt, dass automatisierte IAM-Änderungen in einer DR-Situation riskant sein können, weil sie unbeabsichtigte Rechteöffnungen verursachen könnten. Deshalb bleiben diese in Runbook RB-DR-003 manuell, mit 4-Augen-Prinzip."}
{"ts": "92:10", "speaker": "I", "text": "Verstehe. Wie sieht es mit Abhängigkeiten zu Poseidon Networking oder Nimbus Observability aus?"}
{"ts": "92:27", "speaker": "E", "text": "Poseidon stellt das inter-regionale VPN-Mesh bereit. Ohne das könnten die replizierten Datenbanken nicht synchronisieren. Nimbus liefert uns die Metriken, z.B. Latenz zwischen den Regionen, die wiederum Trigger in NovaOps auslösen können. Hier ist die Verzahnung sehr eng."}
{"ts": "92:55", "speaker": "I", "text": "Und wie werden API-Gateways und IAM-Systeme im DR-Fall berücksichtigt?"}
{"ts": "93:11", "speaker": "E", "text": "API-Gateways werden als Teil des Services 'Gateway-Cluster-DR' mit repliziert. Wir nutzen hier ein Blue-Green-Setup, so dass im Failover nur ein Switch des DNS-Records erfolgt. IAM-Systeme sind in einem separaten Tenant gespiegelt, um den BLAST_RADIUS zu begrenzen."}
{"ts": "93:42", "speaker": "I", "text": "Wie war der Ablauf beim letzten GameDay-Test, TEST-DR-2025-Q1?"}
{"ts": "94:00", "speaker": "E", "text": "Wir haben am 15. Februar ein simuliertes Ausfall-Szenario für Region 'eu-central-2' gefahren. Innerhalb von 38 Minuten waren alle kritischen Services in 'eu-west-1' hochgefahren. Allerdings gab es bei der Observability-Anbindung eine Verzögerung von 7 Minuten wegen eines fehlerhaften Prometheus-Exporters, siehe Ticket DR-INC-221."}
{"ts": "94:34", "speaker": "I", "text": "Welche Probleme wurden identifiziert und wie adressiert?"}
{"ts": "94:48", "speaker": "E", "text": "Neben dem Exporter-Problem haben wir gesehen, dass das Payment-System im Failover zunächst im Read-Only-Modus startete. Das war so gewollt, aber die Umschaltung auf Read-Write wurde verzögert. Wir haben im Runbook RB-DR-005 nun einen automatischen Healthcheck eingebaut, der das beschleunigt."}
{"ts": "95:15", "speaker": "I", "text": "Welche Metriken nutzen Sie, um die Wirksamkeit der DR-Strategie zu bewerten?"}
{"ts": "95:29", "speaker": "E", "text": "Hauptsächlich Recovery Time Actual (RTA) und Recovery Point Actual (RPA) im Vergleich zu den SLA-Vorgaben, plus Service Availability in den ersten 60 Minuten nach Failover. Wir tracken auch die Anzahl manueller Eingriffe, um den Automatisierungsgrad zu messen."}
{"ts": "104:00", "speaker": "I", "text": "Könnten Sie bitte genauer auf RB-DR-001 eingehen? Mich interessiert, welche Schritte dort für den Start des Failovers definiert sind."}
{"ts": "104:15", "speaker": "E", "text": "Ja, klar. RB-DR-001 beschreibt in sieben Hauptschritten, wie wir beim Ausfall der Primärregion vorgehen. Schritt eins ist die Auslösung des DR-Trigger-Jobs im Orchestrator, danach folgen API-Calls an Poseidon Networking, um die DNS-Zuordnung umzuschalten."}
{"ts": "104:37", "speaker": "E", "text": "Im dritten Schritt aktivieren wir in der Sekundärregion die Compute-Pools, laut Konfigdatei conf-dr-2025.yml. Schritt vier bis sechs behandeln Datenbank-Replikation und IAM-Sync, und der letzte Schritt ist die Validierung über Nimbus Observability Dashboards."}
{"ts": "104:59", "speaker": "I", "text": "Und wie hoch ist aktuell der Automatisierungsgrad dieser Runbook-Schritte?"}
{"ts": "105:06", "speaker": "E", "text": "Etwa 85 % laufen vollautomatisch, ausgelöst durch den DR-Trigger. The remaining 15% require manual verification, for example checking specific audit logs before the DNS cutover."}
{"ts": "105:23", "speaker": "I", "text": "Warum sind genau diese 15% noch manuell?"}
{"ts": "105:29", "speaker": "E", "text": "Das liegt an internen Compliance-Vorgaben. Wir haben ein internes RFC-Template RFC-DR-042, das vor kritischen Umschaltungen eine menschliche Freigabe verlangt, um regulatorische Risiken zu minimieren."}
{"ts": "105:46", "speaker": "I", "text": "Verstanden. Welche weiteren Abhängigkeiten bestehen zu Poseidon Networking oder Nimbus Observability, die im DR-Fall kritisch sind?"}
{"ts": "105:55", "speaker": "E", "text": "Poseidon liefert nicht nur DNS, sondern auch Layer-4 Load Balancing. Nimbus Observability ist unser primäres Alerting- und Metriksystem. Wenn Nimbus nicht korrekt auf die Sekundärregion umschaltet, würden wir Blindflug riskieren."}
{"ts": "106:14", "speaker": "I", "text": "Wie binden Sie API-Gateways und IAM-Systeme in den DR-Prozess ein?"}
{"ts": "106:20", "speaker": "E", "text": "API-Gateways werden via Script dr_api_switch.sh auf Region-B Endpoints umgestellt. IAM-Sync läuft über ein Cross-Region-Replication-Feature, das in Runbook RB-DR-002 beschrieben ist."}
{"ts": "106:38", "speaker": "I", "text": "Kommen wir zum letzten GameDay-Test, TEST-DR-2025-Q1. Wie lief dieser ab?"}
{"ts": "106:46", "speaker": "E", "text": "Der Drill wurde am 15. Januar durchgeführt. We simulated a full outage of Region A, triggered RB-DR-001, und beobachteten dann die KPIs: RTO lag bei 42 Minuten, RPO bei 18 Sekunden."}
{"ts": "107:05", "speaker": "I", "text": "Gab es Probleme, die Sie danach adressieren mussten?"}
{"ts": "107:10", "speaker": "E", "text": "Ja, wir hatten in Ticket DR-INC-7789 dokumentiert, dass zwei Microservices falsche Endpoints gecacht hatten. Lösung war ein Update im Service-Mesh-Config, um TTLs auf 5s zu setzen."}
{"ts": "107:28", "speaker": "I", "text": "Welche Metriken nutzen Sie generell, um die Wirksamkeit der DR-Strategie zu bewerten?"}
{"ts": "107:40", "speaker": "E", "text": "Neben RTO und RPO tracken wir den Automation Success Rate, die Alert Delivery Time und den Recovery Consistency Index, der misst, ob alle Systeme auf dem gleichen Stand sind."}
{"ts": "112:00", "speaker": "I", "text": "Kommen wir zum letzten GameDay-Test, der ja als TEST-DR-2025-Q1 protokolliert wurde. Können Sie bitte schildern, wie dieser im Detail ablief?"}
{"ts": "112:10", "speaker": "E", "text": "Ja, der Test startete um 02:00 UTC, simulierte einen kompletten Ausfall der Primärregion FRA-1, und wir haben gemäß Runbook RB-DR-001 sowie RB-DR-004 (für Datenbank-Failover) gearbeitet. Innerhalb von 11 Minuten war der Traffic umgeleitet."}
{"ts": "112:28", "speaker": "I", "text": "Gab es während dieses Ablaufs signifikante Probleme oder Abweichungen von den Erwartungen?"}
{"ts": "112:35", "speaker": "E", "text": "Ja, bei der Umschaltung des API-Gateways gab es eine Race Condition mit Poseidon Networking, wodurch 3% der Requests für 90 Sekunden fehlschlugen. Ticket DR-INC-4521 dokumentiert das."}
{"ts": "112:50", "speaker": "I", "text": "Und wie haben Sie dieses Problem anschließend adressiert?"}
{"ts": "112:56", "speaker": "E", "text": "Wir haben eine Sequenzverzögerung von 15 Sekunden zwischen DNS-Update und Gateway-Routing eingeführt und das in RB-DR-001 v2.3 ergänzt. Außerdem wurde ein Canary-Test in Nimbus Observability integriert."}
{"ts": "113:12", "speaker": "I", "text": "Welche Metriken nutzen Sie, um die Wirksamkeit Ihrer DR-Strategie zu bewerten?"}
{"ts": "113:18", "speaker": "E", "text": "Primär RTO und RPO, klar, aber auch Error Budget Consumption in den ersten 30 Minuten, Mean Time to Detect (MTTD) und Recovery Success Rate. Wir hatten im Drill 98,7% Recovery Success."}
{"ts": "113:33", "speaker": "I", "text": "Lassen Sie uns zu den Kosten-Performance-Tradeoffs kommen: Wie balancieren Sie bei der Regionenauswahl?"}
{"ts": "113:40", "speaker": "E", "text": "Wir wählen sekundäre Regionen mit moderater Latenz, z.B. DUB-2 statt NYC-3, um Netzwerk-Kosten gering zu halten, akzeptieren aber 20ms höhere RTT. Wir kalkulieren das gegen SLA-Puffer."}
{"ts": "113:55", "speaker": "I", "text": "Welche Optimierungen haben Sie zur Reduzierung von Standby-Kosten implementiert?"}
{"ts": "114:02", "speaker": "E", "text": "Wir fahren Core-Services im Warm Standby mit AutoPause nach 15 Minuten Idle, und Storage ist via S3-IA-Equivalent ausgelagert. Laut Kostenreport KST-APR-2025 spart das 22%."}
{"ts": "114:18", "speaker": "I", "text": "Sehen Sie Risiken bei dieser aktuellen Kostenstrategie?"}
{"ts": "114:24", "speaker": "E", "text": "Ja, wenn ein Failover in Peak-Zeiten erfolgt, kann das Hochfahren aus AutoPause bis zu 3 Minuten dauern. Das ist innerhalb des RTO, aber verringert den Puffer. Außerdem besteht Risiko bei Storage-Rehydration."}
{"ts": "114:39", "speaker": "I", "text": "Zum Abschluss: Welche Änderungen planen Sie für die nächste Drill-Phase?"}
{"ts": "114:46", "speaker": "E", "text": "Wir wollen RB-DR-005 für IAM-Failover einführen, regulatorische Vorgaben aus der neuen EU-Cloud-Resilienz-Verordnung einarbeiten und die Runbooks vierteljährlich durch automatisierte Simulationen verifizieren."}
{"ts": "120:00", "speaker": "I", "text": "Kommen wir nun zur Teststrategie. Wie ist der letzte GameDay-Test, also TEST-DR-2025-Q1, konkret abgelaufen?"}
{"ts": "120:25", "speaker": "E", "text": "Der Test begann mit einem simulierten Ausfall der primären Region Frankfurt. Wir folgten strikt dem Ablauf in RB-DR-001 und den zugehörigen Checklisten aus RB-DR-004 für Netzwerkvalidierung. Die Failover-Auslösung wurde um 09:00 MEZ gestartet, und wir konnten innerhalb von 14 Minuten auf die sekundäre Region in Stockholm umschalten."}
{"ts": "120:55", "speaker": "I", "text": "Gab es während des Tests unerwartete Probleme?"}
{"ts": "121:10", "speaker": "E", "text": "Ja, bei einem Subsystem für Batch-Processing kam es zu einer Verzögerung, weil ein Legacy-Skript nicht mit dem neuen IAM-Token-Refresh harmonierte. Das wurde als Incident INC-DR-342 im Ticket-System erfasst und wir haben anschließend einen Patch in das Runbook aufgenommen."}
{"ts": "121:40", "speaker": "I", "text": "Welche KPIs haben Sie zur Bewertung der Wirksamkeit genutzt?"}
{"ts": "121:58", "speaker": "E", "text": "Wir haben RTO und RPO gemessen, zusätzlich die Mean Time to Detect (MTTD) und den Service Availability Index. Besonders im Fokus: das Einhalten der 15-Minuten-RTO laut SLA-DR-2023. Der Drill lag bei 14 Minuten und 12 Sekunden, also im grünen Bereich."}
{"ts": "122:28", "speaker": "I", "text": "Jetzt zum Thema Kosten und Performance – wie balancieren Sie die Wahl der Regionen?"}
{"ts": "122:50", "speaker": "E", "text": "Wir führen quartalsweise Cost-Benefit-Analysen durch. Stockholm bietet gute Latenz zu unseren EU-Kunden und moderate Betriebskosten. Andere Regionen wie Singapore sind teurer im Standby, daher nutzen wir dort nur Cold Standby für sekundäre Services."}
{"ts": "123:20", "speaker": "I", "text": "Welche Optimierungen haben Sie umgesetzt, um Standby-Kosten zu reduzieren?"}
{"ts": "123:36", "speaker": "E", "text": "Wir haben Auto-Shutdown-Skripte implementiert, die nichtkritische VMs in der DR-Region nach den wöchentlichen Tests herunterfahren. Außerdem verwenden wir Shared Storage Snapshots anstelle von replizierten Volumes für statische Daten, gemäß RFC-DR-219."}
{"ts": "124:05", "speaker": "I", "text": "Welche Risiken sehen Sie bei dieser Kostenstrategie?"}
{"ts": "124:20", "speaker": "E", "text": "Ein Risiko ist, dass bei einem sofortigen echten Failover die Kaltstart-Zeit für manche Systeme höher ist. Wir mitigieren das, indem wir vierteljährlich Full Warm Starts simulieren und die Kaltstart-Zeit dokumentieren."}
{"ts": "124:48", "speaker": "I", "text": "Und wie sieht die Roadmap für die nächste Drill-Phase aus?"}
{"ts": "125:02", "speaker": "E", "text": "Wir planen, das DR-Setup um eine dritte Region in Warschau zu erweitern, um regulatorische Anforderungen der neuen EU-Cloud-Compliance-Verordnung EU-CCV-2026 zu erfüllen. Außerdem sollen Runbooks halbjährlich auditiert werden."}
{"ts": "125:32", "speaker": "I", "text": "Wie stellen Sie sicher, dass die Runbooks aktuell bleiben?"}
{"ts": "125:50", "speaker": "E", "text": "Wir haben ein internes Review-Board, das jede Änderung in Jira als CR-Task erfasst, z. B. CR-DR-558. Änderungen werden im Staging-Cluster getestet, bevor sie ins produktive Runbook-Repository übernommen werden."}
{"ts": "136:00", "speaker": "I", "text": "Bevor wir zum Abschluss kommen, würde mich interessieren, welche Lessons Learned aus dem letzten Drill Sie persönlich als kritisch einstufen."}
{"ts": "136:10", "speaker": "E", "text": "Eines der größten Learnings war ganz klar, dass unser Failover-Prozess für die Authentifizierungsserver in der Region West deutlich zu lang war. Laut Ticket DR-INC-472 haben wir knapp 15 Minuten allein für DNS-Propagation benötigt."}
{"ts": "136:25", "speaker": "I", "text": "Gab es dafür eine konkrete Ursachenanalyse, oder war das eher ein generelles Problem der Infrastruktur?"}
{"ts": "136:35", "speaker": "E", "text": "Wir haben festgestellt, dass die TTL-Werte in der Poseidon Networking-Konfiguration nicht einheitlich waren. Einige CNAME-Records hatten 300 Sekunden, andere 1800 Sekunden. Das steht so nicht in Runbook RB-DR-003, daher war es ein blind spot."}
{"ts": "136:50", "speaker": "I", "text": "Und wie wurden diese Inkonsistenzen behoben?"}
{"ts": "137:00", "speaker": "E", "text": "Wir haben ein Skript im Automation-Repo ergänzt, das bei jedem Drill die Zonendateien prüft. Zusätzlich wurde im Nimbus Observability ein Alert eingerichtet, der uns auf TTL-Abweichungen hinweist."}
{"ts": "137:15", "speaker": "I", "text": "Das ist eine gute Prävention. Gab es weitere technische Stolpersteine?"}
{"ts": "137:25", "speaker": "E", "text": "Ja, der Storage-Sync zwischen den Regionen hat bei einem der kritischen Datenbanken nur 85 % des RPO erfüllt. Laut SLA für P-TIT sollte der RPO bei max. 5 Minuten liegen, wir hatten 7 Minuten Verzögerung."}
{"ts": "137:40", "speaker": "I", "text": "Woran lag das, war das ein Bandbreitenproblem oder ein Bug im Sync-Mechanismus?"}
{"ts": "137:50", "speaker": "E", "text": "Es war eine Kombination: temporär zu geringe Bandbreite in der interregionalen Verbindung und ein nicht optimierter Commit-Mechanismus im Storage-Cluster. Wir haben daraus ein RFC-Doc erstellt, ID RFC-DR-209, um die Optimierungen zu tracken."}
{"ts": "138:05", "speaker": "I", "text": "Wie gehen Sie mit der Priorisierung solcher RFCs um, gerade wenn es um Kosten-Performance-Tradeoffs geht?"}
{"ts": "138:15", "speaker": "E", "text": "Wir setzen auf ein Bewertungsmodell, das die Auswirkung auf RTO/RPO, Kosten und Risiko gegeneinanderstellt. In diesem Fall ist der Performance-Gewinn klar kritischer als die leichten Mehrkosten für Bandbreiten-Reserven."}
{"ts": "138:30", "speaker": "I", "text": "Sehen Sie Risiken bei dieser Entscheidung, langfristig?"}
{"ts": "138:40", "speaker": "E", "text": "Das Hauptrisiko ist, dass wir uns an teurere Bandbreiten-Reserven gewöhnen und diese nicht mehr hinterfragen. Deshalb ist in Runbook RB-COST-002 ein jährlicher Review vorgesehen, um Anpassungen vorzunehmen."}
{"ts": "138:55", "speaker": "I", "text": "Welche nächsten Schritte sind für die kommende Drill-Phase konkret geplant?"}
{"ts": "139:00", "speaker": "E", "text": "Wir wollen eine vollständige Simulation mit gleichzeitiger Deaktivierung von zwei Regionen fahren, um den Blast Radius wirklich zu testen. Außerdem planen wir, das IAM-Failover zu containerisieren, basierend auf den Lessons Learned aus TEST-DR-2025-Q1."}
{"ts": "144:00", "speaker": "I", "text": "Lassen Sie uns noch einmal auf die im letzten Drill identifizierten Risiken eingehen – speziell jene, die wir in Ticket DR-RISK-078 dokumentiert haben. Können Sie da bitte ein Update geben?"}
{"ts": "144:05", "speaker": "E", "text": "Ja, gern. In DR-RISK-078 ging es um die Latenzspitzen beim Failover ins Secondary Region Frankfurt-2. Wir haben inzwischen mit dem Poseidon Networking Team eine neue Route-Policy getestet, die diese Spikes um rund 35% reduziert."}
{"ts": "144:12", "speaker": "I", "text": "Das klingt nach einem Fortschritt. Wurde dafür auch eine Anpassung im Runbook RB-DR-001 vorgenommen?"}
{"ts": "144:17", "speaker": "E", "text": "Genau, wir haben in RB-DR-001 Abschnitt 4.2 den Schritt zur manuellen Routenprüfung ergänzt. Das ist zwar noch manuell, aber es verhindert falsche BGP-Announcements während des Drills."}
{"ts": "144:24", "speaker": "I", "text": "Verstehe. Und wie stellen Sie sicher, dass diese Änderungen nicht zu höherer Komplexität im Drill führen?"}
{"ts": "144:29", "speaker": "E", "text": "Wir haben einen Pre-Drill-Check eingeführt, ein kleines Python-Skript, das die Konfigurations-Hashes vergleicht. So fällt der manuelle Schritt kaum ins Gewicht."}
{"ts": "144:36", "speaker": "I", "text": "Gibt es schon erste Metriken aus dem letzten Mini-Testlauf dazu?"}
{"ts": "144:39", "speaker": "E", "text": "Ja, im Test-Log TL-DR-2025-05 sehen wir, dass die Failover-Zeit um 22 Sekunden verkürzt wurde und die Latenz im Schnitt bei 87 ms lag, statt wie vorher bei 132 ms."}
{"ts": "144:46", "speaker": "I", "text": "Das ist eine beachtliche Verbesserung. Gab es in diesem Zuge auch Koordination mit dem Nimbus Observability Team?"}
{"ts": "144:51", "speaker": "E", "text": "Absolut. Nimbus hat uns neue Alert-Regeln in das DR-Dashboard eingebaut, die genau auf diese Routenänderungen triggern. So sehen wir in Echtzeit, ob der BLAST_RADIUS wirklich lokalisiert bleibt."}
{"ts": "144:58", "speaker": "I", "text": "Sehr gut. Blicken wir kurz in die Zukunft: Welche Änderungen planen Sie für die nächste Drill-Phase, um solche Netzwerk-Risiken weiter zu minimieren?"}
{"ts": "145:03", "speaker": "E", "text": "Wir evaluieren gerade ein automatisiertes Route-Fencing, das beim Umschalten temporär alle nicht notwendigen Transit-Verbindungen kappt. Das reduziert Angriffsfläche und Fehlerrisiko."}
{"ts": "145:10", "speaker": "I", "text": "Klingt schlüssig. Sehen Sie dabei Risiken in Bezug auf regulatorische Anforderungen, insbesondere was Datenlokalität betrifft?"}
{"ts": "145:15", "speaker": "E", "text": "Ja, wir müssen strikt auf die Vorgaben der Verordnung IT-SI-2024 achten. Route-Fencing darf nicht dazu führen, dass Logs oder Transaktionsdaten unzulässig ins Ausland geroutet werden, das muss in den Prüfpfad aufgenommen werden."}
{"ts": "145:22", "speaker": "I", "text": "Dann sollten wir das auch in den SLA-Anhang aufnehmen. Könnten Sie bis Ende der Woche einen Vorschlag formulieren?"}
{"ts": "145:27", "speaker": "E", "text": "Ja, ich schreibe das als RFC-DR-NET-092 nieder, inklusive Testplan und Compliance-Checkliste, damit es in der nächsten Steering-Session abgestimmt werden kann."}
{"ts": "146:00", "speaker": "I", "text": "Wir haben gerade die Tradeoffs bei den Kosten angesprochen. Mich würde interessieren, ob Sie konkrete Szenarien haben, in denen Sie bewusst auf Performance verzichten, um Kosten einzusparen."}
{"ts": "146:05", "speaker": "E", "text": "Ja, tatsächlich. Bei den Backup-Replikationen in die sekundäre Region fahren wir im Normalbetrieb mit einer niedrigeren Netzwerk-Throughput-Klasse. Das erhöht die Transferzeit um ca. 7 %, spart uns aber laut Ticket FIN-DR-112 monatlich rund 2.300 €. Im Drill erhöhen wir diesen Parameter temporär – dafür gibt es einen Step im Runbook RB-DR-001 unter Abschnitt 4.3."}
{"ts": "146:20", "speaker": "I", "text": "Interessant. Gab es schon mal den Fall, dass diese Drosselung im Ernstfall Probleme gemacht hat?"}
{"ts": "146:27", "speaker": "E", "text": "Nicht direkt. Im GameDay-Test TEST-DR-2025-Q1 haben wir simuliert, was passiert, wenn die Replikation unter der gedrosselten Bandbreite läuft. Ergebnis: Wir bleiben innerhalb der RPO von 15 Minuten, solange keine zusätzlichen Lastspitzen auftreten. Allerdings zeigt unser internes Dashboard aus Nimbus Observability, dass bei gleichzeitigen Security-Scans die RPO auf 18 Minuten ansteigt."}
{"ts": "146:43", "speaker": "I", "text": "Das heißt, die Abhängigkeit zu den Security-Scans ist ein potenzielles Risiko?"}
{"ts": "146:48", "speaker": "E", "text": "Genau. Deshalb haben wir im RFC DR-OPT-2025-07 vorgeschlagen, Security-Scans in der Failover-Phase zeitlich zu verschieben. Das ist noch nicht produktiv gesetzt, aber im nächsten Drill wollen wir das testen."}
{"ts": "146:59", "speaker": "I", "text": "Wie gehen Sie bei solchen RFCs vor? Gibt es ein festes Review-Board?"}
{"ts": "147:05", "speaker": "E", "text": "Ja, es gibt das DR-Architektur-Board, das alle Änderungen prüft. Dort sitzen Vertreter aus dem Poseidon Networking Team, aus Compliance und aus Operations. Änderungen an Runbooks oder Automatisierungs-Workflows müssen laut interner Policy DOC-DR-STD-05 von mindestens zwei dieser Bereiche freigegeben werden."}
{"ts": "147:19", "speaker": "I", "text": "Gut. Abschließend zu diesem Themenblock – gibt es geplante Investitionen, um den Engpass dauerhaft zu lösen?"}
{"ts": "147:25", "speaker": "E", "text": "Ja, wir haben in der Budgetplanung 2026 einen Posten für adaptive Bandbreitensteuerung eingeplant. Die Idee ist, dass das System automatisch mehr Bandbreite anfordert, wenn Nimbus Observability eine RPO-Gefährdung erkennt. Das würde den manuellen Eingriff laut RB-DR-001 ersetzen."}
{"ts": "147:39", "speaker": "I", "text": "Klingt nach einem komplexen Feature. Wie würden Sie die Zuverlässigkeit dieser Automatik testen?"}
{"ts": "147:44", "speaker": "E", "text": "Wir planen eine Serie von Chaos-Tests, bei denen wir simultan Netzwerkdrosselung und hohe Last triggern. Die Kriterien sind in TESTPLAN-DR-CHAOS-03 dokumentiert. Ziel ist, in 95 % der Fälle die RPO wieder unter 15 Minuten zu drücken, bevor der Drill abgeschlossen ist."}
{"ts": "147:58", "speaker": "I", "text": "Sehr gut, danke. Eine letzte Frage zur Roadmap: Gibt es regulatorische Änderungen, die uns zwingen könnten, diese Bandbreitenanpassung schneller zu implementieren?"}
{"ts": "148:04", "speaker": "E", "text": "Ja, die neue EU-Cloud-Resilience Directive, die 2026 in Kraft tritt, schreibt vor, dass kritische Finanzdienste eine RPO von maximal 10 Minuten im Ernstfall garantieren. Das würde unser aktuelles Setup nicht schaffen, daher haben wir eine Taskforce DR-RPO-10MIN gebildet."}
{"ts": "148:18", "speaker": "I", "text": "Das bedeutet, wir müssen nicht nur die adaptive Steuerung einführen, sondern auch unsere Region-Strategie prüfen?"}
{"ts": "148:23", "speaker": "E", "text": "Richtig. Wir überlegen, eine dritte Region als Hot-Standby einzubinden. Laut Kostenschätzung in FIN-DR-EST-2026-03 wären das +15 % OPEX, aber es wäre die sicherste Option, um die neue Vorgabe zu erfüllen. Risiken wie erhöhte Komplexität und mögliche Fehltrigger im Failover sind im Risk-Register RR-DR-2025 vermerkt."}
{"ts": "148:00", "speaker": "I", "text": "Lassen Sie uns noch einmal konkret auf die geplanten Änderungen für die nächste Drill-Phase eingehen. Welche Anpassungen sind im Architekturplan bereits fixiert?"}
{"ts": "148:15", "speaker": "E", "text": "Wir haben im RFC-DR-2025-07 festgelegt, dass die sekundäre Region künftig auch einen aktiven Read-Replica-Cluster für die Kern-Datenbank erhält, um die RPO von 5 Minuten auf 1 Minute zu reduzieren. Außerdem wird das Failover-Skript FS-DR-Trigger-02 erweitert, sodass die DNS-Umschaltung gleichzeitig mit der IAM-Policy-Aktivierung passiert."}
{"ts": "148:42", "speaker": "I", "text": "Verstehe. Gab es für diese Entscheidung bereits ein Kostenmodell oder ist das noch in Arbeit?"}
{"ts": "148:50", "speaker": "E", "text": "Das Kostenmodell CM-DR-2025-Q3 liegt als Entwurf vor. Darin haben wir die zusätzlichen Compute-Kosten in der Standby-Region gegen die verkürzte Ausfallzeit gerechnet. Ergebnis: Ein Anstieg um 14 %, aber die simulierte Produktiv-Verfügbarkeit steigt von 99,85 % auf 99,94 %."}
{"ts": "149:15", "speaker": "I", "text": "Wie wird sichergestellt, dass die Runbooks wie RB-DR-001 oder RB-DR-004 diese Änderungen widerspiegeln?"}
{"ts": "149:23", "speaker": "E", "text": "Wir haben im Confluence-DR-Space einen Update-Hook definiert. Jeder Merge in das DR-Automations-Repo triggert eine Checkliste, die u.a. den Abgleich der Runbook-Versionen erfordert. Zusätzlich gibt es ein wöchentliches Review-Meeting mit Ops und DevSec."}
{"ts": "149:48", "speaker": "I", "text": "Könnten regulatorische Änderungen hier eingreifen? Ich denke da an die neue EU-Cloud-Resilienzrichtlinie."}
{"ts": "149:56", "speaker": "E", "text": "Ja, die CRD-EU-2026 wird strengere Nachweise für DR-Tests verlangen. Wir planen deshalb, die GameDays mit Nimbus Observability zu protokollieren und die Logs in einem WORM-Storage in der Drittregion abzulegen."}
{"ts": "150:20", "speaker": "I", "text": "Das klingt vernünftig. Gibt es Risiken, dass diese zusätzlichen Protokollierungen den Drill verlangsamen?"}
{"ts": "150:28", "speaker": "E", "text": "Minimal. Wir haben bei TEST-DR-2025-Q1 gemessen, dass die zusätzliche Latenz im Failover-Schritt 3, also beim API-Gateway-Switch, nur 1,2 Sekunden betrug. Kritischer wird eher der Speicherkostenfaktor, der um etwa 6 % steigt."}
{"ts": "150:50", "speaker": "I", "text": "Welche Maßnahmen sind geplant, um diese Kosten zu kontrollieren?"}
{"ts": "150:57", "speaker": "E", "text": "Wir prüfen Deduplizierung auf Storage-Ebene und wollen ältere Drill-Logs nach 24 Monaten in ein Glacier-ähnliches Archiv verschieben. Das ist in Ticket DR-KOSTEN-112 dokumentiert."}
{"ts": "151:15", "speaker": "I", "text": "Zum Abschluss: Gibt es Lessons Learned aus der letzten Drill-Runde, die direkt in die Zukunftsplanung einfließen?"}
{"ts": "151:23", "speaker": "E", "text": "Ja, das größte Learning war die Notwendigkeit einer klaren Entscheidungsmatrix. In TEST-DR-2025-Q1 gab es eine Verzögerung, weil das Incident-Commander-Team nicht eindeutig wusste, wann ein partielles Failover zulässig ist. Das wird nun in RB-DR-002 ergänzt."}
{"ts": "151:47", "speaker": "I", "text": "Das ist vermutlich auch ein kultureller Punkt, oder?"}
{"ts": "151:54", "speaker": "E", "text": "Absolut. Wir müssen die Bereitschaft stärken, in Drill-Szenarien schnell zu entscheiden, auch wenn noch nicht alle Daten vorliegen. Das muss in Trainingsformaten verankert werden, sonst laufen wir Gefahr, dass im Ernstfall wertvolle Zeit verloren geht."}
{"ts": "152:00", "speaker": "I", "text": "Lassen Sie uns bitte noch einmal konkret auf die regulatorischen Änderungen eingehen, die Sie im Ausblick erwähnt hatten. Welche betreffen Titan DR direkt?"}
{"ts": "152:05", "speaker": "E", "text": "Ja, wir müssen ab Q3 2025 die neuen Bundes-IT-Sicherheitsverordnungen berücksichtigen, die unter anderem strengere Nachweise für RTO/RPO-Compliance erfordern. Das heißt, wir müssen unsere Audit-Trails im Runbook RB-DR-001 und RB-DR-004 erweitern."}
{"ts": "152:13", "speaker": "I", "text": "Heißt das, dass wir auch Änderungen an den Automatisierungsskripten vornehmen müssen?"}
{"ts": "152:17", "speaker": "E", "text": "Genau, die Automatisierung muss künftig Signaturen und Hashes der Logs automatisch ablegen, damit sie revisionssicher sind. In Ticket DR-SEC-542 haben wir dazu bereits einen Proof of Concept verlinkt."}
{"ts": "152:25", "speaker": "I", "text": "Und wie wirkt sich das auf die Failover-Zeit aus?"}
{"ts": "152:29", "speaker": "E", "text": "Minimal, wir kalkulieren eine Verlängerung um 3–4 Sekunden, da die Hashing-Prozesse parallel zum eigentlichen Failover laufen. Wir haben das in Testlauf TEST-DR-2025-Q2 simuliert."}
{"ts": "152:36", "speaker": "I", "text": "Kommen wir zu den geplanten Änderungen für die nächste Drill-Phase – was steht da oben auf der Liste?"}
{"ts": "152:40", "speaker": "E", "text": "Wir wollen den Multi-Region-LoadBalancer von statischen Health-Checks auf dynamisches Traffic Shaping umstellen. Das wird in RFC-DR-017 beschrieben und soll Ausfälle granularer isolieren."}
{"ts": "152:48", "speaker": "I", "text": "Das klingt, als würde der BLAST_RADIUS dadurch nochmal reduziert?"}
{"ts": "152:52", "speaker": "E", "text": "Ja, genau. Wenn etwa Region West nur zu 40% beeinträchtigt ist, können wir mit dem dynamischen Shaping den übrigen Traffic dort belassen, statt komplett auf Region Süd zu failovern. Das spart Kosten und reduziert Latenz."}
{"ts": "152:59", "speaker": "I", "text": "Apropos Kosten – sehen Sie durch diese Umstellung Risiken?"}
{"ts": "153:03", "speaker": "E", "text": "Ein Risiko ist, dass die Traffic-Analyse-Logik fehlerhaft eingestuft wird und zu falschen Routing-Entscheidungen führt. Wir mitigieren das mit Canary-Tests vor jeder Umschaltung und definieren klare Rollback-Pfade im Runbook RB-DR-007."}
{"ts": "153:11", "speaker": "I", "text": "Wie stellen wir sicher, dass unsere Runbooks in diesem dynamischen Umfeld immer aktuell bleiben?"}
{"ts": "153:15", "speaker": "E", "text": "Wir haben eine interne Policy, dass jede Architekturänderung ein Update der relevanten Runbooks triggert. Das ist im Confluence-Space DR-Ops dokumentiert, Checkliste CL-DR-Update-02. Zusätzlich gibt es vierteljährliche Runbook-Reviews."}
{"ts": "153:23", "speaker": "I", "text": "Und wie wird das versioniert?"}
{"ts": "153:27", "speaker": "E", "text": "Versionierung erfolgt über unser Git-basierendes Doku-Repo mit Tags für jede Drill-Phase, z.B. v2025.Q2-DR. So können wir jederzeit nachvollziehen, welche Prozeduren zu welchem Zeitpunkt galten."}
{"ts": "153:36", "speaker": "I", "text": "Lassen Sie uns nun, äh, noch einmal auf die Änderungen eingehen, die Sie für die nächste Drill-Phase planen. Was steht konkret auf der Roadmap?"}
{"ts": "153:41", "speaker": "E", "text": "Für Q3 ist geplant, das Failover-Skript aus RB-DR-001 so zu erweitern, dass auch die API-Gateways automatisch die Routen in beiden Regionen synchronisieren. Zusätzlich wollen wir im Rahmen von RFC-DR-2025-07 die IAM-Rollen-Replikation beschleunigen."}
{"ts": "153:48", "speaker": "I", "text": "Verstehe. Gibt es regulatorische Änderungen, die unsere DR-Architektur beeinflussen könnten?"}
{"ts": "153:53", "speaker": "E", "text": "Ja, es gibt einen Entwurf der EU-Cloud-Compliance, der kürzere RPOs für Finanzdaten fordert. Das würde bedeuten, dass wir in der Region Central-2 unsere asynchrone Replikation auf synchrone umstellen müssten, was sowohl Latenz- als auch Kostenfolgen hätte."}
{"ts": "154:00", "speaker": "I", "text": "Wie stellen Sie sicher, dass unsere Runbooks aktuell bleiben?"}
{"ts": "154:05", "speaker": "E", "text": "Wir haben einen vierteljährlichen Review-Prozess, gekoppelt an das Change Management. Jede Änderung in den produktiven DR-Komponenten löst einen Ticket-Typ DR-DOC-Update aus, der eine Aktualisierung im Confluence-Workspace erzwingt."}
{"ts": "154:12", "speaker": "I", "text": "Könnten Sie ein Beispiel für so ein Ticket geben?"}
{"ts": "154:16", "speaker": "E", "text": "Ja, Ticket DR-DOC-2025-14 wurde erstellt, als wir den neuen LoadBalancer-Typ in der US-West-Region eingeführt haben. Das erforderte Anpassungen in Schritt 4 und 7 des RB-DR-001, um Health Checks korrekt zu initialisieren."}
{"ts": "154:24", "speaker": "I", "text": "Wie gehen Sie mit Risiken um, wenn Kostenoptimierungen im Widerspruch zu Performancezielen stehen?"}
{"ts": "154:29", "speaker": "E", "text": "Wir führen vor jeder Optimierung eine Risikoanalyse nach Runbook RB-RISK-002 durch. Wenn z.B. eine Reduzierung der Standby-Kapazität mehr als 5 Sekunden auf die RTO addiert, wird sie abgelehnt oder nur in einer Region umgesetzt."}
{"ts": "154:36", "speaker": "I", "text": "Gab es zuletzt einen Fall, bei dem Sie so entscheiden mussten?"}
{"ts": "154:40", "speaker": "E", "text": "Ja, im März. Wir wollten den Autoscaling-Puffer in der Region North-1 halbieren, um Kosten zu sparen. Die Simulation zeigte eine Verlängerung der Anlaufzeit um 7 Sekunden. Entscheidung war, Änderung nur in einer nicht-kritischen Region zu testen."}
{"ts": "154:48", "speaker": "I", "text": "Das klingt nach einem guten Kompromiss. Gibt es offene Risiken, die Sie besonders im Blick behalten?"}
{"ts": "154:53", "speaker": "E", "text": "Ein Risiko ist die Abhängigkeit vom Poseidon Networking Release-Zyklus. Verzögerungen dort können unseren DR-Testplan verschieben. Außerdem beobachten wir die Lieferzeiten neuer Storage-Nodes, da wir im Drill immer knapper kalkulieren."}
{"ts": "155:00", "speaker": "I", "text": "Zum Abschluss: Welche KPIs werden Sie im nächsten Drill besonders fokussieren?"}
{"ts": "155:06", "speaker": "E", "text": "Primär die Recovery Time Actual (RTA) im Vergleich zur SLA-RTO von 60 Sekunden, und die Fehlerrate der automatischen Playbooks. Ziel ist unter 2% Abbruchquote bei Skriptläufen, gemessen über die letzten drei Tests."}
{"ts": "155:06", "speaker": "I", "text": "Zum Abschluss würde ich gern noch einmal auf die geplanten Änderungen in der nächsten Drill-Phase eingehen. Welche konkreten Anpassungen sind aktuell im RFC-Entwurf DR-RFC-2025-07 vorgesehen?"}
{"ts": "155:10", "speaker": "E", "text": "Ja, also im RFC-Entwurf ist vorgesehen, das Failover-Skript aus RB-DR-001 mit dem neuen API-Hook aus Poseidon v3 zu verheiraten, sodass wir die Netzwerkschwenkung innerhalb von 90 Sekunden schaffen. Außerdem wollen wir die automatisierte Validierung der Datenintegrität erweitern, damit RPO laut SLA konstant unter 4 Minuten bleibt."}
{"ts": "155:17", "speaker": "I", "text": "Das klingt ambitioniert. Gibt es da schon eine Testumgebung oder läuft das initial direkt im Drill?"}
{"ts": "155:21", "speaker": "E", "text": "Wir haben eine isolierte Staging-Region in Südosteuropa hochgezogen, die identische IAM-Policies nutzt. Dort fahren wir gerade nightly Dry-Runs, um die Änderungen auf Ticketbasis – DR-TKT-882 bis -889 – abzusichern, bevor wir sie in den Live-Drill einbinden."}
{"ts": "155:28", "speaker": "I", "text": "Wie stellen Sie sicher, dass die Runbooks aktuell bleiben, insbesondere wenn Poseidon und Nimbus parallel weiterentwickelt werden?"}
{"ts": "155:32", "speaker": "E", "text": "Das ist tatsächlich eine Herausforderung. Wir haben ein internes Review-Intervall von 60 Tagen etabliert. Jedes Mal, wenn im Poseidon-Netzwerkmodul oder in Nimbus Observability ein Major Release kommt, wird ein Trigger gesetzt, der den Runbook-Owner benachrichtigt. Zusätzlich pflegen wir eine Change-Log-Matrix im Confluence-DR-Space."}
{"ts": "155:39", "speaker": "I", "text": "Gibt es in der nächsten Phase regulatorische Änderungen, die unsere Architektur beeinflussen könnten?"}
{"ts": "155:44", "speaker": "E", "text": "Ja, die EU-DSR-Novelle (Directive on System Resilience) tritt Mitte 2025 in Kraft. Sie erfordert, dass alle kritischen Systeme auch bei regionalem Totalausfall innerhalb von 2 Stunden wiederhergestellt werden. Unser aktuelles SLA (RTO 1h, RPO 5 Min) liegt darunter, aber wir müssen Audit-Logs manipulationssicher in beiden Regionen speichern."}
{"ts": "155:52", "speaker": "I", "text": "Welche Risiken sehen Sie bei der Umsetzung dieser neuen Vorgaben, gerade im Hinblick auf Kosten?"}
{"ts": "155:56", "speaker": "E", "text": "Das Offensichtlichste ist der erhöhte Storage-Aufwand für redundante Audit-Logs. Wir schätzen Mehrkosten von 12% pro Jahr. Ein weiteres Risiko ist, dass die Schreiblatenz steigt, wenn wir synchrone Cross-Region-Replikation forcieren. Hier müssen wir einen Sweet Spot finden, damit Performance nicht leidet."}
{"ts": "156:03", "speaker": "I", "text": "Haben Sie dafür schon einen Lösungsansatz?"}
{"ts": "156:07", "speaker": "E", "text": "Wir testen gerade ein hybrides Modell: kritische Audit-Events werden sofort synchron gespiegelt, weniger kritische in Batches asynchron. Das ist in RB-DR-004 dokumentiert. Erste Benchmarks zeigen, dass wir damit unter 150 ms Latenz bleiben, während der Storage-Anstieg auf 7% begrenzt wird."}
{"ts": "156:14", "speaker": "I", "text": "Letzte Frage für heute: Wie priorisieren Sie künftig Investitionen in DR im Vergleich zu anderen Infrastrukturprojekten?"}
{"ts": "156:18", "speaker": "E", "text": "Wir haben im Steering Committee beschlossen, DR als Tier-1-Initiative zu behandeln. Das heißt, bei Budgetkonflikten hat DR Vorrang vor Tier-2-Projekten wie Dev-Cluster-Erweiterungen. Entscheidungsgrundlage sind Risikoanalysen aus den letzten drei GameDays und der erwartete Compliance-Impact."}
{"ts": "156:25", "speaker": "I", "text": "Also fließen Lessons Learned direkt in die Budgetplanung ein?"}
{"ts": "156:29", "speaker": "E", "text": "Genau. Wir haben z. B. nach TEST-DR-2025-Q1 erkannt, dass die manuelle DNS-Umschaltung der größte Zeitfresser war. Daraus entstand DR-RFC-2025-02 für vollautomatisches DNS-Failover, was jetzt in die nächste Phase einzieht. Solche Erkenntnisse rechtfertigen Investitionen sehr klar."}
{"ts": "156:42", "speaker": "I", "text": "Bevor wir abschließen, würde mich interessieren, ob es noch offene Punkte aus der letzten Drill-Phase gibt, die wir bislang nicht adressiert haben."}
{"ts": "156:46", "speaker": "E", "text": "Ja, es gibt noch zwei Tickets aus der Serie DR-OPS-217 und DR-OPS-219, die sich auf ein Delay im Failover der Reporting Engine beziehen. Da haben wir zwar Hotfixes im Staging getestet, sie aber noch nicht im Produktionsdrill angewendet."}
{"ts": "156:52", "speaker": "I", "text": "Liegt das eher an technischen Hürden oder an Priorisierung im Change Board?"}
{"ts": "156:55", "speaker": "E", "text": "Eher Priorisierung – der Change Advisory Board Cycle war gerade voll mit Security-Patches aus RFC-Sec-388. Wir wollten keine Parallelrisiken eingehen."}
{"ts": "157:00", "speaker": "I", "text": "Verstehe. Wie wirkt sich das auf unsere nächste Drill-Planung aus?"}
{"ts": "157:04", "speaker": "E", "text": "Wir haben im Runbook RB-DR-001 ein Flag ergänzt, das im Testmodus die Reporting Engine auf eine reduzierte Datenmenge schaltet. So können wir den Failover-Pfad testen, ohne vollständige Last zu erzeugen."}
{"ts": "157:10", "speaker": "I", "text": "Das klingt nach einer pragmatischen Lösung. Gab es Bedenken hinsichtlich der Aussagekraft der Tests?"}
{"ts": "157:14", "speaker": "E", "text": "Ja, einige Kollegen aus Quality Assurance haben darauf hingewiesen, dass synthetische Last nicht immer dieselben Fehler triggert. Deswegen planen wir einen zweiten Testlauf mit realistischen Daten im Maintenance-Window."}
{"ts": "157:20", "speaker": "I", "text": "Könnte das unsere SLA-Zielwerte für RTO/RPO gefährden, falls der Test länger läuft als geplant?"}
{"ts": "157:24", "speaker": "E", "text": "Kurzfristig nicht, weil wir im Drill-Szenario keine produktiven SLAs verletzen. Langfristig müssen wir aber darauf achten, dass wir die 30-Minuten-RTO auch unter Volllast halten – das ist Teil der nächsten Audit-Prüfung."}
{"ts": "157:30", "speaker": "I", "text": "Gibt es Lessons Learned aus den Verzögerungen, die wir ins Wissensmanagement aufnehmen sollten?"}
{"ts": "157:34", "speaker": "E", "text": "Definitiv: Wir haben im internen Confluence-Bereich eine Sektion für 'Drill Pitfalls' eröffnet. Da steht jetzt explizit drin, dass Security-Changes und DR-Tests zeitlich entkoppelt werden sollten."}
{"ts": "157:40", "speaker": "I", "text": "Gut. Vielleicht noch eine letzte Einschätzung: Sehen Sie aktuell das größte Risiko eher auf der technischen oder auf der organisatorischen Seite?"}
{"ts": "157:44", "speaker": "E", "text": "Im Moment organisatorisch. Technisch sind wir stabil, aber durch die Vielzahl an Projekten – Poseidon, Nimbus, Atlas DB – ist die Koordination komplex. Ein verpasster Abstimmungstermin kann im DR-Fall teure Minuten kosten."}
{"ts": "157:50", "speaker": "I", "text": "Also wäre eine engere Taktung im Status-Reporting sinnvoll?"}
{"ts": "157:54", "speaker": "E", "text": "Ja, wir wollen ab nächstem Quartal ein wöchentliches DR-Standup einführen, um genau diese Koordinationslücken zu schließen. Das ist bereits als Maßnahme DR-IMP-042 im Maßnahmenplan erfasst."}
{"ts": "158:02", "speaker": "I", "text": "Zum Abschluss würde ich gern noch einmal auf die Roadmap eingehen. Welche Änderungen planen Sie konkret für die nächste Drill-Phase des Titan DR?"}
{"ts": "158:10", "speaker": "E", "text": "Wir haben für die nächste Phase vor, die Failover-Zeit durch Pre-Warm von kritischen Services um etwa 15 % zu reduzieren. Das bedeutet, dass wir einige Komponenten in der sekundären Region im sogenannten 'warm-standby' Modus laufen lassen – laut Runbook RB-DR-004 ist das der Modus, der am besten mit unserem SLA RTO ≤ 20 min harmoniert."}
{"ts": "158:28", "speaker": "I", "text": "Heißt das, dass sich dadurch auch die Kostenstruktur ändert?"}
{"ts": "158:34", "speaker": "E", "text": "Ja, klar. Die Betriebskosten steigen dadurch um etwa 8 %, aber wir vermeiden im Gegenzug das Risiko, dass ein Cold-Start bei einer Datenbank-Cluster-Initialisierung den RTO sprengt. Wir haben das in Ticket DR-COST-112 dokumentiert und mit dem Controlling abgestimmt."}
{"ts": "158:48", "speaker": "I", "text": "Gibt es regulatorische Änderungen, die Sie im Blick haben, die auf die DR-Architektur Einfluss nehmen könnten?"}
{"ts": "158:55", "speaker": "E", "text": "Ja, die EU plant eine Verschärfung der Cloud-Compliance-Richtlinien, speziell für Finanzdaten. Das betrifft unsere Kunden in der Region EU-Central. Wir prüfen gerade, ob wir für diese Daten einen zusätzlichen Geo-Fence im Failover-Plan berücksichtigen müssen, siehe RFC-DR-2025-07."}
{"ts": "159:12", "speaker": "I", "text": "Wie stellen Sie sicher, dass die Runbooks aktuell bleiben?"}
{"ts": "159:18", "speaker": "E", "text": "Wir haben einen Quartalszyklus für die Review aller DR-Runbooks. Änderungen wie neue API-Endpunkte oder geänderte IAM-Rollen werden in unseren Confluence-Seiten erfasst und dann in den YAML-basierten Automatisierungsskripten nachgezogen. Das wird durch den DR-Guild-Lead freigegeben."}
{"ts": "159:34", "speaker": "I", "text": "Gab es zuletzt Fälle, in denen veraltete Runbook-Informationen im Drill aufgefallen sind?"}
{"ts": "159:40", "speaker": "E", "text": "Im letzten GameDay haben wir einen veralteten DNS-Switch-Befehl bemerkt. Der stammte noch aus der Zeit vor der Umstellung auf Poseidon Networking v3. Wir haben das sofort als Incident INC-DR-775 erfasst und innerhalb von zwei Tagen korrigiert."}
{"ts": "159:56", "speaker": "I", "text": "Sie hatten vorhin die Kostensteigerung durch warm-standby erwähnt. Welche Risiken sehen Sie bei dieser Strategie?"}
{"ts": "160:03", "speaker": "E", "text": "Das größte Risiko ist, dass wir Ressourcen für Systeme vorhalten, die im Ernstfall vielleicht gar nicht betroffen wären. Außerdem kann eine fehlerhafte Synchronisierung zwischen aktiver und standby-Region zu Dateninkonsistenzen führen. Wir minimieren das Risiko durch stündliche Replication-Checks und Monitoring-Alerts über Nimbus Observability."}
{"ts": "160:20", "speaker": "I", "text": "Werden diese Checks automatisiert ausgelöst oder manuell?"}
{"ts": "160:25", "speaker": "E", "text": "Die Checks laufen vollautomatisch basierend auf Runbook RB-MON-009. Nur im Fall einer Abweichung > 50 ms Latenz oder > 100 MB Differenz wird ein manueller Review angestoßen. Das spart uns im Alltag einiges an Aufwand."}
{"ts": "160:39", "speaker": "I", "text": "Danke, das ist sehr konkret. Gibt es noch offene Punkte, die Sie vor dem nächsten Drill unbedingt adressieren möchten?"}
{"ts": "160:45", "speaker": "E", "text": "Ja, wir wollen die Cross-Region-IAM-Rollenzuweisungen härten. Im letzten Security-Audit wurde ein verwaistes Service-Konto in der Backup-Region gefunden. Das soll laut Plan bis Ende des Quartals bereinigt werden, siehe SecTicket SEC-DR-441."}
{"ts": "162:02", "speaker": "I", "text": "Kommen wir noch einmal auf die regulatorischen Aspekte zu sprechen – sehen Sie aktuell irgendwelche Änderungen, die unsere DR-Architektur unmittelbar betreffen könnten?"}
{"ts": "162:07", "speaker": "E", "text": "Ja, das Bundes-IT-Sicherheitsgesetz 3.0 sieht verschärfte Meldepflichten für Ausfälle vor. Das heißt, unsere Runbooks, speziell RB-DR-004 für Incident Communication, müssen angepasst werden, um innerhalb von max. 4 Stunden nach Ausfall eine Meldung abzusetzen."}
{"ts": "162:16", "speaker": "I", "text": "Verstehe. Würde das bedeuten, dass wir auch die Schnittstelle zu Nimbus Observability erweitern müssen, damit Alerts automatisch einen Draft-Report generieren?"}
{"ts": "162:21", "speaker": "E", "text": "Genau. Wir planen dazu ein Modul in Nimbus zu implementieren, das auf Alert-IDs wie DR-AL-2025-17 reagiert und ein vorbefülltes Incident-Template im Compliance-Format erstellt."}
{"ts": "162:28", "speaker": "I", "text": "Das bringt mich zur Frage der Aktualisierung: Wie stellen wir sicher, dass unsere Runbooks wirklich auf dem neuesten Stand bleiben?"}
{"ts": "162:33", "speaker": "E", "text": "Wir haben einen quartalsweisen Review-Prozess, gekoppelt an das Change Advisory Board. Jede Änderung in Poseidon Networking oder in der Cloud-Region-Policy triggert ein Ticket im DR-Board, z.B. CHG-DR-558, das dann eine Runbook-Revision erzwingt."}
{"ts": "162:42", "speaker": "I", "text": "Und wie wird das auf operativer Ebene nachvollzogen?"}
{"ts": "162:45", "speaker": "E", "text": "Wir pflegen im internen Confluence einen Runbook-Index mit Versionsnummern. Beim Drill müssen alle Operatoren laut Checkliste RB-CHK-002 bestätigen, dass sie die aktuelle Version vorliegen haben."}
{"ts": "162:54", "speaker": "I", "text": "Sie hatten vorhin die Standby-Kosten angesprochen. Gab es in den letzten Monaten Entscheidungen, diese weiter zu optimieren?"}
{"ts": "162:59", "speaker": "E", "text": "Ja, wir haben in der AP-SEA-2 Region von aktiven auf ‘warm standby’ umgestellt. Laut unserem internen Kostenreport CR-2025-Q1 hat das die monatlichen Ausgaben um etwa 27% gesenkt, bei nur +12 Sekunden RTO-Verlängerung."}
{"ts": "163:08", "speaker": "I", "text": "Gab es dabei Risiken, die Sie bewusst in Kauf genommen haben?"}
{"ts": "163:12", "speaker": "E", "text": "Natürlich, der Trade-off ist, dass bei gleichzeitigen Ausfällen in zwei Regionen die Failover-Kette länger braucht. Wir haben das Risiko akzeptiert, da laut SLA-Berechnungen die Eintrittswahrscheinlichkeit < 0,1% p.a. liegt."}
{"ts": "163:21", "speaker": "I", "text": "Wie haben Sie diese Wahrscheinlichkeit modelliert?"}
{"ts": "163:24", "speaker": "E", "text": "Basierend auf historischen Incident-Daten aus den letzten fünf Jahren, kombiniert mit der Failure-Correlation-Matrix aus Poseidon Networking. Die Matrix zeigt, dass simultane Netzausfälle in AP-SEA-2 und EU-CENT-1 extrem selten sind."}
{"ts": "163:33", "speaker": "I", "text": "Letzte Frage: Welche Änderungen planen Sie konkret für die nächste Drill-Phase im Hinblick auf diese Erkenntnisse?"}
{"ts": "163:38", "speaker": "E", "text": "Wir wollen einen Simultan-Ausfall zweier Regionen in den Scope aufnehmen, um genau diese seltene, aber kritische Situation zu trainieren. Das wird als TEST-DR-2025-Q4 geplant, mit angepasster Runbook-Fassung RB-DR-009-Beta."}
{"ts": "163:38", "speaker": "I", "text": "Zum Abschluss möchte ich noch auf die konkreten Maßnahmen eingehen, die Sie aus den letzten Drill-Ergebnissen abgeleitet haben. Was ist in die nächste Iteration eingeflossen?"}
{"ts": "163:43", "speaker": "E", "text": "Wir haben basierend auf TEST-DR-2025-Q1 mehrere Tickets aufgemacht, z.B. INC-DR-774 für die Optimierung des Replica-Lag in der sekundären Region. Außerdem wurde Runbook RB-DR-004 für IAM-Recovery ergänzt, weil beim Drill ein Berechtigungs-Propagation-Delay aufgefallen ist."}
{"ts": "163:49", "speaker": "I", "text": "Gab es hierbei auch Anpassungen an den Failover-Skripten selbst, oder eher nur an den dokumentierten Prozessen?"}
{"ts": "163:53", "speaker": "E", "text": "Beides. Die Python-Skripte für die DNS-Umschaltung wurden refactored, um die TTL-Anpassung dynamisch zu machen. Parallel haben wir in RB-DR-002 die manuelle Verification-Checklist erweitert, um sicherzustellen, dass auch Third-Party-APIs wie das interne Payment-Gateway korrekt swappen."}
{"ts": "163:59", "speaker": "I", "text": "Wie stellen Sie sicher, dass diese Änderungen nicht wieder zu unerwarteten Nebeneffekten führen?"}
{"ts": "164:04", "speaker": "E", "text": "Wir fahren jetzt für jede Script-Änderung eine Sandbox-Simulation in der Staging-Region 'eu-west-sim'. Das ist im Runbook RB-DR-TEST beschrieben. Zusätzlich gibt es ein Pre-Flight Monitoring durch Nimbus Observability, das Blast-Radius-Berechnungen automatisch aktualisiert."}
{"ts": "164:10", "speaker": "I", "text": "Interessant. Und wie binden Sie Poseidon Networking in diese Sandbox-Tests ein?"}
{"ts": "164:14", "speaker": "E", "text": "Poseidon liefert uns die isolierten Overlay-Netze für die Simulation. Wir können in diesen Netzen gezielt Latenzen oder Paketverluste injizieren, um das Verhalten der Routing-Policies unter Failover-Bedingungen zu beobachten."}
{"ts": "164:20", "speaker": "I", "text": "Gab es bei den Kostenüberlegungen Änderungen, nachdem diese Tests durchgeführt wurden?"}
{"ts": "164:24", "speaker": "E", "text": "Ja, slight adjustment: Wir haben die Anzahl der warm-standby Instances in der US-East-Region um 15 % reduziert, weil die Sandbox gezeigt hat, dass wir durch beschleunigte Snapshot-Restore-Prozesse die RTO noch einhalten können. Das spart laut Kalkulation ca. 4.200 € pro Monat."}
{"ts": "164:30", "speaker": "I", "text": "Aber birgt das nicht ein Risiko, falls der Restore-Prozess einmal länger als erwartet dauert?"}
{"ts": "164:34", "speaker": "E", "text": "Das Risiko ist da, ja. Deshalb haben wir in SLA-Dokument DR-SLA-2025 eine Klausel ergänzt, dass bei kritischen Events sofort ein Parallel-Spinup in zwei Regionen erfolgt. Das kostet temporär mehr, sichert aber die RTO < 45 Minuten ab."}
{"ts": "164:40", "speaker": "I", "text": "Wie halten Sie diese SLA- und Runbook-Dokumente aktuell?"}
{"ts": "164:44", "speaker": "E", "text": "Wir haben ein vierteljährliches Review-Board, DR-GOV, das alle Runbooks und SLAs gegen aktuelle Drill-Erkenntnisse prüft. Änderungen werden als RFC im internen System eingereicht, z.B. RFC-DR-2025-07 für die eben genannte Klauselanpassung."}
{"ts": "164:50", "speaker": "I", "text": "Können Sie ein Beispiel geben, wo eine solche Aktualisierung kurzfristig entscheidend war?"}
{"ts": "164:54", "speaker": "E", "text": "Ja, im April hat ein Minor-Firmware-Bug in den Loadbalancern der sekundären Region dazu geführt, dass wir RB-DR-005 um eine zusätzliche Health-Check-Sequence ergänzt haben. Ohne das Review-Board wäre das wahrscheinlich erst im nächsten Major-Drill aufgefallen."}
{"ts": "165:08", "speaker": "I", "text": "Sie hatten vorhin die Optimierungen bei den Standby-Kosten erwähnt. Mich würde interessieren, wie sich diese seit Implementierung der letzten Änderungen in Q2 ausgewirkt haben."}
{"ts": "165:14", "speaker": "E", "text": "Ja, wir haben im April mit RFC-DR-072 die dynamische Skalierung der Warm-Standby-Knoten eingeführt. Das hat die monatlichen Kosten um ca. 18 % gesenkt, ohne die RTO von 15 Minuten laut SLA-DR-2024-01 zu gefährden."}
{"ts": "165:20", "speaker": "I", "text": "Gab es dabei Risiken, die Sie in Kauf nehmen mussten?"}
{"ts": "165:26", "speaker": "E", "text": "Das Hauptrisiko war, dass bei gleichzeitigen Ausfällen in zwei Regionen die Auto-Scaling-Policy zu langsam greift. Wir haben dafür ein Runbook-Addendum RB-DR-001a erstellt, das einen manuellen Pre-Warm-Prozess beschreibt."}
{"ts": "165:33", "speaker": "I", "text": "Wie stellen Sie sicher, dass dieses Addendum aktuell bleibt, wenn sich die Architektur ändert?"}
{"ts": "165:40", "speaker": "E", "text": "Wir haben einen halbjährlichen Review-Zyklus im DR-Chapter etabliert. Jede Änderung am Poseidon Networking oder an der Nimbus Observability Pipeline triggert ein automatisches Ticket im Change-Tracker, z.B. CHG-DR-558."}
{"ts": "165:46", "speaker": "I", "text": "Interessant. Beim letzten GameDay-Test, TEST-DR-2025-Q1, gab es ja einige Verzögerungen im Failover der API-Gateways. Woran lag das?"}
{"ts": "165:53", "speaker": "E", "text": "Das war eine IAM-Policy-Mismatch zwischen Region West-EU und East-AP. Die Replikation der Secrets via Vault-Mirror lief 90 Sekunden hinterher. Wir haben daraus die Lessons Learned LL-DR-2025-03 abgeleitet und die Sync-Intervalle halbiert."}
{"ts": "165:59", "speaker": "I", "text": "Haben diese Anpassungen die Performance messbar verbessert?"}
{"ts": "166:05", "speaker": "E", "text": "Ja, im Retest vom Mai lagen wir bei 12 Sekunden Latenz für den Secret-Abgleich, was innerhalb der Toleranzgrenze von 15 Sekunden liegt. Das Monitoring via Nimbus zeigt auch eine 99,95 % Verfügbarkeit im Drill-Modus."}
{"ts": "166:12", "speaker": "I", "text": "Welche langfristigen Änderungen planen Sie für die nächste Drill-Phase in Bezug auf diese Schnittstellen?"}
{"ts": "166:18", "speaker": "E", "text": "Wir wollen Cross-Region IAM Federation einführen, um die Abhängigkeit von Sync-Tasks zu verringern. Allerdings müssen wir dafür neue Compliance-Checks nach REG-DR-SEC-09 bestehen."}
{"ts": "166:25", "speaker": "I", "text": "Könnte das zusätzliche Kosten verursachen?"}
{"ts": "166:31", "speaker": "E", "text": "Ja, die Federation-Lizenzen erhöhen die Betriebskosten um ca. 4 %, aber wir sparen potenziell Minuten beim Failover. Das ist im DR-Kontext ein vertretbarer Trade-off."}
{"ts": "166:38", "speaker": "I", "text": "Wie bewerten Sie das Risiko, dass durch diese Änderung neue Angriffsflächen entstehen?"}
{"ts": "166:44", "speaker": "E", "text": "Das Risiko ist real, daher planen wir eine zusätzliche Penetration-Testing-Phase, wie im Security-Runbook RB-SEC-004 beschrieben. Erst nach bestandenen Tests wird die Federation in die produktive DR-Architektur aufgenommen."}
{"ts": "167:48", "speaker": "I", "text": "Lassen Sie uns jetzt noch auf die anstehenden regulatorischen Änderungen eingehen. Gibt es Vorgaben von der Bundes-IT-Aufsicht, die sich direkt auf unser Titan DR-Setup auswirken könnten?"}
{"ts": "168:02", "speaker": "E", "text": "Ja, es gibt eine neue Richtlinie, die ab Q3 2025 gilt und verlangt, dass alle kritischen Systeme in maximal 30 Minuten nach Ausfall wieder verfügbar sind. Das verschärft unser SLA von aktuell 45 Minuten RTO auf 30 Minuten."}
{"ts": "168:20", "speaker": "I", "text": "Das heißt, wir müssen unsere Runbooks und Automatisierungsprozesse entsprechend anpassen, korrekt?"}
{"ts": "168:27", "speaker": "E", "text": "Genau. Wir prüfen aktuell RB-DR-003, das den Failover-Prozess für die Finanz-API beschreibt. Eventuell müssen wir den manuellen DNS-Check durch einen automatisierten Health Check ersetzen, um 2-3 Minuten zu sparen."}
{"ts": "168:45", "speaker": "I", "text": "Wie sieht es mit den Kosten aus, wenn wir zusätzliche Automatisierung einführen?"}
{"ts": "168:52", "speaker": "E", "text": "Das initiale Setup der neuen Automation kostet etwa 15.000 €, aber wir erwarten durch reduzierte Ausfallzeiten und geringere Eskalationen langfristig Einsparungen. In Ticket DR-COST-221 haben wir das ROI-Modell hinterlegt."}
{"ts": "169:10", "speaker": "I", "text": "Verstehe. Gab es Überlegungen, ob wir statt manueller Updates Continuous Compliance Checks integrieren?"}
{"ts": "169:17", "speaker": "E", "text": "Ja, das haben wir im RFC-DR-2025-04 vorgeschlagen. Der Plan ist, dass ein Compliance-Agent alle 5 Minuten kritische Parameter prüft, inklusive IAM-Policies, um Konfigurationsdrift früh zu erkennen."}
{"ts": "169:35", "speaker": "I", "text": "Das klingt ambitioniert. Welche Risiken sehen Sie dabei?"}
{"ts": "169:40", "speaker": "E", "text": "Ein Risiko ist, dass wir bei zu aggressiven Checks false positives erzeugen, die unnötige Failover-Trigger auslösen. Wir müssen also sensible Thresholds definieren."}
{"ts": "169:55", "speaker": "I", "text": "Und wie stellen wir sicher, dass die Runbooks aktuell bleiben, insbesondere wenn sich die Architektur ändert?"}
{"ts": "170:02", "speaker": "E", "text": "Wir haben im Ops-Wiki den Prozess DR-RB-Update hinterlegt: nach jedem Architektur-Change-Request wird ein Review der betroffenen Runbooks initiiert. Das ist auch in unserem Change-Management-Tool als Pflichtfeld hinterlegt."}
{"ts": "170:20", "speaker": "I", "text": "Gut. Welche Änderungen sind konkret für die nächste Drill-Phase geplant?"}
{"ts": "170:26", "speaker": "E", "text": "Wir wollen in der Drill-Phase Q3 2025 erstmals den simultanen Ausfall von zwei Regionen simulieren, um den BLAST_RADIUS weiter zu minimieren und zu prüfen, ob unsere Cross-Region-Replication stabil genug ist."}
{"ts": "170:44", "speaker": "I", "text": "Das wird sicher anspruchsvoll. Werden dafür zusätzliche Tests mit Poseidon Networking nötig sein?"}
{"ts": "170:50", "speaker": "E", "text": "Ja, wir müssen Multi-Region-BGP-Policies anpassen. Poseidon-Team hat in Ticket NET-POSE-552 bereits zugesagt, die Failover-Routen vorab in einer isolierten Staging-Umgebung zu testen."}
{"ts": "175:48", "speaker": "I", "text": "Könnten wir jetzt noch etwas tiefer in die geplanten Änderungen für die nächste Drill-Phase eintauchen? Mich interessiert, welche konkreten Verbesserungen Sie aus den letzten Testläufen ableiten konnten."}
{"ts": "176:00", "speaker": "E", "text": "Ja, wir planen in P-TIT Phase Drill+1 vor allem den Failback-Prozess zu optimieren. Bei TEST-DR-2025-Q1 hatten wir im Ticket DR-INC-442 eine Verzögerung von knapp fünf Minuten, weil das DNS-Cutover-Skript in RB-DR-001 nicht automatisch in allen Subnetzen lief."}
{"ts": "176:18", "speaker": "I", "text": "Verstehe. Und wie wollen Sie das konkret adressieren?"}
{"ts": "176:22", "speaker": "E", "text": "Wir ergänzen RB-DR-001 um ein Pre-Check-Modul, das via API auf Poseidon Networking prüft, ob alle Routen-Tabellen in beiden Regionen synchron sind. Zusätzlich wird ein Rollback-Trigger in RB-DR-004 implementiert."}
{"ts": "176:40", "speaker": "I", "text": "Wie sieht es mit regulatorischen Änderungen aus, die unsere DR-Architektur betreffen könnten?"}
{"ts": "176:46", "speaker": "E", "text": "Ab Q3 2025 tritt die neue BSI-Richtlinie §12a in Kraft, die eine Protokollierung aller Failover-Events mit Zeitstempeln unter ±1 Sekunde Genauigkeit fordert. Nimbus Observability wird dafür um ein Audit-Log-Exporter-Plugin ergänzt."}
{"ts": "177:04", "speaker": "I", "text": "Das heißt, wir müssen auch unsere Monitoring-Runbooks anpassen?"}
{"ts": "177:09", "speaker": "E", "text": "Genau. RB-MON-002 bekommt eine neue Sektion 'Compliance Hooks', die die Exporter-Logs gegen die SLA-Definition RTO ≤ 15 Min / RPO ≤ 300 Sek verifiziert."}
{"ts": "177:24", "speaker": "I", "text": "Gibt es Risiken, dass diese zusätzlichen Checks die Wiederherstellungszeit verlängern?"}
{"ts": "177:30", "speaker": "E", "text": "Minimal. Wir haben in einer Simulation (SIM-DR-2025-05) festgestellt, dass der Overhead etwa 12 Sekunden beträgt. Das bleibt im Rahmen und ist ein akzeptabler Trade-off für die Compliance."}
{"ts": "177:46", "speaker": "I", "text": "Welche weiteren Optimierungen planen Sie, um Kosten und Performance auszubalancieren?"}
{"ts": "177:52", "speaker": "E", "text": "Wir evaluieren gerade den Einsatz von 'Warm Standby Light' in der sekundären Region. Das reduziert Compute-Kosten um ca. 35%, ohne dass das RTO merklich steigt, laut Benchmarks aus TEST-DR-2025-Q2-prep."}
{"ts": "178:08", "speaker": "I", "text": "Und wie stellen Sie sicher, dass die Runbooks bei all diesen Änderungen aktuell bleiben?"}
{"ts": "178:12", "speaker": "E", "text": "Wir haben im internen Confluence ein Living Document, verlinkt mit unserem Git-Repo. Jeder Merge-Request zu Scripts wie failover_dns.sh triggert automatisch ein Review des zugehörigen Runbook-Abschnitts."}
{"ts": "178:28", "speaker": "I", "text": "Klingt solide. Gibt es aus Ihrer Sicht noch ein Risiko, das wir bisher nicht adressiert haben?"}
{"ts": "178:34", "speaker": "E", "text": "Ein offener Punkt ist die Abhängigkeit von externen Auth-Providern. Falls deren API im DR-Fall nicht reagiert, greifen unsere Fallback-IAM-Policies erst nach 90 Sekunden. Das gilt es in DR-RFC-2025-07 noch zu verkürzen."}
{"ts": "180:08", "speaker": "I", "text": "Zum Abschluss möchte ich noch auf die potenziellen Risiken eingehen, die Sie im letzten Abschnitt angedeutet haben. Können Sie bitte konkretisieren, welche Sie als kritisch einstufen?"}
{"ts": "180:20", "speaker": "E", "text": "Ja, kritisch sehe ich vor allem die Abhängigkeit von der Region Nord-2, die wir im Stand-by fahren. Laut Risk Assessment RA-DR-042 könnte ein längerer Ausfall dort auch unsere synchronen Replikationen beeinträchtigen."}
{"ts": "180:42", "speaker": "I", "text": "Gibt es dazu schon ein konkretes Ticket oder eine interne Maßnahme?"}
{"ts": "180:50", "speaker": "E", "text": "Ja, das ist in Ticket OPS-DR-1186 dokumentiert. Wir prüfen gerade, ob wir eine zusätzliche asynchrone Replikation nach West-1 aufbauen, um den BLAST_RADIUS in so einem Fall zu minimieren."}
{"ts": "181:12", "speaker": "I", "text": "Das würde natürlich die Kosten beeinflussen. Haben Sie eine grobe Schätzung, wie sich das auf unser DR-Budget auswirkt?"}
{"ts": "181:22", "speaker": "E", "text": "Schätzungen aus der Finanzplanung gehen von +8–10 % OPEX aus, hauptsächlich wegen zusätzlicher Storage- und Netzwerkgebühren. Allerdings könnte das die RTO im Ernstfall um etwa 40 % verbessern."}
{"ts": "181:44", "speaker": "I", "text": "Wie stehen Sie persönlich zu diesem Trade-off?"}
{"ts": "181:50", "speaker": "E", "text": "Ich halte ihn für gerechtfertigt. Wenn wir die SLA-Vorgabe von 45 Minuten RTO auf unter 30 Minuten drücken können, stärken wir unser Standing bei den Kunden. Aus dem Lessons Learned von TEST-DR-2025-Q1 wissen wir, wie wertvoll jede gesparte Minute ist."}
{"ts": "182:14", "speaker": "I", "text": "Sie hatten in vorigen Antworten regulatorische Änderungen angedeutet. Können diese unsere Entscheidung beeinflussen?"}
{"ts": "182:22", "speaker": "E", "text": "Ja, die neue EU-Cloud-Resilience-Guideline CRG-2026 fordert eine geographische Entkopplung von Kern- und Backup-Regionen um mindestens 500 km. Das passt zur West-1 Option, während Nord-2 zu nah an Ost-3 liegt."}
{"ts": "182:46", "speaker": "I", "text": "Verstehe. Beeinflusst das auch unsere Runbooks wie RB-DR-001?"}
{"ts": "182:54", "speaker": "E", "text": "Ja, wir müssten die Failover-Sequenzen anpassen, da die Latenz nach West-1 höher ist. In RB-DR-001.3 würden die Health-Checks und DNS-Switches um 15–20 Sekunden verlängert, laut Simulation SIM-DR-77."}
{"ts": "183:18", "speaker": "I", "text": "Gibt es Pläne, diese Anpassungen zeitnah zu testen?"}
{"ts": "183:24", "speaker": "E", "text": "Ja, wir planen für Q3 einen Mini-Drill mit Fokus auf West-1, codename DR-MINI-2025-W1. Ziel ist es, die Änderungen im Runbook und die neuen Poseidon-Routing-Regeln unter Last zu validieren."}
{"ts": "183:46", "speaker": "I", "text": "Alles klar. Gibt es aus Ihrer Sicht sonst noch offene Themen, die wir vor der nächsten Drill-Phase adressieren sollten?"}
{"ts": "183:54", "speaker": "E", "text": "Wir sollten das Monitoring in Nimbus erweitern, um asynchrone Replikationsverzögerungen in Echtzeit anzuzeigen. Das war eine Lücke, die wir in OPS-DR-1190 bereits erfasst haben und die wir vor Q3 schließen sollten."}
{"ts": "186:08", "speaker": "I", "text": "Könnten Sie noch einmal konkret erläutern, wie beim letzten Drill die Poseidon-Verbindungen umgeschwenkt wurden?"}
{"ts": "186:15", "speaker": "E", "text": "Ja, beim TEST-DR-2025-Q1 haben wir laut Runbook RB-DR-001 in Schritt 4.3 die BGP-Routen im Poseidon-Core umgelegt. Das ging über das Automationsskript `poseidon_failover.sh`, das die neuen Endpunkte innerhalb von 120 Sekunden aktiviert hat."}
{"ts": "186:27", "speaker": "I", "text": "Gab es dabei Abweichungen von der erwarteten RTO?"}
{"ts": "186:31", "speaker": "E", "text": "Minimal – wir lagen bei 7 Minuten Gesamtzeit für den Netzwerkswitch, SLA erlaubt bis 10 Minuten. Die Verzögerung entstand, weil ein API-Token für den Nimbus-Alert-Handler abgelaufen war und manuell erneuert werden musste."}
{"ts": "186:44", "speaker": "I", "text": "Das heißt, hier kam eine Schnittstellenabhängigkeit zum Tragen?"}
{"ts": "186:48", "speaker": "E", "text": "Genau. Nimbus Observability hat die Health Checks der neuen Region nicht sofort verifiziert, weil ohne gültigen Token kein Write ins Incident-Log möglich war. Wir haben daraus eine RFC erstellt – RFC-DR-2025-07 – um Token-Rotation automatisiert einzubauen."}
{"ts": "187:02", "speaker": "I", "text": "Welche Auswirkungen hatte das auf die Gesamtbewertung des Drills?"}
{"ts": "187:06", "speaker": "E", "text": "Die KPI 'Detection-to-Mitigation' stieg leicht von 9 auf 11 Minuten, was noch im grünen Bereich liegt. In der Post-Mortem-Analyse haben wir aber empfohlen, dass Security-Tokens als DR-Critical-Assets in den Pre-Check aufgenommen werden."}
{"ts": "187:20", "speaker": "I", "text": "Klingt nach einem klaren Improvement-Point. Wie sieht es kostenmäßig aus, wenn wir zusätzliche Pre-Checks und Automatisierung einführen?"}
{"ts": "187:27", "speaker": "E", "text": "Laut Kostenkalkulation aus Ticket COST-DR-2025-02 liegen wir bei etwa +3% Betriebsausgaben pro Drill, sparen aber potenziell 15% Downtime-Kosten bei einem echten Incident. Das ist für uns ein vertretbarer Trade-off."}
{"ts": "187:40", "speaker": "I", "text": "Gab es Diskussionen im Steering Committee dazu?"}
{"ts": "187:44", "speaker": "E", "text": "Ja, wir haben das im Protokoll SC-DR-2025-03 festgehalten. Mehrheitlich Zustimmung, mit der Auflage, dass wir nach zwei Drills die Wirksamkeit neu bewerten."}
{"ts": "187:56", "speaker": "I", "text": "Welche Risiken bestehen Ihrer Einschätzung nach weiterhin trotz dieser Optimierungen?"}
{"ts": "188:00", "speaker": "E", "text": "Ein Restrisiko ist die gleichzeitige Störung in zwei Regionen – unser BLAST_RADIUS ist minimiert, aber nicht null. Außerdem könnten regulatorische Änderungen in der EU Cloud Compliance kurzfristig Anpassungen erzwingen."}
{"ts": "188:13", "speaker": "I", "text": "Wie stellen Sie sicher, dass die Runbooks bei solchen Änderungen aktuell bleiben?"}
{"ts": "188:18", "speaker": "E", "text": "Wir haben einen quartalsweisen Review-Zyklus, Ticketserie RB-VERIFY-2025, und koppeln das an die Lessons Learned der letzten GameDays. Änderungen werden versioniert und im internen Git-Repo `dr-runbooks` mit Tagging nach SLA-Revision gepflegt."}
{"ts": "193:28", "speaker": "I", "text": "Sie hatten vorhin schon den GameDay TEST-DR-2025-Q1 erwähnt. Können Sie bitte genauer beschreiben, wie der Ablauf strukturiert war?"}
{"ts": "193:39", "speaker": "E", "text": "Ja, also wir sind streng nach Runbook RB-DR-001 vorgegangen. Zunächst wurde der simulierte Ausfall in Region EU-Central-2 durch das Poseidon Networking-Team initiiert, dann folgten laut Schritt 4.3 die DNS-Failover-Regeln. Danach griff Nimbus Observability ein, um die Latenz- und Health-Checks zu überwachen."}
{"ts": "193:56", "speaker": "I", "text": "Und gab es bei dieser Sequenz etwas, das nicht wie geplant lief?"}
{"ts": "194:02", "speaker": "E", "text": "Ja, beim Cross-Region-DB-Replication gab es eine Verzögerung von 3,8 Minuten. Laut SLA hätten wir unter 2 Minuten bleiben müssen. Ursache war ein nicht aktualisierter IAM-Token-Cache, was in Ticket DR-INC-482 dokumentiert wurde."}
{"ts": "194:20", "speaker": "I", "text": "Wie haben Sie diesen IAM-Token-Cache-Fehler in den Griff bekommen?"}
{"ts": "194:27", "speaker": "E", "text": "Wir haben ein Pre-Check-Skript in RB-DR-002 aufgenommen, das vor dem Failover die Token validiert und bei Bedarf erneuert. Zusätzlich wurde in Poseidon Networking eine Hook-Funktion eingebaut, die diesen Check triggert."}
{"ts": "194:44", "speaker": "I", "text": "Hat das auch Einfluss auf die Monitoring-Strategie von Nimbus Observability gehabt?"}
{"ts": "194:50", "speaker": "E", "text": "Ja, wir haben einen neuen Alert-Typ 'TokenStale' eingeführt, der parallel zu den Latency- und Error-Rate-Metriken läuft. So können wir den Fehlerpfad schneller erkennen, bevor der eigentliche Failover beginnt."}
{"ts": "195:05", "speaker": "I", "text": "Kommen wir zu den Kosten-Performance-Tradeoffs – wie haben Sie hier zwischen Standby-Kosten und Reaktionszeit abgewogen?"}
{"ts": "195:14", "speaker": "E", "text": "Wir nutzen in der Standby-Region eine Mischung aus warm-standby für kritische Services und cold-standby für weniger zeitkritische. Das spart laut Kostenbericht Q1 rund 18% monatlich, bringt aber bei cold-standby einen zusätzlichen Spin-up von ca. 90 Sekunden mit sich."}
{"ts": "195:31", "speaker": "I", "text": "Gab es Diskussionen dazu im Change Advisory Board?"}
{"ts": "195:37", "speaker": "E", "text": "Ja, im CAB-Meeting vom 14.02.2025 wurde diese Entscheidung mit Verweis auf RFC-DR-2025-07 abgesegnet. Risikoakzeptanz war gegeben, da die betroffenen Services nicht in SLA-Kategorie A fallen."}
{"ts": "195:52", "speaker": "I", "text": "Wie sichern Sie, dass diese Abwägungen auch bei regulatorischen Änderungen Bestand haben?"}
{"ts": "196:00", "speaker": "E", "text": "Wir haben einen vierteljährlichen Compliance-Review eingeführt. Falls neue Anforderungen wie kürzere RTOs vorgeschrieben werden, passt das DR-Architekturgremium sofort die Runbooks an. Letztes Audit hat das Verfahren positiv bewertet."}
{"ts": "196:15", "speaker": "I", "text": "Und was planen Sie für die nächste Drill-Phase konkret?"}
{"ts": "196:21", "speaker": "E", "text": "Wir wollen einen simultanen Ausfall zweier Regionen simulieren, um den BLAST_RADIUS-Test zu verschärfen. Dazu wird Poseidon Networking zwei unabhängige Routing-Blackholes setzen und Nimbus Observability parallel Cross-Region-Metriken korrelieren."}
{"ts": "202:48", "speaker": "I", "text": "Sie hatten vorhin die automatisierten Failover-Skripte erwähnt. Können Sie konkret sagen, wie diese aktuell im Titan DR Drill integriert sind?"}
{"ts": "203:01", "speaker": "E", "text": "Ja, klar. Im aktuellen Drill-Setup werden die Skripte aus dem Runbook RB-DR-001 direkt durch unseren Orchestrator in Region EU-Central-1 getriggert, sobald der Nimbus Observability-Alarm AL-DR-PO-07 feuert. Das reduziert die Reaktionszeit signifikant, wir sprechen hier von unter 90 Sekunden bis zum Start der Umschaltung."}
{"ts": "203:24", "speaker": "I", "text": "Und wie wird die Ausführung dieser Skripte überwacht, um sicherzustellen, dass keine Teilschritte hängen bleiben?"}
{"ts": "203:38", "speaker": "E", "text": "Wir haben da einen zweistufigen Mechanismus: Erstens schreibt jedes Skript Status-Events in den Poseidon Networking EventBus, zweitens greift ein Kontroll-Job, der in RB-DR-004 dokumentiert ist, diese Events ab und vergleicht sie mit der erwarteten Sequenz. Bei Abweichungen geht eine Eskalation an das DR-OnCall-Team."}
{"ts": "203:58", "speaker": "I", "text": "Gab es in den letzten Tests Auslöser für solche Eskalationen?"}
{"ts": "204:12", "speaker": "E", "text": "Beim TEST-DR-2025-Q1 hatten wir in Ticket DR-INC-482 tatsächlich einen Fall, bei dem das API-Gateway in der Zielregion nicht innerhalb der RTO hochgefahren ist. Der Kontroll-Job hat das erkannt, und es wurde manuell ein Fallback durchgeführt."}
{"ts": "204:34", "speaker": "I", "text": "Welche Anpassungen wurden daraufhin vorgenommen?"}
