{"ts": "00:00", "speaker": "I", "text": "Können Sie bitte kurz Ihre Rolle im Orion Edge Gateway Projekt umreißen?"}
{"ts": "02:15", "speaker": "E", "text": "Ja, klar. Ich bin als DevOps Engineer für die komplette Build-Phase verantwortlich, das heißt von der Provisionierung der Infrastruktur bis zur Auslieferung über unsere CI/CD-Pipelines. Der Scope umfasst hier das API-Gateway mit Rate-Limiting und die Integration in unser Auth-System Aegis IAM."}
{"ts": "05:30", "speaker": "I", "text": "Und welche Hauptziele verfolgt das Projekt derzeit in dieser Phase?"}
{"ts": "08:10", "speaker": "E", "text": "Primär wollen wir die Gateway-Komponenten so aufsetzen, dass sie in der späteren Run-Phase unsere SLA-ORI-02 erfüllen – also p95 Latenz unter 120ms und 99,95% Verfügbarkeit. Parallel integrieren wir die Authentifizierung und bauen das Monitoring mit Nimbus Observability auf."}
{"ts": "12:00", "speaker": "I", "text": "Wie ordnet sich das Gateway in die Gesamtarchitektur von Novereon Systems ein?"}
{"ts": "15:20", "speaker": "E", "text": "Das Gateway sitzt am Rand unseres Orion-Clusters und vermittelt zwischen externen Clients und den internen Microservices. Es ist quasi der zentrale Eintrittspunkt, an dem wir sowohl API-Sicherheit als auch Traffic-Shaping realisieren."}
{"ts": "19:45", "speaker": "I", "text": "Welche Infrastructure-as-Code Tools setzen Sie ein?"}
{"ts": "23:10", "speaker": "E", "text": "Wir nutzen Terraform für die Cloud-Ressourcen und Ansible für das Konfigurationsmanagement auf den Edge-Nodes. Die Module sind im Repo IaC-ORI hinterlegt, und jede Änderung muss über Merge-Requests mit automatisierten Checks gehen."}
{"ts": "28:50", "speaker": "I", "text": "Wie stellen Sie sicher, dass Blue/Green Deployments gemäß Runbook RB-GW-011 ablaufen?"}
{"ts": "32:30", "speaker": "E", "text": "Wir haben in der Jenkins-Pipeline einen Stage-Step, der den Traffic-Switch nur nach erfolgreichem Canary-Test in Staging freigibt. RB-GW-011 beschreibt die Sequenz genau – inkl. Health-Checks und manueller Approval-Gates."}
{"ts": "37:00", "speaker": "I", "text": "Welche Metriken nutzen Sie, um die SLA-ORI-02 p95 Latency unter 120ms zu halten?"}
{"ts": "41:20", "speaker": "E", "text": "Neben der Latenz selbst tracken wir auch Upstream-Response-Zeiten, Queue-Längen und Rate-Limit-Verletzungen. Nimbus Observability gibt uns dafür Dashboards und Alert-Definitionen, die wir nach den SLA-Vorgaben eingestellt haben."}
{"ts": "46:40", "speaker": "I", "text": "Wie wird POL-QA-014 im Kontext des Gateways angewendet?"}
{"ts": "50:30", "speaker": "E", "text": "POL-QA-014 verlangt für jede Core-Komponente unit-, integration- und load-tests. Für das Gateway haben wir alle drei Stufen automatisiert, und sie werden bei jedem Merge-Request bzw. Nacht-Build getriggert."}
{"ts": "55:00", "speaker": "I", "text": "Und wie dokumentieren Sie Abweichungen und Lessons Learned aus Tickets wie GW-4821?"}
{"ts": "60:00", "speaker": "E", "text": "GW-4821 war ein MTLS-Handshake-Bug, den wir im Test entdeckt haben. Wir dokumentieren solche Fälle im Confluence-Bereich 'Gateway Incidents' mit Link zum Jira-Ticket, Root-Cause-Analyse und Anpassungen an Runbooks."}
{"ts": "90:00", "speaker": "I", "text": "Lassen Sie uns jetzt einmal tiefer auf die Entscheidung eingehen, Blue/Green gegenüber Canary zu bevorzugen. Können Sie bitte die Architekturüberlegungen dahinter darlegen?"}
{"ts": "90:12", "speaker": "E", "text": "Ja, gern. Blue/Green war für uns im Orion Edge Gateway deutlich kalkulierbarer, weil wir die vollständige Trennung der Stages im Runbook RB-GW-011 genau definiert haben. Canary hätte im Zusammenspiel mit Aegis IAM und den strengen Auth-Policies zu unvorhersehbaren Auth-Token-Invalidierungen geführt, was bei SLA-ORI-02 problematisch wäre."}
{"ts": "90:35", "speaker": "I", "text": "Verstehe. Gab es auch Überlegungen bezüglich der Observability-Signale, die gegen Canary sprachen?"}
{"ts": "90:45", "speaker": "E", "text": "Definitiv. Nimbus Observability liefert uns zwar feingranulare Metriken, aber bei Canary hätten wir Signale aus zwei Versionen gleichzeitig korrelieren müssen. Das erhöht die Komplexität bei der Fehlerursachenanalyse enorm, vor allem wenn Rate-Limiting-Events und Auth-Fehler gleichzeitig auftreten."}
{"ts": "91:06", "speaker": "I", "text": "Wie wirken sich diese Entscheidungen auf das Risiko-Management aus, insbesondere im Hinblick auf Latenz?"}
{"ts": "91:15", "speaker": "E", "text": "Mit Blue/Green können wir bei einem Latenz-Anstieg über 120 ms p95 gemäß SLA-ORI-02 sofort auf die andere Color umschalten. Dieses Vorgehen ist auch in unserem Incident-Runbook RB-INC-004 dokumentiert. Bei Canary wäre der Rollback zeitaufwendiger, weil Teilmengen von Traffic umgeroutet werden müssten."}
{"ts": "91:39", "speaker": "I", "text": "Falls ein MTLS-Handshake-Bug wie in GW-4821 erneut auftritt – was passiert als Erstes?"}
{"ts": "91:49", "speaker": "E", "text": "Erstens aktivieren wir sofort das Failover im Gateway-Cluster, wie in RB-SEC-021 beschrieben. Parallel wird der betroffene Service-Port in der Target-Group deaktiviert, um fehlerhafte Handshakes zu isolieren. Danach starten wir einen Hotfix-Build aus unserer CI/CD-Pipeline."}
{"ts": "92:12", "speaker": "I", "text": "Welche Lessons Learned haben Sie aus GW-4821 in Bezug auf Testautomatisierung gezogen?"}
{"ts": "92:22", "speaker": "E", "text": "Wir haben die mTLS-Testcases in unserer Staging-Umgebung erweitert und in POL-QA-014 verankert. Zusätzlich läuft seitdem ein Pre-Deployment-Skript, das Handshake-Simulationen gegen Aegis IAM durchführt, um Zertifikats- und Cipher-Kompatibilität zu prüfen."}
{"ts": "92:46", "speaker": "I", "text": "Gibt es aktuelle Risiken, die Sie im Build-Prozess sehen, die noch nicht mitigiert sind?"}
{"ts": "92:55", "speaker": "E", "text": "Ja, wir haben ein latentes Risiko bei extremen Lastspitzen, wenn mehrere Downstream-APIs gleichzeitig throttlen. Das kann zu einer Kaskade von Retries führen. Wir evaluieren gerade ein dynamisches Backpressure-Modul, das die Retry-Rate adaptiv drosselt."}
{"ts": "93:18", "speaker": "I", "text": "Wie dokumentieren Sie solche Evaluations- und Entscheidungsprozesse?"}
{"ts": "93:27", "speaker": "E", "text": "Wir verwenden Confluence-Seiten mit Verlinkung auf die jeweiligen RFC-Dokumente, z. B. RFC-GW-022 für das Backpressure-Modul. Zusätzlich werden alle Designentscheidungen in unserem Architecture Decision Record (ADR) Repository versioniert."}
{"ts": "93:49", "speaker": "I", "text": "Und abschließend: Welche Maßnahme würden Sie sofort priorisieren, wenn SLA-ORI-02 zweimal in Folge verletzt wird?"}
{"ts": "93:58", "speaker": "E", "text": "Unmittelbar einen Traffic-Shed über die Load Balancer Layer einleiten, um die Kern-User Journeys zu schützen. Danach ein Post-Mortem nach Muster PM-GW-005 durchführen, um Ursachen zu identifizieren und Runbooks wie RB-GW-011 oder RB-INC-004 anzupassen."}
{"ts": "98:00", "speaker": "I", "text": "Lassen Sie uns noch etwas tiefer in die Lessons Learned aus GW-4821 gehen. Was genau haben Sie im Nachgang angepasst?"}
{"ts": "98:15", "speaker": "E", "text": "Wir haben unmittelbar nach der Analyse des MTLS-Handshake-Bugs die Handshake-Timeouts im Gateway-Config-Modul von 5s auf 8s erhöht und parallel in Ansible-Playbooks die Cipher-Suites aktualisiert. Außerdem wurde in RB-GW-011 ein zusätzlicher Schritt für Pre-Prod Smoke-Tests ergänzt."}
{"ts": "98:38", "speaker": "I", "text": "War das eine rein technische Anpassung oder auch prozessual?"}
{"ts": "98:50", "speaker": "E", "text": "Beides. Technisch wie beschrieben, prozessual haben wir ein neues Runbook RB-GW-014 erstellt, das alle MTLS-Handshake-Checks dokumentiert. Dieses wird jetzt in der CI/CD-Pipeline vor Blue/Green-Rollouts automatisch aufgerufen."}
{"ts": "99:15", "speaker": "I", "text": "Gab es bei der Umsetzung Hindernisse, zum Beispiel in Verbindung mit Aegis IAM?"}
{"ts": "99:28", "speaker": "E", "text": "Ja, die Zertifikatsrotation im Aegis IAM lief asynchron zu unseren Gateways. Wir mussten über einen kleinen Wrapper-Service einen Sync-Mechanismus implementieren, der bei jedem Deployment den gültigen Public Key aus Aegis abruft und in den Gateway-Keystore schreibt."}
{"ts": "99:55", "speaker": "I", "text": "Und wie haben Sie das getestet, um die SLA-ORI-02 nicht zu gefährden?"}
{"ts": "100:08", "speaker": "E", "text": "Wir haben Lasttests mit JMeter-Skripten gefahren, die parallel MTLS-Verbindungen und normale Token-Verbindungen simulierten. Die p95-Latenz blieb unter 110 ms, was wir in Nimbus Observability Dashboards verifiziert haben."}
{"ts": "100:34", "speaker": "I", "text": "In Bezug auf Backpressure-Handling: Haben sich durch GW-4821 auch hier Änderungen ergeben?"}
{"ts": "100:46", "speaker": "E", "text": "Indirekt ja. Wir haben den Circuit Breaker in der Downstream-API-Komposition strenger konfiguriert. Wenn MTLS-Fehler auftreten, wird der betroffene Service schneller aus dem Pool entfernt, um keine Kaskadeneffekte zu verursachen."}
{"ts": "101:10", "speaker": "I", "text": "Gab es Widerstand im Team gegen diese restriktiveren Limits?"}
{"ts": "101:22", "speaker": "E", "text": "Ein wenig, weil ein engerer Circuit Breaker zu mehr Fallbacks auf Caches führt. Aber wir haben anhand von Metrics aus den letzten drei Incidents (INC-GW-772, INC-GW-789, GW-4821) gezeigt, dass so die Gesamtverfügbarkeit steigt."}
{"ts": "101:50", "speaker": "I", "text": "Wie dokumentieren Sie diese Erkenntnisse, damit sie in künftige Projekte einfließen?"}
{"ts": "102:02", "speaker": "E", "text": "Neben der Aktualisierung der Runbooks pflegen wir ein internes Confluence-Space 'Gateway Patterns', in dem jedes Ticket mit Root Cause, Impact, Fix und Prevention-Maßnahmen gelistet ist. GW-4821 ist dort als Pattern 'MTLS Pre-Validation' dokumentiert."}
{"ts": "102:25", "speaker": "I", "text": "Wenn Sie auf die Build-Phase zurückblicken: Welche Entscheidung würden Sie im Nachhinein vielleicht anders treffen?"}
{"ts": "102:40", "speaker": "E", "text": "Vielleicht hätte ein früherer Canary-Test mit kleiner Traffic-Subset-Ausleitung einige der Probleme frühzeitig gezeigt. Wir haben uns aus Deploy-Risiko-Gründen für Blue/Green entschieden, aber retrospektiv war das in Bezug auf MTLS-Bugs weniger optimal."}
{"ts": "114:00", "speaker": "I", "text": "Lassen Sie uns noch einmal konkret auf die Lessons Learned aus GW-4821 zurückkommen. Gab es im Nachgang Anpassungen am Runbook RB-GW-011?"}
{"ts": "114:05", "speaker": "E", "text": "Ja, wir haben tatsächlich die Pre-Deployment Checks erweitert. Vorher haben wir MTLS-Handshake-Fehler nur in Stage-Logs gesucht, jetzt gibt es einen zusätzlichen automatisierten Curl-Test im Blue-Cluster, bevor der Traffic-Switch erfolgt."}
{"ts": "114:12", "speaker": "I", "text": "Und diese Tests, werden die über die CI/CD-Pipeline getriggert oder manuell?"}
{"ts": "114:17", "speaker": "E", "text": "Die sind voll in die Jenkins-Pipeline integriert. Step 'preSwitchValidation' nutzt ein Ansible-Play, das auf Basis der in Terraform hinterlegten Endpunkte arbeitet."}
{"ts": "114:24", "speaker": "I", "text": "Wie stellen Sie sicher, dass in dieser Phase keine Latenzspitzen auftreten, die die SLA-ORI-02 verletzen könnten?"}
{"ts": "114:30", "speaker": "E", "text": "Wir haben in Nimbus Observability einen speziellen p95-Live-Check konfiguriert. Bevor der Switch ausgelöst wird, prüfen wir 30 Sekunden lang die Antwortzeiten gegen den Schwellenwert 120ms."}
{"ts": "114:38", "speaker": "I", "text": "Interessant. Gab es seit diesen Änderungen noch mal eine ähnliche Störung?"}
{"ts": "114:42", "speaker": "E", "text": "Nein, bisher nicht. Wir hatten einmal eine knappe Unterschreitung des Limits, aber der PreSwitchCheck hat den GoLive verzögert, bis es stabil war."}
{"ts": "114:49", "speaker": "I", "text": "Wie gehen Sie in solchen Fällen mit dem Deployment-Zeitfenster um?"}
{"ts": "114:54", "speaker": "E", "text": "Wir haben im Change-Window laut RFC-ORI-DEP-07 einen Puffer von 30 Minuten. Wird der überschritten, planen wir laut Runbook einen Rollback oder verschieben auf das nächste Fenster."}
{"ts": "115:02", "speaker": "I", "text": "Gab es intern Diskussionen, Canary Deployments zu nutzen, um flexibler zu sein?"}
{"ts": "115:06", "speaker": "E", "text": "Ja, aber wir haben uns dagegen entschieden, weil das Gateway als zentraler Auth- und Rate-Limit-Knoten einheitlich agieren muss. Mischzustände hätten das Debugging erschwert."}
{"ts": "115:14", "speaker": "I", "text": "Verstehe. Würden Sie sagen, dass diese Entscheidung auch Risiken birgt?"}
{"ts": "115:19", "speaker": "E", "text": "Natürlich, der größte Nachteil ist, dass ein Fehler im Blue-Cluster beim Switch sofort alle Clients betrifft. Deswegen investieren wir so stark in Vorab-Validierungen und Staging-Tests."}
{"ts": "115:27", "speaker": "I", "text": "Abschließend: Wenn morgen ein neuer MTLS-Bug auftritt, wie würden Sie vorgehen?"}
{"ts": "115:33", "speaker": "E", "text": "Sofort den Traffic zurück auf Green leiten, Incident nach INC-GW-09 eröffnen, Logs und Observability-Metriken sichern, dann mit dem Security-Team und IAM-Verantwortlichen aus Aegis eine Root-Cause-Analyse starten."}
{"ts": "116:00", "speaker": "I", "text": "Lassen Sie uns bitte noch einmal konkret auf das Monitoring im Betrieb eingehen – wie stellen Sie sicher, dass Anomalien auch bei niedriger Last erkannt werden?"}
{"ts": "116:10", "speaker": "E", "text": "Wir haben im Build bereits Low-Traffic-Scenarios in Nimbus Observability modelliert. Das heißt, wir nutzen synthetische Transactions, um auch bei off-peak Zeiten Latenzspitzen oder Auth-Failures zu erkennen. Die Runbook-Referenz hier ist RB-OBS-007."}
{"ts": "116:23", "speaker": "I", "text": "Und wie binden Sie diese synthetischen Tests in die CI/CD-Pipeline ein?"}
{"ts": "116:31", "speaker": "E", "text": "Nach jedem erfolgreichen Blue/Green-Switch wird ein Jenkins-Stage 'Synthetic Probes' getriggert, der gegen beide Env-Varianten läuft. Erst wenn die Metriken im SLA-ORI-02-Fenster bleiben, wird der alte Slot deprovisioned."}
{"ts": "116:44", "speaker": "I", "text": "Gab es in letzter Zeit Fälle, in denen dieser Schritt Probleme aufgezeigt hat?"}
{"ts": "116:50", "speaker": "E", "text": "Ja, bei Build #1425 hat eine synthetische Auth-Transaktion einen MTLS-Handshake-Timeout entdeckt – das war eine Regression ähnlich wie im GW-4821 Bug, aber verursacht durch ein fehlerhaftes Ansible-Template."}
{"ts": "117:03", "speaker": "I", "text": "Wie sind Sie da vorgegangen?"}
{"ts": "117:09", "speaker": "E", "text": "Zuerst haben wir laut Incident-Runbook RB-INC-003 auf die alte Green-Umgebung zurückgeschwenkt, dann den Template-Fix in einem Hotfix-Branch implementiert und via Pull-Request mit QA-Check nach POL-QA-014 deployed."}
{"ts": "117:25", "speaker": "I", "text": "Könnten solche Checks auch für Performance-Degradation im Rate Limiting genutzt werden?"}
{"ts": "117:31", "speaker": "E", "text": "Genau. Wir haben Threshold-Tests, die gezielt an der Grenze von 95% des erlaubten TPS laufen, um zu prüfen, ob der Backpressure-Mechanismus korrekt greift und keine Downstream-APIs überlastet."}
{"ts": "117:45", "speaker": "I", "text": "Wie dokumentieren Sie solche Vorfälle im Projekt?"}
{"ts": "117:51", "speaker": "E", "text": "Es gibt ein Confluence-Log 'Gateway Lessons Learned', wo wir Tickets wie GW-4821 oder den Build #1425-Fall mit Root-Cause, Impact-Analyse und Runbook-Referenzen dokumentieren. Das fließt dann in unsere wöchentlichen Architecture Reviews ein."}
{"ts": "118:05", "speaker": "I", "text": "Welche Rolle spielt dabei das Security-Team?"}
{"ts": "118:11", "speaker": "E", "text": "Security prüft alle Auth- und MTLS-bezogenen Incidents, um sicherzustellen, dass keine Policy-Verletzungen bestehen. Sie geben auch Input für die Anpassung von RB-GW-011, falls z.B. neue Cipher-Suites erforderlich sind."}
{"ts": "118:25", "speaker": "I", "text": "Sehen Sie aktuell noch offene Risiken, die nicht hinreichend mitigiert sind?"}
{"ts": "118:40", "speaker": "E", "text": "Ein Restrisiko liegt im Zusammenspiel zwischen Aegis IAM und dem Gateway, wenn bei Lastspitzen beide Seiten simultan ihre Rate-Limits anpassen. Hier planen wir in der nächsten Sprintplanung eine koordinierte Limit-Synchronisation einzubauen, um Latenzspitzen zu vermeiden."}
{"ts": "124:00", "speaker": "I", "text": "Lassen Sie uns bitte noch etwas tiefer in die Lessons Learned aus GW-4821 einsteigen – was hat sich im Post-Mortem als entscheidend herausgestellt?"}
{"ts": "124:20", "speaker": "E", "text": "Das Wichtigste war, dass wir den MTLS-Handshake nicht isoliert betrachtet haben, sondern die gesamte Kette bis zum Downstream-Service. Im Post-Mortem, dokumentiert in Confluence unter PM-GW-202, haben wir gesehen, dass die fehlende Zwischenzertifikatsrotation im Aegis IAM letztlich die Latenzspitzen verursachte."}
{"ts": "124:50", "speaker": "I", "text": "Also ein Zusammenspiel zwischen Auth und Netzwerk-Layer – wie haben Sie das in den Runbooks abgebildet?"}
{"ts": "125:10", "speaker": "E", "text": "Wir haben RB-GW-015 ergänzt, um bei Anomalien im p95 Latency-Chart automatisch ein Zertifikats-Check-Script via Ansible zu triggern. Zusätzlich wurde ein Alert in Nimbus Observability auf den Status des Intermediate CA gesetzt."}
{"ts": "125:35", "speaker": "I", "text": "Und wie wirkt sich das auf die Einhaltung von SLA-ORI-02 aus?"}
{"ts": "125:50", "speaker": "E", "text": "Positiv. Seit Implementierung haben wir keine Überschreitung der 120 ms p95 mehr gesehen. Selbst bei Lasttests mit simulierten 1,2 M Requests/min blieb die Latenz stabil."}
{"ts": "126:15", "speaker": "I", "text": "Gab es bei der Blue/Green-Strategie Anpassungen, um solche Zertifikatsprobleme im Vorfeld zu erkennen?"}
{"ts": "126:30", "speaker": "E", "text": "Ja, wir fahren jetzt vor dem Umschalten einen Pre-Green-Zertifikats-Healthcheck, den wir in RB-GW-011 als Pflichtschritt aufgenommen haben. Das reduziert das Risiko, dass der Green-Cluster fehlerhafte Zertifikate ausliefert."}
{"ts": "126:55", "speaker": "I", "text": "Wie gehen Sie mit der Dokumentation solcher Änderungen um, um Audit-Compliance sicherzustellen?"}
{"ts": "127:10", "speaker": "E", "text": "Wir pflegen jede Runbook-Änderung in unserem internen Change-Management-System nach RFC-CHG-019 ein, mit Verweis auf das auslösende Ticket. Für GW-4821 ist das in CHG-REQ-774 dokumentiert."}
{"ts": "127:35", "speaker": "I", "text": "Stichwort Risiko: Gibt es noch offene Punkte, die Sie als kritisch ansehen?"}
{"ts": "127:50", "speaker": "E", "text": "Die größte offene Baustelle ist das Backpressure-Handling bei gleichzeitigen Authentifizierungen. Wenn Aegis IAM unter Last steht, müssen wir verhindern, dass Requests in der Gateway-Queue verhungern."}
{"ts": "128:15", "speaker": "I", "text": "Wie wollen Sie das technisch adressieren?"}
{"ts": "128:30", "speaker": "E", "text": "Wir evaluieren gerade adaptive Rate-Limits, die sich dynamisch am Health-Status des IAM orientieren. Ein Prototyp ist als Feature-Flag FG-ORI-AL-002 im Staging aktiv."}
{"ts": "128:50", "speaker": "I", "text": "Klingt nach einem weiteren Trade-off zwischen Durchsatz und Stabilität."}
{"ts": "129:00", "speaker": "E", "text": "Genau. Wir müssen den Sweet Spot finden, bei dem wir die SLA einhalten, ohne unnötig viele Requests abzulehnen. Die Observability-Metriken helfen uns dabei, Trends früh zu erkennen und Feature Flags gezielt zu justieren."}
{"ts": "132:00", "speaker": "I", "text": "Könnten Sie bitte noch einmal detaillieren, wie genau Sie die Lessons Learned aus GW-4821 in den aktuellen Build-Prozess integriert haben?"}
{"ts": "132:15", "speaker": "E", "text": "Ja, sicher. Wir haben nach dem MTLS-Handshake-Bug aus GW-4821 ein eigenes Validierungs-Script in unsere Ansible-Rollen eingefügt, das vor Deployments die Zertifikatsketten überprüft. Außerdem wurde im Runbook RB-GW-011 ein zusätzlicher Checkpoint vor dem Traffic-Switch ergänzt."}
{"ts": "132:38", "speaker": "I", "text": "Und wird dieser Check automatisiert oder manuell angestoßen?"}
{"ts": "132:44", "speaker": "E", "text": "Automatisiert im CI-Stage 'pre-prod-verify'. Falls er fehlschlägt, blockiert er das Blue/Green Deployment komplett. Wir haben das so gestaltet, um menschliche Fehler auszuschließen."}
{"ts": "133:02", "speaker": "I", "text": "Wie wirkt sich das auf die Durchlaufzeit Ihres Deployments aus?"}
{"ts": "133:08", "speaker": "E", "text": "Minimal, etwa +90 Sekunden. Aber der Zugewinn an Sicherheit ist hoch, gerade weil SLA-ORI-02 sehr strikt ist und wir bei Latenzspikes keinerlei zusätzliche Ausfälle riskieren wollen."}
{"ts": "133:26", "speaker": "I", "text": "Sie hatten vorhin die p95 Latency angesprochen. Können Sie ein konkretes Beispiel aus der letzten Messung nennen?"}
{"ts": "133:34", "speaker": "E", "text": "Ja, in der letzten Woche lagen wir bei 112 ms p95 unter Produktionslast mit aktiviertem Rate-Limiting. Wir haben das in Nimbus Observability Dashboard ORI-LAT-Graph validiert."}
{"ts": "133:52", "speaker": "I", "text": "Gab es dabei auffällige Korrelationen mit Auth oder Downstream-APIs?"}
{"ts": "134:00", "speaker": "E", "text": "Ja, interestingly, when Aegis IAM token refresh spikes occurred, wir sahen einen leichten Anstieg auf 118 ms. Das Downstream-Team konnte das nach einem Patch im Service 'NovaProfile' stabilisieren."}
{"ts": "134:22", "speaker": "I", "text": "Wie adressieren Sie in solchen Fällen das Zusammenspiel der Teams?"}
{"ts": "134:28", "speaker": "E", "text": "Durch ein cross-team Incident Review. Wir benutzen dafür das interne Template IR-TPL-07, in dem Steps, Root-Cause und Preventive Actions dokumentiert werden. Lessons Learned fließen direkt in unsere Backlog Groomings ein."}
{"ts": "134:50", "speaker": "I", "text": "Gibt es aktuell noch offene Risiken, die Sie als kritisch einstufen würden?"}
{"ts": "134:56", "speaker": "E", "text": "Ja, wir haben noch das Thema 'Burst Traffic' aus Region APAC. Das könnte unser Rate-Limit überschreiten, wenn der Backpressure-Handler nicht schnell genug skalieren kann. Wir testen gerade eine adaptive Queue-Size im Feature-Branch ORI-RL-ADAPT."}
{"ts": "135:20", "speaker": "I", "text": "Falls dieser Handler unter Last ausfällt, wie sieht Ihr Sofortplan aus?"}
{"ts": "135:26", "speaker": "E", "text": "Wir würden gemäß Contingency-Runbook CRB-GW-005 sofort in den 'Safe Mode' gehen: Alle neuen Requests mit HTTP 503 ablehnen, bestehende Sessions sauber terminieren und parallel die Skalierungsregeln anpassen. Dieser Plan wurde nach einem Lasttest im Ticket GW-4972 validiert."}
{"ts": "148:00", "speaker": "I", "text": "Lassen Sie uns noch einmal tiefer in die Lessons Learned aus GW-4821 einsteigen – welche unmittelbaren Changes haben Sie damals in den Deployment-Skripten vorgenommen?"}
{"ts": "148:15", "speaker": "E", "text": "Direkt nach der Root Cause Analyse haben wir in Ansible ein zusätzliches Pre-Check-Task eingeführt, das vor jedem Blue/Green Switch prüft, ob der MTLS-Handshake mit Aegis IAM in <50 ms erfolgt. Diese Änderung wurde als Hotfix in Branch fix/mtls-precheck committed und über unser CI innerhalb von 2h in Prod deployt."}
{"ts": "148:42", "speaker": "I", "text": "Gab es dabei Konflikte mit RB-GW-011 oder musste das Runbook angepasst werden?"}
{"ts": "148:55", "speaker": "E", "text": "Ja, wir haben RB-GW-011 in Abschnitt 4.3 ergänzt: Zusätzlich zu Health-Checks auf HTTP-Ebene gibt es jetzt einen spezifischen TLS-Layer Test. Das war ein Minor-Update, Runbook-Version 1.4.2, genehmigt durch Change Advisory Board am 12.05."}
{"ts": "149:20", "speaker": "I", "text": "Wie haben Sie sichergestellt, dass diese Änderung nicht zu False Positives führt – gerade in Staging, wo Zertifikate manchmal self-signed sind?"}
{"ts": "149:38", "speaker": "E", "text": "Wir haben in den Pre-Check-Tasks eine Environment-Variable STAGE_MODE eingebaut. In Staging werden nur die Handshake-Zeiten geloggt, aber nicht geblockt, um Testläufe nicht unnötig abzubrechen. In Prod wird strikt auf Abbruch bei >50 ms geprüft."}
{"ts": "150:02", "speaker": "I", "text": "Können Sie die Verbindung dieser Änderung zu den SLA-ORI-02 Metriken erläutern?"}
{"ts": "150:15", "speaker": "E", "text": "Klar. Ein langsamer MTLS-Handshake verzögert den ersten Request deutlich. Wenn der Pre-Check das früh erkennt, verhindern wir, dass eine neue Green-Umgebung mit defekter Auth in Betrieb geht. So vermeiden wir p95 Latenzspitzen >120 ms, die sonst ins SLA-Reporting einfließen würden."}
{"ts": "150:42", "speaker": "I", "text": "Gab es auch Anpassungen im Bereich Observability – Dashboards oder Alerts?"}
{"ts": "150:55", "speaker": "E", "text": "Ja, in Nimbus Observability haben wir ein neues Panel 'TLS Handshake Metrics' im Gateway-Board angelegt. Zusätzlich feuert ein Alert GW-TLS-ALRT-07, wenn die 95. Perzentilzeit über 45 ms geht, damit wir proaktiv reagieren."}
{"ts": "151:20", "speaker": "I", "text": "Das klingt nach einer recht engen Verzahnung von Deployments und Monitoring. Gab es Bedenken, dass mehr Checks die Deploymentdauer zu stark verlängern?"}
{"ts": "151:35", "speaker": "E", "text": "Anfangs ja. Ein TLS-Pre-Check dauert ca. 300 ms. Bei 20 parallelen Targets summiert sich das, aber wir haben die Checks parallelisiert. Netto hat sich die Deploymentzeit nur um ~6 Sekunden verlängert, was im Rahmen der Deploy-Zeitfenster gemäß POL-QA-014 liegt."}
{"ts": "151:58", "speaker": "I", "text": "Wie wurde diese Optimierung dokumentiert, um in zukünftigen Projekten wiederverwendbar zu sein?"}
{"ts": "152:10", "speaker": "E", "text": "Wir haben ein internes TechNote-Dokument DOC-GW-DEP-022 erstellt. Darin sind die Ansible-Tasks, Parallelisierungsstrategie und Metrik-Thresholds beschrieben. Dieses Dokument ist jetzt in der internen Knowledge Base unter 'Deployment Patterns' abgelegt."}
{"ts": "152:32", "speaker": "I", "text": "Gibt es Pläne, diese Checks auch auf andere Schnittstellen wie Rate-Limit-Services auszuweiten?"}
{"ts": "152:45", "speaker": "E", "text": "Ja, wir evaluieren gerade einen Pre-Check für den Redis-Cluster des Rate-Limiters. Idee ist, vor dem Umschalten sicherzustellen, dass die Kapazität >95% des erwarteten Traffic-Bursts liegt. Damit würden wir die Resilienz gegen Backpressure-Events weiter erhöhen."}
{"ts": "152:00", "speaker": "I", "text": "Kommen wir noch einmal auf die Lessons Learned aus GW-4821 zurück – haben Sie daraus neue Runbook-Einträge abgeleitet?"}
{"ts": "152:15", "speaker": "E", "text": "Ja, wir haben RB-GW-015 ergänzt, das explizit einen MTLS-Handshake-Retry-Mechanismus beschreibt. Zusätzlich gibt es jetzt im RB-GW-011 einen Hinweis, wie man bei TLS-Fehlercodes 40x sofort den Traffic in die Green-Umgebung schaltet."}
{"ts": "152:42", "speaker": "I", "text": "Und wie ist das in die CI/CD-Pipeline integriert, damit solche Änderungen nicht unbemerkt bleiben?"}
{"ts": "152:55", "speaker": "E", "text": "Wir haben einen Pre-Deploy-Check eingebaut, der die Runbook-Version prüft und mit der im Release-Manifest vergleicht. Wenn RB-GW-015 oder relevante Teile von RB-GW-011 geändert wurden, wird ein zusätzlicher Canary-Testlauf in der Staging-Umgebung gestartet – obwohl wir produktiv Blue/Green fahren."}
{"ts": "153:20", "speaker": "I", "text": "Interessant, also eine Art Hybrid-Test. Gibt es dafür auch ein Monitoring-Hook?"}
{"ts": "153:34", "speaker": "E", "text": "Ja, Nimbus Observability sendet Custom Events mit dem Tag 'runbook_change'. Das wird in unserem Alertmanager auf eine niedrige Schwelle gesetzt, sodass der On-Call sofort eine Benachrichtigung bekommt."}
{"ts": "153:55", "speaker": "I", "text": "Gab es schon Situationen, wo diese schnelle Benachrichtigung kritisch war?"}
{"ts": "154:08", "speaker": "E", "text": "Einmal, bei Ticket GW-4932, hat ein Entwickler versehentlich eine alte Cipher Suite wieder aktiviert. Durch den Hook haben wir das innerhalb von Minuten bemerkt und den Rollout pausiert."}
{"ts": "154:28", "speaker": "I", "text": "Sie sprachen vorhin über p95-Latenz. Haben Sie bei Anpassungen an der Auth-Schicht nach GW-4821 Veränderungen gemessen?"}
{"ts": "154:43", "speaker": "E", "text": "Nach dem Patch ist die p95-Latenz im Auth-Handshake von durchschnittlich 118ms auf 102ms gefallen. Wir führen das auf optimierte Session-Reuse und bessere Connection-Pooling-Parameter zurück."}
{"ts": "155:02", "speaker": "I", "text": "Wie stellen Sie sicher, dass diese Optimierungen langfristig halten und nicht durch spätere Änderungen verloren gehen?"}
{"ts": "155:16", "speaker": "E", "text": "Wir haben in POL-QA-014 jetzt eine feste Regressionstest-Stufe 'AuthPerf' vorgeschrieben, die bei jeder API-Gateway-Änderung getriggert wird. Außerdem gibt es in den Terraform-Modulen einen Default-Parameter-Block, der die Poolgrößen fixiert."}
{"ts": "155:42", "speaker": "I", "text": "Gibt es Risiken, dass diese Fixierung in Terraform zu Inflexibilität führt, beispielsweise bei Lastspitzen?"}
{"ts": "155:56", "speaker": "E", "text": "Das Risiko besteht. Wir haben deshalb einen Override-Mechanismus dokumentiert: Mit einem Feature-Flag kann der On-Call die Poolgrößen temporär anpassen, bis der nächste reguläre Deploy die Defaults wiederherstellt."}
{"ts": "156:18", "speaker": "I", "text": "Klingt pragmatisch. Letzte Frage: Welche offenen Risiken bleiben trotz aller Maßnahmen für das Orion Edge Gateway bestehen?"}
{"ts": "156:36", "speaker": "E", "text": "Größtes Restrisiko ist aktuell ein potenzieller Deadlock zwischen Rate-Limiting- und Auth-Pipelines bei extremen Burst-Lasten. Wir simulieren das im ChaosLab, aber ein vollständiger Beweis fehlt. Wir haben dazu ein offenes RFC-Doc RFC-GW-27, in dem wir mögliche Queue-Bypass-Mechanismen evaluieren."}
{"ts": "160:00", "speaker": "I", "text": "Zum Abschluss würde mich interessieren, wie Sie die Lessons Learned aus GW-4821 konkret in Ihre zukünftigen Blue/Green Rollouts einfließen lassen."}
{"ts": "160:05", "speaker": "E", "text": "Wir haben aus GW-4821 vor allem gelernt, das MTLS-Handshake-Monitoring früher im Pipeline-Flow zu verankern. Im Runbook RB-GW-011 haben wir ein Pre-Deployment Checkpoint ergänzt, der via Ansible-Task einen Handshake-Test gegen die Aegis-IAM Staging-Instanz ausführt."}
{"ts": "160:15", "speaker": "I", "text": "Und wie sichern Sie ab, dass dieser Check nicht nur ein formaler Haken ist, sondern tatsächlich Probleme erkennt?"}
{"ts": "160:20", "speaker": "E", "text": "Wir haben die Testfälle so erweitert, dass sie Fehlercodes und Zeitüberschreitungen in die Nimbus Observability Alerts einspeisen. Das heißt, wenn der Handshake >200 ms dauert oder ein Zertifikat invalid ist, schlägt der Jenkins-Job fehl."}
{"ts": "160:32", "speaker": "I", "text": "Gab es schon einen Fall, wo dieser neue Check gegriffen hat?"}
{"ts": "160:37", "speaker": "E", "text": "Ja, im Sprint S-24 hat er einen Konfigurationsfehler bei einem neuen Zertifikat erkannt, bevor es in die Green-Umgebung ging. Das hat uns einen potenziellen Ausfall im Live-Traffic erspart."}
{"ts": "160:45", "speaker": "I", "text": "Welche Rolle spielt dabei die Dokumentation nach POL-QA-014?"}
{"ts": "160:52", "speaker": "E", "text": "Jeder dieser Vorfälle wird in Confluence nach dem QA-Template abgelegt, inklusive Ticket-ID, Logs und Root-Cause-Analyse. Das Ticket GW-4899 dazu enthält z.B. die vollständige TLS-Dump-Analyse."}
{"ts": "161:03", "speaker": "I", "text": "Können Sie noch etwas zu den Metriken sagen, die Sie für SLA-ORI-02 überwachen, wenn ein neuer Build live geht?"}
{"ts": "161:08", "speaker": "E", "text": "Wir tracken p95 und p99 Latenzen pro Endpoint, Error-Rate <0,1 %, und zusätzlich die Auth-Latenz separat. Die Werte werden in einem 5-Minuten-Intervall an das Team gepusht, um sofort auf Auffälligkeiten reagieren zu können."}
{"ts": "161:20", "speaker": "I", "text": "Wie reagieren Sie, wenn p95 über 120 ms springt?"}
{"ts": "161:25", "speaker": "E", "text": "Dann greifen wir auf das in RB-GW-011 beschriebene Rollback-Prozedere zurück. Parallel wird ein Hotfix-Branch eröffnet, um die Ursache zu beheben – das kann z.B. ein fehlerhaftes Rate-Limit-Config sein."}
{"ts": "161:36", "speaker": "I", "text": "Haben Sie für diese Szenarien einen dedizierten Risikokatalog?"}
{"ts": "161:41", "speaker": "E", "text": "Ja, der Risikokatalog RK-ORI-BUILD listet u.a. Latenzspitzen, Auth-Timeouts und Backpressure auf. Jeder Eintrag enthält Präventionsmaßnahmen, die wir regelmäßig in Retros durchgehen."}
{"ts": "161:52", "speaker": "I", "text": "Welche der Maßnahmen halten Sie aktuell für am kritischsten?"}
{"ts": "161:57", "speaker": "E", "text": "Ganz klar das kontinuierliche Lasttest-Setup. Ohne verlässliche Synthetic Loads in der Staging-Umgebung würden wir Latenzprobleme erst im Livebetrieb sehen – und das Risiko wollen wir nicht eingehen."}
{"ts": "161:30", "speaker": "I", "text": "Wir hatten vorhin die Risiken bei der Latenz angesprochen. Können Sie mir noch einmal konkret sagen, wie Sie im Gateway die p95 unter 120 ms auch bei Lastspitzen absichern?"}
{"ts": "161:36", "speaker": "E", "text": "Ja, also wir kombinieren mehrere Taktiken: Zum einen haben wir in Terraform ein Autoscaling-Setup für die Edge-Pods definiert, das ab 70 % CPU oder 65 % Memory einen neuen Pod hochzieht. Parallel läuft ein Ansible-Playbook, das vorgefertigte Blue/Green-Images ausrollt, falls eine Pod-Gruppe die SLA-ORI-02 zu reißen droht. Das Ganze wird mit den Nimbus Observability Alerts gekoppelt."}
{"ts": "161:48", "speaker": "I", "text": "Und wie spielen diese Alerts in Ihren Runbooks eine Rolle?"}
{"ts": "161:53", "speaker": "E", "text": "In RB-GW-011 ist genau festgelegt, dass bei einem Alert mit Severity 2 ein Task 'BG-Switch' angestoßen wird. Das ist ein Jenkins-Job, der den Traffic sofort auf die Green-Umgebung leitet und parallel die fehlerhafte Blue-Umgebung drainiert. Wir haben das mit Ticket GW-4978 getestet – da konnten wir die Latenz von 180 ms innerhalb von 90 Sekunden wieder unter die 120 ms bringen."}
{"ts": "162:08", "speaker": "I", "text": "Wie stellen Sie sicher, dass solche Switche keine Authentifizierungsabbrüche mit Aegis IAM verursachen?"}
{"ts": "162:14", "speaker": "E", "text": "Das war tatsächlich mal ein Problem. Seit dem Bugfix in GW-4821 haben wir ein Session-Rehydration-Modul eingebaut. Beim Umschalten fragt der neue Podpool die aktiven JWTs im Redis-Cluster ab, validiert sie gegen Aegis IAM und übernimmt sie. Damit entfällt das erneute Login für den Endnutzer."}
{"ts": "162:29", "speaker": "I", "text": "Interessant. Sie hatten im Migrationsplan auch Canary kurz erwähnt. Warum war das hier nicht praktikabel?"}
{"ts": "162:35", "speaker": "E", "text": "Wir haben Canary getestet, aber das Orion Edge Gateway hängt an über 40 Downstream-APIs. In einem Canary-Setup hätten wir komplexes Routing in den Aggregator-Services gebraucht, um nur einen kleinen Prozentsatz der Requests umzuleiten. Das hätte die Latenz verdoppelt und passte nicht zu SLA-ORI-02. Blue/Green ist hier simpler und deterministischer."}
{"ts": "162:50", "speaker": "I", "text": "Welche weiteren Risiken sehen Sie in der Build-Phase?"}
{"ts": "162:56", "speaker": "E", "text": "Zwei Hauptpunkte: Erstens die MTLS-Handshake-Stabilität mit Partner-APIs. Wir hatten bei GW-4821 gesehen, dass falsch konfigurierte Cipher-Suites sofort die Availability runterziehen. Zweitens die Rate-Limit-Konfiguration – ein zu restriktiver Wert kann legitimen Traffic blocken. Wir fahren hier Staging-Tests mit 150 % der erwarteten Peak-Load, um Fehlkonfigurationen früh zu erkennen."}
{"ts": "163:12", "speaker": "I", "text": "Dokumentieren Sie diese Risiken laufend?"}
{"ts": "163:16", "speaker": "E", "text": "Ja, gemäß POL-QA-014 gibt es für jedes Risiko eine Risk Card im Confluence, mit Link zum Jira-Ticket. Lessons Learned aus Incidents wie GW-4821 oder GW-4978 werden dort eingepflegt. Zusätzlich haben wir ein monatliches Review mit dem QA-Team."}
{"ts": "163:28", "speaker": "I", "text": "Wie gehen Sie bei einem erneuten MTLS-Fehler konkret vor?"}
{"ts": "163:33", "speaker": "E", "text": "Runbook RB-GW-021 beschreibt das: Erstens sofortige Umschaltung auf die Backup-Cipher-Suite, zweitens Kontaktaufnahme mit dem Partner, drittens vollständiger Handshake-Dump in unser zentrales Log-Archiv. In GW-4821 hat dieser Ablauf den Ausfall von 45 Minuten auf unter 10 Minuten reduziert."}
{"ts": "163:47", "speaker": "I", "text": "Gibt es Pläne, diese Prozesse noch weiter zu automatisieren?"}
{"ts": "163:52", "speaker": "E", "text": "Ja, wir evaluieren gerade ein Feature in Nimbus Observability, das bei einem MTLS-Alert automatisch den Cipher-Switch triggert. Das müsste aber gut getestet werden, damit wir nicht versehentlich sichere Verbindungen auf eine schwächere Suite downgraden. Das ist der Trade-off zwischen Verfügbarkeit und Security, den wir sehr bewusst abwägen."}
{"ts": "163:00", "speaker": "I", "text": "Können Sie bitte genauer erläutern, wie Sie die IaC-Definitionen für das Orion Edge Gateway modular halten, um in der Build-Phase flexibel auf Änderungen zu reagieren?"}
{"ts": "163:05", "speaker": "E", "text": "Ja, wir haben die Terraform-Module in drei Layers geschnitten – Netzwerk, Compute und Gateway-Services. So können wir z.B. bei einer Anpassung der Auth-Schicht nur das entsprechende Modul neu ausrollen, ohne das VPC-Setup zu berühren."}
{"ts": "163:15", "speaker": "I", "text": "Und wie fließt diese Modularität in Ihr Blue/Green Deployment gem. Runbook RB-GW-011 ein?"}
{"ts": "163:20", "speaker": "E", "text": "Wir mappen die Module auf unterschiedliche Workspaces; für Blue/Green wechseln wir den aktiven Workspace, was Terraform State-seitig sauber getrennt ist. Das Runbook schreibt vor, vor dem Umschalten Smoke Tests gegen den neuen Workspace zu fahren."}
{"ts": "163:32", "speaker": "I", "text": "Sie hatten vorhin SLA-ORI-02 erwähnt. Welche konkreten Metriken tracken Sie kontinuierlich, um die p95 Latenz unter 120ms zu halten?"}
{"ts": "163:37", "speaker": "E", "text": "Wir sammeln mit Nimbus Observability sowohl den `gateway_request_duration_seconds` Histogram als auch den Upstream-Call-Trace. Ein Alert wird getriggert, wenn p95 über zwei Minuten > 110ms ist, damit wir noch vor SLA-Verletzung reagieren."}
{"ts": "163:49", "speaker": "I", "text": "Wie binden Sie dabei die Authentifizierung mit Aegis IAM und das Rate Limiting technisch ein, ohne dass sich die Latenzen hochschaukeln?"}
{"ts": "163:54", "speaker": "E", "text": "Das ist der spannende Teil: Das Gateway cached die JWT-Validation Keys für 15 Minuten, um die Roundtrips zu Aegis IAM zu minimieren. Rate Limiting ist ein Sidecar-Service, der lokal in der Node-Pod-Gruppe läuft, sodass keine zusätzlichen Netzwerkhops anfallen."}
{"ts": "164:06", "speaker": "I", "text": "Gibt es da keine Gefahr, dass bei Key-Rollover im IAM der Cache veraltete Keys nutzt?"}
{"ts": "164:11", "speaker": "E", "text": "Doch, deshalb bekommen wir von Aegis IAM Webhook-Events bei Key-Änderung, die den Cache invalidieren. Diese Integration haben wir im Change-Log von Build 1.4 dokumentiert."}
{"ts": "164:20", "speaker": "I", "text": "Und wie interagieren diese Komponenten mit den automatisierten Tests, die Sie nach POL-QA-014 implementiert haben?"}
{"ts": "164:25", "speaker": "E", "text": "Für jeden Commit werden Unit-Tests und Integrationstests getriggert. Speziell für Auth/RL-Themen haben wir in der Staging-Umgebung Chaos-Tests, die z.B. den IAM-Endpunkt künstlich verlangsamen und die Reaktion des Gateways messen."}
{"ts": "164:37", "speaker": "I", "text": "Gab es aus diesen Tests schon mal Lessons Learned, wie im Ticket GW-4821?"}
{"ts": "164:42", "speaker": "E", "text": "Ja, GW-4821 war ein MTLS-Handshake-Timeout. Aus der Analyse haben wir gelernt, die Handshake-Timeouts asynchron zu behandeln und Retry-Strategien einzubauen, was jetzt Teil des Runbooks RB-GW-015 ist."}
{"ts": "164:53", "speaker": "I", "text": "Verstehe. Und abschließend in diesem Block: Wie gehen Sie vor, wenn Rate-Limit-Überschreitungen auftreten und gleichzeitig Observability Alerts hochgehen?"}
{"ts": "164:58", "speaker": "E", "text": "Dann priorisieren wir laut Incident-Playbook IPB-GW-003 zunächst die Entlastung der Downstream-APIs, indem wir temporär die Limits anheben, aber parallel Ursachenanalyse fahren – häufig sind das fehlerhafte Clients oder Lastspitzen durch Batch-Jobs."}
{"ts": "164:00", "speaker": "I", "text": "Sie hatten vorhin den MTLS-Handshake-Bug GW-4821 angesprochen. Können Sie bitte genauer erklären, wie Sie damals entschieden haben, sofort auf Blue/Green zu wechseln statt einen Hotfix im laufenden Green-Cluster einzuspielen?"}
{"ts": "164:05", "speaker": "E", "text": "Ja, das war eine Abwägung zwischen Geschwindigkeit und Risiko. Laut Runbook RB-GW-011 hätten wir zwar einen Inline-Hotfix versuchen können, aber die Lessons Learned aus GW-4710 zeigten, dass bei MTLS-Fehlern oft auch die Session-Cache-Invalidierung betroffen ist. Blue/Green ermöglichte uns, den defekten Green-Cluster isoliert zu beheben und parallel im Blue die Verbindungstests gemäß POL-QA-014 zu fahren."}
{"ts": "164:12", "speaker": "I", "text": "Gab es in der Situation messbare Auswirkungen auf die SLA-ORI-02 Latenzwerte?"}
{"ts": "164:17", "speaker": "E", "text": "Kurzfristig ja, wir hatten in den ersten drei Minuten einen p95 Peak bei 164 ms. Nimbus Observability hat das sofort getriggert, und wir haben den Traffic Cutover innerhalb von 90 Sekunden abgeschlossen. Danach lagen wir wieder stabil unter den 120 ms."}
{"ts": "164:23", "speaker": "I", "text": "Wie bewerten Sie rückblickend das Risiko, dass beim Umschalten noch nicht alle Downstream-APIs wieder sauber angebunden waren?"}
{"ts": "164:28", "speaker": "E", "text": "Das war tatsächlich ein Risiko. Unser Heuristik-Check sieht vor, dass wir erst umschalten, wenn das Aegis IAM mindestens drei erfolgreiche Mutual-Auth Zyklen mit jeder relevanten API abgeschlossen hat. In GW-4821 mussten wir eine Ausnahme fahren und haben das dann per Ticket-Dokumentation nachgeholt, inklusive Post-Mortem im Confluence."}
{"ts": "164:36", "speaker": "I", "text": "Gab es interne Diskussionen, ob Canary-Deployments in solchen Fällen nicht besser geeignet wären?"}
{"ts": "164:41", "speaker": "E", "text": "Ja, die gab es. Canary hätte uns feineres Traffic-Shaping erlaubt, aber die Risikoanalyse im Architekturboard (RFC-ORI-27) hat ergeben, dass unser Gateway bei Auth-Subsystem-Bugs in Canary-Phasen zu ungleichmäßigen Rate-Limits neigt. Das war für Compliance zu heikel."}
{"ts": "164:48", "speaker": "I", "text": "Verstehe. Und wie gehen Sie mit der Dokumentation solcher Entscheidungen um?"}
{"ts": "164:52", "speaker": "E", "text": "Alle Entscheidungen dieser Kritikalität werden in der Decision-Log Tabelle gespeichert, verknüpft mit Ticket-IDs, Runbook-Referenzen und betroffenen SLAs. Wir nutzen dafür unser internes Tool 'TraceMate', das auch die IaC Commits verlinkt."}
{"ts": "164:58", "speaker": "I", "text": "Gab es aus GW-4821 heraus Änderungen an RB-GW-011 selbst?"}
{"ts": "165:02", "speaker": "E", "text": "Ja, wir haben einen neuen Schritt eingefügt: Vor Blue/Green-Umschaltungen bei Auth-Fehlern müssen jetzt explizit die Observability-Metriken für p95-Latenz und Error Rate im IAM-Handshake geprüft werden. Das ist jetzt Schritt 7a im Runbook."}
{"ts": "165:09", "speaker": "I", "text": "Wie stellen Sie sicher, dass Entwickler diese Ergänzungen auch einhalten?"}
{"ts": "165:13", "speaker": "E", "text": "Wir haben die CI/CD-Pipeline so angepasst, dass ein Deployment-Job für das Gateway die aktuelle Runbook-Version und die geforderten Checkpoints validiert. Fehlt der Nachweis, schlägt der Job fehl."}
{"ts": "165:20", "speaker": "I", "text": "Letzte Frage: Welche offenen Risiken sehen Sie aktuell für Latenz und Verfügbarkeit, speziell in Bezug auf das Zusammenspiel von IAM und Rate Limiting?"}
{"ts": "165:25", "speaker": "E", "text": "Der größte Punkt ist die Gefahr von 'Auth Storms', wenn Aegis IAM Tokens massenhaft refreshen muss. In Kombination mit strikten Rate-Limits kann das einen Kaskadeneffekt erzeugen. Wir testen aktuell ein Pre-Auth-Caching-Modul im Gateway, um dieses Risiko zu mitigieren, aber das ist noch im Experimentierstadium."}
{"ts": "166:00", "speaker": "I", "text": "Sie hatten vorhin erwähnt, dass Blue/Green im Runbook RB-GW-011 sehr klar definiert ist. Können Sie noch erläutern, wie Sie bei einem spontanen MTLS-Handshake-Bug reagieren würden, bevor er in Produktion durchschlägt?"}
{"ts": "166:05", "speaker": "E", "text": "Ja, in so einem Fall greifen wir auf das Incident-Playbook IP-GW-07 zurück. Das sieht vor, dass wir den aktiven Slot sofort einfrieren und den Traffic per DNS-Switch auf die grüne Umgebung umleiten. Parallel starten wir ein Rollback-Skript aus dem IaC-Repo, um die fehlerhafte MTLS-Konfiguration zurückzusetzen."}
{"ts": "166:13", "speaker": "I", "text": "Und wie stellen Sie sicher, dass dieser Switch die SLA-ORI-02 Latenzgrenze von 120ms nicht verletzt?"}
{"ts": "166:18", "speaker": "E", "text": "Wir nutzen in der Pipeline einen Canary-Check innerhalb derselben Region. Dabei wird mit synthetischen Requests die p95-Latenz gemessen. Sobald der Wert über 100ms steigt, stoppen wir den DNS-Switch und suchen in den Logs nach der MTLS-Fehlerursache."}
{"ts": "166:27", "speaker": "I", "text": "Gibt es hier Lessons Learned aus GW-4821, das ja einen ähnlichen Handshake-Bug betraf?"}
{"ts": "166:33", "speaker": "E", "text": "Auf jeden Fall. Aus GW-4821 haben wir gelernt, das mTLS-Zertifikats-Rollover mindestens 48 Stunden vor dem eigentlichen Stichtag zu testen. In POL-QA-014 haben wir dazu einen eigenen QA-Step ergänzt, der automatisiert im Staging-Cluster läuft."}
{"ts": "166:42", "speaker": "I", "text": "Wie verhält sich dieser QA-Step zur Aegis IAM Integration?"}
{"ts": "166:47", "speaker": "E", "text": "Er ist direkt damit verknüpft: der Step führt authentifizierte Requests gegen das Gateway aus, um sicherzustellen, dass sowohl der TLS-Handshake als auch das JWT-Token-Parsing in Kombination funktionieren. Fehler in einem der beiden Pfade blockieren automatisch den Merge ins Main-Branch."}
{"ts": "166:56", "speaker": "I", "text": "Das klingt nach einer engen Verzahnung. Gibt es Risiken, dass bei einem IAM-Update diese Tests plötzlich fehlschlagen?"}
{"ts": "167:01", "speaker": "E", "text": "Ja, dieses Risiko besteht. Deshalb haben wir im Change Management (RFC-ORI-27) festgelegt, dass jedes IAM-Update zunächst in einer isolierten Stage gegen einen geklonten Gateway-Stack getestet wird. Erst wenn alle Observability-Metriken aus Nimbus im grünen Bereich sind, geht der Patch live."}
{"ts": "167:11", "speaker": "I", "text": "Wie prüfen Sie in Nimbus konkret, ob der Patch stabil ist?"}
{"ts": "167:16", "speaker": "E", "text": "Wir haben drei Key Dashboards: Latenz nach Endpoint, Error-Rate nach Auth-Provider und Backpressure-Events. Für den Patch muss p95 unter 110ms liegen, die Error-Rate <0,5% und keine Backpressure-Spitzen über 30 Sekunden auftreten."}
{"ts": "167:25", "speaker": "I", "text": "Sie sprachen vorhin von Backpressure. Wie schnell reagieren Sie auf Rate-Limit-Überschreitungen?"}
{"ts": "167:30", "speaker": "E", "text": "Innerhalb von Sekunden. Ein Alert aus Nimbus löst einen Lambda-Handler aus, der temporär den Burst-Limit-Wert im API-Gateway reduziert. Das ist ein Trade-off: wir schützen die Downstream-APIs, riskieren aber, dass legitime Clients kurzzeitig 429er sehen."}
{"ts": "167:39", "speaker": "I", "text": "Würden Sie für diesen Trade-off eine andere Strategie in Betracht ziehen?"}
{"ts": "167:44", "speaker": "E", "text": "Langfristig ja. Wir evaluieren gerade einen Token-Bucket-Ansatz mit Priorisierung für Service-Accounts. Das würde kritische Integrationen bevorzugen und gleichzeitig die Gesamtsystemlatenz im Rahmen von SLA-ORI-02 halten."}
{"ts": "167:00", "speaker": "I", "text": "Bevor wir weitergehen, möchte ich noch einmal auf die Integration mit Nimbus Observability zurückkommen. Sie hatten vorhin die Metrik-Streams erwähnt – wie genau fließen die in Ihre automatisierten Deployment-Entscheidungen ein?"}
{"ts": "167:12", "speaker": "E", "text": "Wir haben in der CI/CD-Pipeline, Schritt 'Deploy-Validate', einen Hook zur Nimbus-API. Dort ziehen wir die letzten 15 Minuten p95- und p99-Latenzen sowie Error-Rates für die neuen Nodes. Wenn diese Werte gemäß SLA-ORI-02 unter 120 ms bleiben und die Error-Rate < 0,2 % ist, wird der Blue/Green Umschalter wie in RB-GW-011 beschrieben ausgelöst."}
{"ts": "167:36", "speaker": "I", "text": "Und wenn die Metriken außerhalb liegen, gibt es dann einen automatischen Rollback oder muss das manuell angestoßen werden?"}
{"ts": "167:44", "speaker": "E", "text": "Das ist zweistufig: Bei Abweichungen von bis zu 10 % vom SLA-Wert geht ein Alert an den On-Call, und wir prüfen manuell die Logs. Liegt die Latenz > 150 ms oder Error-Rate über 1 %, triggert unsere Pipeline direkt den Rollback-Step 'gw_rollback_v2', dokumentiert in Runbook RB-GW-019."}
{"ts": "168:06", "speaker": "I", "text": "Verstehe. In Bezug auf Aegis IAM – wie stellen Sie sicher, dass Änderungen am Auth-Flow nicht unbemerkt die Gateway-Performance beeinträchtigen?"}
{"ts": "168:16", "speaker": "E", "text": "Wir haben dafür einen speziellen Loadtest-Job 'auth_perf_smoke', der direkt nach jedem Merge in das Auth-Integration-Repo läuft. Er benutzt synthetische MTLS-Handshakes basierend auf dem Szenario aus GW-4821 und prüft nicht nur die Latenz, sondern auch den CPU-Impact auf der Edge-Schicht."}
{"ts": "168:38", "speaker": "I", "text": "Gab es schon Situationen, in denen dieser Job kritische Werte gemeldet hat?"}
{"ts": "168:46", "speaker": "E", "text": "Ja, im Februar hatten wir nach einem Refactor der Token-Parsing-Logik einen Anstieg der p95-Latenz von 95 ms auf 135 ms. Das wurde sofort von 'auth_perf_smoke' geflaggt, und wir haben den Merge rückgängig gemacht, bis ein Patch nach POL-QA-014 erstellt war."}
{"ts": "169:08", "speaker": "I", "text": "Sie erwähnten POL-QA-014 – wie setzen Sie diese Policy in Bezug auf automatisierte Tests im Gateway-Kontext konkret um?"}
{"ts": "169:18", "speaker": "E", "text": "Die Policy fordert, dass für jede kritische Komponente mindestens zwei automatisierte Teststufen existieren: Unit/Integration und Performance. Für das Gateway heißt das: Wir haben API-Contract-Tests, die bei jedem Build laufen, und Performance-Benchmarks, die beim nächtlichen Regression-Run getriggert werden."}
{"ts": "169:40", "speaker": "I", "text": "Wie dokumentieren Sie Abweichungen aus diesen Tests so, dass Lessons Learned später nutzbar bleiben?"}
{"ts": "169:48", "speaker": "E", "text": "Alle Abweichungen werden als Subtasks unter einem QA-Parent-Ticket angelegt, z. B. ORI-QA-2024-17. Wir hängen Screenshots, Log-Excerpts und Metrikgraphen an. Im Confluence-Bereich 'Gateway Retrospectives' schreiben wir eine Kurz-Analyse mit Verweis auf die Ticket-ID."}
{"ts": "170:10", "speaker": "I", "text": "Gut. Lassen Sie uns zu einem möglichen Risiko kommen: Wenn die Downstream-APIs Backpressure signalisieren, wie reagieren Ihre Rate-Limiter?"}
{"ts": "170:20", "speaker": "E", "text": "Unser Rate-Limiter-Modul hat eine adaptive Komponente. Bekommt es von einer Downstream-API einen HTTP 429 oder gRPC 'ResourceExhausted', senkt es das Burst-Limit dynamisch um 20 % und erhöht das Refill-Intervall. Diese Werte sind in ConfigMap 'rl_policy_v3' hinterlegt und werden alle 30 Sekunden reevaluated."}
{"ts": "170:44", "speaker": "I", "text": "Sie haben jetzt mehrfach konkrete Runbooks und Configs erwähnt. Welche Trade-offs haben Sie bei der Wahl dieser adaptiven Strategie bedacht?"}
{"ts": "170:54", "speaker": "E", "text": "Der größte Trade-off ist zwischen Stabilität der Downstream-Services und Endnutzer-Latenz. Zu aggressive Drosselung schützt die Backends, kann aber User-Experience verschlechtern. Wir haben uns für moderate Anpassungen entschieden, weil laut Auswertung der letzten 90 Tage (Report QA-OBS-77) die Backend-Erholung schneller einsetzt, wenn wir unter 25 % Drosselung bleiben."}
{"ts": "172:60", "speaker": "I", "text": "Sie hatten vorhin die Runbooks kurz erwähnt – können Sie mir bitte genauer erläutern, wie RB-GW-011 im Alltag angewendet wird, gerade wenn wir von ungeplanten Traffic-Spitzen sprechen?"}
{"ts": "173:15", "speaker": "E", "text": "Ja, natürlich. RB-GW-011 beschreibt Schritt-für-Schritt, wie wir in der Orion Edge Gateway Pipeline ein Blue/Green Deployment ausrollen, inklusive eines Abschnitts zur Traffic-Drosselung bei Lastspitzen. In der Praxis heißt das, wir triggern über Jenkins den Blue Slot, führen Smoke Tests aus, und schalten dann per Load Balancer Umschaltung um, sobald die Metriken in Nimbus Observability grün sind."}
{"ts": "173:45", "speaker": "I", "text": "Und welche Metriken sind das genau, auf die Sie da achten?"}
{"ts": "174:02", "speaker": "E", "text": "Primär die p95 Latenz unter 120 ms gemäß SLA-ORI-02, Error Rate unter 0,2 %, und ein stabiler MTLS-Handshake innerhalb von 80 ms. Wir haben in unserem Prometheus Setup dafür spezielle Alert-Regeln, die auch automatisiert gegen POL-QA-014 validieren."}
{"ts": "174:32", "speaker": "I", "text": "Wie ist das eigentlich mit der Authentifizierung – ist Aegis IAM da tief integriert oder läuft das eher lose gekoppelt?"}
{"ts": "174:50", "speaker": "E", "text": "Das ist tief integriert. Der Gateway-Service nutzt einen mTLS-gesicherten gRPC-Call zum Aegis IAM Token Service, cached Tokens für 5 Minuten und refresht pro Request-Batch. Dieses Pattern ist in unserem internen IaC-Modul 'gw_auth' hinterlegt, damit es reproduzierbar in allen Stages ausgerollt wird."}
{"ts": "175:20", "speaker": "I", "text": "Hatten Sie im Build bisher Probleme mit dieser Token-Cache-Strategie?"}
{"ts": "175:36", "speaker": "E", "text": "Einmal, ja – Ticket GW-4973. Da haben wir festgestellt, dass bei parallelen Deployments der Cache nicht invalidiert wurde. Wir haben daraufhin im Runbook ergänzt, dass nach jedem Blue/Green-Switch ein forced Cache Flush gemacht wird."}
{"ts": "176:02", "speaker": "I", "text": "Interessant. Und wie hängt das mit den Observability-Mechanismen zusammen, die Sie erwähnten?"}
{"ts": "176:20", "speaker": "E", "text": "Nimbus Observability liefert uns Trace-IDs vom Gateway bis zum IAM-Service. Bei Cache-Invalidierungsfehlern sehen wir erhöhte Auth-Latenzen im Segment 'gw→iam', was direkt in Grafana-Dashboards markiert wird. So konnten wir GW-4973 schnell isolieren."}
{"ts": "176:50", "speaker": "I", "text": "Sie haben also eine Art End-to-End Traceability implementiert, die auch Build-Phase relevanten Code umfasst?"}
{"ts": "177:05", "speaker": "E", "text": "Genau. Wir taggen jede Build-Artifact-Version mit der Git-Commit-ID und der Pipeline-Run-ID. Diese Tags erscheinen in den Logs und Traces, sodass wir bei einer Abweichung sofort zurückverfolgen können, welches IaC-Template und welche Config aktiv waren."}
{"ts": "177:32", "speaker": "I", "text": "Noch mal zu GW-4821: Falls dieses MTLS-Handshake-Problem in der Produktionsphase auftaucht, welche unmittelbaren Schritte würden Sie einleiten?"}
{"ts": "177:50", "speaker": "E", "text": "Sofortiger Switch auf den funktionierenden Slot per RB-GW-011, Aktivierung des Fallback-Zertifikats aus Secrets-Store SSM-GW, und dann Analyse der TLS-Debug-Logs. Parallel würden wir das Rate-Limit temporär anpassen, um Lastspitzen abzufangen und Latenz gemäß SLA-ORI-02 zu halten."}
{"ts": "178:20", "speaker": "I", "text": "Klingt robust. Sehen Sie bei diesem Vorgehen Risiken für die Verfügbarkeit?"}
{"ts": "178:38", "speaker": "E", "text": "Nur das übliche Risiko, dass beim Umschalten ein paar Sessions verloren gehen. Deshalb haben wir eine Session-Drain-Periode von 30 Sekunden eingebaut. Das ist ein Trade-off: minimale Downtime versus sofortige Stabilisierung – in GW-4821 haben wir uns bewusst für Letzteres entschieden."}
{"ts": "180:00", "speaker": "I", "text": "Bevor wir in die abschließende Bewertung gehen, möchte ich noch einmal gezielt auf die Integration mit Aegis IAM zurückkommen – können Sie bitte die technische Tiefe dieser Anbindung skizzieren?"}
{"ts": "180:35", "speaker": "E", "text": "Ja, klar. Wir nutzen den Aegis IAM OIDC Provider mit einer mTLS-gesicherten Verbindung, die gemäß Runbook RB-IAM-007 eingerichtet wurde. Der Gateway-Auth-Filter validiert Access Tokens lokal via JWKs, die wir alle 15 Minuten aus dem Aegis Metadata Endpoint cachen, um Latenzspitzen zu vermeiden."}
{"ts": "181:10", "speaker": "I", "text": "Und wie stellen Sie sicher, dass der JWK-Rollout keine Downtime verursacht, gerade in Kombination mit unseren Rate-Limits?"}
{"ts": "181:38", "speaker": "E", "text": "Wir haben dazu ein Double-Buffering im Key Store implementiert. Alte und neue Keys werden parallel vorgehalten, bis alle aktiven Sessions auslaufen. Das ist im internen RFC-ORI-SEC-04 beschrieben und wurde im letzten Stresstest validiert."}
{"ts": "182:15", "speaker": "I", "text": "Kommen wir zur Observability – wie wird Nimbus Observability bei Ihnen konkret eingesetzt, um SLA-ORI-02 zu überwachen?"}
{"ts": "182:45", "speaker": "E", "text": "Nimbus Agents laufen auf allen Gateway-Knoten. Wir exportieren Metriken wie p95 Latency, Error-Rates und Rate-Limit-Hits in ein zentrales Grafana-Board. Die SLO-Checks sind als Prometheus Alert Rules kodiert, sodass wir bei >100ms p95 sofort reagieren können."}
{"ts": "183:20", "speaker": "I", "text": "Gab es zuletzt eine Situation, in der diese Alerts ausgelöst wurden?"}
{"ts": "183:45", "speaker": "E", "text": "Ja, am 12. Mai hatten wir Ticket GW-4933: Ein Upstream-Service hat bei Lasttests 30% Timeout-Raten erzeugt. Wir haben temporär das Rate-Limit auf 80% des Normalwerts gesenkt, um Backpressure zu vermeiden."}
{"ts": "184:20", "speaker": "I", "text": "Wie wird so eine Maßnahme in Ihrer Deployment-Pipeline umgesetzt?"}
{"ts": "184:48", "speaker": "E", "text": "Wir haben ein Feature-Flag-System. Änderungen am Rate-Limit werden als ConfigMap-Update in Git committed, von der CI/CD-Pipeline erkannt und via Blue/Green unmittelbar ausgerollt. Der Rollback ist im Runbook RB-GW-011 beschrieben."}
{"ts": "185:25", "speaker": "I", "text": "In Bezug auf Backpressure – welche Strategien haben Sie außer Rate-Limit Anpassungen?"}
{"ts": "185:55", "speaker": "E", "text": "Circuit Breaker Patterns. Sobald ein Downstream über einen definierten Fehlerquotienten geht, schließen wir den Circuit für 60 Sekunden, um das System zu stabilisieren. Außerdem nutzen wir eine Warteschlangenlänge als Trigger für Load Shedding."}
{"ts": "186:30", "speaker": "I", "text": "Wie dokumentieren Sie solche Eingriffe, damit Lessons Learned entstehen?"}
{"ts": "186:55", "speaker": "E", "text": "Jeder Eingriff wird als Incident-Record im Confluence Space ORI-OPS dokumentiert, inkl. Metriken vor/nach der Maßnahme. Für GW-4933 haben wir z. B. ein Diagramm der Latenzverteilung angehängt, um künftige Entscheidungen datenbasiert zu treffen."}
{"ts": "187:30", "speaker": "I", "text": "Abschließend: Sehen Sie aktuell noch ungelöste Risiken, die wir im Steering Committee adressieren sollten?"}
{"ts": "187:58", "speaker": "E", "text": "Ja, zwei Punkte: Erstens die Abhängigkeit von einem monolithischen Downstream-Auth-Service – ein Single Point of Failure. Zweitens die noch nicht getesteten Edge-Cases beim TLS 1.3 Resumption. Beide Themen sollten vor Go-Live in einem Chaos-Engineering-Experiment validiert werden."}
{"ts": "188:00", "speaker": "I", "text": "Sie hatten vorhin die Latenzgrenzen aus SLA-ORI-02 erwähnt. Können Sie bitte konkretisieren, wie Sie die Messpunkte im Live-System setzen, um frühzeitig Abweichungen zu erkennen?"}
{"ts": "188:15", "speaker": "E", "text": "Ja, wir nutzen dafür synthetische Checks im 30-Sekunden-Intervall über drei geografische Zonen. Die Messpunkte sind direkt hinter dem Orion Edge Gateway und noch einmal hinter dem ersten Downstream-Service platziert, um festzustellen, ob eine Latenzerhöhung am Gateway oder dahinter entsteht."}
{"ts": "188:35", "speaker": "I", "text": "Und wie gehen Sie vor, wenn die p95 Messung über 120ms liegt, aber nur in einer Zone?"}
{"ts": "188:45", "speaker": "E", "text": "Dann greifen wir in Stufe 1 des Runbooks RB-GW-014: Wir vergleichen die Zone mit historischen Baselines, checken aktuelle Deployments und setzen ggf. temporäre Traffic-Rerouting-Regeln via Gateway Config API, um die Zone zu entlasten."}
{"ts": "189:05", "speaker": "I", "text": "Im Kontext der Authentifizierung – Sie sprachen von Aegis IAM. Wie stellen Sie sicher, dass ein MTLS-Handshake nicht selbst zur Latenzerhöhung führt?"}
{"ts": "189:18", "speaker": "E", "text": "Wir haben vor drei Monaten in RFC-ORI-021 dokumentiert, dass wir Session Resumption über TLS Tickets aktivieren. Zusätzlich loggen wir die Handshake-Dauer separat in Nimbus Observability und triggern bei >30ms eine Warnung."}
{"ts": "189:38", "speaker": "I", "text": "Gab es seit der Einführung dieser Maßnahme konkrete Vorfälle?"}
{"ts": "189:47", "speaker": "E", "text": "Ja, im Ticket GW-4958. Dort hatten wir ein fehlerhaftes Zertifikat im Staging, das in Blue/Green vor dem Switch zu Green entdeckt wurde. Der Handshake war >100ms, wir haben dank der Metrik sofort abgebrochen und den Zertifikatsstore neu aufgebaut."}
{"ts": "190:08", "speaker": "I", "text": "Inwiefern hängen diese Metriken auch mit Ihrer Rate-Limiting-Strategie zusammen?"}
{"ts": "190:16", "speaker": "E", "text": "Ganz klar: Wenn ein Service durch langsame Handshakes blockiert, staut sich Traffic. Unser Leaky-Bucket-Algorithmus im Gateway erkennt steigende Queue-Depths und drosselt neu eintreffende Requests, um Backpressure zu vermeiden."}
{"ts": "190:37", "speaker": "I", "text": "Wie testen Sie solche Szenarien in der Build-Phase?"}
{"ts": "190:46", "speaker": "E", "text": "Wir simulieren mit Gatling-Tests und gezielten TLS-Handshake-Delays. Das ist in POL-QA-014 als Lasttest-Stufe hinterlegt. Die Pipelines triggern das bei jedem Merge in den Build-Branch automatisch."}
{"ts": "191:05", "speaker": "I", "text": "Und welche Trade-offs mussten Sie hier eingehen? Ich meine, zusätzliche Checks können ja auch Overhead erzeugen."}
{"ts": "191:15", "speaker": "E", "text": "Richtig, der Observability-Overhead liegt bei ca. 2–3 % CPU. Wir haben das akzeptiert, weil die Detektionsgeschwindigkeit dadurch massiv steigt. Alternative wäre Sampling, aber das birgt das Risiko, sporadische Latenzspitzen zu übersehen."}
{"ts": "191:35", "speaker": "I", "text": "Letzte Frage dazu: Würden Sie angesichts GW-4821 und GW-4958 künftig an der Blue/Green-Strategie etwas ändern?"}
{"ts": "191:46", "speaker": "E", "text": "Wir überlegen, für kritische Zertifikatsänderungen einen Canary-ähnlichen Pre-Check einzubauen. Kein vollständiges Canary, sondern ein isolierter Zertifikats-Handshake-Test gegen alle Zonen, bevor Green live geht. Das steht bereits als Vorschlag in RFC-ORI-027."}
{"ts": "196:00", "speaker": "I", "text": "Sie hatten eben die Abwägung zwischen Latenz und Verfügbarkeit erwähnt. Können Sie vielleicht ein konkretes Beispiel nennen, wie das in der Praxis beim Orion Edge Gateway gelöst wurde?"}
{"ts": "196:20", "speaker": "E", "text": "Ja, konkret im Build-Cluster haben wir bei Ausfällen des Upstream-Auth-Services in Aegis IAM einen Fallback-Mechanismus aktiviert, der in RB-GW-015 beschrieben ist. Dadurch akzeptieren wir temporär Tokens aus dem Cache, was zwar ein minimales Risiko für Replay-Angriffe birgt, aber die SLA-ORI-02 Latenz unter 120 ms hält."}
{"ts": "196:45", "speaker": "I", "text": "Okay, und wie wird dieser Mechanismus getriggert? Ist das ein manueller Eingriff oder automatisiert?"}
{"ts": "197:05", "speaker": "E", "text": "Das ist voll automatisiert. Wir haben eine Health-Check-Integration mit Nimbus Observability. Wenn drei aufeinanderfolgende pings zum IAM-Service fehlschlagen und der p95 Wert > 200 ms liegt, triggert das Gateway intern das Fallback-Skript aus dem Runbook. Das wird auch als Event in unserem Event-Stream `gw-fallback` geloggt."}
{"ts": "197:30", "speaker": "I", "text": "Wie dokumentieren Sie solche Fallback-Ereignisse im Hinblick auf Lessons Learned?"}
{"ts": "197:50", "speaker": "E", "text": "Wir erstellen jeweils ein Ticket, meistens mit dem Präfix GW-FB, also z. B. GW-FB-102. Darin verlinken wir die Nimbus-Metriken, die Log-Auszüge und eine Bewertung gemäß POL-QA-014. Die Lessons Learned werden dann in Confluence gepflegt, inkl. einer Bewertung, ob wir den Mechanismus optimieren müssen."}
{"ts": "198:15", "speaker": "I", "text": "Gab es schon Fälle, bei denen dieser Fallback zu Problemen geführt hat?"}
{"ts": "198:35", "speaker": "E", "text": "Einmal, ja. Das war ähnlich zu GW-4821, allerdings nicht MTLS-bedingt. Der Cache wurde zu lange genutzt, weil der Rückkehr-Trigger nicht feuerte. Wir haben das in Ticket GW-FB-089 analysiert und die Beobachtungslogik angepasst, sodass jetzt bei zwei erfolgreichen Heartbeats in Folge der Fallback beendet wird."}
{"ts": "199:00", "speaker": "I", "text": "Können Sie erklären, wie das in der CI/CD-Pipeline getestet wird?"}
{"ts": "199:20", "speaker": "E", "text": "Klar, wir haben in der Pipeline eine Stage `simulate_iam_downtime`. Die nutzt ein Mock-IAM, das absichtlich 503er zurückgibt. Wir messen dann die Latenz und prüfen, ob der Cache aktiviert wird und ob nach Recovery die Auth wieder gegen Live-IAM läuft. Die Tests sind in Terraform mit Local-Exec eingebunden, um die IaC-Definitionen und die Ansible-Playbooks gemeinsam zu validieren."}
{"ts": "199:50", "speaker": "I", "text": "Und wie stellen Sie sicher, dass diese Tests nicht den Build zu sehr verzögern?"}
{"ts": "200:10", "speaker": "E", "text": "Wir limitieren die Simulation auf 90 Sekunden und parallelisieren sie mit anderen Integrationstests. Außerdem gibt es einen Schalter `SKIP_FALLBACK_TEST` für Hotfix-Builds, dokumentiert in CI-RB-007."}
{"ts": "200:30", "speaker": "I", "text": "Wenn wir auf die Rate-Limiting-Strategien schauen: Gibt es dort ähnliche Trade-offs?"}
{"ts": "200:50", "speaker": "E", "text": "Ja, durchaus. Wir nutzen ein token-bucket Verfahren mit dynamischer Auffüllrate. Wenn Downstream-APIs Backpressure signalisieren, reduzieren wir die Rate. Das hält die Verfügbarkeit hoch, kann aber einzelnen Clients höhere Latenzen bescheren. Auch hier gilt SLA-ORI-02, daher muss das Balancing sehr fein abgestimmt sein."}
{"ts": "201:15", "speaker": "I", "text": "Und wie reagieren die Teams, wenn solche Anpassungen kurzfristig nötig werden?"}
{"ts": "201:35", "speaker": "E", "text": "Wir haben einen Bereitschaftsplan, OPS-RUN-042, der genau diese Eingriffe beschreibt. Der On-Call-Engineer darf Parameter wie `refill_rate` via Feature-Flag ändern, solange er innerhalb der in RFC-GW-009 definierten Grenzen bleibt. Jede Änderung wird im Gateway-Config-Repo versioniert und mit einem Incident-Post-Mortem verknüpft."}
{"ts": "202:00", "speaker": "I", "text": "Sie hatten vorhin Blue/Green gegenüber Canary bevorzugt. Können Sie jetzt, mit Blick auf GW-4821, noch einmal ausführen, wie sich diese Wahl auf das Risiko-Management ausgewirkt hat?"}
{"ts": "202:20", "speaker": "E", "text": "Ja, also... durch Blue/Green konnten wir beim MTLS-Handshake-Bug die fehlerhafte Green-Umgebung sofort vom Load Balancer trennen. Das hat die p95 Latenz aus SLA-ORI-02 kurzfristig zwar leicht überschritten, aber die Verfügbarkeit blieb über 99,95 %."}
{"ts": "202:45", "speaker": "I", "text": "Gab es dabei spezielle Schritte gemäß Runbook RB-GW-011, die Sie zusätzlich eingezogen haben?"}
{"ts": "203:05", "speaker": "E", "text": "Genau, wir haben die im Anhang C beschriebenen Health-Check-Overrides aktiviert. Damit konnten wir gezielt nur die MTLS-Endpunkte aus dem Traffic nehmen, ohne das gesamte Gateway offline zu setzen."}
{"ts": "203:28", "speaker": "I", "text": "Und wie haben Sie die Ursache im Zusammenspiel von Gateway und Aegis IAM verifiziert?"}
{"ts": "203:50", "speaker": "E", "text": "Wir haben die MTLS-Handshake-Logs auf beiden Seiten korreliert. Über Nimbus Observability konnten wir die fehlerhaften TLS-Session-Tickets sehen und so ausschließen, dass es an der IAM-Config lag."}
{"ts": "204:12", "speaker": "I", "text": "Also war der Defekt rein im Gateway-Termination Layer?"}
{"ts": "204:25", "speaker": "E", "text": "Ja, der Bug entstand durch ein fehlerhaftes Library-Update in der TLS-Termination. POL-QA-014 hätte das in Staging fangen sollen, aber der Test-Trigger war falsch konfiguriert."}
{"ts": "204:48", "speaker": "I", "text": "Wie haben Sie diese Lücke in der Testautomatisierung geschlossen?"}
{"ts": "205:05", "speaker": "E", "text": "Wir haben den CI-Job 'gw-tls-int' so geändert, dass er bei jedem Merge in den Build-Branch automatisch MTLS-Handshake-Tests gegen eine simulierte Aegis IAM Instanz ausführt."}
{"ts": "205:28", "speaker": "I", "text": "Wurden auch Lessons Learned im Ticket System hinterlegt?"}
{"ts": "205:42", "speaker": "E", "text": "Ja, in GW-4821 haben wir neben der Fehleranalyse auch die Sofortmaßnahmen dokumentiert: Blue/Green Rollback-Prozess, Health-Check-Overrides und Fix des Test-Triggers."}
{"ts": "206:05", "speaker": "I", "text": "Und abschließend: Welche Trade-offs mussten Sie zwischen Latenz und Verfügbarkeit in diesem Fall bewusst eingehen?"}
{"ts": "206:25", "speaker": "E", "text": "Wir haben kurzfristig akzeptiert, dass die p95 Latenz auf 135 ms steigt, um den betroffenen Traffic sicher über die stabile Blue-Version zu routen. Das war im Sinne der Kunden besser als eine komplette Downtime."}
{"ts": "206:50", "speaker": "I", "text": "Würden Sie diese Entscheidung auch in künftigen Incidents so treffen?"}
{"ts": "207:10", "speaker": "E", "text": "Ja, solange SLA-ORI-02 nur marginal verletzt wird und die Verfügbarkeit über 99,9 % bleibt, ist das aus meiner Sicht vertretbar. Wir haben das auch als Richtlinie in die Incident-Response-Runbooks aufgenommen."}
{"ts": "210:00", "speaker": "I", "text": "Sie hatten vorhin kurz die Aegis IAM Integration erwähnt – könnten Sie bitte konkret beschreiben, wie die Token-Validierung im Gateway-Cluster umgesetzt ist?"}
{"ts": "210:20", "speaker": "E", "text": "Ja, gern. Wir nutzen im Orion Edge Gateway eine Sidecar-Pattern-Implementierung, die den JWT-Token gegen den Aegis Public Key Endpoint validiert. Die Keys werden über einen konfigurierten JWKS-Cache mit einer TTL von 5 Minuten gehalten, um die Latenz nicht unnötig zu erhöhen. Zusätzlich haben wir einen Fallback-Mechanismus, falls der IAM-Dienst nicht erreichbar ist – gemäß Runbook RB-IAM-004 schalten wir dann in einen Read-Only Modus."}
{"ts": "210:50", "speaker": "I", "text": "Das klingt technisch sauber, aber wie verhindern Sie, dass dieser Fallback die SLA-ORI-02 Latenz sprengt?"}
{"ts": "211:10", "speaker": "E", "text": "Wir haben im Sidecar einen Circuit Breaker mit einem konfigurierten Timeout von 80 ms implementiert. Wird dieser überschritten, wird der Request früh verworfen oder in eine vereinfachte Auth-Path-Logik geleitet. Die p95 Latenz-Metrik wird kontinuierlich über Nimbus Observability getrackt, und in der Pipeline gibt es ein Gate, das Builds blockiert, wenn der Median + 2*StdAbw. die 120 ms überschreitet."}
{"ts": "211:40", "speaker": "I", "text": "Apropos Nimbus – können Sie erläutern, wie die Observability-Komponenten mit dem Deployment-Workflow verzahnt sind?"}
{"ts": "212:05", "speaker": "E", "text": "Klar. Jeder Blue/Green Switch triggert automatisch einen neuen Observability Namespace in Nimbus. Das heißt, wir können Metriken und Traces der neuen Green-Instanz isoliert auswerten, bevor der Traffic vollständig umgeleitet wird. Das ist besonders wichtig, um Cross-Tenant-Latenzen zu identifizieren, die nur unter Last auftreten."}
{"ts": "212:35", "speaker": "I", "text": "Gab es dabei schon einmal einen Fall, wo diese Isolierung ein Problem früh sichtbar gemacht hat?"}
{"ts": "212:55", "speaker": "E", "text": "Ja, bei Build 147 hat Nimbus im Green-Cluster nach 15 Minuten einen Anstieg der Error-Rate auf 3 % gemeldet. Ursache war eine fehlerhafte Konfiguration des Rate Limiters im Downstream API-Connector – das haben wir über Ticket GW-4978 abgewickelt und per Hotfix behoben, bevor der gesamte Traffic migriert wurde."}
{"ts": "213:25", "speaker": "I", "text": "Interessant. Wie stellen Sie sicher, dass solche Fehler auch in der Testphase auffallen?"}
{"ts": "213:45", "speaker": "E", "text": "Wir haben seit POL-QA-014 eine Pflicht, Lasttests mit realitätsnahen Traffic-Profilen durchzuführen. Diese Tests werden in der CI/CD-Pipeline als Stage `perf-green` automatisch gestartet, sobald ein Build das Unit- und Integration-Test-Gate passiert. Die Profile kommen aus aufgezeichneten Produktionsdaten, natürlich anonymisiert."}
{"ts": "214:15", "speaker": "I", "text": "Und wenn im Test ein Ausreißer über der Latenzschwelle liegt, was passiert dann?"}
{"ts": "214:35", "speaker": "E", "text": "Dann greift unser Quality Gate, das den Merge in den Main-Branch verweigert. Für kleine Abweichungen gibt es eine manuelle Freigabe durch den QA-Lead, die in Confluence unter Lessons Learned dokumentiert wird. Große Abweichungen führen zu einem Blocker-Ticket, wie etwa bei GW-4821 geschehen."}
{"ts": "215:05", "speaker": "I", "text": "Wenn wir auf die Risiken schauen – gibt es im aktuellen Build noch offene Punkte?"}
{"ts": "215:25", "speaker": "E", "text": "Ja, wir sehen ein potenzielles Risiko bei MTLS-Handshakes, wenn der Downstream-Service während des TLS-Resumes einen Key-Rollover macht. Das ist noch nicht abschließend getestet. Wir haben dafür ein RFC-Dokument erstellt (RFC-GW-022) und planen negative Tests auf der Staging-Umgebung."}
{"ts": "215:55", "speaker": "I", "text": "Würden Sie in diesem Fall denselben Sofortmaßnahmenplan wie bei GW-4821 anwenden?"}
{"ts": "216:15", "speaker": "E", "text": "Teilweise, ja. Wir würden sofort auf Blue zurückschwenken, das MTLS-Feature temporär deaktivieren und das Observability-Alerting hochsetzen. Zusätzlich wäre hier aber ein gezielter Chaos-Test erforderlich, um sicherzustellen, dass das Problem nicht erneut auftritt."}
{"ts": "218:60", "speaker": "I", "text": "Lassen Sie uns noch einmal auf die Lessons Learned aus GW-4821 eingehen. Welche konkreten Änderungen haben Sie im Runbook RB-GW-011 vorgenommen?"}
{"ts": "219:15", "speaker": "E", "text": "Wir haben nach dem MTLS-Handshake-Bug zwei Punkte ergänzt: Erstens eine explizite Vorprüfung der Zertifikatskette mit unserem internen Tool 'CertScan', und zweitens einen Timeout-Parameter im Blue/Green-Switch-Script, der an SLA-ORI-02 gekoppelt ist. So verhindern wir, dass ein defekter Knoten in Betrieb geht, bevor die Latenz unter 120 ms stabil ist."}
{"ts": "219:48", "speaker": "I", "text": "Und wie wird das technisch im CI/CD-Pipeline-Schritt abgebildet?"}
{"ts": "220:06", "speaker": "E", "text": "Im Jenkins-Job 'gw-deploy-bluegreen' haben wir einen Pre-Deployment-Stage eingefügt. Dieser ruft ein Groovy-Skript auf, das sowohl CertScan gegen den neuen Pod ausführt als auch einen Probe-Request über Nimbus Observability sendet. Failures triggern einen automatischen Rollback per IaC-Template."}
{"ts": "220:37", "speaker": "I", "text": "Sie erwähnen IaC-Templates. Welche Parameter sind dort neu?"}
{"ts": "220:54", "speaker": "E", "text": "Neu ist ein 'latencyBudgetMs' Parameter, default 100 ms, den wir aus SLA-ORI-02 ableiten. Außerdem haben wir 'mtlsRequired=true' als Pflichtfeld für alle Gateway-Services gesetzt. Diese Werte werden in Terraform-Variablen injiziert und dann in den Kubernetes-Manifests berücksichtigt."}
{"ts": "221:22", "speaker": "I", "text": "Gab es durch diese Änderungen messbare Verbesserungen?"}
{"ts": "221:35", "speaker": "E", "text": "Ja, die Fehlerrate bei Blue/Green Umschaltungen ist seitdem um 80 % gesunken. Die mittlere p95 Latenz während Deployments liegt jetzt bei 98 ms, deutlich unter dem Limit. Außerdem konnten wir die Mean Time to Recovery bei fehlerhaften Zertifikaten von 12 auf 4 Minuten reduzieren."}
{"ts": "222:02", "speaker": "I", "text": "Wie dokumentieren Sie diese Verbesserungen für die Nachvollziehbarkeit, auch in Hinblick auf POL-QA-014?"}
{"ts": "222:18", "speaker": "E", "text": "Wir pflegen für jede Release-Welle ein QA-Log, in dem der automatisierte Test-Output, die Observability-Graphen und die IaC-Diffs abgelegt werden. Dieses Log ist Teil unseres Confluence-Workflows, der in POL-QA-014 vorgeschrieben ist. Ticket-Referenzen wie GW-4821 sind dort als Hyperlink eingebettet."}
{"ts": "222:48", "speaker": "I", "text": "Gab es Schwierigkeiten bei der Integration dieser Dokumentation in den Alltag?"}
{"ts": "223:05", "speaker": "E", "text": "Anfangs ja, weil das Team die zusätzlichen Schritte als Overhead empfand. Wir haben dann aber automatisierte Hooks eingebaut: Der Merge in 'main' triggert automatisch die Generierung des QA-Logs, sodass niemand manuell exportieren muss."}
{"ts": "223:29", "speaker": "I", "text": "Abschließend: Welche Risiken sehen Sie, falls wir künftig auf Canary umsteigen müssten?"}
{"ts": "223:44", "speaker": "E", "text": "Das größte Risiko wäre die Komplexität bei der Synchronisierung mit Aegis IAM Tokens, da Canary Traffic oft kleinteilig gesplittet wird. Unsere aktuellen Latency-Budgets und die Zertifikatsprüfungen sind auf binäre Umschaltungen optimiert. Canary würde mehr Metrik-Granularität erfordern, was wiederum die Alert-Noise in Nimbus erhöhen könnte."}
{"ts": "224:15", "speaker": "I", "text": "Würden Sie in so einem Fall Anpassungen an RB-GW-011 empfehlen?"}
{"ts": "224:28", "speaker": "E", "text": "Definitiv. Wir müssten Canary-spezifische Health-Check-Intervalle definieren, vielleicht auf 10 Sekunden runter, und parallel eine feinere Metrikaggregation in Nimbus konfigurieren. Außerdem bräuchten wir einen neuen Abschnitt zu Token-Rotation während inkrementeller Ausrollungen."}
{"ts": "226:00", "speaker": "I", "text": "Sie hatten vorhin die MTLS-Handshake-Problematik aus Ticket GW-4821 erwähnt. Mich interessiert jetzt, wie Sie im Incident-Runbook RB-INC-007 den Abschnitt zur Fehlerisolation konkret anwenden."}
{"ts": "226:18", "speaker": "E", "text": "Ja, also RB-INC-007 schreibt vor, dass wir beim ersten Anzeichen eines SSL- oder MTLS-Fehlers sofort den betroffenen Upstream aus dem Pool nehmen – per automatischem Health-Check in Kubernetes. Wir isolieren das Deployment über einen TrafficShift in der Ingress-Konfiguration und starten dann einen gezielten Pod-Restart, bevor wir tiefer in die Zertifikatskette schauen."}
{"ts": "226:47", "speaker": "I", "text": "Okay, und wie lange dauert es, bis dieser TrafficShift aktiv ist?"}
{"ts": "227:00", "speaker": "E", "text": "In unseren letzten Tests, die auch in den QA-Protokollen zu POL-QA-014 dokumentiert sind, lag die Latenz zwischen Detection und Shift bei etwa 3,5 Sekunden. Das ist innerhalb unseres internen Zielwerts von 5 Sekunden, um SLA-ORI-02 nicht zu verletzen."}
{"ts": "227:19", "speaker": "I", "text": "Gab es schon Situationen, in denen diese 5 Sekunden überschritten wurden?"}
{"ts": "227:29", "speaker": "E", "text": "Einmal, ja – während einer Lastspitze, als gleichzeitig ein Rate-Limit-Alarm aus Nimbus Observability kam. Da hatten wir kurzzeitig 6,2 Sekunden, was wir dann im Post-Mortem unter Lessons Learned vermerkt und im Runbook RB-INC-007 angepasst haben, um parallele Alerts besser zu priorisieren."}
{"ts": "227:56", "speaker": "I", "text": "Das heißt, Sie haben die Alert-Priorisierung verändert, um Kollisionen zu vermeiden?"}
{"ts": "228:05", "speaker": "E", "text": "Genau. Wir haben im Alertmanager eine Regel eingeführt, die Authentifizierungs- und Netzwerkfehler höher priorisiert als reine Throughput-Überschreitungen. So wird der TrafficShift nicht verzögert, selbst wenn die Metrik-Pipeline belastet ist."}
