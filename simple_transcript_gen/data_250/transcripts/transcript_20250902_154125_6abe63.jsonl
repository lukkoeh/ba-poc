{"ts": "00:00", "speaker": "I", "text": "Können Sie bitte Ihre aktuelle Rolle im Projekt Orion Edge Gateway beschreiben?"}
{"ts": "02:15", "speaker": "E", "text": "Ja, klar. Ich bin Lead Engineer für das gesamte Gateway-Deployment. Meine Hauptaufgabe ist es, die Infrastruktur und das API-Routing so zu gestalten, dass wir sowohl die internen Microservices als auch externe Partner-APIs sicher und performant anbinden. In der Build-Phase bedeutet das vor allem IaC-Templates zu erstellen und die CI/CD-Pipelines für automatisierte Deployments einzurichten."}
{"ts": "06:40", "speaker": "I", "text": "Wie fügt sich das Gateway in die Gesamtarchitektur der Novereon Plattform ein?"}
{"ts": "09:05", "speaker": "E", "text": "Das Orion Edge Gateway sitzt quasi ganz vorne im Datenpfad. Es ist der Entry Point für alle API-Calls von Kunden und internen Tools. Danach werden Requests entsprechend des Service Discovery Moduls an die passenden Microservices weitergeleitet. Wir haben es so konzipiert, dass es eng mit unserem internen Service Mesh interagiert, um Policies wie Authentifizierung und Rate Limiting zentral durchzusetzen."}
{"ts": "13:20", "speaker": "I", "text": "Welche Hauptziele verfolgen wir in der aktuellen Build-Phase?"}
{"ts": "16:00", "speaker": "E", "text": "Kurz gesagt: Stabilität, Sicherheit und Skalierbarkeit. Wir wollen sicherstellen, dass die Gateway-Komponenten auf Kubernetes Nodes in mehreren Zonen laufen, dass mTLS zwischen allen Komponenten funktioniert und dass wir das Rate Limiting so implementieren, dass wir SLA-ORI-02 erfüllen können – also maximal 0,1% Throttling bei legitimen Requests."}
{"ts": "20:45", "speaker": "I", "text": "Welche IaC-Tools setzen Sie für die Provisionierung der Gateway-Infrastruktur ein?"}
{"ts": "24:10", "speaker": "E", "text": "Wir nutzen primär Terraform, ergänzt durch Helm Charts für Kubernetes-spezifische Deployments. Die Terraform-Module sind in unserem internen Registry-System versioniert, und wir haben für das Gateway ein eigenes Modul, das VPCs, Load Balancer und Secrets Management automatisiert bereitstellt. Für Secrets binden wir Vault an, das ebenfalls per Terraform konfiguriert wird."}
{"ts": "28:35", "speaker": "I", "text": "Wie ist der aktuelle Stand der CI/CD-Pipelines für Deployments im Projekt?"}
{"ts": "32:00", "speaker": "E", "text": "Die Pipelines sind zu etwa 80% fertig. Wir haben Build-, Test- und Deploy-Stages, die auf Jenkins laufen. Für die Deployments nutzen wir ein eigenes Plugin, das das Runbook RB-GW-011 für Blue/Green Rolling Deployments referenziert. Momentan testen wir noch die automatischen Rollbacks bei fehlerhaften Health Checks."}
{"ts": "36:50", "speaker": "I", "text": "Wie stellen Sie sicher, dass Deployments im Einklang mit RB-GW-011 erfolgen?"}
{"ts": "40:30", "speaker": "E", "text": "Wir haben in der Pipeline eine Stage, die explizit die Schritte aus RB-GW-011 ausführt: Zunächst wird das neue Deployment in einer Parallel-Umgebung hochgezogen, dann laufen Smoke Tests und Latenz-Messungen. Erst wenn die Schnittstellen p95 < 120ms erreichen, wird der Traffic umgeschwenkt. Die Policy ist in Jenkins als groovy Script hinterlegt, sodass niemand sie überspringen kann."}
{"ts": "45:20", "speaker": "I", "text": "Wie haben Sie die mTLS-Handshake-Problematik aus Ticket GW-4821 adressiert?"}
{"ts": "49:10", "speaker": "E", "text": "Das war ein hartnäckiger Fehler, bei dem der Handshake zwischen Gateway und zwei älteren Microservices in unserer Staging-Zone nicht sauber lief. Wir haben die TLS-Version angehoben und in den Helm Charts die Cipher Suites explizit gesetzt. Zusätzlich haben wir einen Pre-Deployment-Check eingebaut, der das Zertifikat-Expiry und die Kompatibilität prüft. Seitdem kein Ausfall mehr im mTLS-Check."}
{"ts": "53:45", "speaker": "I", "text": "Inwiefern berücksichtigen Sie POL-SEC-001 (Least Privilege & JIT Access) in der Gateway-Konfiguration?"}
{"ts": "58:00", "speaker": "E", "text": "Wir folgen strikt dem Prinzip, dass jedes Service Account Token nur minimal benötigte Rechte hat. Die Gateway Pods laufen mit nicht-root Usern, und Admin-Zugänge werden nur per JIT Access für maximal 60 Minuten freigegeben. Das ist in unseren IaC-Templates als Standard konfiguriert, sodass neue Deployments automatisch konform mit POL-SEC-001 sind."}
{"ts": "90:00", "speaker": "I", "text": "Können Sie etwas genauer erläutern, wie Sie die Observability-Daten aus P-NIM ins Gateway-Monitoring integriert haben?"}
{"ts": "90:05", "speaker": "E", "text": "Ja, wir haben einen Sidecar-Agent im Gateway-Pod, der die Metriken über OpenTelemetry sammelt und dann an den Nimbus Collector weiterleitet. Das Mapping der p95 Latenz-Metrik auf unsere interne Metrik-ID ORI_LAT_P95 ist im Config-Repo dokumentiert."}
{"ts": "90:25", "speaker": "I", "text": "Und wie stellen Sie sicher, dass diese Metriken regelmäßig überprüft werden, um die 120 ms Grenze einzuhalten?"}
{"ts": "90:30", "speaker": "E", "text": "Wir haben ein wöchentliches Review im DevOps-Stand-up, plus automatische SLO-Checks. Bei Verletzungen wird ein Incident nach Runbook RB-OBS-033 eröffnet, das die Schritte zur Ursachenanalyse vorgibt."}
{"ts": "90:50", "speaker": "I", "text": "Gab es schon Vorfälle, bei denen RB-OBS-033 tatsächlich zum Einsatz kam?"}
{"ts": "90:55", "speaker": "E", "text": "Ja, einmal im März, als ein fehlerhaftes Rate-Limiting-Rule-Set zu erhöhter Latenz führte. Das Runbook half uns, schnell zwischen Netzwerklast und Konfigurationsfehler zu unterscheiden."}
{"ts": "91:15", "speaker": "I", "text": "Wie koordinieren Sie sich in so einem Fall mit dem Security-Team, gerade wenn Auth-Policies betroffen sein könnten?"}
{"ts": "91:20", "speaker": "E", "text": "Wir haben einen festen Slack-Channel #ori-sec-ops, in dem wir sofort ein Heads-up geben. Wenn Auth-Policies geändert werden müssen, erstellen wir ein RFC nach dem Template RFC-AUTH-v2, das von Security reviewed wird."}
{"ts": "91:40", "speaker": "I", "text": "Und diese RFCs, wie dokumentieren Sie die Umsetzung für die Nachvollziehbarkeit laut POL-QA-014?"}
{"ts": "91:45", "speaker": "E", "text": "Alle Änderungen landen in unserem Change-Log-System OrionTrack. Dort referenzieren wir das RFC, die zugehörigen Tickets und Commit-Hashes. Das ist unser Audit-Trail."}
{"ts": "92:05", "speaker": "I", "text": "Kommen wir zu den Trade-offs: Welche Abwägungen mussten Sie bei der Rate-Limiting-Implementierung treffen?"}
{"ts": "92:10", "speaker": "E", "text": "Der größte Trade-off war zwischen Genauigkeit und Performance. Wir hätten sehr feingranulare Token-Bucket-Algorithmen nutzen können, aber die wären CPU-intensiv gewesen. Wir haben uns für eine etwas gröbere Sliding-Window-Variante entschieden, die in Tests 8 % weniger Ressourcen verbrauchte."}
{"ts": "92:35", "speaker": "I", "text": "Gab es Risiken, die Sie bewusst akzeptiert haben, um die Build-Phase zu beschleunigen?"}
{"ts": "92:40", "speaker": "E", "text": "Ja, wir haben beim mTLS-Handshake die Retry-Logik zunächst vereinfacht implementiert, obwohl das in GW-4821 als potenzieller Engpass dokumentiert war. Das hat uns zwei Wochen gespart, wird aber in der nächsten Phase gehärtet."}
{"ts": "93:00", "speaker": "I", "text": "Können Sie ein Beispiel nennen, wo ein Runbook oder Ticket als Beweismittel für eine Entscheidung diente?"}
{"ts": "93:05", "speaker": "E", "text": "Ja, bei der Entscheidung für die Sliding-Window-Limits haben wir Incident-Ticket ORI-INC-1179 und die Analyse aus RB-RATE-014 angeführt. Das hat im Architekturgremium die Zustimmung beschleunigt."}
{"ts": "96:00", "speaker": "I", "text": "Lassen Sie uns jetzt noch auf die Risiken und Trade-offs eingehen, die Sie in dieser Build-Phase bewusst eingegangen sind."}
{"ts": "96:15", "speaker": "E", "text": "Ja, also eines der größten Trade-offs war tatsächlich beim Rate Limiting. Wir haben uns entschieden, zunächst nur den globalen Token-Bucket-Ansatz zu implementieren und den per-User-Bucket in einen späteren Sprint zu verschieben, um den Go-Live-Termin zu halten."}
{"ts": "96:37", "speaker": "I", "text": "Und wie haben Sie das Risiko bewertet, dass per-User-Limits fehlen?"}
{"ts": "96:49", "speaker": "E", "text": "Wir haben das in der Risk Log RSK-ORI-07 dokumentiert. Die Einschätzung war: mittleres Risiko für SLA-ORI-02, da unsere Haupt-APIs wenig Missbrauch erwarten lassen. Als Gegenmaßnahme nutzen wir P-NIM Alerts auf ungewöhnliche Traffic-Spitzen."}
{"ts": "97:09", "speaker": "I", "text": "Gab es ein Runbook oder Ticket, das diese Entscheidung untermauert?"}
{"ts": "97:20", "speaker": "E", "text": "Ja, Runbook RB-RL-021 beschreibt genau, wie wir im Fall von Abuse temporär IP-basierte Limits via Feature Flag setzen können. Wir haben dazu Ticket GW-5278 erstellt, das die Flag-Implementierung trackte."}
{"ts": "97:44", "speaker": "I", "text": "Können Sie ein weiteres Beispiel nennen, wo Sie einen bewussten Kompromiss eingegangen sind?"}
{"ts": "97:55", "speaker": "E", "text": "Ein weiteres Beispiel ist mTLS für interne Services. In GW-4821 haben wir den Handshake-Bug adressiert, aber für zwei Low-Traffic-Microservices die Zertifikatsrotation manuell belassen, um nicht das gesamte Cert-Automation-Modul refactoren zu müssen."}
{"ts": "98:20", "speaker": "I", "text": "Wie stellen Sie sicher, dass diese manuellen Prozesse nicht vergessen werden?"}
{"ts": "98:31", "speaker": "E", "text": "Wir haben in unserem Confluence die Checkliste CL-MTLS-005, die quartalsweise vom Ops-Team abgearbeitet wird. Zusätzlich erstellt P-NIM einen Reminder-Alert 14 Tage vor Ablauf der Zertifikate."}
{"ts": "98:52", "speaker": "I", "text": "Sehen Sie langfristige Risiken, falls diese manuellen Schritte beibehalten werden?"}
{"ts": "99:05", "speaker": "E", "text": "Ja, klar. Manual Steps sind immer fehleranfällig. Deshalb haben wir im Backlog das Epic ORI-AUTO-12, um die Rotation in unsere Terraform-Module zu integrieren. Priorität ist mittel, weil aktuelle Prozesse stabil laufen."}
{"ts": "99:27", "speaker": "I", "text": "Gab es in Bezug auf SLA-ORI-02 noch andere Risiken, die Sie akzeptiert haben?"}
{"ts": "99:39", "speaker": "E", "text": "Ja, bei der Auth-Integration haben wir den JWT-Signature-Algorithmus vorerst nicht auf EdDSA umgestellt, weil die Bibliotheksunterstützung in unserem Gateway-Framework noch experimentell war. Das Risiko eines etwas höheren CPU-Loads bei Peak haben wir akzeptiert."}
{"ts": "100:00", "speaker": "I", "text": "Wie wurde diese Entscheidung dokumentiert?"}
{"ts": "100:12", "speaker": "E", "text": "Im RFC-ORI-443 haben wir die Evaluierung der Algorithmen festgehalten, inklusive Benchmarks und Kompatibilitätstests. Das Approval kam vom Architekturgremium, mit Verweis auf RB-AUTH-009 für Monitoring des CPU-Loads."}
{"ts": "112:00", "speaker": "I", "text": "Sie hatten vorhin kurz das Thema Alert Fatigue angesprochen – können Sie erläutern, wie Sie dabei konkret mit Runbook RB-OBS-033 arbeiten?"}
{"ts": "112:15", "speaker": "E", "text": "Ja, klar. RB-OBS-033 gibt uns ein klares Escalation-Pattern vor, mit definierten Cooldown-Zeiten und Prioritäten. Wir haben zusätzlich ein internes Tagging-System implementiert, um Low-Impact Alerts automatisch zu bündeln."}
{"ts": "112:40", "speaker": "I", "text": "Und dieser Tagging-Mechanismus, ist der in der Pipeline oder im Monitoring-Stack verankert?"}
{"ts": "112:53", "speaker": "E", "text": "Der greift direkt im Nimbus Observability Stream. Wir nutzen einen Pre-Processor, der basierend auf Meta-Daten aus dem IaC-Repo erkennt, welche Alerts zueinander gehören."}
{"ts": "113:20", "speaker": "I", "text": "Interessant – das heißt, die IaC-Definitionen beeinflussen auch, wie Alerts korreliert werden?"}
{"ts": "113:32", "speaker": "E", "text": "Genau, das ist auch einer der Punkte, die wir beim letzten RFC-Meeting diskutiert haben. Der Vorteil ist, dass Änderungen an Gateway-Konfigurationen sofort in die Alert-Logik übernommen werden."}
{"ts": "113:55", "speaker": "I", "text": "Gab es dabei Konflikte mit den Security-Vorgaben aus POL-SEC-001?"}
{"ts": "114:08", "speaker": "E", "text": "Nur minimal. Wir mussten sicherstellen, dass getaggte Alerts keine sensiblen Auth-Daten enthalten. Das haben wir mit einem Sanitizer-Modul gelöst, das vor der Speicherung greift."}
{"ts": "114:30", "speaker": "I", "text": "Kommen wir auf die SLO-Überwachung zurück: Welche Metriken sind für Sie im Build-Phase-Tracking am kritischsten?"}
{"ts": "114:43", "speaker": "E", "text": "Primär die p95 Latenz, Error Rate pro Endpoint und Auth-Zeiten für OAuth-Flows. Wir haben einen Threshold-Check, der bei 110ms p95 schon eine Warnung ausgibt, um vor der 120ms-Grenze reagieren zu können."}
{"ts": "115:10", "speaker": "I", "text": "Haben Sie diese Schwellenwerte irgendwo dokumentiert?"}
{"ts": "115:22", "speaker": "E", "text": "Ja, das steht in unserem internen SLA-Dokument ORI-SLA-TRACK, und es ist direkt im Monitoring-Template für P-NIM hinterlegt."}
{"ts": "115:40", "speaker": "I", "text": "Abschließend würde mich interessieren, ob es in dieser Build-Phase Risiken gab, die Sie bewusst akzeptiert haben, um den Zeitplan zu halten."}
{"ts": "115:53", "speaker": "E", "text": "Ja, wir haben beim Rate-Limiting zunächst eine einfachere Token-Bucket-Implementierung gewählt, obwohl wir wussten, dass sie unter Last >10k RPS nicht optimal skaliert. Das war bewusst, um Feature-Freeze einzuhalten."}
{"ts": "116:20", "speaker": "I", "text": "Gab es dafür ein Ticket oder Runbook als Entscheidungsgrundlage?"}
{"ts": "116:33", "speaker": "E", "text": "Ja, das war in Ticket GW-5120 dokumentiert, mit Verweis auf Runbook RB-GW-015, wo wir die Trade-offs zwischen Token-Bucket und Leaky-Bucket diskutiert haben und die Entscheidung für die Build-Phase festgehalten wurde."}
{"ts": "130:00", "speaker": "I", "text": "Könnten Sie bitte genauer beschreiben, welche Metriken Sie zusätzlich zur p95 Latenz überwachen, um proaktiv auf mögliche SLA-Verletzungen reagieren zu können?"}
{"ts": "130:20", "speaker": "E", "text": "Ja, neben der p95 Latenz messen wir auch Error-Rate pro Endpoint, Queue Depth im internen Message-Bus und die Auth-Verifikationsdauer. Diese Metriken sind im Dashboard ORI-GW-Perf-01 hinterlegt und werden von Nimbus Observability (P-NIM) alle 15 Sekunden aktualisiert."}
{"ts": "130:55", "speaker": "I", "text": "Und wie fließen diese Daten dann in Ihre Alerting-Strategie ein? Nutzen Sie da zum Beispiel RB-OBS-033?"}
{"ts": "131:15", "speaker": "E", "text": "Genau, RB-OBS-033 gibt uns die Threshold-Fenster vor, z.B. 2 Minuten über 1% Error-Rate triggert einen Warn-Alert, über 5% einen Critical-Alert. Wir haben zusätzlich einen Suppression-Mechanismus eingebaut, um bei geplanten Deployments keine unnötigen Alarme auszulösen."}
{"ts": "131:50", "speaker": "I", "text": "Wie koordinieren Sie diese Suppressions mit dem Deployment-Team?"}
{"ts": "132:05", "speaker": "E", "text": "Wir binden das direkt in die CI/CD-Pipeline ein. Vor Beginn eines Deployments setzt ein Step ein Maintenance-Flag in P-NIM, das auch im Incident-Channel gepostet wird. So weiß das NOC-Team, dass Alarme in diesem Zeitfenster zu ignorieren sind."}
{"ts": "132:40", "speaker": "I", "text": "Gibt es dabei Abhängigkeiten zu den Auth-Integrationen, die wir zuvor besprochen hatten?"}
{"ts": "133:00", "speaker": "E", "text": "Ja, absolut. Wenn wir Auth-Module aktualisieren, insbesondere bei Änderungen an mTLS-Parametern, müssen wir sicherstellen, dass diese Wartungsfenster auch im Security-Team-Kalender hinterlegt sind. Das verhindert Konflikte, wenn parallel ein Security-Scan läuft."}
{"ts": "133:35", "speaker": "I", "text": "Kommen wir zu Risiken: Gab es Situationen, in denen Sie bewusst ein Risiko akzeptiert haben, um die Build-Phase zu beschleunigen?"}
{"ts": "133:55", "speaker": "E", "text": "Ja, z.B. bei der Rate-Limiting-Implementierung. Wir haben zunächst nur das statische Throttling aus Rollout-Plan ORI-RT-Plan1 aktiviert und das adaptive Modul verschoben. Das war im Ticket GW-4992 dokumentiert, weil wir wussten, dass die Testabdeckung dafür noch lückenhaft war, aber wir den Feature-Freeze halten mussten."}
{"ts": "134:30", "speaker": "I", "text": "Wie haben Sie diese Entscheidung intern begründet?"}
{"ts": "134:45", "speaker": "E", "text": "Wir haben einen Risk Acceptance Record erstellt, RAR-ORI-07. Darin steht, dass das statische Limit bei 500 req/s zwar weniger flexibel ist, aber laut Lasttests in Staging 98% der erwarteten Lastspitzen abdeckt. Das adaptive Modul wird in der nächsten Sprintplanung als Priorität 1 gesetzt."}
{"ts": "135:20", "speaker": "I", "text": "Gab es Beweismittel, die Sie genutzt haben, um diese Entscheidung zu untermauern?"}
{"ts": "135:35", "speaker": "E", "text": "Ja, wir haben die Loadtest-Reports LT-ORI-2024-05 angehängt, zusammen mit den Staging-Dashboards aus P-NIM, die zeigen, dass selbst unter Peak-Load die p95 Latenz bei 110ms blieb. Diese Daten sind im Confluence-Page 'Gateway Build Decisions' verlinkt."}
{"ts": "136:10", "speaker": "I", "text": "Zum Abschluss: Welche Lessons Learned ziehen Sie bisher aus dieser Build-Phase?"}
{"ts": "136:30", "speaker": "E", "text": "Wichtig ist für uns, dass wir frühzeitig Cross-Team-Kalender pflegen, besonders zwischen Security, Infra und DevOps, um Wartungs- und Deploy-Zeiten sauber abzustimmen. Und dass wir Runbooks wie RB-GW-011 und RB-OBS-033 nicht nur formal, sondern wirklich als Arbeitsgrundlage nutzen."}
{"ts": "146:00", "speaker": "I", "text": "Sie hatten vorhin die Optimierung des Rate Limitings erwähnt. Können Sie genauer erklären, welche Abwägungen Sie hier getroffen haben?"}
{"ts": "146:05", "speaker": "E", "text": "Ja, wir standen vor der Wahl zwischen einer sehr granularen Token-Bucket-Implementierung pro Client-ID und einer globalen Bucket-Lösung pro Region. Die erstere erfüllt strenger SLA-ORI-02, aber hätte mehr Latenz verursacht. Wir haben uns für die regionale Aggregation entschieden, dokumentiert in RFC-ORI-217, um die p95-Latenz stabil unter 120 ms zu halten."}
{"ts": "146:18", "speaker": "I", "text": "Gab es dafür formale Genehmigungen oder war das ein Teamentscheid?"}
{"ts": "146:23", "speaker": "E", "text": "Formale Genehmigung über den Architektur-Review-Board-Prozess, referenziert in Ticket ARC-5542. Wir haben die Entscheidung mit Messdaten aus der Staging-Umgebung belegt, die wir via P-NIM gesammelt haben."}
{"ts": "146:36", "speaker": "I", "text": "Wie sind Sie mit den zusätzlichen Risiken umgegangen, die aus der regionalen Aggregation entstanden sind?"}
{"ts": "146:41", "speaker": "E", "text": "Wir haben im Runbook RB-GW-029 einen Fallback-Pfad definiert: Falls ein Region-Bucket ausfällt oder überläuft, schalten wir temporär auf per-Client Limitierung um. Diese Logik ist im Deployment-Manifest mit Feature-Flags abgesichert."}
{"ts": "146:55", "speaker": "I", "text": "Gab es schon einen Incident, bei dem Sie dieses Fallback nutzen mussten?"}
{"ts": "147:00", "speaker": "E", "text": "Einmal, ja – Incident INC-ORI-774, ausgelöst durch einen fehlerhaften Burst im API-Traffic. Wir haben innerhalb von 3 Minuten umgeschaltet, was auch im Post-Mortem PM-ORI-774 dokumentiert ist."}
{"ts": "147:14", "speaker": "I", "text": "Wie stellen Sie sicher, dass solche Lessons Learned breit im Team geteilt werden?"}
{"ts": "147:19", "speaker": "E", "text": "Wir haben ein wöchentliches Tech-Sync, in dem Post-Mortems durch den Incident-Owner vorgestellt werden. Zusätzlich pflegen wir eine interne Confluence-Seite 'Gateway Ops Learnings', auf der alle Runbooks und Incidents verlinkt sind."}
{"ts": "147:33", "speaker": "I", "text": "Vielleicht noch zum Thema Auth-Policies: Gab es Trade-offs bei der Umsetzung neuer Policies in Bezug auf POL-SEC-001?"}
{"ts": "147:38", "speaker": "E", "text": "Ja, um die Build-Phase nicht zu verzögern, haben wir temporär breitere Rollen für Service-Accounts zugelassen, mit strikter Ablaufzeit von 14 Tagen. Das war in Change-Request CR-SEC-312 festgehalten und mit dem Security-Team abgestimmt."}
{"ts": "147:51", "speaker": "I", "text": "Und wie wurde dieser temporäre Zustand überwacht?"}
{"ts": "147:56", "speaker": "E", "text": "Über ein spezielles Alert-Set in P-NIM, das jede Policy mit Ablaufdatum <30 Tage überwacht. Wir hatten sogar einen Alert, der in Incident INC-SEC-198 mündete, weil ein Ablaufdatum übersehen wurde."}
{"ts": "148:10", "speaker": "I", "text": "Zum Abschluss: Gibt es ein konkretes Beispiel, wo ein Runbook oder Ticket als Beweismittel für eine Entscheidung diente?"}
{"ts": "148:15", "speaker": "E", "text": "Definitiv. Bei der Entscheidung, Blue/Green statt Canary zu fahren, haben wir RB-GW-011 als Begründung herangezogen. Das Runbook enthielt Benchmarks aus früheren Projekten (Ticket DEP-4410), die belegen, dass Blue/Green für unsere Auth-Integrationen weniger Ausfallrisiko birgt."}
{"ts": "148:00", "speaker": "I", "text": "Lassen Sie uns noch einmal konkret auf die Rate-Limiting-Implementierung zurückkommen. Welche Parameter haben Sie final gewählt und warum?"}
{"ts": "148:15", "speaker": "E", "text": "Wir haben uns für einen Sliding-Window-Ansatz mit 100 Requests pro Sekunde entschieden, kombiniert mit einem Burst-Puffer von 20. Der Grund ist, dass unsere Tests gemäß SLA-ORI-02 gezeigt haben, dass dies die p95 Latenz bei hoher Last stabil unter 120ms hält."}
{"ts": "148:37", "speaker": "I", "text": "Gab es dabei konkurrierende Anforderungen, die Sie abwägen mussten?"}
{"ts": "148:44", "speaker": "E", "text": "Ja, durchaus. Einerseits wollten die API-Consumer mehr Durchsatz, andererseits hat das Security-Team unter POL-SEC-001 betont, dass zu großzügige Limits Angriffsflächen öffnen. Wir haben uns an RB-GW-022 orientiert, das explizit die Balance zwischen Performance und Abuse-Prevention beschreibt."}
{"ts": "149:08", "speaker": "I", "text": "Wie haben Sie diese Parameteränderung dokumentiert, um Traceability sicherzustellen?"}
{"ts": "149:15", "speaker": "E", "text": "Wir haben ein RFC-Dokument erstellt, RFC-ORI-019, in dem die Messwerte aus den Lasttests, die Diskussion im Architektur-Review und die Freigabe durch den Product Owner protokolliert sind. Zusätzlich ist im Changelog des Terraform-Moduls ein entsprechender Commit mit Ticket-ID GW-5123 verlinkt."}
{"ts": "149:39", "speaker": "I", "text": "Gab es Risiken, die Sie bewusst akzeptiert haben, um die Build-Phase nicht zu verzögern?"}
{"ts": "149:46", "speaker": "E", "text": "Ja, wir haben bewusst auf eine vollständige Integration der neuen Observability-Dashboards aus P-NIM verzichtet, Runbook RB-OBS-047 empfiehlt das zwar, aber wir haben stattdessen Minimal-Metriken angebunden. Das Risiko: geringere Frühwarnung bei Anomalien, was wir in der Go-Live-Phase nachholen wollen."}
{"ts": "150:10", "speaker": "I", "text": "Wie wurde dieses Risiko im Team kommuniziert?"}
{"ts": "150:16", "speaker": "E", "text": "In unserem wöchentlichen Build-Sync habe ich das als 'Known Gap' im Jira-Board markiert und mit der ID RSK-ORI-07 versehen. Das war wichtig, damit alle Stakeholder, besonders Ops, wissen, dass noch ein Monitoring-Upgrade aussteht."}
{"ts": "150:34", "speaker": "I", "text": "Haben Sie eine Rückfallebene definiert, falls die vereinfachte Überwachung nicht ausreicht?"}
{"ts": "150:41", "speaker": "E", "text": "Ja, wir haben in RB-OBS-033 einen manuellen Health-Check-Plan, der alle 30 Minuten die wichtigsten Endpunkte prüft. Zusätzlich haben wir Warnschwellen für Error Rates in der Log-Pipeline gesetzt, um im Notfall reagieren zu können."}
{"ts": "150:59", "speaker": "I", "text": "Gab es ein Beispiel, wo ein Runbook oder Ticket direkt als Entscheidungsgrundlage gedient hat?"}
{"ts": "151:05", "speaker": "E", "text": "Ja, das Ticket GW-4821 mit dem mTLS-Handshake-Fix war ausschlaggebend. Wir haben daraus gelernt, dass die Zertifikats-Rotation synchron mit dem Deployment erfolgen muss. Runbook RB-GW-011 wurde entsprechend angepasst, um solche Race-Conditions zu vermeiden."}
{"ts": "151:27", "speaker": "I", "text": "Welche Lehren ziehen Sie persönlich aus diesen Entscheidungen für künftige Projekte?"}
{"ts": "151:34", "speaker": "E", "text": "Ich habe gelernt, dass es besser ist, Risiken klar zu benennen und bewusst einzuplanen, anstatt sie zu verstecken. Und dass dokumentierte Trade-offs, gestützt durch Tickets und Runbooks, im Nachhinein viel Zeit und Diskussion sparen."}
{"ts": "152:00", "speaker": "I", "text": "Lassen Sie uns noch einmal auf das Thema Monitoring zurückkommen – wie haben Sie die p95 Latenz-Überwachung im Orion Edge Gateway konkret technisch umgesetzt?"}
{"ts": "152:05", "speaker": "E", "text": "Wir haben im Build-Cluster Prometheus-Exporter direkt an die API-Gateway-Instanzen gekoppelt. Die Service-Mesh-Layer liefern die Latenzwerte in Millisekunden, und ein dediziertes Alert-Schema prüft kontinuierlich gegen die 120 ms aus SLA-ORI-02."}
{"ts": "152:13", "speaker": "I", "text": "Gab es dabei Wechselwirkungen mit der Authentifizierungsschicht, die die Latenz beeinflussten?"}
{"ts": "152:18", "speaker": "E", "text": "Ja, definitiv. Gerade beim mTLS-Handshake hatten wir initial Peaks. Wir haben das mit Session Resumption optimiert, und im Runbook RB-SEC-019 dokumentiert, wie die Handshake-Times reduziert werden."}
{"ts": "152:26", "speaker": "I", "text": "Und diese Optimierung, war die in Abstimmung mit dem Security-Team oder eigenständig?"}
{"ts": "152:31", "speaker": "E", "text": "In enger Abstimmung. Wir haben im RFC-SEC-221 den Change beschrieben, Threat Models geprüft und ein Go vom Security-Team erhalten, bevor wir in die Staging-Pipeline deployt haben."}
{"ts": "152:40", "speaker": "I", "text": "Wie binden Sie eigentlich die Observability-Daten aus Nimbus (P-NIM) ins Dashboard des Gateways ein?"}
{"ts": "152:45", "speaker": "E", "text": "Wir nutzen den Nimbus Data Collector als Sidecar, der Gateway-Metriken in ein zentrales Grafana-Board streamt. Über Label-Filter können wir zwischen internen und externen Requests unterscheiden."}
{"ts": "152:53", "speaker": "I", "text": "Gab es Herausforderungen beim Mapping der Nimbus-Daten auf die Gateway-Metriken?"}
{"ts": "152:58", "speaker": "E", "text": "Ja, die Namenskonventionen waren nicht konsistent. Wir haben in Ticket OBS-5587 eine Normalisierungs-Logik implementiert, um Metriken wie 'req_duration_ms' und 'latency' zusammenzuführen."}
{"ts": "153:06", "speaker": "I", "text": "Und wie vermeiden Sie dabei Alert Fatigue, gerade bei so vielen Metriken?"}
{"ts": "153:11", "speaker": "E", "text": "Da greifen wir auf RB-OBS-033 zurück. Wir aggregieren vor dem Alerting, setzen dynamische Schwellenwerte und definieren Quiet Times, um unnötige PagerDuty-Calls zu verhindern."}
{"ts": "153:19", "speaker": "I", "text": "Hat diese Aggregation jemals dazu geführt, dass ein echter Vorfall zu spät erkannt wurde?"}
{"ts": "153:24", "speaker": "E", "text": "Einmal, ja. Bei einem CPU-Spike wurde der Alarm gedämpft. Wir haben danach im Lessons-Learned-Meeting entschieden, CPU-Metriken wieder mit statischen Thresholds zu versehen."}
{"ts": "153:32", "speaker": "I", "text": "Also eine bewusste Anpassung des Runbooks als Reaktion auf reale Incidents?"}
{"ts": "153:37", "speaker": "E", "text": "Genau. Wir haben RB-OBS-033 v1.2 daraus abgeleitet und dokumentiert, welche Metriken nicht mehr dynamisch angepasst werden dürfen, um die Reaktionszeit nicht zu gefährden."}
{"ts": "153:36", "speaker": "I", "text": "Bleiben wir noch kurz beim Thema Risiken – können Sie ein weiteres Beispiel nennen, wo Sie eine technische Schuld bewusst in Kauf genommen haben?"}
{"ts": "153:42", "speaker": "E", "text": "Ja, wir haben beim Auth-Integrationstest die vollständige End-to-End mTLS-Prüfung nicht in jedem Build laufen lassen, um die Pipeline-Zeit unter 15 Minuten zu halten. Das war in Ticket GW-4978 dokumentiert, mit Verweis auf Runbook RB-SEC-017, das vorsieht, diese Tests stichprobenartig nachzuziehen."}
{"ts": "153:55", "speaker": "I", "text": "Und wie stellen Sie sicher, dass dadurch keine sicherheitskritischen Lücken unbemerkt bleiben?"}
{"ts": "154:00", "speaker": "E", "text": "Wir haben einen Canary-Deployment-Ansatz gewählt. Das heißt, ein kleiner Prozentsatz des Traffics geht durch die neue Auth-Konfiguration, während parallel die vollständigen mTLS-Tests in einer isolierten Stage-Umgebung laufen. Falls ein Fehler auftaucht, wird sofort auf die vorherige Policy zurückgerollt."}
{"ts": "154:15", "speaker": "I", "text": "Gab es Situationen, in denen dieser Canary Rollback tatsächlich aktiviert wurde?"}
{"ts": "154:20", "speaker": "E", "text": "Einmal, ja. Beim Merge von RFC-ORI-104 hat die neue Policy Requests aus einer Partnerregion blockiert, weil das Zertifikatsprofil nicht im Truststore war. Der Canary-Alarm griff innerhalb von 3 Minuten, und wir haben auf die alte Policy zurückgeschwenkt."}
{"ts": "154:36", "speaker": "I", "text": "Wie haben Sie das Problem anschließend behoben?"}
{"ts": "154:40", "speaker": "E", "text": "Wir haben das Zertifikatsprofil in den globalen Truststore aufgenommen und zusätzlich in RB-SEC-017 die Testfälle für Partnerregionen erweitert. Außerdem wurde in SLA-ORI-02 ein Prüfpunkt ergänzt, um Auth-Fehler in der p95 Latenz-Messung zu berücksichtigen."}
{"ts": "154:55", "speaker": "I", "text": "Das klingt nach einer engen Verzahnung von Security und Performance. War das ein bewusst gesteuertes Ziel?"}
{"ts": "155:00", "speaker": "E", "text": "Absolut. Wir wollten vermeiden, dass Security-Mechanismen als separate Silo-Metrik behandelt werden. Deshalb tracken wir Auth-Erfolgsraten und Latenzen gemeinsam im Nimbus Observability Dashboard, verlinkt mit P-NIM."}
{"ts": "155:12", "speaker": "I", "text": "Welche Lessons Learned konnten Sie aus dieser Erfahrung ableiten?"}
{"ts": "155:16", "speaker": "E", "text": "Erstens, dass Canary-Rollouts nicht nur für Code, sondern auch für Policy-Änderungen unverzichtbar sind. Zweitens, dass unsere Runbooks lebendige Dokumente sind – RB-SEC-017 wurde direkt aktualisiert. Drittens, dass Performance- und Security-SLOs parallel betrachtet werden sollten."}
{"ts": "155:29", "speaker": "I", "text": "Haben Sie daraufhin den Change-Management-Prozess angepasst?"}
{"ts": "155:33", "speaker": "E", "text": "Ja, RFCs für Auth-Integration müssen jetzt explizit eine Canary-Strategie enthalten. Außerdem wurde in POL-QA-014 ergänzt, dass Testabdeckung für externe Partnerzertifikate vor Freigabe nachzuweisen ist."}
{"ts": "155:46", "speaker": "I", "text": "Gab es darüber hinaus ein Risiko, das Sie langfristig akzeptiert haben?"}
{"ts": "155:50", "speaker": "E", "text": "Wir haben entschieden, die Rate-Limiting-Regeln für interne Dienste erst in der nächsten Phase granular zu gestalten. Kurzfristig riskieren wir so, dass interne Tests mehr Ressourcen beanspruchen, aber das beschleunigt den aktuellen Rollout erheblich und ist im Risikoregister unter ORI-RSK-09 dokumentiert."}
{"ts": "156:00", "speaker": "I", "text": "Lassen Sie uns noch kurz auf das Thema Observability eingehen – wie haben Sie die Integration mit P‑NIM zuletzt optimiert?"}
{"ts": "156:05", "speaker": "E", "text": "Wir haben im letzten Sprint den Sidecar‑Collector auf Version 2.4 gehoben, damit die Traces aus dem Orion Edge Gateway konsistent im Nimbus Observability erscheinen. Das war wichtig für die Korrelation zwischen API‑Latenzen und Auth‑Handshake‑Zeiten."}
{"ts": "156:14", "speaker": "I", "text": "Gab es dabei technische Stolpersteine, die mehrere Subsysteme betroffen haben?"}
{"ts": "156:18", "speaker": "E", "text": "Ja, die mTLS‑Handshake‑Problematik aus GW‑4821 hat wieder reingespielt, weil die Trace‑IDs bei TLS‑Fehlern nicht sauber propagiert wurden. Wir mussten im Gateway‑Ingress und im Auth‑Service die gRPC‑Interceptor synchron anpassen."}
{"ts": "156:28", "speaker": "I", "text": "Das klingt nach einem klassischen Multi‑Hop‑Debugging… haben Sie dafür ein Runbook genutzt?"}
{"ts": "156:32", "speaker": "E", "text": "Genau, RB‑OBS‑033 war unsere Basis, allerdings haben wir einen zusätzlichen Abschnitt ergänzt, um die Korrelation von Trace‑Spans über fehlgeschlagene mTLS‑Sessions hinweg zu dokumentieren."}
{"ts": "156:40", "speaker": "I", "text": "Wie wirkt sich das auf das SLA‑Monitoring, insbesondere SLA‑ORI‑02, aus?"}
{"ts": "156:45", "speaker": "E", "text": "Positiv – wir können jetzt präziser identifizieren, ob Latenzspitzen durch Netzwerklayer‑Retries oder durch Auth‑Delays entstehen. Das hilft, die p95 unter 120 ms zu halten."}
{"ts": "156:54", "speaker": "I", "text": "Und wie gehen Sie mit Alarmmüdigkeit um, wenn mehrere solcher Layer Alerts auslösen?"}
{"ts": "156:59", "speaker": "E", "text": "Wir haben die Alert‑Aggregation auf der Observability‑Plattform so konfiguriert, dass korrelierte Events zu einem Incident zusammengefasst werden. Zusätzlich gibt es einen Cool‑down‑Timer, wie in RB‑OBS‑033 vorgeschlagen."}
{"ts": "157:06", "speaker": "I", "text": "Wechseln wir kurz zum Thema Zusammenarbeit: Wie koordinieren Sie Änderungen an den Auth‑Policies mit dem Security‑Team?"}
{"ts": "157:10", "speaker": "E", "text": "Über wöchentliche Policy‑Sync‑Calls und das gemeinsame RFC‑Board. Beispielsweise lief die letzte JIT‑Access‑Anpassung unter RFC‑AUTH‑219 und wurde innerhalb von drei Tagen durch beide Teams freigegeben."}
{"ts": "157:20", "speaker": "I", "text": "Und wie stellen Sie sicher, dass diese Änderungen traceable bleiben, gemäss POL‑QA‑014?"}
{"ts": "157:24", "speaker": "E", "text": "Jede Änderung wird im zentralen Change‑Log mit Ticket‑ID, Datum und Reviewer‑Signatur erfasst. Die Doku ist Teil des Deploy‑Artifacts, sodass wir jederzeit nachvollziehen können, welche Policy‑Version live ist."}
{"ts": "157:34", "speaker": "I", "text": "Abschließend, gab es eine Entscheidung in dieser Phase, bei der Sie bewusst ein Risiko in Kauf genommen haben, um schneller voranzukommen?"}
{"ts": "157:39", "speaker": "E", "text": "Ja, wir haben bei der Implementierung des dynamischen Rate‑Limiters zunächst auf die vollständige Canary‑Phase verzichtet, um die Build‑Phase nicht zu verzögern. Das war dokumentiert in Ticket ORI‑RL‑087 mit Verweis auf das Risiko 'RL‑Skip‑Canary'. Wir haben es akzeptiert, weil die Lasttests keine kritischen Ausfälle zeigten."}
{"ts": "157:36", "speaker": "I", "text": "Kommen wir noch einmal auf das Thema Observability zurück: Welche konkreten Schritte haben Sie unternommen, um die Daten aus Nimbus Observability in das Gateway-Monitoring zu integrieren?"}
{"ts": "157:46", "speaker": "E", "text": "Wir haben einen dedizierten Exporter-Container im Gateway-Cluster deployed, der per gRPC die Metriken aus P-NIM zieht und im Prometheus-Format aufbereitet. Das Mapping haben wir gemäß RB-OBS-033 umgesetzt, damit die Alerts konsistent bleiben."}
{"ts": "157:58", "speaker": "I", "text": "Und wie stellen Sie sicher, dass diese Alerts nicht zu Alert Fatigue führen?"}
{"ts": "158:05", "speaker": "E", "text": "Wir haben Alert-Deduplizierung in der Alertmanager-Config aktiviert und Schwellenwerte angepasst, basierend auf historischen p95-Latenzen. Außerdem nutzen wir den wöchentlichen Review-Call, um störende Alerts zu klassifizieren und ggf. temporär zu stummschalten."}
{"ts": "158:17", "speaker": "I", "text": "Wie wirkt sich das auf das SLA-ORI-02 Monitoring aus?"}
{"ts": "158:24", "speaker": "E", "text": "Positiv, weil wir weniger Rauschen haben und echte SLA-Verletzungen schneller sehen. Wir haben einen separaten SLA-Dashboard-Tab, der nur die kritischen Metriken wie Throughput und Error Rate aggregiert."}
{"ts": "158:36", "speaker": "I", "text": "Gab es hierbei eine besondere Herausforderung in Bezug auf die Auth-Integration?"}
{"ts": "158:43", "speaker": "E", "text": "Ja, die mTLS-Handshake-Latenzen tauchten zunächst als Ausreißer auf, weil die Logs aus dem Auth-Proxy und dem Gateway nicht synchronisiert waren. Wir haben dann den Zeitabgleich über NTP-Harmonisierung implementiert, Ticket GW-4821 diente als Referenz."}
{"ts": "158:57", "speaker": "I", "text": "Interessant. Wie koordinieren Sie solche Änderungen teamübergreifend?"}
{"ts": "159:03", "speaker": "E", "text": "Über das wöchentliche Sync-Meeting mit Security und Ops. Änderungen an Auth-Policies gehen immer über ein RFC-Board; für die Zeitsynchronisierung haben wir RFC-ORI-27 eingereicht und dokumentiert."}
{"ts": "159:15", "speaker": "I", "text": "Sie hatten vorhin Trade-offs beim Rate Limiting erwähnt. Gab es auch bei der Observability Trade-offs?"}
{"ts": "159:23", "speaker": "E", "text": "Definitiv. Mehr granulare Metriken bedeuten höhere Speicherkosten und etwas mehr Latenz bei Abrufen. Wir haben uns entschieden, nur p50, p90 und p95 zu sammeln, um den Overhead zu begrenzen."}
{"ts": "159:35", "speaker": "I", "text": "Wie haben Sie das gegenüber dem Management begründet?"}
{"ts": "159:41", "speaker": "E", "text": "Mit einer Kosten-Nutzen-Analyse aus dem Runbook RB-OBS-045 und einem Test-Report, der zeigte, dass zusätzliche Quantile kaum Mehrwert für die SLA-Überwachung bringen."}
{"ts": "159:52", "speaker": "I", "text": "Gibt es Lessons Learned, die Sie aus dieser Phase mitnehmen?"}
{"ts": "159:58", "speaker": "E", "text": "Ja, dass eine saubere Datenquelle über mehrere Subsysteme hinweg – in unserem Fall Auth-Proxy, Gateway und Observability-Stack – entscheidend ist. Und dass Runbooks wie RB-OBS-033 nicht nur technische, sondern auch organisatorische Klarheit schaffen."}
{"ts": "160:00", "speaker": "I", "text": "Lassen Sie uns jetzt noch einmal auf das Thema Monitoring eingehen – welche Metriken prüfen Sie aktuell regelmäßig, um sicherzustellen, dass die p95-Latenz unter dem vereinbarten Grenzwert bleibt?"}
{"ts": "160:05", "speaker": "E", "text": "Wir haben dafür einen Satz von Prometheus-Queries, die sowohl `gateway_request_duration_seconds_bucket` als auch den `upstream_service_latency` tracken. Zusätzlich gibt es einen Alert in Alertmanager, der bei 110 ms p95 eine Warnung generiert, sodass wir vor dem 120 ms SLA-Limit reagieren können."}
{"ts": "160:12", "speaker": "I", "text": "Und wie werden diese Daten denn ins zentrale Nimbus Observability Projekt integriert?"}
{"ts": "160:16", "speaker": "E", "text": "Wir nutzen den Sidecar-Exporter, der im P-NIM Standard vorgesehen ist. Der schreibt die Metriken in den zentralen Kafka-Bus, von wo aus sie in den Nimbus Data Lake gehen. Dort laufen dann die SLO-Auswertungen, wie in RB-OBS-033 beschrieben."}
{"ts": "160:23", "speaker": "I", "text": "RB-OBS-033 enthält auch Hinweise zum Umgang mit Alert Fatigue, richtig?"}
{"ts": "160:27", "speaker": "E", "text": "Genau. Ein wichtiger Punkt ist die Aggregation ähnlicher Alerts. Statt zehn separater mTLS-Warnungen bekommen wir einen Sammel-Alert mit Verweis auf das zugehörige Runbook, so dass das On-Call-Team gezielt nachschauen kann."}
{"ts": "160:33", "speaker": "I", "text": "Apropos mTLS – wie hat sich Ihre damalige Lösung aus Ticket GW-4821 langfristig ausgewirkt?"}
{"ts": "160:38", "speaker": "E", "text": "Sehr positiv, muss ich sagen. Wir haben ja die Handshake-Retries in Envoy so konfiguriert, dass transient errors abgefangen werden, und gleichzeitig mit POL-SEC-001 die Zertifikatsrotation auf JIT Access umgestellt. Das hat die Fehlerrate bei neuen Sessions um 60 % reduziert."}
{"ts": "160:46", "speaker": "I", "text": "Wie koordinieren Sie dabei mit dem Security-Team, wenn neue Auth-Policies eingespielt werden?"}
{"ts": "160:50", "speaker": "E", "text": "Wir haben einen wöchentlichen Sync. Vor jeder Rollout-Phase eines Auth-Policy-Changes wird ein RFC erstellt, der alle Änderungen auflistet. Erst nach Freigabe durch Security und QA gehen die Änderungen in den Blue/Green-Deploy gemäß RB-GW-011."}
{"ts": "160:57", "speaker": "I", "text": "Und wie stellen Sie dabei die Traceability nach POL-QA-014 sicher?"}
{"ts": "161:01", "speaker": "E", "text": "Jede Änderung wird im internen Git mit einem Verweis auf das RFC- und das Ticket-IDs versehen. Zusätzlich speichern wir die Policy-Hashes in unserem Config-Registry, damit spätere Audits die exakte Version nachvollziehen können."}
{"ts": "161:08", "speaker": "I", "text": "Gab es Situationen, in denen Sie bewusst ein Risiko akzeptiert haben, um die Build-Phase zu beschleunigen?"}
{"ts": "161:12", "speaker": "E", "text": "Ja, beim Rate Limiting haben wir zunächst auf ein vereinfachtes Token-Bucket-Modell gesetzt, obwohl wir wussten, dass es bei Bursts leicht über dem SLA-ORI-02 liegen könnte. Die Entscheidung wurde in Ticket GW-5007 dokumentiert mit dem Hinweis, nach Go-Live auf ein leaky-bucket mit adaptive shaping umzustellen."}
{"ts": "161:20", "speaker": "I", "text": "Und welches Runbook diente Ihnen dabei als Grundlage?"}
{"ts": "161:24", "speaker": "E", "text": "RB-GW-025 – das beschreibt die Evaluierung verschiedener Rate-Limiting-Algorithmen und enthält auch eine Risiko-Matrix. Wir konnten so klar belegen, warum die temporäre Vereinfachung vertretbar war."}
{"ts": "161:36", "speaker": "I", "text": "Lassen Sie uns noch einmal auf die Schnittstelle zwischen Gateway und Auth-Service zurückkommen. Wie haben Sie mTLS-Handshake-Probleme zuletzt gelöst, ohne die Build-Phase zu verzögern?"}
{"ts": "161:41", "speaker": "E", "text": "Wir haben in Ticket GW-4821 einen Workaround dokumentiert: temporär haben wir das Zertifikats-Reloading asynchron gestaltet, basierend auf RB-SEC-017. So konnten wir die Handshakes entkoppeln, während im Hintergrund der neue Cert-Provider integriert wurde."}
{"ts": "161:48", "speaker": "I", "text": "Und dieser Ansatz, hat der irgendwelche Seiteneffekte auf das Rate Limiting oder die SLA-ORI-02 gehabt?"}
{"ts": "161:52", "speaker": "E", "text": "Minimal, ja. In den ersten 48 Stunden war die p95 Latenz um ca. 8 ms erhöht. Wir haben das in den Nimbus Observability Dashboards verfolgt und nachjustiert, indem wir die Keep-Alive-Werte in der Gateway-Konfiguration angepasst haben."}
{"ts": "161:59", "speaker": "I", "text": "Wie haben Sie das Security-Team bei dieser Änderung eingebunden?"}
{"ts": "162:03", "speaker": "E", "text": "Über unseren wöchentlichen Auth-Policy-Sync. Wir haben vorab im Security-Channel die RFC-ORI-029 gepostet und POL-SEC-001 referenziert, um sicherzustellen, dass auch der temporäre Zustand den Least-Privilege-Prinzipien entspricht."}
{"ts": "162:10", "speaker": "I", "text": "Gab es dabei auch Monitoring-Anpassungen?"}
{"ts": "162:13", "speaker": "E", "text": "Ja, wir haben eine zusätzliche Metrik 'mtls_handshake_time' eingeführt und in RB-OBS-033 als Beobachtungspunkt vermerkt. Alerts wurden zunächst in den Info-Level gesetzt, um Alert Fatigue zu vermeiden."}
{"ts": "162:20", "speaker": "I", "text": "Interessant. Können Sie ein Beispiel nennen, wie Sie Observability-Daten aus P-NIM ins Gateway-Monitoring integriert haben?"}
{"ts": "162:25", "speaker": "E", "text": "Wir ziehen per gRPC-Stream die Latenz- und Fehlerraten-Daten aus dem P-NIM Collector und mappen sie auf unsere Gateway-Service-IDs. Dadurch sehen wir in der gleichen Grafana-View sowohl interne als auch externe Metriken."}
{"ts": "162:33", "speaker": "I", "text": "Gab es beim Mapping zwischen internen IDs und externen P-NIM IDs Herausforderungen?"}
{"ts": "162:37", "speaker": "E", "text": "Ja, weil die Namenskonventionen nicht vollständig übereinstimmten. Wir haben ein kleines Mapping-File im IaC-Repo hinterlegt, Version kontrolliert, sodass CI/CD es bei Deployments automatisch injiziert."}
{"ts": "162:44", "speaker": "I", "text": "Zum Abschluss: Welche Lehre ziehen Sie aus diesen Anpassungen für die nächsten Projektphasen?"}
{"ts": "162:48", "speaker": "E", "text": "Wir müssen frühzeitig cross-funktionale Abhängigkeiten adressieren. Das mTLS-Thema hat gezeigt, dass Security, Observability und Deployment-Teams eng verzahnt sein müssen – und dass Runbooks wie RB-SEC-017 und RB-OBS-033 gemeinsam gepflegt werden sollten."}
{"ts": "162:55", "speaker": "I", "text": "Würden Sie sagen, dass der Trade-off, kurzfristig leicht erhöhte Latenz zu akzeptieren, vertretbar war?"}
{"ts": "162:59", "speaker": "E", "text": "Ja, unter Abwägung der Risiken war es vertretbar. Wir hatten klare Messwerte, ein Rückfall-Plan im Runbook und konnten so die Build-Phase ohne kritische Verzögerung fortsetzen."}
{"ts": "162:06", "speaker": "I", "text": "Wir hatten ja eben über die Build-Phase gesprochen. Können Sie jetzt etwas detaillierter auf die Interaktion zwischen Monitoring und Sicherheitsmechanismen eingehen, speziell wie Sie Metriken und Policy-Compliance verknüpfen?"}
{"ts": "162:12", "speaker": "E", "text": "Ja, klar. Wir haben im Orion Edge Gateway ein internes Correlation-Modul, das sowohl die Metriken aus Nimbus Observability als auch die Security-Events aus unserem Auth-Subsystem aggregiert. So können wir z.B. bei anomalen Latenzspitzen prüfen, ob zeitgleich ein mTLS-Handshake-Fehler wie in GW-4821 auftritt."}
{"ts": "162:23", "speaker": "I", "text": "Das klingt nach einer Cross-Domain-Analyse. Wie stellen Sie sicher, dass das im Rahmen der SLO-Überwachung effizient bleibt?"}
{"ts": "162:28", "speaker": "E", "text": "Wir haben Filter in der Pipeline definiert, die nur Events mit relevanten Tags weiterleiten – also 'latency_p95', 'auth_failure' etc. – und dann greift Runbook RB-OBS-033, um die Alert-Kaskade zu drosseln. So vermeiden wir Alert Fatigue und bleiben innerhalb der SLO-Auswertung."}
{"ts": "162:40", "speaker": "I", "text": "Und wie binden Sie das in die CI/CD-Umgebung ein?"}
{"ts": "162:45", "speaker": "E", "text": "Bei jedem Deployment wird ein Canary-Release mit einem Observability-Webhook versehen. Der sendet in Echtzeit Metriken an das Monitoring-Dashboard, und wenn die p95-Latenz über 120ms steigt, wird der Rollout automatisch pausiert. Das ist konform zu RB-GW-011."}
{"ts": "162:57", "speaker": "I", "text": "Verstehe. Gab es dabei technische Herausforderungen, die nicht in den offiziellen Runbooks standen?"}
{"ts": "163:02", "speaker": "E", "text": "Ja, zum Beispiel mussten wir für die mTLS-Log-Korrelation eine eigene Parser-Extension für den Log-Collector schreiben, weil die Standardkonfiguration von P-NIM die Session-ID nicht mitlieferte. Das haben wir als internen Note in unserem Wiki dokumentiert."}
{"ts": "163:13", "speaker": "I", "text": "Können Sie ein Beispiel nennen, wie Sie mit dem Security-Team zusammenarbeiten, um solche Anpassungen abzusichern?"}
{"ts": "163:18", "speaker": "E", "text": "Klar, wir erstellen für jede Anpassung ein Mini-RFC, der die Änderung beschreibt, inklusive Testplan. Das Security-Team prüft, ob wir POL-SEC-001 weiterhin erfüllen. Erst nach deren Freigabe mergen wir die Änderung in die IaC-Templates."}
{"ts": "163:29", "speaker": "I", "text": "Sie erwähnten IaC. Welche Tools nutzen Sie aktuell dafür?"}
{"ts": "163:33", "speaker": "E", "text": "Terraform für die Cloud-Ressourcen und Ansible für die Gateway-spezifischen Konfigurationen. Das Zusammenspiel ist wichtig, weil Terraform die Netzwerkinfrastruktur inkl. Load Balancer bereitstellt, während Ansible die Auth-Policies deployt."}
{"ts": "163:44", "speaker": "I", "text": "Gab es dabei Konflikte zwischen den Modulen, zum Beispiel beim Rolling Deployment?"}
{"ts": "163:49", "speaker": "E", "text": "Ja, anfangs gab es Race Conditions, wenn Terraform schneller fertig war als Ansible. Dadurch wurden einige Gateways ohne korrekte Auth-Policy live geschaltet. Wir haben das gelöst, indem wir in RB-GW-011 einen zusätzlichen Synchronisationsschritt definiert haben."}
{"ts": "164:00", "speaker": "I", "text": "Wenn Sie auf die bisherigen Lessons Learned schauen, gibt es einen Punkt, der Ihnen für zukünftige Phasen besonders wichtig erscheint?"}
{"ts": "164:05", "speaker": "E", "text": "Definitiv die enge Verzahnung von Observability und Security. Multi-hop Analysen wie die Korrelation von Latenzmetriken mit Auth-Fehlern sparen uns enorm Zeit bei der Fehlerursachenanalyse und sollten von Beginn an in die Architektur gedacht werden."}
{"ts": "165:06", "speaker": "I", "text": "Kommen wir noch einmal auf das Monitoring zu sprechen – welche konkreten Metriken beobachten Sie derzeit am Gateway, um das SLA-ORI-02 konsequent einzuhalten?"}
{"ts": "165:14", "speaker": "E", "text": "Wir tracken vorrangig die p95 Latenz, Error Rates nach API-Route, und die mTLS Handshake-Dauer. Zusätzlich haben wir im letzten Sprint auch die Connection Pool Saturation integriert, um frühzeitig Bottlenecks zu erkennen."}
{"ts": "165:26", "speaker": "I", "text": "Und wie spielen hier die Daten aus dem Nimbus Observability Projekt P-NIM hinein?"}
{"ts": "165:32", "speaker": "E", "text": "Wir haben mit dem Observability-Team ein gRPC-Exportmodul gebaut, das Logs und Traces direkt ins P-NIM schiebt. Dort werden sie mit den Backend-Service-Metriken korreliert, sodass wir end-to-end Sichtbarkeit haben."}
{"ts": "165:45", "speaker": "I", "text": "Gab es bei dieser Integration technische Stolpersteine?"}
{"ts": "165:49", "speaker": "E", "text": "Ja, besonders das Mapping der Trace IDs zwischen Gateway und Backend war tricky. Wir mussten einen Custom Propagator entwickeln, dokumentiert in RFC-GW-133, um konsistente IDs sicherzustellen."}
{"ts": "166:02", "speaker": "I", "text": "Wie stellen Sie sicher, dass Alerts aus diesen Metriken nicht zu Alert Fatigue führen?"}
{"ts": "166:08", "speaker": "E", "text": "Wir nutzen Runbook RB-OBS-033, um Schwellenwerte dynamisch anzupassen. Außerdem bündeln wir ähnliche Events und setzen Suppression Windows nach Deployments."}
{"ts": "166:20", "speaker": "I", "text": "Und zum Thema Zusammenarbeit: wie koordinieren Sie mit dem Security-Team, wenn es um neue Auth-Policies geht?"}
{"ts": "166:27", "speaker": "E", "text": "Wir haben ein wöchentliches Sync-Meeting, in dem wir geplante Änderungen reviewen. Für dringende Anpassungen gibt es einen JIT-Freigabeprozess gemäss POL-SEC-001, den wir strikt einhalten."}
{"ts": "166:39", "speaker": "I", "text": "Wie läuft der RFC-Prozess bei Änderungen an der Auth-Integration konkret ab?"}
{"ts": "166:44", "speaker": "E", "text": "Zuerst erstellen wir ein RFC-Dokument im internen Confluence, referenzieren betroffene Tickets wie GW-4821, und holen sign-off vom Security Lead. Danach erfolgt ein Staging-Testlauf, bevor es in Produktion geht."}
{"ts": "166:58", "speaker": "I", "text": "Dokumentieren Sie diese Schritte auch für die Nachvollziehbarkeit nach POL-QA-014?"}
{"ts": "167:03", "speaker": "E", "text": "Ja, jede Änderung wird im Change Log des Gateways mit Ticketnummer, Datum, und verantwortlicher Person erfasst. Das Audit-Team prüft einmal pro Quartal stichprobenartig."}
{"ts": "167:15", "speaker": "I", "text": "Zum Abschluss dieser Runde: gab es zuletzt ein Beispiel, wo ein Runbook oder Ticket als Beweismittel für eine Architekturentscheidung diente, abseits vom Rate Limiting?"}
{"ts": "167:23", "speaker": "E", "text": "Ja, bei der Entscheidung für den mTLS-Handshake-Optimizer haben wir Runbook RB-GW-021 und Ticket SEC-5123 herangezogen. Das half, die Performance-Verbesserung gegenüber dem Standard-Flow zu belegen und die Freigabe zu beschleunigen."}
{"ts": "167:06", "speaker": "I", "text": "Wir hatten vorhin kurz über die Blue/Green Deployments gesprochen – können Sie genauer erklären, wie Sie im Orion Edge Gateway Projekt sicherstellen, dass wir bei einem Rollback keine Authentifizierungs-States verlieren?"}
{"ts": "167:11", "speaker": "E", "text": "Ja, klar. Wir haben dafür in RB-GW-011 eine Sequenz dokumentiert, bei der wir während des Umschaltens die Session Stores in Redis replizieren. Das heißt, selbst wenn wir von Green zurück zu Blue wechseln, bleiben die Tokens und mTLS-Session-Keys gültig."}
{"ts": "167:17", "speaker": "I", "text": "Und wie wird das technisch umgesetzt?"}
{"ts": "167:22", "speaker": "E", "text": "Wir nutzen einen Sidecar-Container, der die Redis-Persistence überwacht. Der Sidecar hat ein kleines Go-Skript, das auf die PreSwitch- und PostSwitch-Hooks hört – das ist in unserem CI/CD-Job im Stage 'pre_deploy_checks' integriert."}
{"ts": "167:29", "speaker": "I", "text": "Interessant. Und in Bezug auf Observability – wie verifizieren Sie, dass dieser Prozess keine zusätzlichen Latenzen verursacht?"}
{"ts": "167:35", "speaker": "E", "text": "Wir messen p95-Latenzen vor und nach dem Switch mit Nimbus Observability (P-NIM). Die Metriken werden automatisch gelabelt mit 'deploy_event=true', sodass wir in Grafana Dashboards sofort sehen, ob der Switch einen Latenzspike verursacht."}
{"ts": "167:42", "speaker": "I", "text": "Gab es schon mal einen Fall, wo dieser Spike über SLA-ORI-02 hinausging?"}
{"ts": "167:48", "speaker": "E", "text": "Einmal, ja – im Test im Februar. Da war der Sidecar falsch konfiguriert und hat doppelt repliziert. Das wurde in Ticket GW-5172 dokumentiert und wir haben den Runbook-Step 4.3 angepasst, um das zu verhindern."}
{"ts": "167:55", "speaker": "I", "text": "Wie koordinieren Sie solche Änderungen mit dem Security-Team?"}
{"ts": "168:01", "speaker": "E", "text": "Wir haben einen festen Weekly-Slot mit Security, in dem alle Patches, die Auth-Mechanismen betreffen, durchgesprochen werden. Änderungen an mTLS-Parametern oder Session Management gehen zusätzlich als RFC durch das SecOps Board."}
{"ts": "168:08", "speaker": "I", "text": "Und die Dokumentation – wie stellen Sie Traceability gemäß POL-QA-014 sicher?"}
{"ts": "168:13", "speaker": "E", "text": "Jede Änderung kriegt eine eindeutige Change-ID, die sowohl im Git-Commit als auch im Confluence-Change-Log auftaucht. Wir verlinken dort direkt auf die zugehörigen Tickets und Runbooks."}
{"ts": "168:20", "speaker": "I", "text": "Gab es bei der Umsetzung der Auth-Integration Punkte, an denen Sie bewusst ein Risiko akzeptiert haben?"}
{"ts": "168:26", "speaker": "E", "text": "Ja, beim Hotfix für GW-4821. Wir haben damals die mTLS-Handshake-Timeouts temporär von 2s auf 5s erhöht, obwohl das gegen unsere eigene Latenz-Policy ging, um einen Produktionsausfall zu verhindern. Das war im Change-Log als 'Risk Accepted' markiert."}
{"ts": "168:34", "speaker": "I", "text": "Wie wurde das danach wieder rückgängig gemacht?"}
{"ts": "168:38", "speaker": "E", "text": "Nach einer Woche, als der Upstream-Bug im Zertifikats-Parser gefixt war, haben wir den Timeout wieder auf 2s gesetzt. Das haben wir über dieselbe Pipeline ausgerollt und im Post-Mortem dokumentiert, inklusive Verweis auf RB-SEC-019."}
{"ts": "169:42", "speaker": "I", "text": "Sie hatten vorhin die Risiken im Build erwähnt. Können Sie jetzt bitte erläutern, wie Sie in der Integration mit dem Observability-Stack vorgegangen sind, um diese Risiken abzufedern?"}
{"ts": "169:48", "speaker": "E", "text": "Ja, klar. Wir haben schon früh im Build die Schnittstellen zu P-NIM, also Nimbus Observability, fest verdrahtet. Dadurch konnten wir eventbasierte Alerts in der Pre-Prod simulieren. Das war wichtig, um Latenzspitzen sofort zu erkennen und zu verifizieren, dass unsere Workarounds aus Ticket GW-4976 greifen."}
{"ts": "169:59", "speaker": "I", "text": "Und wie haben Sie diese Events in der Gateway-Konfiguration sichtbar gemacht?"}
{"ts": "170:03", "speaker": "E", "text": "Wir haben im IaC-Template ein Modul 'obs-metrics-exporter' ergänzt, das per Helm-Chart deployt wird. Die Config zieht automatisch die p95 und p99 Werte aus Prometheus und bindet sie an das Dashboard 'ORI-GW-LAT'."}
{"ts": "170:14", "speaker": "I", "text": "Gab es dabei Konflikte mit den Auth-Integrationen?"}
{"ts": "170:18", "speaker": "E", "text": "Ein kleines Problem: mTLS-Handshake und der Exporter-Container haben sich in der frühen Version nicht verstanden, weil die Sidecar-Proxys Ports blockierten. Wir haben das per Patch aus Runbook RB-OBS-033 gelöst, Abschnitt 'Sidecar Port Rebind'."}
{"ts": "170:30", "speaker": "I", "text": "Interessant, und das wurde komplett automatisiert?"}
{"ts": "170:34", "speaker": "E", "text": "Ja, wir haben das als Jenkins-Stage implementiert, die nur bei Änderungen an der Observability-Config triggert. So vermeiden wir unnötige Deployments und halten uns an RB-GW-011 für Rolling Blue/Green."}
{"ts": "170:44", "speaker": "I", "text": "Wie koordinieren Sie in so einem Fall mit dem Security-Team?"}
{"ts": "170:48", "speaker": "E", "text": "Wir haben einen wöchentlichen Slot, nennen wir's 'Auth+Obs Sync'. Dort besprechen wir RFCs, die mTLS- oder Policy-Änderungen beinhalten. Für den Patch hatten wir RFC-ORI-202 offen, die Security hat das in 48 Stunden freigegeben."}
{"ts": "170:59", "speaker": "I", "text": "Gab es Bedenken seitens Security wegen der Exporter-Daten?"}
{"ts": "171:03", "speaker": "E", "text": "Ja, sie wollten sicherstellen, dass keine sensiblen Auth-Header in den Metriken landen. Wir haben dafür einen Sanitizer geschrieben, der Header-Felder vor dem Export entfernt. Das war eine bewusste Entscheidung, auch wenn es minimalen Overhead erzeugt."}
{"ts": "171:15", "speaker": "I", "text": "Das klingt nach einem kleinen Trade-off zwischen Sicherheit und Performance."}
{"ts": "171:19", "speaker": "E", "text": "Genau. Wir haben den zusätzlichen Overhead akzeptiert, weil SLA-ORI-02 uns bei der Latenz noch genug Luft lässt. Der Beweis war ein Lasttest aus Ticket GW-5032, der trotz Sanitizer unter 110ms p95 blieb."}
{"ts": "171:30", "speaker": "I", "text": "Hatten Sie dafür ein spezielles Runbook, oder wurde das ad hoc dokumentiert?"}
{"ts": "171:34", "speaker": "E", "text": "Wir haben RB-AUTH-014 aktualisiert, Abschnitt 'Metrics Sanitization'. Darin ist jetzt der Jenkins-Job verlinkt und auch der Testplan aus GW-5032 hinterlegt, damit das bei künftigen Releases reproduzierbar bleibt."}
{"ts": "171:02", "speaker": "I", "text": "Sie hatten vorhin die Risiken erwähnt, die wir bewusst akzeptiert haben. Mich würde jetzt interessieren, wie Sie im Orion Edge Gateway Projekt mit dem Security-Team bei der Einführung neuer Auth-Policies konkret zusammenarbeiten."}
{"ts": "171:07", "speaker": "E", "text": "Wir haben dafür einen festen Sync-Slot jeden Dienstag, in dem wir die geplanten Änderungen gegen POL-SEC-001 prüfen. Dort gehen wir auch die Diff-Logs unserer Gateway-Konfiguration durch, die wir aus der IaC-Pipeline generieren. Außerdem nutzen wir den Policy-Validator aus dem internen Tool 'ValSec', um vorab Konflikte zu erkennen."}
{"ts": "171:14", "speaker": "I", "text": "Und wenn Sie so eine Änderung einreichen – wie läuft dann der RFC-Prozess ab?"}
{"ts": "171:19", "speaker": "E", "text": "Wir starten mit einem RFC-Dokument im Confluence Space 'ORI-ARCH'. Das Template orientiert sich an POL-QA-014. Nach initialem Review im Architekturboard wird das Ticket in Jira (Label RFC-ORI) erstellt, und wir müssen mindestens zwei +1 Reviews vom Security-Team haben, bevor die Änderung in den Build-Branch gemerged werden darf."}
{"ts": "171:27", "speaker": "I", "text": "Können Sie ein Beispiel nennen, wo dieser Prozess entscheidend war, um Probleme zu vermeiden?"}
{"ts": "171:33", "speaker": "E", "text": "Ja, im Ticket GW-4932 ging es um die Erweiterung des mTLS-Handshake-Timeouts. Ohne den Security-Review hätten wir eine zu hohe Timeout-Dauer konfiguriert, was unsere SLOs gefährdet hätte. Durch die Reviews konnten wir das auf 2,5 Sekunden begrenzen und gleichzeitig die Client-Kompatibilität wahren."}
{"ts": "171:41", "speaker": "I", "text": "Wie dokumentieren Sie solche Anpassungen, um die Traceability sicherzustellen?"}
{"ts": "171:46", "speaker": "E", "text": "Alle Änderungen landen in unserem GitOps-Repo, Commit-Messages enthalten das Ticket- oder RFC-Tag. Zusätzlich verlinken wir im Runbook RB-GW-021 die relevanten Abschnitte, damit on-call Engineers bei Incidents die Historie nachvollziehen können."}
{"ts": "171:54", "speaker": "I", "text": "Sie sprachen gerade von Runbooks – wie nutzen Sie die, um Alert Fatigue zu vermeiden?"}
{"ts": "171:59", "speaker": "E", "text": "RB-OBS-033 schreibt vor, dass wir Alerts mit einer p95 Latenz <120ms nur triggern, wenn drei aufeinanderfolgende 1-Minuten-Intervalle überschritten werden. Das filtert Fluktuationen raus. Zusätzlich taggen wir Alerts mit Severity und 'Actionability', sodass das On-Call-Team klar erkennt, ob sofortiges Eingreifen nötig ist."}
{"ts": "172:07", "speaker": "I", "text": "Wie werden die Observability-Daten aus Nimbus Observability (P-NIM) ins Gateway-Monitoring integriert?"}
{"ts": "172:12", "speaker": "E", "text": "Wir haben einen Exporter implementiert, der die P-NIM Streams in Prometheus-kompatible Metriken übersetzt. Diese laufen dann in unser zentrales Grafana-Dashboard 'ORI-GW-Perf'. Dort sehen wir Latenzen, Fehlerraten und den Status der Rate-Limiter pro Tenant."}
{"ts": "172:20", "speaker": "I", "text": "Gibt es bei dieser Integration besondere Herausforderungen?"}
{"ts": "172:25", "speaker": "E", "text": "Ja, P-NIM liefert teilweise hochfrequente Event-Bursts. Wir mussten ein Sampling implementieren, um die Gateway-CPU nicht zu überlasten. Das war auch in RFC-ORI-072 dokumentiert, inklusive Lasttests mit synthetischen Traffic-Profilen."}
{"ts": "172:33", "speaker": "I", "text": "Letzte Frage: Gab es eine Entscheidung in der Build-Phase, die Sie mit konkretem Runbook- oder Ticket-Bezug getroffen haben und die sich im Nachhinein als kritisch herausgestellt hat?"}
{"ts": "172:38", "speaker": "E", "text": "Ja, die Wahl, beim Rate Limiting auf ein token-bucket-Modell mit zentralem Redis-Cluster zu setzen, wurde im Runbook RB-GW-015 festgehalten. Das war Ticket GW-4770. Damals war das Risiko der Cross-Region-Latenz bekannt, wir haben es aber akzeptiert, um die Entwicklungszeit zu verkürzen. Später mussten wir das mit lokalem Caching mitigieren, was in RFC-ORI-088 festgehalten ist."}
{"ts": "174:42", "speaker": "I", "text": "Lassen Sie uns noch einmal auf die Integration der Observability-Daten eingehen – wie genau haben Sie P-NIM angebunden, um die Gateway-Metriken zu erfassen?"}
{"ts": "174:48", "speaker": "E", "text": "Wir haben einen dedizierten Exporter implementiert, der die internen Prometheus-Metriken des Gateways an Nimbus Observability weiterleitet. Dabei nutzen wir den RB-OBS-033 als Leitfaden, um sicherzustellen, dass keine sensiblen Payload-Informationen in den Timeseries landen."}
{"ts": "174:59", "speaker": "I", "text": "Gab es Probleme mit der Konvertierung der Metriken, etwa bei der Aggregation für p95 Latenz?"}
{"ts": "175:05", "speaker": "E", "text": "Ja, anfangs hatten wir falsche Buckets in der Histogrammkonfiguration. Das führte zu einer verzerrten p95-Berechnung. Wir haben das in Ticket OBS-217 nachverfolgt und die Buckets gemäß SLO-ORI-02 angepasst."}
{"ts": "175:15", "speaker": "I", "text": "Und wie reagieren Sie, wenn Alarme zu häufig ausgelöst werden?"}
{"ts": "175:20", "speaker": "E", "text": "Wir nutzen das 'Alert Fatigue'-Kapitel in RB-OBS-033: Wir setzen Deduplizierung und verlängern die Evaluierungszeitfenster, wenn wir feststellen, dass die Alarme eher Rauschen als echte Probleme signalisieren."}
{"ts": "175:34", "speaker": "I", "text": "Wie koordinieren Sie diese Anpassungen mit dem Security-Team, insbesondere wenn Security-relevante Metriken betroffen sind?"}
{"ts": "175:40", "speaker": "E", "text": "Wir haben einen wöchentlichen Sync-Call mit dem Security-Team, in dem wir jede Metrikänderung vorstellen. Wenn es um Auth-Failures geht, binden wir sie direkt ein, um POL-SEC-001 einzuhalten."}
{"ts": "175:53", "speaker": "I", "text": "Können Sie ein Beispiel nennen, bei dem ein Alarm direkt zu einer Policy-Anpassung geführt hat?"}
{"ts": "175:59", "speaker": "E", "text": "Ja, im April hatten wir plötzlich einen Anstieg bei 401-Responses. Der Alarm löste aus, und nach Analyse haben wir die JIT-Access-Policy im Gateway verschärft, basierend auf Incident GW-4932."}
{"ts": "176:12", "speaker": "I", "text": "Interessant. Wie dokumentieren Sie solche Änderungen für die Traceability?"}
{"ts": "176:17", "speaker": "E", "text": "Wir pflegen jede Policy-Änderung in unserem Confluence-basierten Änderungsprotokoll und verlinken das zugehörige RFC, z.B. RFC-ORI-19, damit es später auditierbar ist."}
{"ts": "176:27", "speaker": "I", "text": "Wenn wir auf die Build-Phase schauen: Gab es kürzlich einen Trade-off zwischen Performance und Sicherheit, den Sie bewusst getroffen haben?"}
{"ts": "176:33", "speaker": "E", "text": "Ja, bei der TLS-Handshake-Optimierung (GW-4821) haben wir die Cipher-Suite so gewählt, dass sie minimal langsamer ist, dafür aber Forward Secrecy garantiert. Das hat uns ca. 3ms Latenz gekostet, aber das Risiko eines Downgrades eliminiert."}
{"ts": "176:47", "speaker": "I", "text": "Und wie wurde diese Entscheidung abgesichert?"}
{"ts": "176:51", "speaker": "E", "text": "Wir haben die Empfehlung aus RB-SEC-044 konsultiert und im Architekturboard vorgestellt. Das Meeting-Protokoll ist an Ticket SEC-778 angehängt, um die Nachvollziehbarkeit zu gewährleisten."}
{"ts": "182:42", "speaker": "I", "text": "Bevor wir zu den Lessons Learned übergehen, können Sie noch kurz erläutern, wie Sie die mTLS-Handshake-Problematik aus Ticket GW-4821 final gelöst haben?"}
{"ts": "183:01", "speaker": "E", "text": "Ja, wir haben nach einigen Tests im Staging die TLS-Konfiguration so angepasst, dass die Client-Zertifikatsprüfung bereits auf der Edge-Ebene greift. Zusätzlich haben wir die Zertifikatsrotation in Anlehnung an RB-SEC-017 automatisiert, um Ablaufprobleme zu vermeiden."}
{"ts": "183:28", "speaker": "I", "text": "Gab es dabei größere Abstimmungen mit dem Security-Team?"}
{"ts": "183:36", "speaker": "E", "text": "Absolut, wir haben wöchentliche Syncs eingeführt. Die Policy-Änderungen wurden im RFC-Auth-458 eingereicht und nach Review durch SecOps in der CI/CD-Pipeline verankert."}
{"ts": "183:58", "speaker": "I", "text": "Wie stellen Sie sicher, dass die Deployments weiterhin im Einklang mit RB-GW-011 laufen, speziell bei den Blue/Green-Rollouts?"}
{"ts": "184:14", "speaker": "E", "text": "Wir haben in Jenkins einen Stage-Gate-Job, der die Service-Health-Checks aus P-NIM abruft. Erst wenn p95-Latenz und Error-Rate unter den in SLA-ORI-02 definierten Schwellen liegen, wird der Traffic umgeschwenkt."}
{"ts": "184:36", "speaker": "I", "text": "Und wie gehen Sie mit Alert Fatigue im Alltag um?"}
{"ts": "184:47", "speaker": "E", "text": "Wir nutzen RB-OBS-033 aktiv: Alerts werden in drei Kritikalitätsstufen eingeteilt, und Low-Priority-Events landen nur in einem wöchentlichen Digest. Zudem führen wir vierteljährliche Tuning-Sessions durch."}
{"ts": "185:10", "speaker": "I", "text": "Können Sie ein Beispiel geben, wo diese Tuning-Sessions konkrete Verbesserungen gebracht haben?"}
{"ts": "185:22", "speaker": "E", "text": "Ja, im März haben wir die Schwelle für CPU-Usage-Alerts auf 85% statt 75% gesetzt, basierend auf historischen Lastprofilen. Das hat die unnötigen Alarme um 40% reduziert, ohne reale Incidents zu übersehen."}
{"ts": "185:44", "speaker": "I", "text": "Wie dokumentieren Sie solche Änderungen, um Traceability gemäß POL-QA-014 zu gewährleisten?"}
{"ts": "185:56", "speaker": "E", "text": "Alle Änderungen werden in unserem internen Confluence als Change-Log erfasst, referenziert mit Ticket-IDs und den zugehörigen Runbooks. Zusätzlich committen wir die aktualisierten Configs ins IaC-Repo."}
{"ts": "186:18", "speaker": "I", "text": "Gab es in der Build-Phase Risiken, die Sie bewusst akzeptiert haben, um Zeit zu sparen?"}
{"ts": "186:29", "speaker": "E", "text": "Ja, wir haben den Rollout des erweiterten JWT-Claim-Checks auf das nächste Release verschoben, um die Kernfunktion Rate Limiting termingerecht zu liefern. Das war im Risikolog RL-ORI-07 dokumentiert."}
{"ts": "186:52", "speaker": "I", "text": "Und wie wurde diese Entscheidung abgesichert? Gab es formale Freigaben?"}
{"ts": "187:04", "speaker": "E", "text": "Genau, die Entscheidung wurde im Architekturboard mit Verweis auf Runbook RB-RISK-005 und Ticket DEC-ORI-22 abgesegnet. Wir haben die Security-Implikationen bewertet und ein temporäres Monitoring eingeführt."}
{"ts": "190:42", "speaker": "I", "text": "Bevor wir zum Abschluss kommen, möchte ich nochmal auf das Zusammenspiel zwischen API-Gateway und Observability eingehen. Können Sie den Weg der Latenzmetriken von der Erfassung bis ins Dashboard kurz skizzieren?"}
{"ts": "191:05", "speaker": "E", "text": "Ja, gerne. Also, die Latenzmessung startet direkt im Edge Gateway über den integrierten Metrics Collector, der per Sidecar läuft. Die Rohdaten gehen in unser internes Telemetrie-Bus-System, basierend auf NATS. Dort werden sie von einem Aggregator konsumiert, der die p50, p95 und p99 Werte berechnet. Anschließend pushen wir diese in Nimbus Observability (Projekt P-NIM), wo ein spezielles Dashboard für SLA-ORI-02 gepflegt wird."}
{"ts": "191:31", "speaker": "I", "text": "Und wie stellen Sie sicher, dass die Datenintegrität zwischen Gateway und Nimbus gewährleistet ist?"}
{"ts": "191:50", "speaker": "E", "text": "Wir nutzen Checksummen auf Batch-Ebene. Jede Metrik-Charge erhält einen SHA256-Hash, der beim Empfang in Nimbus validiert wird. Außerdem haben wir in Runbook RB-OBS-033 beschrieben, wie bei Mismatch ein automatischer Re-Export aus dem Raw-Buffer initiiert wird."}
{"ts": "192:14", "speaker": "I", "text": "Gab es in letzter Zeit Fälle, in denen dieser Re-Export nötig war?"}
{"ts": "192:32", "speaker": "E", "text": "Ja, vor drei Wochen. Wir hatten eine Inkonsistenz in den p99-Werten aufgrund eines fehlerhaften Sidecar-Updates. Ticket OBS-2173 dokumentiert den Vorfall; wir haben daraufhin die Sidecar-Images in der CI/CD-Pipeline mit einem zusätzlichen Integrationstest versehen."}
{"ts": "192:59", "speaker": "I", "text": "Apropos CI/CD, wie schnell können Sie aktuell ein Sidecar-Update von Commit bis Rollout durchziehen?"}
{"ts": "193:15", "speaker": "E", "text": "Durch die Optimierung des Build-Caches und parallele Stages in der GitOps-Pipeline sind wir bei ca. 18 Minuten End-to-End. Dabei halten wir uns strikt an RB-GW-011 für Blue/Green-Deployments, um Ausfallzeiten zu vermeiden."}
{"ts": "193:39", "speaker": "I", "text": "Wie koordinieren Sie solche Updates mit dem Security-Team, gerade wenn Auth-Module betroffen sind?"}
{"ts": "193:55", "speaker": "E", "text": "Hier gilt POL-SEC-001: Least Privilege und Just-in-Time Access. Vor einem Update erstellen wir ein temporäres Access-Window, das im Auth-Proxy freigegeben wird. Das Security-Team prüft vorab das Changelog, und wir loggen alle Änderungen im Audit-Trail, wie es POL-QA-014 verlangt."}
{"ts": "194:22", "speaker": "I", "text": "Welche Lessons Learned haben Sie aus der letzten Build-Sprint-Retrospektive mitgenommen?"}
{"ts": "194:41", "speaker": "E", "text": "Eine zentrale Erkenntnis war, dass wir Cross-Team-Tests früher einplanen müssen. Beim letzten Sprint gab es einen Engpass, weil das Auth-Team erst zwei Tage vor dem Release Zugriff auf die Staging-Umgebung bekam. Das führte zu Last-Minute-Fixes, die wir vermeiden wollen."}
{"ts": "195:05", "speaker": "I", "text": "Wie wollen Sie das konkret umsetzen?"}
{"ts": "195:20", "speaker": "E", "text": "Wir haben im neuen Sprint-Plan einen festen Slot 'Inter-Team Integration Testing' in Woche zwei platziert. Außerdem wird jede RFC, die Auth betrifft, automatisch einen Notification-Trigger ans Security-Team auslösen – das ist bereits in unserem Jira-Workflow hinterlegt."}
{"ts": "195:47", "speaker": "I", "text": "Gibt es Risiken, die Sie aktuell im Blick behalten, auch wenn wir sie vorerst akzeptieren?"}
{"ts": "196:08", "speaker": "E", "text": "Ja, wir akzeptieren momentan, dass das Rate Limiting bei Traffic-Spikes oberhalb des 95. Perzentils leicht über den SLA-Wert geht. Der Fix ist in RFC-GW-884 beschrieben, aber wir priorisieren aktuell die Fertigstellung der Auth-Integration. Wir haben das Risiko dokumentiert und mit dem Produktmanagement abgestimmt."}
{"ts": "198:42", "speaker": "I", "text": "Danke für die Ausführung zu den Trade-offs. Lassen Sie uns noch auf das Monitoring eingehen – welche spezifischen Metriken sind aktuell in Ihrem Dashboard für das Orion Edge Gateway prominent?"}
{"ts": "198:55", "speaker": "E", "text": "Wir haben die p95 Latenz, die Fehlerquote nach API-Key und Path, und die aktiven mTLS-Sessions ganz oben. Zusätzlich tracken wir die Rate-Limiting Verstöße pro Tenant, um SLA-ORI-02 in Echtzeit zu überwachen."}
{"ts": "199:12", "speaker": "I", "text": "Und wie fließen die Daten aus Nimbus Observability konkret in dieses Dashboard ein?"}
{"ts": "199:20", "speaker": "E", "text": "Über den P-NIM Connector, der im Runbook RB-OBS-033 beschrieben ist. Der Connector streamt Metriken in unser PromQL Backend, die wir dann im Grafana-Board 'GW-Core' visualisieren."}
{"ts": "199:37", "speaker": "I", "text": "Gab es dabei Integrationsprobleme zwischen dem P-NIM Connector und der Gateway-Telemetrie?"}
{"ts": "199:45", "speaker": "E", "text": "Ja, anfangs gab es ein Problem mit der Label-Normalisierung. Ticket MON-2214 dokumentiert, wie wir per Regex-Rewriter die Labels auf ein einheitliches Schema gebracht haben."}
{"ts": "200:02", "speaker": "I", "text": "Wie gehen Sie mit Alert Fatigue um, gerade wenn so viele Metriken gleichzeitig feuern?"}
{"ts": "200:10", "speaker": "E", "text": "Wir haben eine dedizierte Alert-Suppression-Policy eingeführt, die in RB-OBS-033 Abschnitt 5 beschrieben ist. Außerdem reviewen wir mit dem SRE-Team wöchentlich die Top 10 Alert-Quellen."}
{"ts": "200:27", "speaker": "I", "text": "Kommen wir zur Zusammenarbeit: Wie koordinieren Sie mit dem Security-Team beim Ausrollen neuer Auth-Policies?"}
{"ts": "200:35", "speaker": "E", "text": "Wir nutzen ein gemeinsames RFC-Template, das die Policy-Details, Impact-Analyse und Testplan enthält. Vor dem Merge ins Auth-Repo muss ein Security-Engineer den Pull Request abzeichnen."}
{"ts": "200:52", "speaker": "I", "text": "Und wie dokumentieren Sie diese Änderungen für die Nachvollziehbarkeit nach POL-QA-014?"}
{"ts": "201:00", "speaker": "E", "text": "Jede Änderung erhält eine Change-ID, die in Confluence im Changelog erfasst wird. Zusätzlich referenzieren wir das zugehörige Ticket und die RFC-Nummer."}
{"ts": "201:15", "speaker": "I", "text": "Gab es jüngst einen Fall, wo diese Dokumentationskette besonders wichtig war?"}
{"ts": "201:23", "speaker": "E", "text": "Ja, bei GW-5178 zur Anpassung der JWT-Claim-Struktur. Ohne die lückenlose Doku hätten wir nicht schnell verifizieren können, dass die Änderung POL-SEC-001-konform war."}
{"ts": "201:40", "speaker": "I", "text": "Abschließend: Sehen Sie für die nächsten zwei Sprints noch Risiken, die wir bewusst eingehen müssen?"}
{"ts": "201:48", "speaker": "E", "text": "Ein Restrisiko bleibt bei der Parallelisierung der Blue/Green Deployments laut RB-GW-011. Wir akzeptieren temporär eine leicht erhöhte Latenz während des Green-Warmups, um die Build-Phase termingerecht abzuschließen."}
{"ts": "205:42", "speaker": "I", "text": "Sie hatten eben die Risiken beim Rate Limiting beschrieben. Mich würde jetzt interessieren: wie haben Sie parallel dazu die Observability so erweitert, dass diese Risiken früh erkennbar werden?"}
{"ts": "206:05", "speaker": "E", "text": "Wir haben im Zuge der Anpassungen ein zusätzliches Alert-Set in Nimbus Observability (P-NIM) aktiviert, speziell für die Gateway-Komponenten. Das war kein trivialer Schritt, weil wir RB-OBS-033 strikt befolgen mussten, um Alert Fatigue zu vermeiden. Wir haben Schwellenwerte für p95 Latenz und Error-Rate kombiniert und mit einem Cooldown versehen."}
{"ts": "206:34", "speaker": "I", "text": "Gab es dafür eine formale RFC oder ist das eher ad hoc entstanden?"}
{"ts": "206:45", "speaker": "E", "text": "Es gab tatsächlich eine RFC, RFC-ORI-278, die wir innerhalb von zwei Tagen durch den Change Advisory Board bekommen haben. Darin haben wir auch dokumentiert, wie diese Metrik-Kombination mit bestehenden mTLS-Checks aus Ticket GW-4821 korreliert, um eine End-to-End Sicht zu erhalten."}
{"ts": "207:12", "speaker": "I", "text": "Interessant, also haben Sie Metriken aus der Security-Integration und aus dem Performance-Monitoring zusammengebracht."}
{"ts": "207:25", "speaker": "E", "text": "Genau, das war ein Multi-Hop-Ansatz: erst kommt der mTLS-Handshake, dann Auth-Verification, dann Rate-Limiting-Decision. Wir messen an allen drei Punkten, um SLA-ORI-02 zu überwachen und bei Abweichungen gezielt die Ursache zu finden."}
{"ts": "207:52", "speaker": "I", "text": "Wie haben Sie dabei mit dem Security-Team koordiniert, gerade wenn neue Auth-Policies eingespielt werden?"}
{"ts": "208:03", "speaker": "E", "text": "Wir haben einen wöchentlichen Sync mit dem Security-Team und zusätzlich ein JIT Access-Protokoll gemäß POL-SEC-001 eingerichtet. Jede Policy-Änderung wird in einem Staging-Gateway mit Blue/Green Deployment (RB-GW-011) getestet, bevor sie in Produktion geht."}
