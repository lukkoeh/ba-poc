{"ts": "00:00", "speaker": "I", "text": "Können Sie bitte kurz die Ziele und den aktuellen Stand des Titan DR Projekts skizzieren?"}
{"ts": "01:15", "speaker": "E", "text": "Ja, gern. Titan DR hat als Hauptziel, einen belastbaren Multi-Region-Failover für unsere kritischen Plattformdienste zu ermöglichen, mit einem RTO von maximal 30 Minuten und RPO unter 5 Minuten. Aktuell sind wir in der Drill-Phase, das heißt wir simulieren komplette Region-Ausfälle, um unsere Prozeduren wie in Runbook RB-DR-001 zu verifizieren."}
{"ts": "03:05", "speaker": "I", "text": "Und welche Verantwortung übernehmen Sie konkret als Cloud Architect in diesem Projekt?"}
{"ts": "04:20", "speaker": "E", "text": "Ich entwerfe die technische Architektur für die Cross-Region-Replikation, definiere die Netzwerk- und Storage-Topologien und stelle sicher, dass die SLAs in den Service Design Docs eingehalten werden. Dazu gehört auch, dass ich bei GameDays wie TEST-DR-2025-Q1 die Konfigurationen prüfe und Lessons Learned in die nächste Iteration einfließen lasse."}
{"ts": "06:00", "speaker": "I", "text": "Wie passt das Projekt in die Gesamtstrategie von Novereon Systems, insbesondere im Hinblick auf das Leitprinzip 'Safety First'?"}
{"ts": "07:25", "speaker": "E", "text": "Safety First bedeutet für uns, dass Ausfallsicherheit Vorrang vor Performance-Optimierungen hat. Titan DR ist der operative Ausdruck dieser Strategie: Wir investieren bewusst in redundante Infrastruktur und in automatisierte Failover-Mechanismen, auch wenn das kurzfristig höhere Kosten erzeugt."}
{"ts": "09:10", "speaker": "I", "text": "Wie ist die Architektur für Multi-Region-Failover im Titan DR Projekt gestaltet?"}
{"ts": "11:00", "speaker": "E", "text": "Wir nutzen ein aktives-aktives Modell zwischen zwei Primärregionen und eine passive Tertiärregion für extreme Szenarien. Die Datenbankreplikation erfolgt über synchrones Streaming zwischen den Primärregionen und asynchron zur Tertiärregion, um Latenz und Kosten zu balancieren."}
{"ts": "13:30", "speaker": "I", "text": "Welche SLAs oder SLOs sind für dieses Projekt definiert und wie stellen Sie deren Einhaltung sicher?"}
{"ts": "15:00", "speaker": "E", "text": "Im SLA-Dokument SLA-DR-2025 sind 99,95 % Verfügbarkeit pro Quartal und die genannten RTO/RPO-Werte festgelegt. Wir überwachen die KPIs mit dem Nimbus Observability Stack, triggern Alerts bei Abweichungen und referenzieren dabei klar definierte Eskalationspfade aus RB-DR-001."}
{"ts": "17:45", "speaker": "I", "text": "Wie berücksichtigen Sie RTO und RPO in Ihrem Design?"}
{"ts": "19:10", "speaker": "E", "text": "Wir führen vorab Latenz- und Datenverlust-Simulationen durch, um die Replikationsstrategie zu wählen. Zum Beispiel hat eine synchrone Replikation zwar bessere RPO-Werte, kann aber RTO durch komplexere Recovery-Prozesse verlängern. Deshalb kombinieren wir Synchronität zwischen Primärregionen und Asynchronität zur Tertiärregion."}
{"ts": "21:40", "speaker": "I", "text": "Gibt es Integrationspunkte zwischen Titan DR und dem Nimbus Observability Projekt?"}
{"ts": "23:00", "speaker": "E", "text": "Ja, klar. Nimbus liefert die Metriken und Logs, die unsere DR-Runbooks triggern. Ohne die Echtzeitdaten aus Nimbus könnten wir Failover-Entscheidungen nicht automatisiert und SLA-konform treffen. Zum Beispiel wird bei einem 'Region Health Score' unter 70 % automatisch das RB-DR-001 Playbook gestartet."}
{"ts": "25:20", "speaker": "I", "text": "Wie nutzen Sie Runbooks wie RB-DR-001 in Verbindung mit Observability-Alarmen?"}
{"ts": "27:00", "speaker": "E", "text": "Wir haben die Alarme in Nimbus so konfiguriert, dass sie direkt in unser Incident Response Tool posten. Dort wird automatisch ein Link zu RB-DR-001 angezeigt, mit den passenden Parametern wie betroffene Region und Services. So kann das On-Call-Team ohne Verzögerung die richtigen Schritte einleiten."}
{"ts": "90:00", "speaker": "I", "text": "Könnten Sie bitte nun genauer auf die Architektur eingehen – wie ist das Multi-Region-Failover technisch umgesetzt?"}
{"ts": "90:08", "speaker": "E", "text": "Ja, gerne. Wir haben zwei aktive Regionen in Europa und Nordamerika, verbunden über ein asynchrones Replikationslayer. Die Steuerung übernimmt unser interner Orchestrator 'AegisSwitch', der per Health-Check alle 30 Sekunden den Status prüft. Failover erfolgt automatisiert, aber mit manuellem Approval innerhalb von fünf Minuten, um Fehltrigger zu vermeiden."}
{"ts": "90:38", "speaker": "I", "text": "Welche SLAs und SLOs sind in diesem Kontext definiert?"}
{"ts": "90:44", "speaker": "E", "text": "Wir haben ein internes SLA von 99,95 % Verfügbarkeit für kritische Kunden-APIs. Das SLO für RTO liegt bei maximal 15 Minuten, das RPO bei 120 Sekunden. Die Einhaltung prüfen wir mit simulierten Failovern im Drill-Modus alle sechs Wochen, dokumentiert in Test-Tickets wie TEST-DR-2025-07."}
{"ts": "91:15", "speaker": "I", "text": "Wie stellen Sie sicher, dass RTO und RPO im Design berücksichtigt sind?"}
{"ts": "91:21", "speaker": "E", "text": "Durch die Kombination aus synchronisierten Metadaten in einem globalen KvStore und asynchron replizierten Datenbank-Instanzen. Wir haben in RB-DR-001 genau festgelegt, in welchen Sequenzen Services hochgefahren werden, um Abhängigkeiten einzuhalten und das RTO nicht zu reißen."}
{"ts": "91:50", "speaker": "I", "text": "Gibt es Integrationspunkte zwischen Titan DR und dem Nimbus Observability Projekt?"}
{"ts": "91:56", "speaker": "E", "text": "Ja, Nimbus liefert die Metriken und Alarme, die AegisSwitch triggern. Außerdem haben wir gemeinsam ein Observability-Playbook ergänzt, das im Runbook RB-DR-001 als Anhang C referenziert wird. Das erlaubt es, Alarm-IDs direkt mit DR-Workflows zu verknüpfen."}
{"ts": "92:20", "speaker": "I", "text": "Wie nutzen Sie Runbooks wie RB-DR-001 in Verbindung mit den Observability-Alarmen?"}
{"ts": "92:26", "speaker": "E", "text": "Wenn Nimbus einen Severity-1 Alarm ausgibt, verweist der Alarmtext bereits auf den relevanten Abschnitt im Runbook, z. B. RB-DR-001 §4.2. Dadurch spart das Incident-Team wertvolle Minuten, weil niemand erst suchen muss, welche Prozedur für diese Fehlerklasse gilt."}
{"ts": "92:48", "speaker": "I", "text": "Welche Lessons Learned aus Helios Datalake oder Poseidon Networking fließen hier konkret ein?"}
{"ts": "92:54", "speaker": "E", "text": "Aus Helios haben wir gelernt, dass Schema-Änderungen vor einem Failover propagiert werden müssen, um Inkonsistenzen zu vermeiden. Poseidon hat uns gezeigt, dass Netzwerksegmentierung den BLAST_RADIUS stark reduziert – wir haben daher isolierte VPCs pro Region eingeführt."}
{"ts": "93:16", "speaker": "I", "text": "Balancieren Sie die Kosten für zusätzliche Regionen mit den Performancegewinnen?"}
{"ts": "93:22", "speaker": "E", "text": "Ja, wir haben ein Cost-Performance-Matrix erstellt, wo jede Region nach Latenz und Betriebskosten bewertet wird. Regionen mit unter 80 ms Latenz und moderaten Kosten laufen aktiv, andere sind im Warm-Standby, um die Budgets nicht zu sprengen."}
{"ts": "93:45", "speaker": "I", "text": "Welche Maßnahmen haben Sie ergriffen, um den BLAST_RADIUS zu minimieren?"}
{"ts": "93:51", "speaker": "E", "text": "Neben den isolierten VPCs haben wir Service-Mesh-Routing-Regeln, die Traffic pro Mandant trennen. So können wir bei einem Ausfall gezielt nur betroffene Mandanten umleiten, statt die gesamte Plattform zu bewegen."}
{"ts": "96:00", "speaker": "I", "text": "Könnten Sie bitte etwas genauer auf die SLAs eingehen, die Sie für Titan DR definiert haben, und wie diese technisch abgesichert werden?"}
{"ts": "96:15", "speaker": "E", "text": "Ja, also wir haben ein 99,95% Availability SLA für die kritischen Services und ein RTO von maximal 15 Minuten, RPO 5 Minuten. Technisch setzen wir dafür auf synchrones Replikations-Setup zwischen den Primär- und Sekundärregionen und automatisiertes Failover via dem Orchestrator-Modul 'AuroraSwitch'."}
{"ts": "96:45", "speaker": "I", "text": "Und wie wird das in Ihren Monitoring- und Observability-Tools reflektiert?"}
{"ts": "97:00", "speaker": "E", "text": "Die Metriken laufen in Nimbus Observability ein. Wir haben dort spezielle Dashboards, die RB-DR-001 referenzieren, mit Thresholds für Latenz, Heartbeats und Datenkonsistenz. Sobald ein Alert triggert, wird automatisch ein Ticket im DR-Queue erstellt — zum Beispiel DR-TCK-8821 im letzten Drill."}
{"ts": "97:30", "speaker": "I", "text": "Sie hatten erwähnt, dass Lessons Learned aus Helios Datalake eingeflossen sind. Können Sie ein konkretes Beispiel nennen?"}
{"ts": "97:45", "speaker": "E", "text": "Aus Helios Datalake haben wir gelernt, dass wir vor allem die Cross-Region-Datenintegrität nicht nur stichprobenartig, sondern kontinuierlich prüfen müssen. Damals gab es ein Inkonsistenzfenster von 12 Minuten, was für DR zu lang ist. Daher haben wir im Titan DR Projekt eine Prüfroutine alle 90 Sekunden implementiert."}
{"ts": "98:15", "speaker": "I", "text": "Interessant. Und wie spielt Poseidon Networking hier hinein?"}
{"ts": "98:30", "speaker": "E", "text": "Poseidon hat uns gelehrt, möglichst wenige, klar definierte Routing-Policies zu verwenden, um Failover-Zeiten zu verkürzen. Wir nutzen jetzt ein vereinfachtes BGP-Failover-Template, das im Runbook RB-NET-042 dokumentiert ist und bei Titan DR integriert wurde."}
{"ts": "99:00", "speaker": "I", "text": "Gibt es bei der Integration von Nimbus Observability und Titan DR noch manuelle Schritte?"}
{"ts": "99:15", "speaker": "E", "text": "Der Großteil ist automatisiert, aber wir haben noch einen manuellen Sanity-Check vor dem finalen Umschalten der Region. Dieser Schritt ist in RB-DR-001 als 'Step 7b' dokumentiert und soll menschliche Plausibilitätskontrolle sicherstellen."}
{"ts": "99:45", "speaker": "I", "text": "Wie stellen Sie sicher, dass diese manuellen Schritte nicht zum Bottleneck werden?"}
{"ts": "100:00", "speaker": "E", "text": "Wir trainieren die Operatoren regelmäßig in GameDays, sodass der Check unter zwei Minuten bleibt. Außerdem wird im Drill-Report ein KPI dazu erfasst, DR-KPI-07, um Abweichungen sofort zu erkennen."}
{"ts": "100:30", "speaker": "I", "text": "Können Sie mir ein Beispiel geben, wie eine Alarmkette gemäß RB-DR-001 konkret abläuft?"}
{"ts": "100:45", "speaker": "E", "text": "Sicher. Nehmen wir einen Latenz-Alarm >200ms: Nimbus triggert Alert-Policy DR-LAT-02, erstellt Ticket DR-TCK-8850, Operator prüft Step 3-5 in RB-DR-001, wenn bestätigt, wird 'AuroraSwitch' manuell bestätigt, Failover startet, und RB-DR-001 Step 8 dokumentiert die Rückverlagerung."}
{"ts": "101:15", "speaker": "I", "text": "Sehen Sie noch Verbesserungspotenzial in der Integration dieser Systeme?"}
{"ts": "101:30", "speaker": "E", "text": "Ja, wir wollen die Schnittstelle zwischen Nimbus und dem Orchestrator komplett API-basiert machen, um den manuellen Bestätigungsprozess optional zu automatisieren, sobald wir genügend Vertrauen in die False-Positive-Rate haben."}
{"ts": "112:00", "speaker": "I", "text": "Lassen Sie uns jetzt auf das Thema Risikomanagement eingehen. Im letzten GameDay, also TEST-DR-2025-Q1, welche Risiken sind Ihnen da besonders aufgefallen?"}
{"ts": "112:20", "speaker": "E", "text": "Ja, also ähm, im GameDay haben wir festgestellt, dass die Failover-Latenz in der Region Oslo-2 um knapp 15 Sekunden über dem RTO-Ziel lag. Grund war ein fehlerhaftes Health-Check-Skript, das in Ticket DR-INC-573 dokumentiert ist. Zusätzlich haben wir eine Überschneidung von Observability-Alarmen in Nimbus gesehen, die zu doppelten Eskalationen führten."}
{"ts": "112:50", "speaker": "I", "text": "Und wie hat dieses Incident-Postmortem Ihre Architekturentscheidungen beeinflusst?"}
{"ts": "113:05", "speaker": "E", "text": "Wir haben daraufhin in RFC-DR-2025-04 beschlossen, die Health-Checks zu entkoppeln, sodass die Failover-Entscheidung nicht mehr von einer einzigen Monitoring-Quelle abhängt. Außerdem haben wir im Runbook RB-DR-001 einen neuen Step eingefügt, der vor der Eskalation einen Cross-Check mit einem unabhängigen Probe-Cluster macht."}
{"ts": "113:35", "speaker": "I", "text": "Wie dokumentieren Sie diese Tradeoffs und Lessons Learned für künftige Audits?"}
{"ts": "113:48", "speaker": "E", "text": "Wir pflegen ein internes Confluence-Repository, in dem jedes Postmortem mit einer eigenständigen \"Decision Record\"-Sektion versehen wird. Da listen wir das Problem, die Optionen, den gewählten Weg und die Begründung auf. Für Audit-Zwecke packen wir auch Metriken rein – zum Beispiel die Latenzkurven aus Testlauf DR-GD-2025-01."}
{"ts": "114:20", "speaker": "I", "text": "Können Sie ein Beispiel für so eine Metrik nennen, die besonders wichtig war?"}
{"ts": "114:33", "speaker": "E", "text": "Ja, die 95th-Percentile-Failover-Zeit. In unserem Drill lag sie bei 47 Sekunden, das Ziel war ≤35 Sekunden. Diese Abweichung war der Auslöser für die Health-Check-Überarbeitung."}
{"ts": "114:55", "speaker": "I", "text": "Kommen wir zu den nächsten Prioritäten für Titan DR. Was steht da ganz oben auf Ihrer Liste?"}
{"ts": "115:10", "speaker": "E", "text": "Ganz oben steht die Automatisierung der Region-Spinup-Prozesse. Bisher brauchen wir noch manuelle Bestätigungsschritte aus Compliance-Gründen. Wir wollen über RFC-DR-2025-07 einen Approval-Workflow in unser Deployment-Tool einbauen, der diese Checks automatisiert, aber auditierbar macht."}
{"ts": "115:40", "speaker": "I", "text": "Und welche Verbesserungen wollen Sie im nächsten Drill-Zyklus umsetzen?"}
{"ts": "115:54", "speaker": "E", "text": "Wir planen, die aktiven Standby-Knoten so zu konfigurieren, dass sie kontinuierlich Traffic verarbeiten, wenn auch nur zu 5 % Load. Das verringert die Cold-Start-Zeit. Außerdem wollen wir die Blast-Radius-Analyse ausweiten, inspiriert von den Poseidon Networking-Findings, um Segmentierungsfehler zu vermeiden."}
{"ts": "116:25", "speaker": "I", "text": "Könnten Sie noch etwas zu geplanten RFCs zur Anpassung der Failover-Prozeduren sagen?"}
{"ts": "116:38", "speaker": "E", "text": "Neben RFC-DR-2025-07 haben wir RFC-DR-2025-09 in der Pipeline, der vorsieht, dass wir die Failover-Trigger auch auf Applikationsebene testen – nicht nur auf Infrastruktur. Das kam als Lesson Learned aus Helios Datalake, wo ein App-Level-Bottleneck unbemerkt blieb."}
{"ts": "117:05", "speaker": "I", "text": "Zum Abschluss: Wie schätzen Sie das Risiko ein, dass trotz all dieser Maßnahmen beim nächsten echten Ausfall etwas schiefgeht?"}
{"ts": "117:20", "speaker": "E", "text": "Risiko null gibt es nicht. Aber wir haben mit den jetzigen Anpassungen die Eintrittswahrscheinlichkeit kritischer Ausfälle deutlich reduziert. Die Kombination aus Multi-Source-Monitoring, aktiven Standby-Knoten und klaren Runbooks wie RB-DR-001 senkt die Impact-Schwere signifikant. Wichtig ist, dass wir das im nächsten GameDay wieder validieren."}
{"ts": "128:00", "speaker": "I", "text": "Sie hatten vorhin die Lessons Learned aus anderen Projekten erwähnt. Mich würde jetzt interessieren: Welche Risiken sind Ihnen im letzten GameDay (TEST-DR-2025-Q1) konkret aufgefallen?"}
{"ts": "128:20", "speaker": "E", "text": "Ja, im GameDay gab es ein paar unerwartete Findings. Zum Beispiel haben wir bei der simulierten Region-East-Outage festgestellt, dass unser Cross-Region DNS-Propagation länger dauerte als im SLA-RFC-DR-07 definiert. Konkret waren es 87 Sekunden statt der vereinbarten maximal 60."}
{"ts": "128:45", "speaker": "E", "text": "Außerdem fiel auf, dass ein Teil der Poseidon-Netzwerksegmente in West‑2 nicht wie geplant automatisch isoliert wurde. Das hat das Blast-Radius-Minimization-Pattern verletzt, das wir in RB-DR-004 dokumentiert haben."}
{"ts": "129:10", "speaker": "I", "text": "Wie haben Sie auf diese Abweichungen reagiert? Gab es sofortige Architekturänderungen oder eher Prozessanpassungen?"}
{"ts": "129:28", "speaker": "E", "text": "Wir haben kurzfristig ein Hotfix-Change im Change-Request CR-DR-221 eingespielt, um die DNS-TTLs in der Secondary-Region zu reduzieren. Prozessseitig wurde im Runbook RB-DR-001 ein zusätzlicher Schritt ergänzt, der die Isolations-Checks explizit triggert, bevor der Traffic umgelenkt wird."}
{"ts": "129:55", "speaker": "I", "text": "Gab es auch ein Postmortem zu diesen Findings?"}
{"ts": "130:08", "speaker": "E", "text": "Ja, das Incident-Postmortem PM-DR-2025-03. Darin haben wir dokumentiert, dass die längere DNS-Propagation teilweise auf einen veralteten Resolver-Cache in einer Subsystem-VM zurückging. Das war ein Konfigurationsdrift, der weder im Helios Monitoring- noch im Nimbus Observability-Alerting auftauchte."}
{"ts": "130:34", "speaker": "I", "text": "Das klingt nach einem typischen 'Hidden Dependency' Problem. Wie dokumentieren Sie solche Tradeoffs und Lessons Learned für künftige Audits?"}
{"ts": "130:52", "speaker": "E", "text": "Wir pflegen dafür ein Confluence-DR-Knowledge-Space, in dem jedes Postmortem als Audit-Asset markiert wird. Zusätzlich verlinken wir die entsprechenden Jira-Tickets wie DR-4521 direkt mit den betroffenen Runbooks und RFCs."}
{"ts": "131:18", "speaker": "E", "text": "So können Auditoren nachvollziehen, warum wir zum Beispiel die TTL-Werte geändert haben und wie wir den Trade-off zwischen schnelleren Failovers und höherem DNS-Traffic bewertet haben."}
{"ts": "131:42", "speaker": "I", "text": "Wenn Sie diesen Trade-off zwischen Kosten und Performance betrachten: Gab es intern Widerstand gegen die TTL-Reduktion?"}
{"ts": "131:58", "speaker": "E", "text": "Ja, das Network Ops Team hat argumentiert, dass kürzere TTLs die Anzahl der DNS-Queries deutlich erhöhen und damit die Betriebskosten in West‑2 um ca. 7% steigern. Wir haben aber anhand der Testdaten gezeigt, dass der Gewinn in der Recovery Time Objective (RTO) kritischer war."}
{"ts": "132:24", "speaker": "I", "text": "Wie gehen Sie mit solchen divergierenden Prioritäten um?"}
{"ts": "132:36", "speaker": "E", "text": "Wir nutzen ein DR-Decision-Board, das aus Vertretern von Architektur, Ops und Compliance besteht. Entscheidungen wie bei CR-DR-221 werden dort mit Risiko-Matrix und Cost-Benefit-Analyse präsentiert. Bei diesem Fall hat das Board nach 2 Sessions zugestimmt."}
{"ts": "133:02", "speaker": "I", "text": "Verstanden. Lassen Sie uns zum Abschluss noch kurz auf die nächsten Schritte schauen: Was sind die unmittelbar nächsten Prioritäten?"}
{"ts": "144:00", "speaker": "E", "text": "Für Q3 wollen wir einen erweiterten Drill durchführen, der auch simultane Ausfälle in zwei Regionen simuliert. Außerdem planen wir ein RFC-Update RFC-DR-15, um die Failover-Prozeduren robuster gegen Netzwerk-Latenzanomalien zu machen."}
{"ts": "144:00", "speaker": "I", "text": "Lassen Sie uns nun auf die konkreten Risiken eingehen, die Sie im letzten GameDay TEST-DR-2025-Q1 identifiziert haben."}
{"ts": "144:05", "speaker": "E", "text": "Ja, klar. Wir haben beim Drill festgestellt, dass die Replikationslatenz in der passiven Region zeitweise über unser RPO von 15 Minuten hinausging – konkret bis zu 22 Minuten. Das war in Ticket DR-INC-094 dokumentiert."}
{"ts": "144:15", "speaker": "I", "text": "Woran lag das denn genau?"}
{"ts": "144:19", "speaker": "E", "text": "Hauptursache war eine unerwartete Queue-Bildung im Poseidon Networking Layer. Das hat die Cross-Region-Streams verlangsamt. Unsere Runbook-Prozedur RB-DR-004 hatte diesen Pfad nicht explizit abgedeckt."}
{"ts": "144:29", "speaker": "I", "text": "Wie haben Sie darauf reagiert?"}
{"ts": "144:33", "speaker": "E", "text": "Wir haben kurzfristig den Throttle-Parameter in den Replikationsjobs angepasst, um den Burst zu reduzieren. Langfristig gibt es jetzt ein RFC-Update – RFC-DR-2025-12 – für RB-DR-004, das diesen Netzwerkpfad explizit testet."}
{"ts": "144:45", "speaker": "I", "text": "Gab es weitere Risiken, die Sie für die Architektur als kritisch einstufen?"}
{"ts": "144:49", "speaker": "E", "text": "Ja, wir haben den BLAST_RADIUS bei simultanem Ausfall von zwei Regionen unterschätzt. In der Simulation verloren 18 % der Non-Critical Workloads. Das haben wir im Lessons-Learned-Dokument LL-DR-2025-Q1 festgehalten."}
{"ts": "144:59", "speaker": "I", "text": "Das klingt nach einer schwierigen Abwägung. Wie dokumentieren Sie solche Tradeoffs für spätere Audits?"}
{"ts": "145:03", "speaker": "E", "text": "Wir nutzen ein Confluence-basiertes Decision-Log, in dem jede Abwägung mit Ticket-ID, Runbook-Referenz und Metriken hinterlegt ist. Dazu hängen wir Screenshots aus Nimbus Observability an, um die Metrikverläufe zu belegen."}
{"ts": "145:15", "speaker": "I", "text": "Haben Sie ein Beispiel für so eine dokumentierte Entscheidung?"}
{"ts": "145:19", "speaker": "E", "text": "Klar, Decision-Log-Eintrag DL-DR-045 beschreibt den Wechsel von aktiver zu hybrider Standby-Strategie. Belegt mit Metriken aus TEST-DR-2024-Q4, die zeigten, dass aktive Standby zwar 30 % teurer war, aber das RTO um 40 % reduzierte."}
{"ts": "145:31", "speaker": "I", "text": "Wie gehen Sie mit dem Risiko um, dass bei hybrider Standby manche Ressourcen ungenutzt bleiben?"}
{"ts": "145:35", "speaker": "E", "text": "Das ist tatsächlich ein Kostenfaktor. Wir haben dafür in Runbook RB-DR-007 einen wöchentlichen Kapazitätsreview eingeführt. So können wir ungenutzte Ressourcen in der Hybrid-Region temporär für Batch-Jobs freigeben."}
{"ts": "145:47", "speaker": "I", "text": "Und wie wird diese Flexibilität in den SLAs abgebildet?"}
{"ts": "145:51", "speaker": "E", "text": "In SLA-DR-2025 haben wir definiert, dass Non-Critical Workloads in der Hybrid-Region preemptible sein dürfen, solange das Core-RTO von 30 Minuten nicht verletzt wird. Das erlaubt uns, Kosten zu sparen und trotzdem resilient zu bleiben."}
{"ts": "146:00", "speaker": "I", "text": "Wenn wir den Blick nach vorne richten: Was sind jetzt, direkt nach den letzten Tests, die ganz konkreten Prioritäten für Titan DR?"}
{"ts": "146:05", "speaker": "E", "text": "Ganz oben auf der Liste steht die Automatisierung der Cross-Region-DNS-Updates. Im Drill haben wir noch zwei manuelle Schritte aus RB-DR-001 benötigt, die wir in RB-DR-002 automatisieren wollen, um die Recovery-Zeit um ca. 40 Sekunden zu verkürzen."}
{"ts": "146:15", "speaker": "I", "text": "Verstehe. Gibt es dafür schon ein RFC oder ist das noch in der Konzeptionsphase?"}
{"ts": "146:20", "speaker": "E", "text": "Das RFC-DR-2025-04 ist bereits im Draft-Status. Wir haben es letzte Woche im Architecture Review vorgestellt. Die Hauptdiskussion drehte sich um die Frage, wie wir die Health-Checks im Nimbus Observability zuverlässig triggern, bevor der DNS-Switch erfolgt."}
{"ts": "146:34", "speaker": "I", "text": "Apropos Health-Checks – werden diese im nächsten Drill-Zyklus erweitert?"}
{"ts": "146:38", "speaker": "E", "text": "Ja, wir erweitern von Level-2 Checks (Port erreichbar, HTTP 200) auf Level-3 Checks, die auch App-spezifische KPIs aus dem Helios Datalake auswerten. Das ist eine Lesson Learned aus TEST-DR-2025-Q1, wo ein Service zwar 'grün' war, aber eine interne Queue bereits 90% voll."}
{"ts": "146:55", "speaker": "I", "text": "Das heißt, Sie koppeln direkt die Observability-Metriken mit den Failover-Triggers?"}
{"ts": "147:00", "speaker": "E", "text": "Genau. Wir nutzen die Nimbus API, um vor dem Failover ein Metrik-Snapshot zu ziehen und mit den Schwellenwerten aus RB-OBS-015 zu vergleichen. Erst wenn beides – Infrastruktur und Applikationszustand – OK ist, ziehen wir den Traffic um."}
{"ts": "147:15", "speaker": "I", "text": "Wie gehen Sie mit der Balance zwischen zu frühem und zu spätem Failover um? Das ist ja oft ein heikler Punkt."}
{"ts": "147:20", "speaker": "E", "text": "Wir haben hysteresis margins eingeführt – also eine zweistufige Bestätigung. Ein Service muss in zwei aufeinanderfolgenden Prüfzyklen fehlschlagen, bevor der Failover-Prozess startet. Laut Ticket INC-DR-445 hat uns das im April einen unnötigen Region-Switch erspart."}
{"ts": "147:38", "speaker": "I", "text": "Gab es Überlegungen, diese Margins dynamisch anzupassen? Beispielsweise abhängig von Tageszeit oder Lastprofil?"}
{"ts": "147:43", "speaker": "E", "text": "Ja, das ist Teil des RFC-DR-2025-05. Da wollen wir die Werte aus dem Poseidon Networking nutzen, um bei hoher Backbone-Latenz schneller zu reagieren, während wir bei niedriger Last konservativer agieren."}
{"ts": "147:56", "speaker": "I", "text": "Klingt durchdacht. Welche Verbesserungen wollen Sie im nächsten Drill-Zyklus abgesehen davon noch implementieren?"}
{"ts": "148:00", "speaker": "E", "text": "Wir wollen das Chaos Testing ausweiten. Bisher haben wir nur DB-Cluster-Ausfälle simuliert. Im nächsten Zyklus werden wir gezielt auch Netzwerk-Partitionen zwischen Regionen erzeugen, um den BLAST_RADIUS bei Cross-Region Traffic zu bewerten."}
{"ts": "148:15", "speaker": "I", "text": "Und wie dokumentieren Sie diese neuen Testarten für spätere Audits?"}
{"ts": "148:20", "speaker": "E", "text": "Jeder Test bekommt einen Eintrag in unserem DR-Testregister. Wir verlinken die Testbeschreibung, die Observability-Daten, und das Postmortem-Dokument. Für Audit-Zwecke legen wir zudem ein Sign-Off-Protokoll nach Vorlage AUD-DR-02 bei."}
{"ts": "148:00", "speaker": "I", "text": "Lassen Sie uns noch einmal auf die Integrationsaspekte eingehen – konkret, wie Titan DR mit anderen Plattformprojekten zusammenspielt. Können Sie das schildern?"}
{"ts": "148:10", "speaker": "E", "text": "Ja, gern. Titan DR greift direkt auf die Metriken aus Nimbus Observability zu, um Health-Checks der Region-Cluster zu validieren. Über die Alert-Pipeline wird dann automatisch das Runbook RB-DR-001 getriggert, wenn z. B. ein SLA-Breach droht."}
{"ts": "148:23", "speaker": "I", "text": "Das heißt, es gibt eine Verknüpfung zwischen Observability Alarmen und den DR-Prozessen?"}
{"ts": "148:29", "speaker": "E", "text": "Genau. Wir haben in der Drill-Phase einen Middleware-Service integriert, der aus dem Nimbus-Event-Stream nur die Severity 'critical' Events filtert. Diese lösen über unser Control Plane API ein Failover-Skript aus, das im Runbook unter Abschnitt 3.2 dokumentiert ist."}
{"ts": "148:45", "speaker": "I", "text": "Gab es bei der Integration technische Stolpersteine?"}
{"ts": "148:50", "speaker": "E", "text": "Ja, wir mussten die Latenz zwischen Event-Erkennung und Failover-Signalierung unter zwei Sekunden bringen. Das war nur möglich, nachdem wir im Poseidon Networking Projekt das interne Routing optimiert hatten – Multi-Hop-Pfade wurden reduziert."}
{"ts": "149:05", "speaker": "I", "text": "Interessant, das ist also ein direkter Nutzen aus Poseidon?"}
{"ts": "149:10", "speaker": "E", "text": "Ja, absolut. Poseidon hat uns eine Low-Latency-Mesh-Struktur zwischen den Rechenzentren gegeben. Das wiederum verbessert den RTO, der bei Titan DR auf maximal 90 Sekunden festgelegt ist."}
{"ts": "149:23", "speaker": "I", "text": "Wie stellen Sie sicher, dass diese RTO-Ziele auch im Ernstfall eingehalten werden?"}
{"ts": "149:30", "speaker": "E", "text": "Wir fahren monatliche Mini-Drills, bei denen wir absichtlich eine Region isolieren und die Recovery-Zeit messen. Die Ergebnisse landen im Ticket-System unter z. B. QA-DR-2025-07, und werden gegen die SLA-Vorgaben geprüft."}
{"ts": "149:45", "speaker": "I", "text": "Gibt es auch einen Abgleich mit Lessons Learned aus Helios Datalake?"}
{"ts": "149:51", "speaker": "E", "text": "Ja, aus Helios wissen wir, dass Datenreplikations-Lags oft unterschätzt werden. Deshalb haben wir im DR-Design asynchrone Replikation nur dort zugelassen, wo der RPO größer als 60 Sekunden tolerierbar ist."}
{"ts": "150:05", "speaker": "I", "text": "Kommen wir zum Abschluss auf die nächsten Schritte. Was ist für den kommenden Drill-Zyklus geplant?"}
{"ts": "150:12", "speaker": "E", "text": "Wir wollen die aktive Standby-Konfiguration in einer Region testen, um zu prüfen, ob wir die Failover-Zeit nochmals halbieren können. Dazu wird es ein RFC geben – RFC-DR-2025-08 – mit Anpassungen in den Prozeduren."}
{"ts": "150:25", "speaker": "I", "text": "Sehen Sie dabei besondere Risiken?"}
{"ts": "150:30", "speaker": "E", "text": "Ja, vor allem Kostenexplosion durch dauerhaft laufende Ressourcen und potenzielle Inkonsistenzen bei gleichzeitigen Write-Operationen. Wir mitigieren das mit Quorum-Mechanismen und enger Kostenkontrolle, dokumentiert im Audit-Log DR-AUD-2025-Q3."}
{"ts": "152:00", "speaker": "I", "text": "Wir hatten gerade über die Dokumentation der Tradeoffs gesprochen. Mich würde interessieren, welche nächsten Prioritäten Sie für Titan DR sehen."}
{"ts": "152:05", "speaker": "E", "text": "Die Top-Priorität ist aktuell die Optimierung der Failover-Zeiten zwischen den Regionen Nord und West. Wir haben zwar im Drill RTO unter sechs Minuten erreicht, aber unser SLO liegt bei fünf. Das heißt, wir müssen laut Runbook RB-DR-001 Schritt 4 und 5 beschleunigen."}
{"ts": "152:12", "speaker": "I", "text": "Welche konkreten Maßnahmen planen Sie dazu?"}
{"ts": "152:17", "speaker": "E", "text": "Wir wollen die Replikations-Queues im Storage-Layer vorwärmen und das Netzwerk-Routing automatisieren, sodass der BGP-Failover ohne manuellen Approval-Delay greift. Ein RFC-Entwurf (RFC-DR-2025-07) ist schon in Review."}
{"ts": "152:24", "speaker": "I", "text": "Und wie sieht es mit Verbesserungen im nächsten Drill-Zyklus aus?"}
{"ts": "152:29", "speaker": "E", "text": "Im nächsten Zyklus wollen wir auch Szenarien testen, bei denen gleich zwei Subsysteme ausfallen, z. B. ComputeCluster und MessagingBus gleichzeitig. Das war eine Schwachstelle im letzten GameDay laut Ticket INC-DR-882."}
{"ts": "152:36", "speaker": "I", "text": "Gab es dazu schon geplante Anpassungen in den Prozeduren?"}
{"ts": "152:42", "speaker": "E", "text": "Ja, wir ergänzen in RB-DR-002 eine Parallelisierung von Wiederanlauf-Schritten, inspiriert von Lessons Learned aus Poseidon Networking, wo wir ähnliche Dependency-Chains verkürzt haben."}
{"ts": "152:50", "speaker": "I", "text": "Könnten Sie das ein wenig ausführen, wie Poseidon da Einfluss hatte?"}
{"ts": "152:55", "speaker": "E", "text": "Klar, bei Poseidon haben wir ein automatisiertes Dependency-Mapping eingeführt. Dadurch konnten wir erkennen, dass bestimmte Netzwerkservices unabhängig wieder hochgefahren werden können. Diese Erkenntnis übertragen wir jetzt ins Titan DR Playbook."}
{"ts": "153:02", "speaker": "I", "text": "Wie dokumentieren Sie diese Änderungen für künftige Audits?"}
{"ts": "153:07", "speaker": "E", "text": "Wir pflegen jede Änderung sowohl im Confluence-DR-Space mit Versionierung als auch im Git-Repo für Runbooks. Jede Anpassung bekommt einen Verweis auf das auslösende Incident- oder Testticket, z. B. [REF:INC-DR-882]."}
{"ts": "153:14", "speaker": "I", "text": "Gibt es dabei auch Herausforderungen bei der Nachvollziehbarkeit?"}
{"ts": "153:19", "speaker": "E", "text": "Ja, vor allem wenn mehrere Teams parallel Änderungen einpflegen. Wir haben daher einen Merge-Review-Workflow etabliert, der zwingend von mindestens einem DR-Engineer und einem Observability-Spezialisten freigegeben werden muss."}
{"ts": "153:26", "speaker": "I", "text": "Zum Abschluss: Wo sehen Sie mittelfristig die größten Chancen, das Projekt noch resilienter zu machen?"}
{"ts": "153:31", "speaker": "E", "text": "Mittelfristig wollen wir ein aktives-active Setup in allen Kernregionen etablieren. Das erhöht zwar die Kosten, reduziert aber den BLAST_RADIUS deutlich. Wir planen dazu ein Proof of Concept mit nur zwei Services, um die Machbarkeit und SLA-Einhaltung zu messen."}
{"ts": "153:36", "speaker": "I", "text": "Gut, dann würde mich jetzt noch interessieren, welche konkreten Schritte Sie nach diesen Erkenntnissen eingeleitet haben, um die identifizierten Risiken zu adressieren."}
{"ts": "153:41", "speaker": "E", "text": "Wir haben direkt nach dem GameDay ein RFC, konkret RFC-DR-2025-07, aufgesetzt, um die Failover-Sequenz in Runbook RB-DR-001 zu ändern. Das Ziel war, den Traffic-Switch in zwei Phasen zu splitten, um die Lastspitzen besser abzufedern."}
{"ts": "153:48", "speaker": "I", "text": "Das klingt nach einem eher vorsichtigen Vorgehen. Welche Vorteile hat diese zweiphasige Umschaltung?"}
{"ts": "153:54", "speaker": "E", "text": "Ja, genau, vorsichtig, um den Blast Radius weiter zu limitieren. In Phase 1 routen wir nur 30 % des Traffics auf die Zielregion, beobachten dann über unser Nimbus Observability Dashboard die KPIs wie Latenz und Error-Rate, und erst wenn die stabil sind, gehen wir in Phase 2 mit den restlichen 70 %."}
{"ts": "154:03", "speaker": "I", "text": "Gab es dazu besondere Abstimmungen mit dem Networking-Team, gerade wegen der Routing-Policies?"}
{"ts": "154:07", "speaker": "E", "text": "Ja, das Poseidon Networking Team war früh involviert. Wir mussten die BGP-Failover-Policies in den Edge Routern anpassen, um asymmetrisches Routing zu vermeiden. Das war eine Lehre aus einem älteren Incident, TCK-INC-4412, bei dem genau so etwas den RTO überschritten hat."}
{"ts": "154:15", "speaker": "I", "text": "Können Sie kurz schildern, wie Sie diese Änderungen getestet haben, bevor sie in Produktion gingen?"}
{"ts": "154:20", "speaker": "E", "text": "Wir haben im Staging eine simulierte Region-Outage erzeugt und die zweiphasige Umschaltung durchgespielt. Parallel liefen Synthetic Transactions, und wir haben mit dem Observability-Team die Alarme aus RB-OBS-005 gegen die erwarteten Werte korreliert."}
{"ts": "154:28", "speaker": "I", "text": "Und wie haben sich die Kosten durch diese Anpassung verändert?"}
{"ts": "154:32", "speaker": "E", "text": "Minimal höher, weil die Standby-Ressourcen in der Zielregion früher hochgefahren werden müssen. Aber das bleibt im Budgetrahmen des Cost Center CC-DR-2025, vor allem, weil wir in anderen Bereichen durch Reserved Instances sparen konnten."}
{"ts": "154:40", "speaker": "I", "text": "Gab es auch Diskussionen, diese Anpassung wieder zu verwerfen, falls die Performancegewinne nicht groß genug sind?"}
{"ts": "154:45", "speaker": "E", "text": "Ja, das wurde im Architecture Review AR-2025-03 kurz erwogen. Aber die Metriken aus dem Drill zeigen 18 % weniger Fehler während des Failovers, das war ein klares Signal, beizubehalten."}
{"ts": "154:53", "speaker": "I", "text": "Wie stellen Sie sicher, dass diese Lessons Learned auch in zukünftigen Projekten genutzt werden?"}
{"ts": "154:57", "speaker": "E", "text": "Wir pflegen ein internes Confluence-Wiki, in dem jede Änderung an Runbooks und die zugehörigen Evidenzen verlinkt werden, inkl. Screenshots aus den Drill-Dashboards und Verweise auf Tickets wie TCK-DR-2025-112."}
{"ts": "155:05", "speaker": "I", "text": "Und abschließend: Welche kurzfristigen Prioritäten stehen jetzt bis zum nächsten Drill an?"}
{"ts": "155:09", "speaker": "E", "text": "Wir wollen die automatisierte Validierung der Failover-Reihenfolge implementieren, damit menschliche Fehler ausgeschlossen werden. Außerdem planen wir, im nächsten Drill auch einen gleichzeitigen Ausfall von zwei Non-Critical Services zu simulieren, um die Resilienz wirklich zu stressen."}
{"ts": "155:06", "speaker": "I", "text": "Lassen Sie uns noch etwas tiefer auf die Lessons Learned aus dem letzten Drill eingehen. Gab es aus Ihrer Sicht einen Aspekt, der besonders überraschend war?"}
{"ts": "155:17", "speaker": "E", "text": "Ja, tatsächlich. Beim TEST-DR-2025-Q1 hatten wir angenommen, dass die Latenz zwischen Region West und Region Nord stabil unter 50ms bleibt. In der Peak-Phase sind wir aber auf 85ms hochgeschossen, und das hat einige unserer Replikationsjobs verzögert."}
{"ts": "155:38", "speaker": "I", "text": "Wie sind Sie mit dieser Verzögerung umgegangen?"}
{"ts": "155:44", "speaker": "E", "text": "Wir haben ad hoc den Runbook-Abschnitt RB-DR-001-§4.2 angewendet, der vorsieht, temporär auf inkrementelle Snapshots statt kontinuierlicher Stream-Replikation umzustellen. Das reduziert die Netzlast und stabilisiert die Latenz."}
{"ts": "156:05", "speaker": "I", "text": "Und das hat gereicht, um die SLA-Vorgaben einzuhalten?"}
{"ts": "156:11", "speaker": "E", "text": "Gerade so. Unser SLO für RPO liegt bei 15 Minuten, wir hatten in diesem Szenario 13 Minuten. Wir haben danach ein RFC-Update (RFC-DR-2025-07) initiiert, um die Failover-Mechanik proaktiv um diesen Fallback zu erweitern."}
{"ts": "156:34", "speaker": "I", "text": "Gab es andere Teams, die in die Umsetzung des RFC eingebunden waren?"}
{"ts": "156:40", "speaker": "E", "text": "Ja, sowohl das Poseidon Networking Team zur Optimierung der Interconnect-Routen als auch Nimbus Observability, damit wir künftig solche Latenzspitzen früher als Alarm sehen. Das war eine direkte Cross-Projekt-Kollaboration."}
{"ts": "157:02", "speaker": "I", "text": "Das klingt nach einem guten Beispiel für die Verzahnung verschiedener Subsysteme."}
{"ts": "157:08", "speaker": "E", "text": "Absolut. Wir haben sogar ein gemeinsames Dashboard in Nimbus erstellt, das Metriken aus Helios Datalake zieht, um historische Latenzverläufe mit den Live-Werten zu korrelieren. So können wir Ursachen wie saisonale Lastspitzen besser vorhersagen."}
{"ts": "157:31", "speaker": "I", "text": "Welche Risiken sehen Sie noch für den nächsten Drill-Zyklus?"}
{"ts": "157:37", "speaker": "E", "text": "Ein Risiko ist, dass wir bei einem gleichzeitigen Ausfall von zwei Regionen in die passive Kapazität der dritten Region geraten. Das würde unsere Performance spürbar drücken. Wir müssen entscheiden, ob wir die Kostenschraube lockern und mehr Warm-Standby-Ressourcen provisionieren."}
{"ts": "158:00", "speaker": "I", "text": "Wie dokumentieren Sie solche Abwägungen für spätere Audits?"}
{"ts": "158:06", "speaker": "E", "text": "Wir nutzen unser internes Decision-Log-System. Jeder Tradeoff wird dort mit Ticket-IDs, z. B. CHG-DR-882 und den zugehörigen Runbook-Referenzen hinterlegt. Zusätzlich hängen wir die Metriken aus dem Drill als CSV an, damit Auditoren die Zahlenbasis sehen."}
{"ts": "158:26", "speaker": "I", "text": "Welche Prioritäten setzen Sie jetzt kurzfristig?"}
{"ts": "158:31", "speaker": "E", "text": "Kurzfristig wollen wir das RFC-DR-2025-07 ausrollen, die Latenzalarme in Nimbus schärfen und mit Helios Analytics ein Predictive-Modell trainieren. Damit können wir im nächsten Drill proaktiv auf Anomalien reagieren, bevor sie SLA-kritisch werden."}
{"ts": "160:06", "speaker": "I", "text": "Können Sie uns bitte noch einmal durch den Integrationspfad zwischen Titan DR und dem Nimbus Observability Projekt führen?"}
{"ts": "160:12", "speaker": "E", "text": "Ja, klar. Wir haben im mittleren Drillzyklus eine direkte API-Verbindung etabliert, sodass Alarme aus Nimbus instantan in unseren RB-DR-001 Ablauf einspeisen. Das heißt, wenn Nimbus einen KPI-Breach erkennt, wird unser Step 'Failover-Prep' automatisch angestoßen."}
{"ts": "160:23", "speaker": "I", "text": "Und wie genau wird dieser Trigger in der Multi-Region-Architektur verarbeitet?"}
{"ts": "160:30", "speaker": "E", "text": "Der Trigger läuft über unseren Event-Bus, der in beiden Regionen gespiegelt ist. Im aktiven Rechenzentrum aktiviert er ein Lambda-ähnliches Modul, das die RTO/RPO Parameter aus dem SLA-Dokument DR-SLA-2025-01 liest und die Sequenz einleitet."}
{"ts": "160:45", "speaker": "I", "text": "Gab es da besondere Lessons Learned aus vorherigen Projekten wie Helios Datalake oder Poseidon Networking?"}
{"ts": "160:52", "speaker": "E", "text": "Absolut. Aus Helios haben wir die Segmentierung der Datenströme übernommen, um den BLAST_RADIUS zu verkleinern. Aus Poseidon stammt unser BGP-Failover-Ansatz, der in RB-DR-002 dokumentiert ist."}
{"ts": "161:05", "speaker": "I", "text": "Wie balancieren Sie diese zusätzlichen Integrationsschritte mit den Performancezielen?"}
{"ts": "161:12", "speaker": "E", "text": "Wir setzen asynchrone Verarbeitung ein, um Latenzen zu minimieren. Parallel wird ein 'warm standby' in der Sekundärregion betrieben, was zwar etwa 15% Mehrkosten verursacht, aber das Failover auf unter 90 Sekunden bringt."}
{"ts": "161:27", "speaker": "I", "text": "Haben Sie im letzten GameDay konkrete Probleme bei diesen Schnittstellen festgestellt?"}
{"ts": "161:33", "speaker": "E", "text": "Ja, im TEST-DR-2025-Q1 hatten wir einen Edge-Case, wo Nimbus einen Alarm nicht korrekt weitergab, weil der Auth-Token abgelaufen war. Das haben wir in Ticket DR-BUG-457 erfasst und mit einem Auto-Renew-Mechanismus gefixt."}
{"ts": "161:48", "speaker": "I", "text": "Wie dokumentieren Sie solche Fixes für das Audit?"}
{"ts": "161:54", "speaker": "E", "text": "Wir hängen das Ticket an das Change-Log im Confluence-DR-Space an, verlinken es auf RB-DR-001 und notieren die Änderung in unserem RFC, hier war's RFC-DR-2025-04."}
{"ts": "162:06", "speaker": "I", "text": "Gibt es für den nächsten Drill-Zyklus geplante Änderungen an den Failover-Prozeduren?"}
{"ts": "162:12", "speaker": "E", "text": "Ja, wir wollen den manuellen Approval-Step optional machen, wenn die Confidence-Scores aus Nimbus über 95% liegen. Das reduziert weitere 20 Sekunden von der RTO."}
{"ts": "162:23", "speaker": "I", "text": "Welche Risiken sehen Sie bei dieser Automatisierung?"}
{"ts": "162:29", "speaker": "E", "text": "Das Hauptrisiko ist ein False Positive bei den Alarms. Deshalb implementieren wir einen Cross-Check mit zwei unabhängigen Metrik-Quellen bevor der auto-approve greift, wie in unserem Risk Log Eintrag RL-DR-2025-09 beschrieben."}
{"ts": "161:30", "speaker": "I", "text": "Können Sie auf Basis dessen noch einmal kurz schildern, wie die nächsten konkreten Schritte im Titan DR Projekt aussehen?"}
{"ts": "161:36", "speaker": "E", "text": "Ja, also direkt im Anschluss an diesen Drill wollen wir zunächst die Findings aus den Logs in Confluence dokumentieren, speziell in der Seite 'Titan DR Drill Q1 2025'. Danach folgt ein RFC, um das Failover-Skript in RB-DR-002 zu erweitern."}
{"ts": "161:46", "speaker": "I", "text": "Geht es bei diesem RFC um Performance oder mehr um Prozesseffizienz?"}
{"ts": "161:50", "speaker": "E", "text": "Primär um Prozesseffizienz. Wir haben gesehen, dass der Schritt 'DNS Cutover' im Runbook manuell länger dauert als unsere RTO-Vorgabe erlaubt. Wir wollen diesen Schritt automatisieren und die Validierung parallelisieren."}
{"ts": "161:59", "speaker": "I", "text": "Verändert das auch die Integrationspunkte mit Nimbus Observability?"}
{"ts": "162:03", "speaker": "E", "text": "Ja, leicht. Wir müssen den Health-Check-Webhook von Nimbus so anpassen, dass er den Status des automatisierten Cutovers erkennt und nicht fälschlich einen Incident auslöst. Das ist im Ticket OBS-INT-942 festgehalten."}
{"ts": "162:12", "speaker": "I", "text": "Wie stellen Sie sicher, dass solche Anpassungen nicht neue Risiken einführen?"}
{"ts": "162:16", "speaker": "E", "text": "Wir fahren dafür eine Sandbox-Simulation, bei der Nimbus-Alerts in einem isolierten Namespace getestet werden. Außerdem schreiben wir die Tests in unser CI/CD, sodass jede Änderung an RB-DR-002 automatisch gegen diese Checks läuft."}
{"ts": "162:25", "speaker": "I", "text": "Sie hatten vorhin die Lessons Learned aus Helios Datalake erwähnt. Fließt davon auch etwas hier ein?"}
{"ts": "162:29", "speaker": "E", "text": "Ja, aus Helios haben wir gelernt, dass Cross-Region-Datenreplikation nur dann sauber funktioniert, wenn die Netzwerk-Latenz in den Observability-Alerts berücksichtigt wird. Deshalb binden wir jetzt Latenzmetriken direkt in die Failover-Entscheidung ein."}
{"ts": "162:38", "speaker": "I", "text": "Gibt es für diese Metriken spezielle Schwellenwerte?"}
{"ts": "162:42", "speaker": "E", "text": "Ja, im SLA-Dokument SLA-DR-2025 Abschnitt 4.2 ist definiert: Wenn die Latenz zwischen Region A und B über 250 ms für mehr als 3 Minuten liegt, wird ein automatischer Switch eingeleitet, sofern auch die Error-Rate über 1% steigt."}
{"ts": "162:51", "speaker": "I", "text": "Und wie dokumentieren Sie diese automatischen Switches für Audits?"}
{"ts": "162:55", "speaker": "E", "text": "Jeder Switch generiert einen Eintrag in unserem DR-Event-Log, das revisionssicher in der Audit-Datenbank liegt. Dort verknüpfen wir die Event-ID mit dem zugehörigen Test oder Incident, z.B. TEST-DR-2025-Q2-07."}
{"ts": "163:04", "speaker": "I", "text": "Sehen Sie in dieser Automatisierung auch potenzielle Nachteile?"}
{"ts": "163:08", "speaker": "E", "text": "Ja, das Risiko von 'false positives' steigt. Deshalb definieren wir im RFC, dass mindestens zwei unabhängige Metriken den Schwellenwert überschreiten müssen, bevor der Switch ausgelöst wird. Das reduziert unnötige Ausfälle und wahrt die Balance zwischen Safety First und Servicekontinuität."}
{"ts": "162:60", "speaker": "I", "text": "Sie hatten vorhin erwähnt, dass Sie nach dem letzten Drill eine Anpassung der Failover-Prozeduren planen. Können Sie das bitte etwas genauer ausführen?"}
{"ts": "163:05", "speaker": "E", "text": "Ja, klar. Wir haben im Drill gesehen, dass die Runbook-Sequenz in RB-DR-001 bei Schritt 7 einen unnötigen manuellen Check enthält, der die RTO um etwa 3 Minuten verlängert hat. Wir wollen diesen Schritt durch ein automatisiertes Health-Check-Script ersetzen, das bereits in unserem internen Git-Repo als Branch 'feat-auto-health' vorliegt."}
{"ts": "163:20", "speaker": "I", "text": "Das klingt nach einer klaren Verbesserung. Haben Sie schon geprüft, ob das Script mit den bestehenden Observability-Hooks aus Nimbus kompatibel ist?"}
{"ts": "163:34", "speaker": "E", "text": "Ja, wir haben einen Integrationstest mit dem Nimbus Alerting Service gemacht. Das Script feuert ein Event vom Typ 'DR_HEALTH_OK' oder 'DR_HEALTH_FAIL', das in der Alert-Pipeline von Nimbus direkt in die Drill-Dashboards einfließt. Die Anpassung erfordert nur ein Minor-Update der Alert-Rules in ConfigMap 'nimbus-drill-rules'."}
{"ts": "163:51", "speaker": "I", "text": "Wie sieht es mit dem Change-Management aus – planen Sie eine RFC-Erstellung?"}
{"ts": "164:03", "speaker": "E", "text": "Genau, wir bereiten gerade RFC-DR-2025-02 vor. Darin dokumentieren wir die Automatisierung, die Testfälle aus dem Drill, sowie die Anpassungen an den Observability-Hooks. Der RFC wird auch die Lessons Learned referenzieren, die wir aus dem Incident INC-DR-2024-19 gezogen haben."}
{"ts": "164:20", "speaker": "I", "text": "Gibt es besondere Risiken, die Sie bei dieser Automatisierung sehen?"}
{"ts": "164:34", "speaker": "E", "text": "Ein kleines, aber relevantes Risiko ist ein False Positive im Health-Check. Wenn der Check fehlschlägt, obwohl der Standby-Cluster gesund ist, könnten wir unnötig failovern. Das mitigieren wir durch eine doppelte Verifikation: Das Script prüft sowohl interne Metriken als auch externe Pingdom-ähnliche Checks innerhalb von 15 Sekunden."}
{"ts": "164:55", "speaker": "I", "text": "Sie hatten vorhin auch von der Minimierung des BLAST_RADIUS gesprochen. Wird dieser durch die neue Automatisierung beeinflusst?"}
{"ts": "165:10", "speaker": "E", "text": "Positiv, ja. Weil wir schneller und präziser erkennen, ob ein Failover nötig ist, vermeiden wir großflächige Umschaltungen. Das schützt Teilregionen, die nicht betroffen sind, und reduziert den BLAST_RADIUS auf im Mittel 1/3 der vorherigen Ausdehnung. Das ist auch im KPI-Dashboard 'DR-Containment' dokumentiert."}
{"ts": "165:28", "speaker": "I", "text": "Wie fassen Sie diese Verbesserungen für die Audit-Trails zusammen?"}
{"ts": "165:40", "speaker": "E", "text": "Wir pflegen im Confluence-Space 'Titan DR' einen Abschnitt 'Change Log'. Dort verlinken wir RFCs, Testprotokolle, Screenshots der Metriken vor/nach der Änderung, und die Genehmigungstickets aus unserem ITSM-System, in diesem Fall TCK-DR-2025-44."}
{"ts": "165:58", "speaker": "I", "text": "Gibt es schon einen Zeitplan, wann der nächste Drill mit diesen Änderungen durchgeführt wird?"}
{"ts": "166:10", "speaker": "E", "text": "Ja, wir haben für Q3 2025 einen Mini-Drill angesetzt, speziell um die Automatisierung zu testen. Das ist Teil unseres 'Continuous DR Improvement'-Programms, das nach jedem größeren Change einen fokussierten Test vorsieht."}
{"ts": "166:26", "speaker": "I", "text": "Wie schätzen Sie die Erfolgschancen dieser Änderung ein, basierend auf Ihren bisherigen Erfahrungen?"}
{"ts": "166:40", "speaker": "E", "text": "Sehr hoch. Die Kombination aus Lessons Learned aus Helios Datalake, wo wir ähnliche Health-Checks in den ETL-Pipelines eingeführt haben, und der engen Verzahnung mit Nimbus gibt uns eine solide Basis. Auch die Akzeptanz im DR-Team ist hoch, weil sie im Drill die Effizienzgewinne direkt gesehen haben."}
{"ts": "164:00", "speaker": "I", "text": "Bevor wir zum Abschluss kommen, würde mich noch interessieren: welche konkreten Prioritäten setzen Sie jetzt für die nächsten zwei Sprints im Titan DR?"}
{"ts": "164:05", "speaker": "E", "text": "Wir fokussieren uns in Sprint 27 und 28 auf die Optimierung der automatischen Region-Promotion. Das bedeutet, dass wir im Runbook RB-DR-004 die manuellen Steps reduzieren wollen, um die Failover-Zeit um ca. 15 % zu senken."}
{"ts": "164:15", "speaker": "I", "text": "Heißt das, es gibt auch Anpassungen an den bestehenden SLAs oder bleiben die unverändert?"}
{"ts": "164:19", "speaker": "E", "text": "Die SLAs bleiben formal unverändert – 99,95 % Availability –, aber wir wollen die internen SLOs für RTO von 12 auf 10 Minuten anpassen. Das dokumentieren wir dann im SLA-Register und verknüpfen es mit Ticket OPS-DR-223."}
{"ts": "164:32", "speaker": "I", "text": "Und wie gehen Sie mit den Lessons Learned aus dem letzten Drill in Bezug auf Observability um?"}
{"ts": "164:37", "speaker": "E", "text": "Da hatten wir beim TEST-DR-2025-Q1 ein Thema mit verzögerten Alarmen im Nimbus Observability Stack. Wir haben daraufhin die Alert-Latenz in unserem Prometheus-Cluster analysiert und die Alertmanager-Konfiguration entsprechend in RFC-DR-056 angepasst."}
{"ts": "164:50", "speaker": "I", "text": "Können Sie kurz beschreiben, wie Sie die Änderung getestet haben?"}
{"ts": "164:54", "speaker": "E", "text": "Wir haben einen simulierten Region-Ausfall in der Sandbox-Umgebung ausgelöst und mittels Synthetic Checks die Alarmierungskette gemessen. Ergebnis: von Ausfall bis Incident-Flag jetzt im Schnitt 18 Sekunden schneller."}
{"ts": "165:05", "speaker": "I", "text": "Welche Risiken sehen Sie noch vor dem nächsten GameDay?"}
{"ts": "165:09", "speaker": "E", "text": "Ein Restrisiko bleibt bei der Netzwerksegmentierung zwischen den Regionen. Wenn das Poseidon Networking Layer asynchron driftet, kann das zu inkonsistenten Routen führen. Wir haben ein Monitoring-Skript in RB-NET-019 ergänzt, um das früh zu erkennen."}
{"ts": "165:22", "speaker": "I", "text": "Gibt es Überlegungen, die Failover-Prozeduren formell via RFC anzupassen?"}
{"ts": "165:26", "speaker": "E", "text": "Ja, RFC-DR-060 ist bereits in Draft. Dort wollen wir u. a. den Switch von passivem zu teilweise aktivem Standby in der APAC-Region beschreiben, um die Cold-Start-Zeiten zu verringern."}
{"ts": "165:38", "speaker": "I", "text": "Heißt das nicht gleichzeitig höhere Kosten?"}
{"ts": "165:41", "speaker": "E", "text": "Ja, wir rechnen mit +8 % OPEX, aber die Risikominimierung ist es wert. Wir dokumentieren die Kostensteigerung mit einem klaren Business Case im Audit-Wiki, inkl. Verweis auf das Incident-Postmortem PM-DR-019."}
{"ts": "165:53", "speaker": "I", "text": "Zum Abschluss: wie stellen Sie sicher, dass all diese Änderungen im nächsten Drill evaluiert werden?"}
{"ts": "165:57", "speaker": "E", "text": "Wir bauen die Szenarien direkt in das GameDay-Skript GD-DR-2025-Q3 ein, versehen sie mit Metrik-Checkpoints und binden das an unseren Confluence-basierten Drill-Report an, damit die Auditspur lückenlos bleibt."}
{"ts": "166:00", "speaker": "I", "text": "Bevor wir zu den nächsten Schritten kommen, möchte ich noch einmal kurz auf die Lessons Learned aus dem letzten Drill eingehen. Gab es dabei überraschende Erkenntnisse im Bereich der Netzwerk-Latenzen zwischen den Regionen?"}
{"ts": "166:20", "speaker": "E", "text": "Ja, wir haben im Drill festgestellt, dass die Latenz zwischen Region EU-Central und AP-Southeast unter Last höher war als in unseren synthetischen Tests. Das hat uns gezwungen, die Parameter in Runbook RB-DR-001, Abschnitt 4.2, zur automatischen Traffic-Shift-Geschwindigkeit anzupassen."}
{"ts": "166:45", "speaker": "I", "text": "Wie wirkt sich das konkret auf die Einhaltung der definierten RTOs aus?"}
{"ts": "167:00", "speaker": "E", "text": "Unsere RTO von 45 Minuten konnten wir trotzdem halten, aber nur, weil wir in Echtzeit die Failover-Pipeline in der Orchestrierung umgestellt haben. Die Anpassung der Traffic-Shift-Rate hat eine Überlastung der AP-Southeast-Knoten verhindert, was im Drill eine kritische Rolle spielte."}
{"ts": "167:25", "speaker": "I", "text": "Gab es bei dieser Umstellung Koordinationsbedarf mit anderen Projekten, etwa Nimbus Observability?"}
{"ts": "167:40", "speaker": "E", "text": "Absolut. Das Alerting-Modul von Nimbus musste temporär auf einen erweiterten Schwellenwert gesetzt werden, um das Rauschen durch die Latenzspitzen herauszufiltern. Wir haben dazu eine Schnellfreigabe über Ticket OBS-NTF-882 erstellt und gemäß der vereinbarten Eskalationsmatrix freigegeben."}
{"ts": "168:05", "speaker": "I", "text": "Das klingt nach einer reibungslosen Ad-hoc-Kooperation. Haben Sie diese Anpassung schon in die Standardprozeduren aufgenommen?"}
{"ts": "168:20", "speaker": "E", "text": "Ja, wir haben einen Nachtrag zum Runbook RB-DR-001 als Revision C eingereicht. Der RFC-Entwurf RFC-DR-2025-04 liegt aktuell im Review des Architekturboards. Darin ist die neue Koordination mit Observability als Pflichtschritt im Failover-Play definiert."}
{"ts": "168:50", "speaker": "I", "text": "Wie gehen Sie mit den zusätzlichen Kosten für die temporäre Skalierung um, die im Drill notwendig waren?"}
{"ts": "169:05", "speaker": "E", "text": "Wir haben in der Kostenanalyse Q1/2025 berechnet, dass die temporäre Skalierung um 20 % günstiger war als ein dauerhafter Ausbau. Deshalb wird diese Maßnahme nur Drill- oder Incident-basiert ausgelöst, nicht im Dauerbetrieb."}
{"ts": "169:30", "speaker": "I", "text": "Gab es im GameDay-Protokoll noch weitere Punkte, die in die Architektur zurückgespiegelt wurden?"}
{"ts": "169:45", "speaker": "E", "text": "Ja, unter anderem eine Empfehlung, die Health-Checks zwischen den Loadbalancern interregionale Timeouts um 500 ms zu verlängern. Das war Ticket DR-NET-443, das wir nach Abstimmung mit Poseidon Networking umgesetzt haben, um unnötige Rebalancing-Events zu vermeiden."}
{"ts": "170:15", "speaker": "I", "text": "Wie stellen Sie sicher, dass solche Änderungen revisionssicher dokumentiert werden?"}
{"ts": "170:30", "speaker": "E", "text": "Wir nutzen das interne Audit-Repository mit signierten Merge-Requests. Jeder Merge erzeugt automatisch einen Audit-Trail-Eintrag mit Referenz zu Runbooks und Tickets. Bei DR-relevanten Änderungen ist zusätzlich die Freigabe durch den Sicherheitsbeauftragten erforderlich."}
{"ts": "170:55", "speaker": "I", "text": "Das klingt sehr stringent. Abschließend: Was sind für Sie die Top-Prioritäten im nächsten Drill-Zyklus?"}
{"ts": "171:10", "speaker": "E", "text": "Wir wollen die Failback-Prozedur beschleunigen und die Cross-Region-Datenbank-Replikation von asynchron auf semi-synchron umstellen, um das RPO auf unter 90 Sekunden zu senken. Außerdem planen wir ein Chaos-Engineering-Experiment, um die Robustheit gegen partielle Netzwerkpartitionen zu verifizieren."}
{"ts": "174:00", "speaker": "I", "text": "Bevor wir ins Detail gehen, können Sie bitte noch einmal den aktuellen Stand des Titan DR Drills kurz zusammenfassen?"}
{"ts": "174:10", "speaker": "E", "text": "Ja, also wir sind derzeit in der dritten Drill-Woche, Fokus liegt auf den Failover-Skripten für die Region Oslo. Laut Runbook RB-DR-001 haben wir jetzt erstmals die automatisierte DNS-Umschaltung getestet."}
{"ts": "174:26", "speaker": "I", "text": "Und das hat soweit wie geplant funktioniert?"}
{"ts": "174:30", "speaker": "E", "text": "Größtenteils ja, aber wir hatten eine Latenz von knapp 14 Sekunden beim Propagieren, was für unser SLO von 10 Sekunden knapp drüber ist. Wir prüfen gerade, ob es am internen Resolver-Limit liegt."}
{"ts": "174:50", "speaker": "I", "text": "Sie hatten vorhin schon einmal Nimbus Observability erwähnt – wie greifen hier die Integrationen mit Titan DR ineinander?"}
{"ts": "175:00", "speaker": "E", "text": "Nimbus liefert uns die Echtzeit-Metriken, und über die Alert-Bridge RB-OBS-005 triggern wir direkt die DR-Runbooks. Das heißt, wenn der HealthCheck in Region A mehrfach fehlschlägt, startet automatisch RB-DR-002 für die Zielregion."}
{"ts": "175:20", "speaker": "I", "text": "Gab es dabei besondere Herausforderungen?"}
{"ts": "175:24", "speaker": "E", "text": "Ja, die größte war die Synchronisierung der Metrics-Timestamps. Ohne UTC-Normalisierung hatten wir Phantom-Alerts. Das haben wir mit einem Patch aus dem Poseidon Networking Projekt behoben."}
{"ts": "175:44", "speaker": "I", "text": "Spannend, also ein direkter Know-how-Transfer zwischen Projekten."}
{"ts": "175:48", "speaker": "E", "text": "Genau, und das ist auch ein Ziel der Safety-First-Strategie: Lessons Learned nicht isoliert lassen. Helios Datalake hat uns z.B. beim schnellen Rehydrieren der Backups in neuer Region geholfen."}
{"ts": "176:06", "speaker": "I", "text": "Wenn Sie an die Kosten denken – wie haben Sie in diesem Drill den BLAST_RADIUS minimiert?"}
{"ts": "176:12", "speaker": "E", "text": "Wir haben nur die kritischen Services im aktiven Standby betrieben und den Rest kalt gehalten. Das spart laut Kalkulation TK-DR-2025-07 etwa 38% der Compute-Kosten, ohne die Kern-SLAs zu gefährden."}
{"ts": "176:32", "speaker": "I", "text": "Gab es Feedback aus den Audits dazu?"}
{"ts": "176:36", "speaker": "E", "text": "Ja, Audit-Report AR-DR-Q1 lobt die Evidenzlage, empfiehlt aber, die Entscheidungskriterien für aktives vs. passives Standby noch klarer im Confluence-DR-Space zu dokumentieren."}
{"ts": "176:54", "speaker": "I", "text": "Wie setzen Sie diese Empfehlung um?"}
{"ts": "177:00", "speaker": "E", "text": "Wir haben ein RFC DR-2025-14 aufgesetzt, das die Tradeoffs mit Messwerten aus den GameDays verknüpft. Ziel ist, bis zum nächsten Drill-Zyklus eine verbindliche Matrix zu haben, die Kosten, RTO und RPO gegeneinanderstellt."}
{"ts": "180:00", "speaker": "I", "text": "Bevor wir zum Abschluss kommen, könnten Sie noch kurz schildern, welche Schnittstellen Titan DR aktuell zum Nimbus Observability Projekt hat?"}
{"ts": "180:15", "speaker": "E", "text": "Ja, gerne. Wir haben eine direkte Integration der Failover-Trigger in die Nimbus Alert Engine. Das bedeutet, dass bei Region-Health-Events automatisch Runbook RB-DR-001 referenziert wird. Die Observability-Dashboards beinhalten jetzt auch die DR-spezifischen Metriken, um RTO und RPO in Echtzeit zu überwachen."}
{"ts": "180:38", "speaker": "I", "text": "Und welche Lessons Learned aus früheren Projekten wie Helios Datalake oder Poseidon Networking fließen hier konkret ein?"}
{"ts": "180:55", "speaker": "E", "text": "Aus Helios haben wir vor allem die Erkenntnis mitgenommen, dass Storage-Latenzen während eines Failovers massiv unterschätzt werden können. Poseidon hat uns gezeigt, wie wichtig eine saubere Segmentierung des Netzwerktraffics ist, um beim Failover keinen unnötigen BLAST_RADIUS zu erzeugen. Diese beiden Punkte sind fest in den Design-Guidelines von Titan DR verankert."}
{"ts": "181:20", "speaker": "I", "text": "Gab es im letzten Drill besondere Herausforderungen bei der Schnittstelle zwischen Alert Engine und Failover-Skripten?"}
{"ts": "181:35", "speaker": "E", "text": "Ja, bei TEST-DR-2025-Q1 trat eine Race Condition auf: Der Alert löste aus, bevor alle Health-Checks abgeschlossen waren. Wir haben daraufhin in Ticket DR-INC-7743 dokumentiert, dass ein zusätzlicher Wait-Step von 90 Sekunden nötig ist, um falsche Positivmeldungen zu vermeiden."}
{"ts": "181:58", "speaker": "I", "text": "Wie stellen Sie sicher, dass solche Anpassungen auch bei Audits nachvollziehbar bleiben?"}
{"ts": "182:12", "speaker": "E", "text": "Wir pflegen jede Abweichung und jedes Fix in unserem DR-Change-Log, das mit den RFC-Nummern verknüpft ist, z.B. RFC-DR-2025-04. Jeder Eintrag enthält Verweise auf die relevanten Runbooks und Drill-Reports, um die Kette von Ursache und Entscheidung lückenlos darzustellen."}
{"ts": "182:35", "speaker": "I", "text": "Gab es auch Überlegungen, die Failover-Prozeduren stärker zu automatisieren, um menschliche Fehler zu minimieren?"}
{"ts": "182:48", "speaker": "E", "text": "Definitiv. Wir evaluieren gerade einen aktiven Standby-Ansatz mit automatischem DNS-Switching. Das ist kostenintensiver, wie wir in unserer Kosten-Nutzen-Analyse DR-CBA-2025-Q2 festgehalten haben, könnte aber die RTO um bis zu 40% reduzieren."}
{"ts": "183:10", "speaker": "I", "text": "Welche Risiken würden Sie bei einer solchen Umstellung sehen?"}
{"ts": "183:24", "speaker": "E", "text": "Hauptsächlich das Risiko von Split-Brain-Szenarien, falls die Heartbeat-Signale falsch interpretiert werden. Das würde zu inkonsistenten Daten führen. Wir haben deshalb ein zusätzliches Quorum-Verfahren in RFC-DR-2025-07 vorgeschlagen."}
{"ts": "183:45", "speaker": "I", "text": "Wie gehen Sie mit der Balance zwischen Budgetrestriktionen und der Notwendigkeit solcher Sicherheitsmaßnahmen um?"}
{"ts": "184:00", "speaker": "E", "text": "Wir priorisieren Maßnahmen nach einem Risk-Impact-Score. Wenn ein Sicherheits-Feature wie das Quorum-Verfahren ein hohes Risiko signifikant reduziert, dann argumentieren wir für die Budgetfreigabe mit Verweis auf unsere 'Safety First'-Strategie und die SLA-Verpflichtungen von 99,95% Availability."}
{"ts": "184:22", "speaker": "I", "text": "Und zum Ausblick: Welche RFCs sind für die nächsten Schritte geplant?"}
{"ts": "184:35", "speaker": "E", "text": "Neben RFC-DR-2025-07 für das Quorum steht auch RFC-DR-2025-09 an, in dem wir die Integration eines Chaos-Testing-Frameworks beschreiben. Damit wollen wir Failover-Pfade unter Produktionslast simulieren und so weitere Schwachstellen frühzeitig erkennen."}
{"ts": "185:20", "speaker": "I", "text": "Sie hatten vorhin schon die Lessons Learned erwähnt – könnten Sie etwas genauer auf die Integration mit dem Nimbus Observability Projekt eingehen?"}
{"ts": "185:36", "speaker": "E", "text": "Ja, gern. Wir haben Titan DR so gebaut, dass die Failover-Trigger direkt in Nimbus Observability eingespeist werden. Das heißt, wenn in Region EU-Central ein SLA-Breach erkannt wird, feuert sofort ein Alarm, der gemäß Runbook RB-DR-001 eine automatisierte Umschaltung anstößt."}
{"ts": "185:58", "speaker": "I", "text": "Das klingt nach einer engen Kopplung. Wie stellen Sie sicher, dass diese Automatisierung nicht zu Fehlalarmen führt?"}
{"ts": "186:15", "speaker": "E", "text": "Wir haben da ein zweistufiges Validierungsverfahren. Step 1 ist ein Cross-Check über Poseidon Networking Logs, Step 2 ein manueller Review durch den DR-On-Call. Diese Logik ist in Ticket CHG-DR-452 dokumentiert und wurde im letzten Drill validiert."}
{"ts": "186:40", "speaker": "I", "text": "Und gab es bei TEST-DR-2025-Q1 Situationen, in denen diese Validierung entscheidend war?"}
{"ts": "186:55", "speaker": "E", "text": "Ja, ein Beispiel: ein transienter Paketverlust wurde fälschlich als Region-Failure interpretiert. Die Poseidon-Daten zeigten aber nur ein 3-Minuten-Fenster mit 15% Loss. Ohne Cross-Check hätten wir unnötig umgeschaltet."}
{"ts": "187:18", "speaker": "I", "text": "Interessant. Wie dokumentieren Sie solche Erkenntnisse für zukünftige Audits?"}
{"ts": "187:33", "speaker": "E", "text": "Wir pflegen ein internes DR-Wiki. Dort verlinken wir Runbook-Revisionen, Tickets wie INC-DR-882 und die Postmortems. Alle Änderungen werden mit RFCs, z.B. RFC-DR-019, revisionssicher abgelegt."}
{"ts": "187:58", "speaker": "I", "text": "Sie hatten vorhin die aktive vs. passive Standby-Strategie angesprochen. Welche Entscheidung fiel am Ende?"}
{"ts": "188:14", "speaker": "E", "text": "Wir haben uns für ein aktives-active Pairing entschieden, aber nur für kritische Services. Hintergrund: die Kosten für passive Standby in drei Regionen wären zwar niedriger, aber das RTO hätte sich verdoppelt. Unser SLO von 5 Minuten wäre nicht haltbar gewesen."}
{"ts": "188:38", "speaker": "I", "text": "Gab es Bedenken wegen der Komplexität bei aktiv-active?"}
{"ts": "188:50", "speaker": "E", "text": "Natürlich. Die Synchronisation der State Stores war kritisch. Wir mussten eine Lösung aus Helios Datalake übernehmen, wo wir schon Multi-Master-Replikation getestet hatten. Das hat die Latenz leicht erhöht, aber den BLAST_RADIUS stark reduziert."}
{"ts": "189:15", "speaker": "I", "text": "Wie messen Sie die Einhaltung der RPO in diesem Setup?"}
{"ts": "189:28", "speaker": "E", "text": "Über kontinuierliche Snapshot-Checks. Nimbus erzeugt stündlich einen RPO-Report. Wenn wir über 30 Sekunden Lag feststellen, wird automatisch ein Maintenance-Window gemäß RB-DR-004 geplant."}
{"ts": "189:50", "speaker": "I", "text": "Und zum Abschluss: Welche Verbesserungen planen Sie für den nächsten Drill-Zyklus?"}
{"ts": "190:05", "speaker": "E", "text": "Wir wollen die Cross-Region-Kompression optimieren, um Kosten zu sparen, ohne die Failover-Zeit zu erhöhen. Außerdem steht RFC-DR-023 an, um die Fehlersimulationen realistischer zu gestalten."}
{"ts": "194:40", "speaker": "I", "text": "Zum Abschluss würde mich interessieren, welche unmittelbaren Prioritäten Sie jetzt nach dem Drill setzen."}
{"ts": "194:48", "speaker": "E", "text": "Direkt nach dem Drill konzentrieren wir uns auf die Umsetzung der im Ticket DR-IMP-223 beschriebenen Hotfixes für das Failover-Skript. Außerdem priorisieren wir die Anpassung von RB-DR-001, um die neuen Steps für den Cross-Region-DNS-Switch aufzunehmen."}
{"ts": "194:59", "speaker": "I", "text": "Gibt es dabei Abhängigkeiten zu anderen Teams oder Projekten?"}
{"ts": "195:05", "speaker": "E", "text": "Ja, wir sind auf das Networking-Team angewiesen, das gerade ein Update aus Poseidon Networking ausrollt. Die BGP-Routenänderungen, die sie einführen, müssen in unseren Failover-Tests berücksichtigt werden, sonst riskieren wir inkonsistente Routen im Recovery-Szenario."}
{"ts": "195:20", "speaker": "I", "text": "Wie stellen Sie sicher, dass diese Koordination gelingt?"}
{"ts": "195:25", "speaker": "E", "text": "Wir haben einen gemeinsamen Change-Kalender, der mit den RFC-Einträgen verknüpft ist. Für jede Änderung am Netzpfad gibt es ein Pflichtfeld zur DR-Kompatibilität. Außerdem führen wir wöchentliche Sync-Calls durch, um die Abhängigkeiten zu besprechen."}
{"ts": "195:38", "speaker": "I", "text": "Und im Hinblick auf Verbesserungen für den nächsten Drill-Zyklus – was steht da auf Ihrer Liste?"}
{"ts": "195:44", "speaker": "E", "text": "Wir wollen den automatisierten Health-Check erweitern, damit er nicht nur Infrastrukturmetriken aus Nimbus Observability zieht, sondern auch Applikations-Transaktionen prüft. Das ist eine direkte Lehre aus dem GameDay, wo wir einen 15-minütigen Blind Spot hatten, weil die App-Layer-Probe fehlte."}
{"ts": "195:58", "speaker": "I", "text": "Das klingt nach einer engeren Integration mit Nimbus. Wie gehen Sie da vor?"}
{"ts": "196:03", "speaker": "E", "text": "Wir haben bereits ein Nimbus-Plugin in Entwicklung, das die Runbook-Schritte aus RB-DR-003 automatisch triggert, wenn bestimmte Metrik-Thresholds überschritten werden. Damit verkürzen wir die Mean Time to Detect im Failover-Fall signifikant."}
{"ts": "196:15", "speaker": "I", "text": "Sind dafür auch neue SLAs oder SLOs geplant?"}
{"ts": "196:20", "speaker": "E", "text": "Ja, wir wollen ein internes SLO für die Detection-Zeit von maximal 60 Sekunden etablieren. Dazu wird ein Draft-RFC im nächsten Steering Committee vorgestellt. Sollte das angenommen werden, passt sich auch unser SLA gegenüber internen Kunden an."}
{"ts": "196:33", "speaker": "I", "text": "Wie dokumentieren Sie diese Änderungen für spätere Audits?"}
{"ts": "196:38", "speaker": "E", "text": "Alle Anpassungen werden in Confluence im Kapitel 'DR Changes' versioniert. Zusätzlich hängen wir die Verweise auf die zugehörigen Tickets und Testprotokolle an, damit wir bei einer Audit-Anfrage die komplette Kette von Anforderung bis Umsetzung nachvollziehen können."}
{"ts": "196:50", "speaker": "I", "text": "Abschließend: Gibt es schon konkrete RFCs zu Anpassungen der Failover-Prozesse?"}
{"ts": "196:55", "speaker": "E", "text": "Ja, RFC-DR-2025-07 beschreibt den Wechsel auf ein aktives-aktives Setup zwischen zwei Primärregionen. Wir haben die Entscheidung nach einer Kosten-Nutzen-Analyse getroffen, die auch die Erkenntnisse aus TEST-DR-2025-Q1 einbezieht. Der RFC ist derzeit im Review beim Architekturboard."}
{"ts": "201:40", "speaker": "I", "text": "Könnten Sie bitte noch einmal genauer beschreiben, wie Titan DR aktuell mit dem Nimbus Observability Projekt integriert ist?"}
{"ts": "201:48", "speaker": "E", "text": "Ja, gerne. Wir haben eine direkte Integration über das Alert-API von Nimbus. Das heißt, wenn in Region A ein Latenz-Threshold von 250ms überschritten wird, löst Nimbus automatisch den Runbook-Trigger RB-DR-001 aus. Dieser Schritt ist in unserem Drill-Skript DS-TDR-03 festgehalten."}
{"ts": "201:59", "speaker": "E", "text": "Zusätzlich haben wir ein Mapping in der Observability-UI, sodass der Status der Failover-Cluster in beiden Regionen visualisiert ist. Das ist ein Multi-Hop-Flow, weil die Alarme aus Nimbus über unser internes Event-Bus-System an den DR-Orchestrator weitergegeben werden."}
{"ts": "202:14", "speaker": "I", "text": "Das klingt recht komplex. Gab es hier besondere Herausforderungen bei der Synchronisierung zwischen den Systemen?"}
{"ts": "202:19", "speaker": "E", "text": "Ja, vor allem beim Timestamp-Abgleich. Nimbus arbeitet mit einer eigenen Zeitquelle, der DR-Orchestrator hingegen mit NTP-Sync. Wir mussten eine Normalisierung einbauen, um Drift von mehr als 2 Sekunden zu vermeiden. Das war eine Lektion aus dem Helios Datalake Projekt, wo Zeitabweichungen zu falschen Alarm-Korrelationen geführt hatten."}
{"ts": "202:34", "speaker": "I", "text": "Interessant. Können Sie noch etwas zu den Lessons Learned aus Poseidon Networking in diesem Kontext sagen?"}
{"ts": "202:39", "speaker": "E", "text": "Klar. Poseidon hat uns gezeigt, dass Netzwerksegmentierung den BLAST_RADIUS erheblich reduzieren kann. Für Titan DR heißt das: Die Replikationslinks zwischen Regionen sind über dedizierte, isolierte VLANs geführt. So verhindern wir, dass ein Routing-Fehler im Produktionsnetz den DR-Traffic beeinflusst."}
{"ts": "202:54", "speaker": "I", "text": "Sie hatten vorhin den BLAST_RADIUS schon mal erwähnt. Haben Sie dazu konkrete Metriken definiert?"}
{"ts": "203:00", "speaker": "E", "text": "Ja, wir messen den BLAST_RADIUS als Anteil der betroffenen Services im Falle eines Region-Ausfalls. Unser Zielwert liegt bei maximal 20 % Service-Degradation innerhalb von 5 Minuten nach Failover. Die Metrik ist Teil unseres SLO-Dokuments SLO-DR-2025-01."}
{"ts": "203:14", "speaker": "I", "text": "Wenn wir auf die Kosten-Performance-Balance schauen – hatten Sie Überlegungen, den aktiven Standby doch wieder zu verwerfen?"}
{"ts": "203:20", "speaker": "E", "text": "Kurzzeitig ja, während der Budget-Review im Ticket FIN-REQ-882. Wir haben aber entschieden, beim aktiven Standby zu bleiben, weil dadurch die RTO von 10 auf 3 Minuten sinkt. Die Mehrkosten von ca. 18 % wurden durch das Risiko-Profil gerechtfertigt."}
{"ts": "203:36", "speaker": "I", "text": "Gab es dazu formale Entscheidungsdokumente?"}
{"ts": "203:40", "speaker": "E", "text": "Ja, die Entscheidung ist im RFC-DR-2025-07 dokumentiert, inkl. einer Risiko-Matrix und Verweisen auf das Postmortem INC-DR-2024-11. Letzteres zeigte, dass passive Standby-Strategien in unserem Setup zu langen Cold-Starts führen."}
{"ts": "203:55", "speaker": "I", "text": "Wie fließen diese Erkenntnisse in den nächsten Drill-Zyklus ein?"}
{"ts": "204:00", "speaker": "E", "text": "Wir planen, beim nächsten Drill TEST-DR-2025-Q3 zusätzlich Cross-Region-Capacity-Checks in Echtzeit zu simulieren. Das soll sicherstellen, dass das aktive Standby wirklich die Last aufnehmen kann, ohne die SLAs zu verletzen."}
{"ts": "204:12", "speaker": "I", "text": "Und abschließend, gibt es schon geplante RFCs zur Anpassung der Failover-Prozeduren?"}
{"ts": "204:16", "speaker": "E", "text": "Ja, RFC-DR-2025-12 ist in Draft. Darin schlagen wir vor, den Failover-Schritt 4 aus RB-DR-001 so zu ändern, dass DNS-Cutover und Cache-Invalidierung parallel laufen. Das könnte die End-to-End-RTO um weitere 30 Sekunden verbessern."}
{"ts": "210:40", "speaker": "I", "text": "Bevor wir zum Ausblick kommen – können Sie noch einmal konkret sagen, welche SLAs aktuell für Titan DR gelten und wie Sie diese technisch überwachen?"}
{"ts": "210:55", "speaker": "E", "text": "Ja, also wir haben ein formales SLA von 99,95 % Verfügbarkeit pro Region und ein internes SLO für Failover-Zeit von maximal 90 Sekunden. Die Überwachung läuft über unser internes Monitoring-Framework \"Nimbus MonX\". Dort haben wir spezifische Probes hinterlegt, die auch den Runbook-Trigger RB-DR-001 automatisch starten, falls zwei aufeinanderfolgende Checks fehlschlagen."}
{"ts": "211:21", "speaker": "I", "text": "Und diese Probes, sind die in allen Regionen identisch konfiguriert oder gibt es Anpassungen?"}
{"ts": "211:35", "speaker": "E", "text": "Grundsätzlich identisch, aber wir haben regionenspezifische Thresholds. Zum Beispiel ist in der Region EU-Central ein Latenz-Threshold von 120 ms hinterlegt, in AP-South hingegen 180 ms, weil die Netztopologie dort ein anderes Grundrauschen hat."}
{"ts": "211:56", "speaker": "I", "text": "Verstehe. Gab es im letzten Drill besondere Erkenntnisse zu diesen Grenzwerten?"}
{"ts": "212:10", "speaker": "E", "text": "Ja, im TEST-DR-2025-Q1 ist uns aufgefallen, dass unser Alert in AP-South zu spät kam, weil der Threshold zu großzügig war. Wir haben das im Ticket OPS-DR-553 dokumentiert und den Wert inzwischen um 20 ms gesenkt."}
{"ts": "212:33", "speaker": "I", "text": "Sie hatten vorhin erwähnt, dass Lessons Learned aus Poseidon Networking eingeflossen sind – können Sie das an einem Beispiel festmachen?"}
{"ts": "212:49", "speaker": "E", "text": "Klar, bei Poseidon hatten wir mal eine Route-Flap-Situation, die unser Failover unnötig oft getriggert hat. Für Titan DR haben wir deshalb eine BGP-Flap-Dampening-Policy übernommen, die im Runbook RB-NET-042 beschrieben ist. Das reduziert Fehlalarme erheblich."}
{"ts": "213:15", "speaker": "I", "text": "Gab es dazu auch Abhängigkeiten zum Nimbus Observability Projekt?"}
{"ts": "213:28", "speaker": "E", "text": "Ja, Nimbus liefert uns die Metriken für die BGP-Session-Stabilität. Ohne diese Integration könnten wir die Dampening-Policy gar nicht sauber evaluieren. Das ist ein klassischer Multi-Hop-Zusammenhang: Netzwerk-Health beeinflusst DR-Trigger, und die Metriken kommen aus einem anderen Projekt."}
{"ts": "213:52", "speaker": "I", "text": "Stichwort Multi-Region: Nutzen Sie aktive oder passive Standby-Strategien derzeit?"}
{"ts": "214:05", "speaker": "E", "text": "Aktuell fahren wir ein aktives/aktives Setup für die kritischen API-Services und ein passives Standby für Batch-Workloads. Das ist ein Kompromiss: aktives/aktives erhöht Performance und verteilt Last, passives spart Kosten bei weniger zeitkritischen Prozessen."}
{"ts": "214:28", "speaker": "I", "text": "Wie dokumentieren Sie diese Entscheidungen für spätere Audits?"}
{"ts": "214:41", "speaker": "E", "text": "Wir führen pro Entscheidung ein RFC-Dokument, z. B. RFC-DR-2025-07 für die Standby-Strategie. Darin sind die Alternativen, die Tradeoffs und die beschlossenen Maßnahmen enthalten, plus Verweise auf Testprotokolle und Tickets."}
{"ts": "215:02", "speaker": "I", "text": "Und zum Abschluss – was sind Ihre nächsten Prioritäten für Titan DR?"}
{"ts": "215:15", "speaker": "E", "text": "Kurzfristig wollen wir die Drill-Frequenz von halbjährlich auf quartalsweise erhöhen, um schneller Feedback zu erhalten. Außerdem planen wir ein RFC zur Anpassung der Failover-Prozeduren, damit RTO und RPO noch enger an unser internes SLO gebunden sind."}
{"ts": "219:40", "speaker": "I", "text": "Könnten Sie noch etwas genauer ausführen, wie die Erkenntnisse aus dem letzten Drill in konkrete RFCs überführt werden?"}
{"ts": "219:55", "speaker": "E", "text": "Ja, wir haben nach TEST-DR-2025-Q1 gleich drei RFCs aufgesetzt: RFC-DR-2025-07 für die Anpassung der Failover-Latenz, RFC-DR-2025-08 für neue Health-Check-Probes und RFC-DR-2025-09 für verbesserte Log-Korrelation. Diese sind im internen Confluence als Entwurf verlinkt und folgen dem Standard-Review-Prozess mit Architekturboard und Security-Team."}
{"ts": "220:20", "speaker": "I", "text": "Gab es bei der Umsetzung irgendwelche technischen Stolpersteine, die Sie nicht erwartet hatten?"}
{"ts": "220:35", "speaker": "E", "text": "Ein kleiner, aber signifikanter Punkt war, dass unsere bestehenden Runbooks – insbesondere RB-DR-001 – keine klaren Schritte für die Diagnose von partiellen Netzwerksegmentierungen enthielten. Das mussten wir in RB-DR-001a als Addendum ergänzen, basierend auf Logs aus Region West-2."}
{"ts": "220:57", "speaker": "I", "text": "Wie haben Sie die Ergänzungen validiert?"}
{"ts": "221:10", "speaker": "E", "text": "Wir haben ein internes Mini-Drill-Szenario, 'SegNetSim', in der Staging-Umgebung gefahren. Dort wurde gezielt ein Paket-Drop von 30% zwischen zwei Availability Zones simuliert. Das neue Runbook-Addendum hat die Identifikation auf unter 4 Minuten verkürzt – vorher lag der Median bei knapp 12."}
{"ts": "221:35", "speaker": "I", "text": "Interessant. Und wie fließen diese Erkenntnisse in die Observability ein?"}
