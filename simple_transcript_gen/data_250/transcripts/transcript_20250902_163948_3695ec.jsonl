{"ts": "00:00", "speaker": "I", "text": "Können Sie mir bitte den aktuellen Stand des Helios Datalake Projekts im Rahmen der Scale-Phase schildern, und vielleicht auch, äh, die grobe Architektur?"}
{"ts": "04:15", "speaker": "E", "text": "Ja, gern. Wir sind jetzt im dritten Quartal der Scale-Phase, das heißt wir haben die Grund-ELT-Strecken zu Snowflake bereits stabilisiert. Im Zentrum steht unser Unified ELT Framework, das sowohl Streaming-Daten aus Kafka als auch Batch-Loads aus verschiedenen Quellsystemen orchestriert. Die wichtigsten Komponenten sind ein Kafka-Cluster mit drei Regionen, unsere Airflow DAGs für Batch-Transformationen, und das dbt-Projekt, das die modellierten Schichten in Snowflake aufbaut. Unser Ziel ist es, die Latenz auf unter 5 Minuten für Streaming und unter 2 Stunden für Batch zu halten."}
{"ts": "09:05", "speaker": "I", "text": "Und welche SLOs und SLAs sind aktuell für den Datalake definiert?"}
{"ts": "13:40", "speaker": "E", "text": "Wir haben ein SLO für End-to-End-Verfügbarkeit von 99,8% im Monatsmittel, und ein SLA für Datenvollständigkeit von 99,5% pro Batch-Load. Die Einhaltung stellen wir durch ein kombiniertes Monitoring sicher – technische Checks in Prometheus und funktionale Checks per Data Quality Framework. Bei Abweichungen greifen Runbooks wie RB-MON-015 für Monitoring-Alerts oder RB-ING-042 für Ingestion-Failover."}
{"ts": "18:50", "speaker": "I", "text": "Könnten Sie beschreiben, wie Kafka-Ingestion und Batch-Partitionierung ineinandergreifen?"}
{"ts": "23:30", "speaker": "E", "text": "Klar. Wir nutzen Kafka hauptsächlich für Near-Real-Time Events. Für Quellsysteme, die nur Batch liefern, partitionieren wir die Loads gemäß RFC-1287. Diese beschreibt eine Zeitfenster-basierte Partitionierung, die wiederum mit den Kafka-Topics synchronisiert wird, sodass z. B. die 'Late Arrivals' im nächsten Microbatch nachgezogen werden. Airflow triggert dann dbt-Modelle, die sowohl die Streaming- als auch Batch-Daten konsolidieren."}
{"ts": "28:45", "speaker": "I", "text": "RFC-1287 klingt zentral – was steht da konkret drin, und wie setzen Sie das um?"}
{"ts": "34:05", "speaker": "E", "text": "RFC-1287 legt fest, dass wir für Batch-Loads eine feste Bucketgröße von einer Stunde verwenden, unabhängig von der tatsächlichen Quell-Upload-Zeit. Das erleichtert das Zusammenführen mit Kafka-Streams. Technisch setzen wir das über ein Preprocessing-Skript in Airflow um, das den Partition Key generiert. Zusätzlich haben wir in RFC-1287 dokumentiert, wie Replays aussehen, wenn ein Bucket unvollständig ist."}
{"ts": "39:20", "speaker": "I", "text": "Wie werden Änderungen an den dbt-Modellen in Produktion integriert und getestet?"}
{"ts": "43:55", "speaker": "E", "text": "Wir haben dafür eine gestaffelte Pipeline: Entwickler pushen auf Feature-Branches, es laufen Unit-Tests und Slim CI für dbt. Danach deployen wir in eine Staging-Snowflake-Umgebung, wo Integrationstests gegen anonymisierte Echtdaten laufen. Erst nach Freigabe durch den Data Steward geht's mit einem Merge auf Main weiter und wird per ArgoCD in die Produktion gebracht. Dazu gibt es die interne Checkliste aus RUN-DBT-021."}
{"ts": "49:10", "speaker": "I", "text": "Welche Maßnahmen setzen Sie ein, um die Performance bei wachsender Datenmenge zu sichern?"}
{"ts": "54:00", "speaker": "E", "text": "Wir haben drei Hauptmaßnahmen: Erstens Clustering Keys in Snowflake auf den Zeitstempelfeldern, um Scans zu minimieren. Zweitens nutzen wir Materialized Views für häufig abgefragte Aggregationen. Drittens haben wir in Kafka eine Retention-Policy eingeführt, die nur relevante Events länger vorhält. All das ist in POL-PERF-004 beschrieben."}
{"ts": "58:30", "speaker": "I", "text": "Und wie gehen Sie mit Kostenoptimierung im Cloud-Kontext um, speziell im Hinblick auf POL-FIN-007?"}
{"ts": "63:25", "speaker": "E", "text": "POL-FIN-007 gibt uns vor, dass wir Warehouse-Sizes dynamisch anpassen und Off-Peak-Loads bündeln. Wir haben auch Query-Tagging eingeführt, um Kosten pro Fachbereich zu tracken. Monatlich gibt es ein FinOps-Review, das Abweichungen diskutiert und Optimierungen beschließt."}
{"ts": "68:40", "speaker": "I", "text": "Gibt es Runbooks wie RB-ING-042, die Sie bei Failover-Szenarien nutzen?"}
{"ts": "75:00", "speaker": "E", "text": "Ja, RB-ING-042 beschreibt Schritt für Schritt, wie wir bei einem Ausfall einer Kafka-Partition auf eine Ersatzpartition in einer anderen Region umschwenken. Enthalten sind auch Kommandos für die Neuinitialisierung der Consumer-Gruppen, sowie Checks zur Datenvollständigkeit nach dem Failover."}
{"ts": "90:00", "speaker": "I", "text": "Lassen Sie uns jetzt auf die Skalierungsmaßnahmen eingehen. Sie hatten vorhin angedeutet, dass beim Übergang in die Scale-Phase einige Optimierungen notwendig wurden – können Sie das bitte konkretisieren?"}
{"ts": "90:15", "speaker": "E", "text": "Ja, klar. Wir haben zunächst die Snowflake-Warehouse-Größen dynamisch angepasst – das sogenannte Auto-suspend/Auto-resume Feature, gemäß interner Richtlinie POL-FIN-007, um Kosten zu senken. Parallel haben wir die Kafka Consumer Groups so skaliert, dass die Latenz unter den SLA-Wert von 90 Sekunden bleibt, auch bei Peak-Loads."}
{"ts": "90:38", "speaker": "I", "text": "Und wie messen Sie in diesem Kontext den Erfolg dieser Maßnahmen?"}
{"ts": "90:49", "speaker": "E", "text": "Wir tracken Metriken über unser internes Monitoring-Tool *AsterMon*. Dort sind Dashboards konfiguriert, die sowohl Warehouse Credit Usage als auch den Kafka Lag visualisieren. Ein Beispiel: Ticket OPS-5432 dokumentiert, wie wir durch ein Warehouse-Scaling von M auf L die Query-Laufzeit um 35% reduzieren konnten, bei nur 12% Mehrkosten."}
{"ts": "91:10", "speaker": "I", "text": "Interessant. Gibt es auch Runbooks, die Sie bei solchen Performance-Optimierungen stützen?"}
{"ts": "91:21", "speaker": "E", "text": "Ja, wir nutzen etwa RB-ING-042 für Failover- und Performance-Tuning im Ingestion Layer. Das beschreibt Schritt für Schritt, wie zusätzliche Kafka-Partitions on-the-fly deployed werden, um Backlogs abzubauen, ohne den BLAST_RADIUS zu vergrößern."}
{"ts": "91:42", "speaker": "I", "text": "Stichwort BLAST_RADIUS – wie spielt das Thema in Ihre Multi-Region-Strategie hinein?"}
{"ts": "91:53", "speaker": "E", "text": "Wir replizieren kritische Snowflake-Schemas in eine zweite Region, allerdings nur read-only, um Kosten gering zu halten. Gemäß RB-DR-001, unserem Titan DR Runbook, können wir im Ernstfall innerhalb von 20 Minuten umschalten, was unserem RTO-Ziel entspricht."}
{"ts": "92:14", "speaker": "I", "text": "Und wie stellen Sie sicher, dass diese RTO/RPO-Werte nicht nur auf dem Papier existieren?"}
{"ts": "92:25", "speaker": "E", "text": "Wir führen vierteljährliche DR-Tests durch, bei denen realistisch simuliert wird, dass Region A ausfällt. Dabei messen wir den tatsächlichen Datenverlust – zuletzt 3,5 Minuten, also unterhalb unseres RPO von 5 Minuten."}
{"ts": "92:44", "speaker": "I", "text": "Gab es bei diesen Tests schon einmal Überraschungen oder unerwartete Probleme?"}
{"ts": "92:55", "speaker": "E", "text": "Ja, einmal hat ein fehlerhaftes dbt-Deployment in der Secondary Region dazu geführt, dass ein dringendes Reporting nicht lief. Das haben wir im Post-Mortem DOC-PM-221 aufgearbeitet und als RFC-1452 in unseren Change-Prozess integriert."}
{"ts": "93:16", "speaker": "I", "text": "Das klingt nach einem klaren Trade-off zwischen Redundanz und Komplexität. Können Sie ein weiteres Beispiel geben, wo Sie eine bewusste Entscheidung treffen mussten?"}
{"ts": "93:28", "speaker": "E", "text": "Klar – wir haben uns gegen eine vollständige Multi-Master-Replikation entschieden, obwohl das die Resilienz erhöht hätte. Die Kosten und der höhere Koordinationsaufwand hätten laut unserer TCO-Analyse (siehe FIN-ANA-009) den Nutzen übertroffen."}
{"ts": "93:48", "speaker": "I", "text": "Wie kommunizieren Sie solche Entscheidungen intern, damit alle Stakeholder sie nachvollziehen können?"}
{"ts": "94:00", "speaker": "E", "text": "Wir dokumentieren die Entscheidungsgrundlagen in RFCs, verlinken relevante Tickets und Monitoring-Daten, und präsentieren sie im wöchentlichen Architecture Stand-up. So ist sichergestellt, dass sowohl Dev- als auch Ops-Teams im Bilde sind."}
{"ts": "96:00", "speaker": "I", "text": "Lassen Sie uns jetzt noch einmal auf die Lessons Learned eingehen – speziell nach dem letzten Scale-Test im März. Was waren aus Ihrer Sicht die wichtigsten Erkenntnisse?"}
{"ts": "96:20", "speaker": "E", "text": "Also, eine wesentliche Erkenntnis war, dass unser Streaming-Layer in Kombination mit den Batch-Loads aus RFC-1287 stabil lief, aber die Snowflake-Warehouse-Größe im Peak zu klein dimensioniert war. Wir haben daraufhin im Runbook RB-ING-042 einen Schritt ergänzt, der einen automatischen Scale-up bei über 85 % CPU Last triggert."}
{"ts": "96:48", "speaker": "I", "text": "Gab es bei diesem Scale-up Mechanismus irgendwelche unerwarteten Nebeneffekte oder Kostenfallen?"}
{"ts": "97:05", "speaker": "E", "text": "Ja, tatsächlich. Der Scale-up war technisch korrekt, aber POL-FIN-007 verpflichtet uns, Kostensteigerungen über 15 % sofort zu melden. Wir mussten einen Alert im Monitoring ergänzen, der FinOps informiert, sobald ein Scale-up länger als 3 Stunden aktiv ist."}
{"ts": "97:34", "speaker": "I", "text": "Wie haben Sie die Alerting-Logik getestet, um Fehlalarme zu vermeiden?"}
{"ts": "97:50", "speaker": "E", "text": "Wir haben ein synthetisches Lastszenario in der Staging-Region DE-2 erzeugt, ähnlich wie beim Failover-Test aus RB-DR-001. Dabei haben wir die Warehouse-Größe manuell hochgesetzt und geprüft, ob das Alerting genau nach 3 Stunden anschlägt. Falsch-Positiv-Rate lag bei null."}
{"ts": "98:22", "speaker": "I", "text": "Interessant. Und wie wurde diese Änderung intern kommuniziert? Gab es ein RFC?"}
{"ts": "98:35", "speaker": "E", "text": "Ja, RFC-1452 – 'Adaptive Warehouse Scaling Alerts' – wurde im Architektur-Channel geteilt. Wir haben Monitoring-Screenshots und ein kurzes Loom-Video zur Funktionsweise angehängt, damit Ops und FinOps das Verhalten nachvollziehen können."}
{"ts": "99:02", "speaker": "I", "text": "Gab es bei der Multi-Region-Architektur noch offene Punkte, die Sie jetzt nach den Tests anpassen wollen?"}
{"ts": "99:18", "speaker": "E", "text": "Ja, beim BLAST_RADIUS haben wir festgestellt, dass einige Kafka-Topics noch nicht regional isoliert waren. Ticket OPS-3872 adressiert genau das; wir segmentieren jetzt die Consumer-Gruppen pro Region, um Cross-Region Traffic im Failover zu vermeiden."}
{"ts": "99:45", "speaker": "I", "text": "Wie wirkt sich diese Segmentierung auf die Latenz und das Datenkonsistenz-Modell aus?"}
{"ts": "100:00", "speaker": "E", "text": "Die Latenz sinkt leicht, weil weniger Cross-Region Hops nötig sind. Konsistenz bleibt eventual, aber wir haben in RB-DR-001 vermerkt, dass bei Region-Failover ein Reconciliation-Job laufen muss, um innerhalb von RPO=15 Minuten die Deltas zu korrigieren."}
{"ts": "100:28", "speaker": "I", "text": "Haben Sie diese Reconciliation-Jobs bereits automatisiert oder laufen die noch manuell?"}
{"ts": "100:42", "speaker": "E", "text": "Teilautomatisiert. Der Trigger ist automatisiert, aber die finale Validierung erfolgt noch manuell per Checkliste aus RB-VAL-009. Wir wollen das bis Q3 voll automatisieren, um die RTO besser einzuhalten."}
{"ts": "101:05", "speaker": "I", "text": "Letzte Frage: Welche Risiken sehen Sie, wenn diese Automatisierung verzögert umgesetzt wird?"}
{"ts": "101:20", "speaker": "E", "text": "Das Hauptrisiko ist, dass wir bei einem echten Multi-Region-Failover das RTO von 30 Minuten nicht schaffen und temporär inkonsistente Reports ausliefern. Das könnte gegen SLA-DS-004 verstoßen und Kundenvertrauen beeinträchtigen. Deshalb ist das Thema bei uns als High Priority eingestuft."}
{"ts": "112:00", "speaker": "I", "text": "Wir hatten eben die generellen Skalierungsmaßnahmen betrachtet. Mich würde nun interessieren, wie Sie konkret im Helios Datalake Lastspitzen vorhersehen und abfangen."}
{"ts": "112:18", "speaker": "E", "text": "Wir nutzen ein kombiniertes Predictive Scaling. Einerseits haben wir in Snowflake Auto-Suspend/Resume konfiguriert, andererseits laufen in unserer Control Plane eigene Forecast-Algorithmen, die über Kafka-Lag-Metriken und historische Batch-Load-Dauer Prognosen erstellen. Daraufhin triggern wir vorab die Skalierung der Warehouses."}
{"ts": "112:48", "speaker": "I", "text": "Und wie fließt dieses Forecasting in Ihre SLO-Überwachung ein?"}
{"ts": "113:02", "speaker": "E", "text": "Die SLOs für Batch-Latenz (max 45 Minuten pro Partition laut SLA-DL-004) werden pro Joblauf evaluiert. Wenn das Forecasting einen Wert >80% der Schwelle erwartet, bekommt das On-Call-Team ein Frühwarn-Ticket im System Orion, z.B. TCK-ING-982, um proaktiv Ressourcen anzupassen."}
{"ts": "113:32", "speaker": "I", "text": "Verstehe. Sie hatten vorhin RFC-1287 erwähnt. Können Sie erklären, wie dessen Vorgaben zur Partitionierung bei Lastspitzen helfen?"}
{"ts": "113:50", "speaker": "E", "text": "Ja, RFC-1287 definiert eine hybride Strategie: Kafka-Streams werden nach Event-Typ partitioniert, Batch-Loads hingegen nach Zeitfenstern und Region. So können wir gezielt nur die Partitionen hochskalieren, die gerade kritisch sind, ohne das ganze Warehouse zu vergrößern."}
{"ts": "114:18", "speaker": "I", "text": "Interessant. Wie testen Sie, dass diese Partitionierungs-Logik in Produktion fehlerfrei greift?"}
{"ts": "114:34", "speaker": "E", "text": "Wir haben im Staging eine komplette Spiegelung des Partitionierungslayers. Dort werden synthetische Lags injiziert. Das Test-Runbook TRB-PTN-009 beschreibt Schritt für Schritt, wie wir Monitoring-Alerts simulieren und prüfen, ob die Auto-Scaling-Regeln korrekt feuern."}
{"ts": "115:02", "speaker": "I", "text": "Gab es in letzter Zeit einen Vorfall, bei dem diese Mechanismen tatsächlich entscheidend waren?"}
{"ts": "115:16", "speaker": "E", "text": "Ja, im März hatten wir eine unvorhergesehene Datenwelle aus dem CRM-System. Die Partition 'eu-west-2-Q1' ging auf 1,4 Mio. Events Rückstand. Dank Forecast-Alarm und RFC-1287 konnten wir zielgerichtet nur diesen Partition-Cluster verdoppeln. Der SLA wurde eingehalten, und die Mehrkosten waren minimal."}
{"ts": "115:44", "speaker": "I", "text": "Wie stellen Sie sicher, dass solche Anpassungen nicht zu negativen Nebeneffekten führen, etwa auf Kostenoptimierung gem. POL-FIN-007?"}
{"ts": "116:02", "speaker": "E", "text": "Wir haben in POL-FIN-007 einen Threshold für temporäre Überschreitungen definiert. Jede Skalierungsaktion wird mit einem Cost Impact Tag versehen. Unser FinOps-Dashboard zeigt in Echtzeit, ob wir im tolerierten Budgetfenster liegen. Falls nicht, greift Runbook RB-CST-015 mit Eskalationspfad zum Architekten."}
{"ts": "116:32", "speaker": "I", "text": "Macht Sinn. Im Multi-Region-Kontext: greifen diese Mechanismen auch synchronisiert über alle Regionen hinweg?"}
{"ts": "116:48", "speaker": "E", "text": "Teilweise. Für Helios haben wir asynchrone Replikation zwischen eu-central-1 und us-east-1. Scaling-Decisions werden lokal getroffen, aber ein Metadaten-Stream über Kafka teilt den Zustand. Das Titan DR Runbook RB-DR-001 beschreibt, wie im Notfall die Skalierungsregeln auch in die Secondary-Region kopiert werden."}
{"ts": "117:20", "speaker": "I", "text": "Gab es schon mal einen Fall, wo Sie bewusst auf synchronisierte Skalierung verzichtet haben, um Resilienz zu wahren?"}
{"ts": "117:38", "speaker": "E", "text": "Ja, im Januar. Wir hatten in us-east-1 eine Netzwerkdegradation. Hätten wir die Skalierung synchronisiert, wäre der Engpass repliziert worden. Stattdessen haben wir eu-central-1 hochgefahren und us-east-1 isoliert. Monitoring-Evidenz aus Alert-IDs AL-DR-223 und AL-NET-910 bestätigte, dass so der BLAST_RADIUS minimiert wurde."}
{"ts": "128:00", "speaker": "I", "text": "Können Sie bitte ein wenig genauer erläutern, wie Sie aktuell die Skalierung in der Snowflake-Schicht handhaben, gerade bei steigender Event-Rate aus Kafka?"}
{"ts": "128:20", "speaker": "E", "text": "Ja, also wir haben in der Scale-Phase den Wechsel auf Multi-Cluster Warehouses vollzogen. Das erlaubt uns, bei Peaks wie Monatsendverarbeitungen, automatisch zusätzliche Cluster zu starten. Wir koppeln das an unser internes Load-Monitoring aus dem Modul 'helios-metrics'."}
{"ts": "128:45", "speaker": "I", "text": "Nutzen Sie dabei bestimmte Schwellenwerte oder ist das rein reaktiv?"}
{"ts": "129:00", "speaker": "E", "text": "Wir haben feste Thresholds definiert, etwa 65% Warehouse-Auslastung über 5 min, dann wird laut Runbook RB-PRF-019 ein neues Cluster gespawnt. Bei unter 40% geht es wieder runter. Das steht so auch in unserem SLO-Dokument DL-SLO-002."}
{"ts": "129:28", "speaker": "I", "text": "Und wie spielt das mit der Kafka-Ingestion-Logik zusammen?"}
{"ts": "129:45", "speaker": "E", "text": "Hier kommt der Multi-Hop ins Spiel: Unsere Kafka-Consumer schreiben erst in eine Landing-Zone in S3, partitioniert nach RFC-1287. Snowpipe zieht das dann quasi near-real-time in Snowflake, und dbt-Modelle werden asynchron getriggert. Die Partitionierungsstrategie ist wichtig, weil sie die Load-Parallelisierung im Warehouse direkt beeinflusst."}
{"ts": "130:15", "speaker": "I", "text": "Gibt es da bekannte Pain Points?"}
{"ts": "130:30", "speaker": "E", "text": "Ja, bei sehr kleinen Partitionen produzieren wir zu viel Overhead. Das haben wir im Incident INC-HEL-442 gesehen, wo die Query-Queue auf 120 s hochging. Als Reaktion haben wir in RFC-1342 die Min-Partition-Size auf 128 MB gesetzt."}
{"ts": "130:58", "speaker": "I", "text": "Sie hatten vorhin Failover-Szenarien erwähnt, können Sie ein Beispiel geben?"}
{"ts": "131:15", "speaker": "E", "text": "Klar, bei einem Ausfall der Region eu-central-1 letzte Woche haben wir RB-ING-042 und Titan DR Runbook RB-DR-001 kombiniert. Wir haben die Kafka-Streams auf us-east-2 umgeleitet und den Snowflake Account Failover aktiviert. Innerhalb von 18 min waren wir wieder live, RTO war somit unter dem Ziel von 30 min."}
{"ts": "131:50", "speaker": "I", "text": "Wie testen Sie diese RTO/RPO-Werte praktisch?"}
{"ts": "132:05", "speaker": "E", "text": "Wir fahren vierteljährlich DR-Drills. Die letzte Simulation war am 15.03., dokumentiert im Testreport TR-DR-2024-03. Wir simulieren komplette Region-Ausfälle und messen die Recovery-Zeiten. Das Monitoring liefert uns dabei Time-to-First-Record und Full-Load-Metrics."}
{"ts": "132:35", "speaker": "I", "text": "Gab es schwierige Entscheidungen zwischen Kosten und Resilienz in diesem Kontext?"}
{"ts": "132:50", "speaker": "E", "text": "Ja, ein klassischer Trade-off war die Frage, ob wir ein drittes Warm-Standby in ap-southeast-1 aufbauen. Die Kosten lt. POL-FIN-007 wären ca. +22% jährlich. Monitoring aus den letzten 12 Monaten zeigte aber, dass unsere BLAST_RADIUS-Risiken durch zwei Regionen schon auf 1,8% reduziert sind. Daher haben wir uns dagegen entschieden und es im RFC-1399 festgehalten."}
{"ts": "133:25", "speaker": "I", "text": "Wie fließen solche Entscheidungen zurück ins Team-Wissen?"}
{"ts": "144:00", "speaker": "E", "text": "Wir pflegen eine interne Confluence-Seite 'Helios Arch Decisions', verlinken die RFCs, Incident-Postmortems und relevante Runbooks. Zusätzlich besprechen wir kritische Punkte im wöchentlichen Architektur-Review, damit alle den Kontext und die Evidenz kennen."}
{"ts": "144:00", "speaker": "I", "text": "Wir hatten ja gerade schon über die Kosten-Performance-Resilienz-Trade-offs gesprochen. Können Sie ein konkretes Beispiel nennen, wo Sie Monitoring-Daten als Grundlage genutzt haben?"}
{"ts": "144:06", "speaker": "E", "text": "Ja, klar. Wir haben z. B. im Ticket OPS-HEL-452 gesehen, dass die Latenz bei den Kafka-Streams im Peak um 27 % hochging. Die Metriken aus unserem Prometheus-Cluster haben uns gezeigt, dass die Batch-Fenster zu eng waren. Daraufhin haben wir gemäß RFC-1321 die Partitionierungsstrategie angepasst."}
{"ts": "144:14", "speaker": "I", "text": "Das heißt, Sie haben hier sowohl Streaming- als auch Batch-Aspekte neu bewertet?"}
{"ts": "144:18", "speaker": "E", "text": "Genau. Wir haben die Batch-Partitionen in Snowflake so verschoben, dass sie nicht mehr mit den Peak-Zeiten der Kafka-Ingestion kollidieren. Das wurde in RB-ING-042 als Best Practice nachgetragen."}
{"ts": "144:25", "speaker": "I", "text": "Gab es dabei auch Auswirkungen auf die SLAs oder SLOs, die Sie erfüllen müssen?"}
{"ts": "144:29", "speaker": "E", "text": "Ein Stück weit. Unser SLO für End-to-End-Datenverfügbarkeit liegt bei 99,7 % innerhalb von 15 Minuten nach Erfassung. Durch die Anpassung sind wir jetzt sogar bei durchschnittlich 99,82 %, was wir im letzten Quartalsreport nachgewiesen haben."}
{"ts": "144:38", "speaker": "I", "text": "Wie kommunizieren Sie solche Änderungen intern?"}
{"ts": "144:42", "speaker": "E", "text": "Über unseren internen Confluence-Space, speziell das Helios-Change-Log. Zusätzlich wird jede größere Anpassung als RFC dokumentiert – wie gesagt, RFC-1321 war genau so ein Fall – und mit dem Architecture Review Board diskutiert."}
{"ts": "144:51", "speaker": "I", "text": "Gab es im Rahmen der Multi-Region-Strategie noch weitere Entscheidungen, die durch solche Evidenz getrieben waren?"}
{"ts": "144:56", "speaker": "E", "text": "Ja, wir haben etwa den BLAST_RADIUS beim Region-Failover getestet. In RB-DR-001 ist beschrieben, dass wir nur die minimal nötigen Topics nach Region B spiegeln, um Kosten zu sparen. Monitoring aus unserem Chaos-Testlauf CT-HEL-09 hat gezeigt, dass wir damit die RTO von 12 auf 8 Minuten senken konnten."}
{"ts": "145:07", "speaker": "I", "text": "Das klingt nach einem bewussten Trade-off. Gab es Risiken, die Sie dafür in Kauf genommen haben?"}
{"ts": "145:11", "speaker": "E", "text": "Natürlich. Der Hauptpunkt ist, dass bei einem Totalausfall einer Region für nicht-spiegelpflichtige Streams ein Datenverlust von bis zu 5 Minuten entstehen kann. Das ist im SLA als Ausnahme definiert und wurde mit den Fachbereichen abgestimmt."}
{"ts": "145:20", "speaker": "I", "text": "Und wie halten Sie das Team auf Stand, wenn solche Ausnahmen greifen?"}
{"ts": "145:24", "speaker": "E", "text": "Wir haben dafür eine Notification-Policy in PagerDuty, die bei Auslösung eines RB-DR-001-Events automatisch eine interne Statusseite aktualisiert und einen Slack-Channel notifiziert. Das ist auch in Runbook RB-COM-017 dokumentiert."}
{"ts": "145:33", "speaker": "I", "text": "Wie schätzen Sie die Nachhaltigkeit dieser Architektur für die nächsten Skalierungsstufen ein?"}
{"ts": "145:38", "speaker": "E", "text": "Ich denke, wir sind gut aufgestellt. Die Kombination aus adaptiver Partitionierung, selektiver Multi-Region-Spiegelung und klaren Runbooks erlaubt uns, weitere 18–24 Monate zu wachsen, bevor wir über ein Redesign nachdenken müssen."}
{"ts": "146:00", "speaker": "I", "text": "Könnten Sie bitte noch einmal erläutern, wie im Helios Datalake in der Scale-Phase die Kafka-Ingestion mit den Batch Loads zusammenspielt?"}
{"ts": "146:05", "speaker": "E", "text": "Ja, klar. Wir haben ein hybrides Processing-Pattern: Echtzeit-Streams aus Kafka fließen direkt in unsere Raw-Schicht in Snowflake über einen dedizierten Connector, während Batch Loads, die wir z.B. aus Legacy-DBs ziehen, nach RFC-1287 partitioniert werden. Diese Partitionierungsstrategie sorgt dafür, dass die Daten kompatibel zu den Streaming-Partitionen sind, sodass dbt-Modelle beides konsistent zusammenführen können."}
{"ts": "146:16", "speaker": "I", "text": "Und wie genau wird diese Konsistenz in der Praxis überprüft?"}
{"ts": "146:19", "speaker": "E", "text": "Wir haben einen Validierungs-Job in Airflow, der nach jedem Load-Lauf stichprobenartig Hashes der Kafka- und Batch-Datensätze vergleicht. Das ist im Runbook RB-VAL-019 beschrieben, und es gibt automatische Tickets im System, wenn Abweichungen über 0,1% auftreten."}
{"ts": "146:30", "speaker": "I", "text": "Interessant. Gab es in letzter Zeit Fälle, wo dieser Mechanismus ausgelöst wurde?"}
{"ts": "146:33", "speaker": "E", "text": "Ja, im März hatten wir ein Incident-Ticket INC-HEL-443, da war die Abweichung bei 0,27%. Ursache war ein verspäteter Kafka-Partition Commit, der wegen einer Region-Latenz im Multi-Region-Setup verzögert wurde."}
{"ts": "146:44", "speaker": "I", "text": "Wie haben Sie auf diese Latenz reagiert?"}
{"ts": "146:47", "speaker": "E", "text": "Wir haben kurzfristig die Commit-Intervalle angepasst und die Retry-Policy aus RB-ING-042 aktiviert. Langfristig planen wir, den Cross-Region-Replication-Mechanismus zu optimieren, um den BLAST_RADIUS zu minimieren."}
{"ts": "146:58", "speaker": "I", "text": "Das bringt mich zu den RTO/RPO-Zielen – wie setzen Sie diese im Kontext der Multi-Region-Architektur praktisch um?"}
{"ts": "147:02", "speaker": "E", "text": "Unser RTO liegt bei 30 Minuten, RPO bei 5 Minuten. Getestet wird das quartalsweise mit dem Titan DR Runbook RB-DR-001. Wir simulieren den Ausfall einer Region, leiten den Traffic auf die sekundäre Region um und messen, wie schnell die Pipelines wieder stabil laufen."}
{"ts": "147:14", "speaker": "I", "text": "Gab es beim letzten Test besondere Erkenntnisse?"}
{"ts": "147:17", "speaker": "E", "text": "Ja, wir haben festgestellt, dass der Failover für Kafka-Streams nur 4 Minuten dauerte, aber die dbt-Modelle 12 Minuten brauchten, um wieder im Sync zu sein. Das Monitoring-Log HEL-MON-202304 zeigt, dass ein paar Long-Running Queries in Snowflake während des Switches blockierten."}
{"ts": "147:30", "speaker": "I", "text": "Wie sind Sie mit dieser Erkenntnis umgegangen?"}
{"ts": "147:33", "speaker": "E", "text": "Wir haben die Query Timeouts angepasst und ein Preemptive Abort Script in den Deployment-Playbooks ergänzt. Damit konnten wir im letzten Test die Sync-Zeit auf 6 Minuten reduzieren, was unsere SLA deutlich entlastet."}
{"ts": "147:44", "speaker": "I", "text": "Das klingt nach einem bewussten Trade-off zwischen Performance, Stabilität und Kosten. Wie haben Sie diesen intern dokumentiert?"}
{"ts": "147:48", "speaker": "E", "text": "Wir haben einen RFC-1422 erstellt, der die Monitoring-Daten, die Testresultate und die Kostenanalyse nach POL-FIN-007 enthält. So konnten wir transparent darstellen, warum wir zusätzliche Compute-Kosten akzeptieren, um die RTO-Ziele sicher einzuhalten."}
{"ts": "148:00", "speaker": "I", "text": "Sie hatten vorhin angedeutet, dass Sie im Rahmen der Scale-Phase schon spezielle Query-Optimierungen in Snowflake umgesetzt haben. Können Sie das bitte etwas detaillierter erläutern?"}
{"ts": "148:15", "speaker": "E", "text": "Ja, wir haben unter anderem Materialized Views für die am häufigsten abgefragten Aggregationen eingeführt und gleichzeitig das Clustering von großen Tabellen angepasst. Speziell die Event-Streams aus Kafka werden nach dem Write-Stage in Snowflake so reorganisiert, dass die Partitionierung besser zu den Zeitfenstern unserer Batch-Loads passt. Das hat die Scan-Kosten pro Query um etwa 18 % gesenkt."}
{"ts": "148:38", "speaker": "I", "text": "Haben Sie dazu auch interne Benchmarks gefahren, um die Verbesserungen zu validieren?"}
{"ts": "148:47", "speaker": "E", "text": "Ja, wir nutzen dafür ein internes Tool namens 'QueryProbe'. Es misst die Ausführungszeiten vor und nach Änderungen, basierend auf einer repräsentativen Query-Suite. Im Ticket HEL-PERF-221 haben wir z. B. dokumentiert, wie der Median-Laufzeitwert von 3,2 auf 2,6 Sekunden gefallen ist."}
{"ts": "149:12", "speaker": "I", "text": "Und wie fließen diese Optimierungen in Ihre Kostenkontroll-Strategie gemäß POL-FIN-007 ein?"}
{"ts": "149:24", "speaker": "E", "text": "POL-FIN-007 definiert Schwellenwerte für monatliche Compute-Kosten pro Business Unit. Die Performance-Optimierungen senken nicht nur die Abfragezeit, sondern reduzieren auch die Credits, die Snowflake pro Scan verbraucht. Wir haben ein Monitoring-Alert aufgesetzt, das bei 85 % des Budgets einen Review triggert – so konnten wir im letzten Quartal 12 % unter Budget bleiben."}
{"ts": "149:48", "speaker": "I", "text": "Wie stellen Sie dabei sicher, dass diese Maßnahmen nicht die Datenaktualität oder Resilienz beeinträchtigen?"}
{"ts": "150:00", "speaker": "E", "text": "Das ist immer ein Trade-off. Wir fahren jede wesentliche Optimierung zunächst in der Staging-Region und prüfen per Regressionstests, ob die Latenz der ELT-Pipelines innerhalb der SLOs bleibt. Zusätzlich simulieren wir in einer Schattenumgebung Failover gemäß Runbook RB-ING-042, um zu sehen, ob das veränderte Clustering im Ernstfall schnell genug repliziert wird."}
{"ts": "150:28", "speaker": "I", "text": "Apropos Failover: Können Sie ein Beispiel geben, wie RB-ING-042 in einer realen Situation angewendet wurde?"}
{"ts": "150:40", "speaker": "E", "text": "Letzten Monat hatten wir in Region eu-central-1 eine Netzwerkanomalie, die den Kafka-Consumer-Lag stark erhöhte. RB-ING-042 beschreibt Schritt für Schritt, wie wir den Traffic kurzfristig auf unsere us-east-1-Cluster umleiten. Durch die vorbereiteten Terraform-Skripte konnten wir in 14 Minuten umschwenken und blieben unter dem definierten RTO von 20 Minuten."}
{"ts": "151:08", "speaker": "I", "text": "Das klingt nach einer engen Verzahnung mit Ihrer Multi-Region-Architektur. Wie koordinieren Sie das mit den Disaster-Recovery-Vorgaben aus RB-DR-001?"}
{"ts": "151:20", "speaker": "E", "text": "RB-DR-001 ist unser Master-Runbook für Titan DR-Szenarien. Es legt fest, wie Datenreplikation, DNS-Umschaltungen und User-Failover zusammenspielen. RB-ING-042 ist darin als Modul integriert, sodass bei einem großflächigen Ausfall automatisch auch die Ingestion-Failover-Sequenz getriggert wird. Wir testen das quartalsweise in sogenannten 'DR-Gamedays'."}
{"ts": "151:46", "speaker": "I", "text": "Gab es in diesen Tests auch Situationen, in denen Sie Ihre ursprünglichen Annahmen revidieren mussten?"}
{"ts": "151:55", "speaker": "E", "text": "Ja, in einem Test im März zeigte das Monitoring, dass die Cross-Region-Replication-Latenz bei sehr hohen Lastspitzen um 35 % anstieg. Das war höher als unsere RPO-Vorgabe erlaubte. Basierend auf diesen Evidenzen aus dem Metrik-Set HEL-MON-DR-07 haben wir zusätzliche Bandbreitenreserven gebucht und den Replikationsplan angepasst."}
{"ts": "152:22", "speaker": "I", "text": "Wie dokumentieren Sie solche Anpassungen, um sicherzustellen, dass alle Teams informiert sind?"}
{"ts": "152:33", "speaker": "E", "text": "Wir erstellen für jede signifikante Anpassung ein RFC-Dokument – in diesem Fall RFC-1329 – mit einer Zusammenfassung der Metriken, den identifizierten Risiken und den geplanten Änderungen. Dieses wird im internen Confluence veröffentlicht und im wöchentlichen Architekturrat vorgestellt, damit alle Stakeholder Feedback geben können."}
{"ts": "152:00", "speaker": "I", "text": "Wir hatten vorhin ja schon die grundsätzlichen Skalierungsmaßnahmen angesprochen. Mich würde jetzt interessieren, wie genau Sie die Partitionierungslogik für steigendes Datenvolumen angepasst haben."}
{"ts": "152:15", "speaker": "E", "text": "Ja, also wir haben im Zuge von RFC-1287 die Batch-Partitionierung auf stündliche Buckets umgestellt, statt wie zuvor täglich. Das erlaubt uns, parallelere Loads in Snowflake zu fahren und gleichzeitig die Latenz für near-real-time Reports zu senken."}
{"ts": "152:36", "speaker": "I", "text": "Und diese Anpassung, hat die sich direkt auch auf die Kafka-Ingestion ausgewirkt?"}
{"ts": "152:45", "speaker": "E", "text": "Indirekt schon, denn die Consumer-Gruppen, die wir für die Ingestion nutzen, committen Offsets jetzt ebenfalls in kleineren Intervallen. Damit sinkt das Risiko, bei einem Failover viel nachladen zu müssen, was laut RB-ING-042 ja kritisch ist."}
{"ts": "153:05", "speaker": "I", "text": "Verstehe. Gab es dadurch auch Kostenimplikationen, im Rahmen von POL-FIN-007?"}
{"ts": "153:14", "speaker": "E", "text": "Ja, die stündlichen Loads führen zu mehr, aber kleineren Snowflake-Compute-Jobs. Wir haben das mit Auto-Suspend kombiniert, sodass die Warehouses in Pausen sofort runterfahren. Laut unserem letzten POL-FIN-007 Audit hat das die Compute-Kosten um etwa 12% gesenkt."}
{"ts": "153:35", "speaker": "I", "text": "Gab es dabei Engpässe oder vielleicht auch unerwartete Nebenwirkungen?"}
{"ts": "153:44", "speaker": "E", "text": "Ja, ein Nebeneffekt war, dass einige Downstream-dbt-Modelle häufiger getriggert wurden, was wiederum die Transformation-SLAs beeinflusst hat. Wir mussten die CI-Pipeline so anpassen, dass nur betroffene Modelle neu gebaut werden."}
{"ts": "154:05", "speaker": "I", "text": "Wie testen Sie solche Änderungen vor dem Rollout in Produktion?"}
{"ts": "154:13", "speaker": "E", "text": "Wir fahren Smoke-Tests in einer isolierten Staging-Region, die eine Kopie der kritischen Streams enthält. Zusätzlich simulieren wir mit einem Replay-Tool aus Ticket T-HEL-443 einen Failover, wie es auch in RB-DR-001 beschrieben ist, um RTO und RPO zu validieren."}
{"ts": "154:36", "speaker": "I", "text": "Und wie sahen die letzten Tests in Bezug auf die RTO/RPO-Werte aus?"}
{"ts": "154:44", "speaker": "E", "text": "Beim letzten Durchlauf lagen wir bei einer RTO von 7 Minuten und einem RPO von unter 30 Sekunden für kritische Topics. Das entspricht den Zielwerten aus unserem SLA-Dokument DL-SLA-2022-09."}
{"ts": "155:02", "speaker": "I", "text": "Gab es bei der Multi-Region-Strategie besondere Trade-offs?"}
{"ts": "155:10", "speaker": "E", "text": "Ja, wir mussten uns zwischen vollständiger aktiver Aktiver-Setups und aktiven/passiven Replikas entscheiden. Aufgrund der Kosten und der Ergebnisse aus Monitoring-Report MR-HEL-58 haben wir uns für aktiv/passiv entschieden, um den BLAST_RADIUS zu minimieren und trotzdem Budgetgrenzen einzuhalten."}
{"ts": "155:33", "speaker": "I", "text": "Wie kommunizieren Sie solche Entscheidungen im Team?"}
{"ts": "155:41", "speaker": "E", "text": "Wir verfassen ein kurzes Decision Record im Confluence, verlinken die relevanten RFCs und Monitoring-Grafiken, und markieren es im Change-Log. So können alle Entwickler später nachvollziehen, warum wir uns auf Basis der Evidenz so entschieden haben."}
{"ts": "160:00", "speaker": "I", "text": "Können Sie bitte noch einmal genauer erklären, wie im Helios Datalake die Kafka-Ingestion mit den Batch-Partitionierungen zusammenspielt?"}
{"ts": "160:05", "speaker": "E", "text": "Ja, gern. Wir haben Kafka-Topics für die wichtigsten Domains, z.B. 'orders' und 'inventory'. Diese werden in Microbatches von jeweils 15 Minuten geschnitten, bevor sie in Snowflake geladen werden. Der Batch-Prozess nutzt dabei die Partitionierungsregeln aus RFC-1287 – das heißt wir partitionieren primär nach Event-Zeitstempel und sekundär nach Region, um spätere Joins im dbt zu optimieren."}
{"ts": "160:17", "speaker": "I", "text": "Und diese Partitionierungsstrategie, wie wirkt die sich auf die ELT-Pipeline aus?"}
{"ts": "160:21", "speaker": "E", "text": "Sie sorgt dafür, dass die ELT-Jobs in Snowflake effizienter scannen, weil sie nur die relevanten Microbatches laden. In der dbt-Build-Phase greifen wir dann gezielt auf die Partitionen zu, die im letzten Processing-Window aktualisiert wurden. Das reduziert die Kosten für Compute, was auch in Einklang mit POL-FIN-007 steht."}
{"ts": "160:35", "speaker": "I", "text": "Wie gehen Sie mit Änderungen an dbt-Modellen um?"}
{"ts": "160:39", "speaker": "E", "text": "Das läuft über einen Staging-Branch in GitLab. Jeder Merge Request triggert einen Testlauf in unserer Pre-Prod-Umgebung. Wir nutzen ein eigenes Runbook RB-DBT-013, das die Tests, Schema-Validierungen und Performance-Benchmarks beschreibt, bevor wir in Produktion deployen."}
{"ts": "160:53", "speaker": "I", "text": "Gibt es da einen direkten Zusammenhang zwischen Kafka-Ingestion und der dbt-Deployment-Pipeline?"}
{"ts": "160:57", "speaker": "E", "text": "Ja, indirekt. Die Ingestion liefert die Rohdaten in einer Form, die wir in der dbt-Entwicklung simulieren können. Über einen Replay-Mechanismus in der Pre-Prod ziehen wir ausgewählte Kafka-Batches in das Test-Schema. So sehen wir früh, ob z.B. ein neues Feld in 'orders' unerwartete Auswirkungen auf unsere Transformationsmodelle hat."}
{"ts": "161:13", "speaker": "I", "text": "Interessant. Können Sie ein Beispiel nennen, wo so ein Replay ein Problem aufgedeckt hat?"}
{"ts": "161:17", "speaker": "E", "text": "Ja, im Ticket HEL-QA-774. Da war ein Null-Wert im Feld 'customer_segment', der in einem dbt-Join zu einer Vervielfachung der Zeilen führte. Wir konnten das im Replay feststellen und vor dem Go-Live eine COALESCE-Logik implementieren."}
{"ts": "161:32", "speaker": "I", "text": "Welche Rolle spielt Snowflake-Optimierung hier noch?"}
{"ts": "161:36", "speaker": "E", "text": "Snowflake-Cluster-Größen passen wir dynamisch an. Batch-Loads aus Kafka triggern via Task-Scheduler ein Scale-up auf ein Medium-Warehouse, und nach der dbt-Transformation wird wieder heruntergefahren. Das ist in RB-ING-042 in einem separaten Abschnitt dokumentiert, obwohl es ursprünglich für Failover gedacht war."}
{"ts": "161:52", "speaker": "I", "text": "Nutzen Sie diese Mechanismen auch im Multi-Region-Betrieb?"}
{"ts": "161:56", "speaker": "E", "text": "Teilweise. In der EU-Region läuft die Kafka-Ingestion identisch wie in den USA, aber die Partitionierungsregeln haben wir leicht angepasst, weil dort die Spitzenlast anders verteilt ist. Die Runbooks enthalten pro Region eine Tabelle mit den optimalen Batch-Größen und Warehouse-Settings."}
{"ts": "162:09", "speaker": "I", "text": "Wie stellen Sie sicher, dass diese regionalen Anpassungen nicht zu Inkonsistenzen führen?"}
{"ts": "162:13", "speaker": "E", "text": "Das regeln wir über Cross-Region-Checks, bei denen wir Hash-Summen der transformierten Tabellen vergleichen. Wenn Abweichungen auftreten, greift ein Alert-Workflow aus RB-DR-001, der dann je nach Schweregrad einen manuellen Review erzwingt."}
{"ts": "161:30", "speaker": "I", "text": "Könnten Sie bitte noch einmal konkret schildern, wie die Kafka-Ingestion mit den Batch-Partitionierungen nach RFC-1287 verzahnt ist?"}
{"ts": "161:37", "speaker": "E", "text": "Ja, klar. Die Kafka-Ingestion läuft bei uns über den Helios Connector Layer, der Topic-basierte Streams in S3 Stage-Buckets schreibt. Die Batch-Partitionierung greift dort an, indem sie die Daten nach dem in RFC-1287 definierten Zeit- und Region-Key-Schema aufteilt. So können wir später im dbt-Modeling gezielt nur die relevanten Partitionen verarbeiten."}
{"ts": "161:49", "speaker": "I", "text": "Und wie geht es von dort in den dbt-Deploy-Flow über?"}
{"ts": "161:55", "speaker": "E", "text": "Sobald die Partitionierung abgeschlossen ist, wird über unseren Orchestrator 'AuroraFlow' ein dbt Run-Job getriggert. Dieser Job zieht die Metadaten der neuen Partitionen, updated die Incremental Models in der Snowflake-Schicht und deployt sie nach einem Canary-Testlauf gemäß unserem internen Runbook RB-DBT-015."}
{"ts": "162:08", "speaker": "I", "text": "Das heißt, Sie haben eine Art Multi-Hop-Verkettung zwischen Ingestion, Transformation und Snowflake-Performanceoptimierung?"}
{"ts": "162:14", "speaker": "E", "text": "Ganz genau. Die erste Hop ist der Kafka-zu-S3 Batch Export, der zweite Hop ist das dbt-Inkremental-Update, und der dritte ist die Snowflake-Cluster-Skalierung. Letzterer wird durch AuroraFlow gesteuert, basierend auf den Query-Histories, sodass wir die Warehouse-Größe kurzzeitig hochskalieren können, um die neuen Partitionen ohne Latenzspitzen zu laden."}
{"ts": "162:29", "speaker": "I", "text": "Gab es bei diesem Multi-Hop-Setup besondere Herausforderungen?"}
{"ts": "162:34", "speaker": "E", "text": "Ja, anfangs hatten wir Probleme mit Out-of-Order Events im Kafka-Stream. Das führte zu Partitionen, die nicht vollständig waren und dbt-Jobs fehlschlugen. Wir haben daraufhin in RFC-1287 einen Puffer-Mechanismus eingeführt, der erst nach einem Watermark-Checkpoint von T+5 Minuten die Partition als 'complete' markiert."}
{"ts": "162:50", "speaker": "I", "text": "Wie wirkt sich das auf Ihre SLAs aus?"}
{"ts": "162:55", "speaker": "E", "text": "Unsere SLA für Batch-Verfügbarkeit liegt bei D+0 02:00 UTC. Mit dem Puffer schaffen wir das trotzdem, weil wir die dbt-Läufe parallelisieren. Wir haben das in SLA-Dokument SL-HEL-003 festgehalten und mit Monitoring-Alerts in Grafana hinterlegt."}
{"ts": "163:08", "speaker": "I", "text": "Können Sie ein Beispiel geben, wie Sie Snowflake-Performance anpassen, wenn ein größerer Kafka-Burst kommt?"}
{"ts": "163:14", "speaker": "E", "text": "Wenn unser Kafka-Lag-Monitor (Ticket MON-2345) meldet, dass mehr als 500k Messages im Rückstand sind, skaliert AuroraFlow den Snowflake-Warehouse-Typ von 'XSMALL' auf 'LARGE' für maximal 90 Minuten. Danach fahren wir automatisch zurück, um Kosten zu sparen. Dieser Mechanismus ist in Runbook RB-SF-009 dokumentiert."}
{"ts": "163:28", "speaker": "I", "text": "Und wie testen Sie, dass dieser Ablauf in der Produktion stabil funktioniert?"}
{"ts": "163:33", "speaker": "E", "text": "Wir haben monatliche DRY-Runs, bei denen wir synthetische Kafka-Bursts einspeisen. Die Ergebnisse vergleichen wir mit unseren Baseline-Query-Zeiten in Snowflake. Bei Abweichungen >10% gibt es einen Post-Mortem-Review laut Prozess PM-HEL-004."}
{"ts": "163:46", "speaker": "I", "text": "Gab es schon mal einen Fall, wo dieser Prozess fehlschlug?"}
{"ts": "163:51", "speaker": "E", "text": "Einmal, ja. Beim Test im März hat der Canary-Deploy der dbt-Modelle einen fehlerhaften SQL-Ref in Stage 'customer_orders' nicht erkannt, wodurch das Warehouse unnötig lange auf LARGE lief. Wir haben daraus gelernt und in RB-DBT-015 einen zusätzlichen Schema-Check vor Deploy eingeführt."}
{"ts": "162:06", "speaker": "I", "text": "Lassen Sie uns noch einmal auf die Integration eingehen: Wie greifen die Kafka-Batch-Ingestion und der dbt-Deploy-Flow konkret ineinander, gerade wenn wir an RFC-1287 denken?"}
{"ts": "162:10", "speaker": "E", "text": "Also, RFC-1287 definiert ja die Zeitfenster und die Segmentgrößen für die Batch-Partitionierung. Wir haben das so umgesetzt, dass der Kafka-Connector die Daten bis zu einer definierten Watermark sammelt, dann werden sie in S3 als Stage-Buckets abgelegt, und genau an diesem Punkt triggert unser dbt-Deploy-Flow die Transformation, die schon auf die erwartete Partition-Granularität abgestimmt ist."}
{"ts": "162:18", "speaker": "I", "text": "Okay, das heißt, die Batch-Partitionen sind schon voraggregiert, bevor dbt ins Spiel kommt?"}
{"ts": "162:21", "speaker": "E", "text": "Genau, dadurch minimieren wir die Anzahl der Snowflake COPY-Befehle pro Batch. Das bringt uns, äh, stabilere Ladezeiten und reduziert die Transient Storage Kosten. Das war eine der Lessons Learned aus den Performance-Tests in der Scale-Phase."}
{"ts": "162:28", "speaker": "I", "text": "Und wie stellen Sie sicher, dass Änderungen an dbt-Modellen nicht die Partitionierungslogik brechen?"}
{"ts": "162:33", "speaker": "E", "text": "Wir haben da zwei Schutzmechanismen: Erstens, ein Pre-Deploy-Hook im CI, der die Partition Keys im Modell mit den in RFC-1287 beschriebenen Keys vergleicht. Zweitens, eine Canary-Execution im Staging-Schema in Snowflake, bevor das Ganze nach 'prod' geht."}
{"ts": "162:39", "speaker": "I", "text": "Interessant. Gibt es dazu ein internes Ticket oder Runbook?"}
{"ts": "162:43", "speaker": "E", "text": "Ja, das ist in RB-ING-042 dokumentiert, Abschnitt 3.2. Da steht auch, wie im Fehlerfall ein Fallback auf die vorherige dbt-Version gemacht wird."}
{"ts": "162:50", "speaker": "I", "text": "Wie wirkt sich diese Kopplung zwischen Ingestion und Transformation auf die Gesamtperformance in Snowflake aus?"}
{"ts": "162:54", "speaker": "E", "text": "Positiv, in den meisten Fällen. Durch konsistente Partitionierung erreichen wir eine bessere Cluster Key Nutzung in Snowflake, weniger Data Skew und damit schnelleres Querying. Das Monitoring zeigt, dass die mediane Ladezeit um 18 % gesunken ist seit wir den Flow so fahren."}
{"ts": "163:02", "speaker": "I", "text": "Gab es dafür spezielle Tests oder Benchmarks?"}
{"ts": "163:05", "speaker": "E", "text": "Ja, wir haben im Ticket PERF-HEL-221 eine Vergleichsmessung mit und ohne RFC-1287-Konformität gefahren. Das war ein multi-hop Test: von Kafka-Ingestion über dbt bis hin zu analytischen Queries in Snowflake."}
{"ts": "163:11", "speaker": "I", "text": "Und wie fließen solche Ergebnisse in zukünftige Architekturentscheidungen ein?"}
{"ts": "163:15", "speaker": "E", "text": "Wir dokumentieren sie in den RFCs, und aktualisieren auch die SLO-Dokumentation. Zum Beispiel haben wir das Ladezeit-SLO auf 15 Minuten pro Batch verschärft, weil wir gesehen haben, dass es machbar ist."}
{"ts": "163:21", "speaker": "I", "text": "Gibt es Grenzen dieser Integration, die vielleicht in der nächsten Phase adressiert werden müssen?"}
{"ts": "163:26", "speaker": "E", "text": "Ja, das Thema Multi-Region-Consistency. Momentan ist der dbt-Deploy-Flow nur für die primäre Region optimiert. Für DR-Szenarien müssten wir RB-DR-001 enger verzahnen, damit auch im Failover die Partitionierungs- und Transformations-Logik konsistent bleibt."}
{"ts": "164:42", "speaker": "I", "text": "Können wir jetzt noch einmal konkret auf die Multi-Region-Architektur eingehen? Mich interessiert, welche Maßnahmen Sie implementiert haben, um den BLAST_RADIUS bei einem Ausfall zu minimieren."}
{"ts": "165:00", "speaker": "E", "text": "Ja, klar. Wir haben in der Scale-Phase das Deployment in zwei AWS-Regionen parallel hochgezogen, mit asynchroner Replikation der Snowflake-Staging- und Core-Schemas. Das minimiert den BLAST_RADIUS, weil ein Ausfall in einer Region nicht den gesamten Ingest stoppt. Zusätzlich nutzen wir im Kafka-Cluster MirrorMaker 2, um Topics zu spiegeln, sodass bei Region-Failover keine Daten verloren gehen."}
{"ts": "165:22", "speaker": "I", "text": "Und wie greifen Sie dabei auf Runbooks wie RB-DR-001 zurück?"}
{"ts": "165:33", "speaker": "E", "text": "RB-DR-001, das Titan DR Runbook, beschreibt genau die Schritte für Failover-Tests und den manuellen Switch. Wir haben das angepasst, um spezifische Checks für Helios einzubauen, z. B. Validierung der dbt-Schemas nach Failover. Der Prozess ist dokumentiert im internen Confluence und wird quartalsweise geübt, zuletzt am 14.03., Ticket DR-TEST-22."}
{"ts": "165:57", "speaker": "I", "text": "Welche RTO- und RPO-Ziele haben Sie für Helios definiert, und wie testen Sie diese praktisch?"}
{"ts": "166:10", "speaker": "E", "text": "RTO liegt bei 45 Minuten, RPO bei maximal 5 Minuten Datenverlust. Wir testen das, indem wir im Staging-Cluster gezielt einen Knoten isolieren und den Replikations-Lag messen. Letzter Test zeigte, dass wir im Schnitt 3 Minuten hinterherhinken, was innerhalb des SLA ist."}
{"ts": "166:34", "speaker": "I", "text": "Gab es dabei Trade-offs zwischen Kosten und Resilienz?"}
{"ts": "166:45", "speaker": "E", "text": "Absolut. Die zusätzliche Region verdoppelt die Storage-Kosten. Wir haben abgewogen, ob wir nur kritische Topics replizieren, aber aufgrund der regulatorischen Anforderungen haben wir uns für Vollreplikation entschieden. Monitoring-Daten aus Grafana-Board HEL-PERF-07 haben gezeigt, dass die zusätzliche Latenz tolerierbar ist, sodass wir Performance nicht opfern mussten."}
{"ts": "167:12", "speaker": "I", "text": "Wie dokumentieren Sie solche Entscheidungen intern?"}
{"ts": "167:22", "speaker": "E", "text": "Über RFCs, konkret hier RFC-1332. Darin sind Szenarien, Kostenanalysen und Performance-Metriken enthalten. Wir hängen auch die Testprotokolle aus den Runbooks an, damit die Entscheidung nachvollziehbar bleibt."}
{"ts": "167:43", "speaker": "I", "text": "Gab es Risiken, die Sie im Zusammenhang mit der Multi-Region-Strategie identifiziert haben?"}
{"ts": "167:54", "speaker": "E", "text": "Ja, ein Risiko ist inkonsistente Transformationen, wenn während des Failovers ein dbt-Deploy läuft. Wir mitigieren das, indem wir Deploy-Fenster in beiden Regionen synchronisieren und vor Failover einen Freeze setzen. Das ist in RB-ING-042 als Sonderfall dokumentiert."}
{"ts": "168:15", "speaker": "I", "text": "Können Sie ein Beispiel nennen, wo Monitoring-Evidenz eine Architektur-Entscheidung beeinflusst hat?"}
{"ts": "168:27", "speaker": "E", "text": "Ja, beim Thema Batch-Partitionierung gemäß RFC-1287 hatten wir ursprünglich eine 1-Stunden-Partition. Monitoring via Snowflake Query History zeigte aber, dass dadurch das Clustering ineffizient wurde. Wir sind dann auf 30-Minuten-Partitionen umgestiegen, was die Query-Laufzeiten um 18% verbessert hat. Das war ein klarer Performance-Gewinn, den wir gegen die leicht höheren Compute-Kosten abgewogen haben."}
{"ts": "168:55", "speaker": "I", "text": "Wie gehen Sie in solchen Fällen mit der Kommunikation an Stakeholder um?"}
{"ts": "169:07", "speaker": "E", "text": "Wir nutzen einen wöchentlichen Helios-Architektur-Review-Call, in dem wir Änderungen vorstellen. Für größere Anpassungen, wie die Partitionierungsänderung, gab es ein spezielles Briefing mit Finance, um POL-FIN-007-konforme Kostenfreigabe zu sichern. Außerdem wird das Change-Log im DataOps-Channel im Chat aktualisiert."}
{"ts": "172:42", "speaker": "I", "text": "Sie hatten vorhin die Integration von Kafka und den Batch-Loads erwähnt – können Sie mir bitte nochmal den aktuellen Stand beschreiben, besonders in Bezug auf die Scale-Phase des Projekts?"}
{"ts": "172:51", "speaker": "E", "text": "Ja, klar. In der Scale-Phase haben wir mittlerweile drei dedizierte Kafka-Cluster, die über MirrorMaker2 replizieren. Die Batch-Loads laufen in Snowflake in sogenannten Micro-Batches, die wir über Airflow orchestrieren. Ziel ist, dass wir die SLOs – 95 % der Datenverfügbarkeit innerhalb von 15 Minuten nach Event-Ankunft – auch bei wachsendem Datenvolumen halten."}
{"ts": "173:15", "speaker": "I", "text": "Und wie genau wird sichergestellt, dass diese 15-Minuten-Vorgabe eingehalten wird?"}
{"ts": "173:20", "speaker": "E", "text": "Wir haben dafür in Runbook RB-ING-042 ein Alerting beschrieben, das auf Lags in den Kafka-Consumer-Gruppen prüft und gleichzeitig die Snowflake-Task-Latenzen überwacht. Falls der Wert über 10 Minuten steigt, wird automatisch eine Skalierungs-Policy laut Ticket OPS-4256 ausgelöst, um die Snowpipe-Queues zu vergrößern."}
{"ts": "173:47", "speaker": "I", "text": "Interessant. Können Sie die Rolle von RFC-1287 in diesem Kontext erläutern?"}
{"ts": "173:53", "speaker": "E", "text": "RFC-1287 war entscheidend für die Partitionierungsstrategie. Wir haben dort festgelegt, dass Batch-Partitionen nach Zeit und Quell-ID getrennt werden, um Hotspots in Snowflake zu vermeiden. Das wirkt sich direkt auf den dbt-Deploy-Flow aus, weil die Modelle jetzt auf kleinere, gezieltere Partitionen zugreifen und Transformationen parallelisieren."}
{"ts": "174:21", "speaker": "I", "text": "Das heißt, die dbt-Deployments profitieren unmittelbar von der Kafka-Partitionierung?"}
{"ts": "174:27", "speaker": "E", "text": "Genau. Durch die feiner geschnittenen Partitionen können wir im dbt-Flow inkrementelle Modelle effizienter fahren. Vorher mussten große Tabellen komplett transformiert werden; jetzt reichen kleinere Slices, was die Snowflake-Performance laut unseren Benchmarks in TST-ENV-12 um ca. 30 % verbessert hat."}
{"ts": "174:54", "speaker": "I", "text": "Gab es Herausforderungen bei der Integration dieser Änderungen in die Produktion?"}
{"ts": "175:00", "speaker": "E", "text": "Ja, insbesondere mussten wir den Deploy-Flow im CI/CD anpassen. Wir haben Feature-Branches in GitLab, die über ein Staging-Schema in Snowflake laufen. Erst nach erfolgreichem Data-Quality-Check – definiert in Runbook RB-QA-015 – wird in PROD gemerged. Das Testing der Partitionierung war tricky, weil Replays aus Kafka in einer isolierten Umgebung gefahren werden mussten."}
{"ts": "175:29", "speaker": "I", "text": "Wie wirkt sich das Ganze auf die Betriebskosten aus, vor allem im Hinblick auf POL-FIN-007?"}
{"ts": "175:35", "speaker": "E", "text": "POL-FIN-007 schreibt vor, dass wir Compute-Kosten pro Query unter 0,05 € halten sollen. Mit der Partitionierung und inkrementellen dbt-Runs haben wir die Query-Laufzeiten reduziert. Das senkt nicht nur die Latenz, sondern auch die Credits, die Snowflake verbraucht. Monatlich sehen wir dadurch rund 18 % Kostenersparnis."}
{"ts": "175:57", "speaker": "I", "text": "Gab es dafür spezielle Monitoring-Metriken oder Dashboards?"}
{"ts": "176:02", "speaker": "E", "text": "Ja, wir haben in Grafana ein Dashboard 'DLK-Perf-Overview', das sowohl Kafka-Lags, Snowflake-Query-Performance als auch dbt-Run-Zeiten darstellt. Zusätzlich speichern wir Metriken in Prometheus, was uns erlaubt, historische Trends zu analysieren – hilfreich bei Kapazitätsplanungen."}
{"ts": "176:25", "speaker": "I", "text": "Wie sieht es mit Ausfallsicherheit aus, speziell bei der Kopplung von Kafka und Snowflake?"}
{"ts": "176:31", "speaker": "E", "text": "Wir setzen auf eine Multi-Region-Replikation von Kafka und nutzen Snowflake Failover-Mechanismen. Im Runbook RB-DR-001 ist definiert, dass wir bei Ausfall einer Region den BLAST_RADIUS auf maximal 20 % der Streams begrenzen, indem wir nur nicht-kritische Topics kurzzeitig pausieren. Diese Strategie wurde im DR-Test TST-DR-09 erfolgreich verifiziert."}
{"ts": "180:42", "speaker": "I", "text": "Könnten Sie jetzt bitte erläutern, wie diese Optimierungen konkret in der Produktionsumgebung ausgerollt wurden? Mich interessiert besonders der Ablauf zwischen Staging und Live-Deployment."}
{"ts": "181:05", "speaker": "E", "text": "Klar, also wir nutzen einen zweistufigen Deploy-Prozess. Zuerst werden die dbt-Modelle in unserer Helios-Staging-Umgebung gegen einen anonymisierten Daten-Snapshot getestet. Danach erfolgt ein Canary-Deploy in Produktions-Snowflake-Cluster C2, der nur 10% der Kafka-Batches konsumiert."}
{"ts": "181:33", "speaker": "I", "text": "Und wie lange läuft so ein Canary-Durchlauf im Schnitt, bevor Sie auf Vollauslastung gehen?"}
{"ts": "181:47", "speaker": "E", "text": "In der Regel 45 Minuten. Wir haben in RB-DEP-019 festgelegt, dass mindestens zwei Batch-Zyklen ohne SLA-Verletzung durchlaufen müssen, bevor wir den Switch vollziehen."}
{"ts": "182:12", "speaker": "I", "text": "Stichwort SLA: Welche konkreten Metriken überwachen Sie hier im Canary-Test?"}
{"ts": "182:26", "speaker": "E", "text": "Wir checken Batch-Latency < 300 Sekunden, Fehlerquote < 0.5% und Snowflake-Query-Response-Zeit unter 1.2 Sekunden für kritische Modelle. Zusätzlich auch den Kafka Lag, um keine Rückstaus zu riskieren."}
{"ts": "182:54", "speaker": "I", "text": "Verstehe. Wie greifen hier die Multi-Region-Aspekte ein, falls während des Canary-Deploy ein Failover nötig wäre?"}
{"ts": "183:11", "speaker": "E", "text": "Da kommt RB-DR-001 ins Spiel, unser Titan DR Runbook. Bei Auffälligkeiten im Primär-Cluster in Frankfurt wird automatisch der Region-Consumer in Stockholm aktiviert. Das Canary-Segment wird dann dort gespiegelt, um die Vergleichbarkeit zu sichern."}
{"ts": "183:39", "speaker": "I", "text": "Das heißt, auch die Batch-Partitionierung nach RFC-1287 wird in beiden Regionen identisch gefahren?"}
{"ts": "183:52", "speaker": "E", "text": "Genau. Die Partition Keys basieren auf Event-Timestamps und Tenant-ID, das ist deterministisch und daher problemlos replizierbar. Die Snowflake-Staging-Tabellen sind via Zero-Copy Cloning synchron."}
{"ts": "184:18", "speaker": "I", "text": "Gab es schon Fälle, in denen ein Canary zwar in Region A fehlerfrei lief, aber in Region B Auffälligkeiten zeigte?"}
{"ts": "184:34", "speaker": "E", "text": "Ja, einmal im März. In Stockholm hatten wir höhere Latenzen aufgrund einer unerwarteten Netzwerk-Drossel zwischen Kafka-Broker und Snowpipe-Endpunkt. Ticket OP-HEL-442 dokumentiert das, wir mussten temporär den Batch-Size-Parameter anpassen."}
{"ts": "185:01", "speaker": "I", "text": "Interessant. Das klingt nach einem Trade-off zwischen Stabilität und Performance. Wie haben Sie die Entscheidung damals gefällt?"}
{"ts": "185:15", "speaker": "E", "text": "Wir haben uns für Stabilität entschieden, basierend auf Evidence aus dem Monitoring – konkret den Metriken aus unserem Helios-Observability-Dashboard. Das war auch Thema im Architektur-Review zu RFC-1302, wo wir dokumentierten, dass 5% Performance-Verlust akzeptabel ist, wenn dadurch der BLAST_RADIUS minimiert wird."}
{"ts": "185:46", "speaker": "I", "text": "Und wie kommunizieren Sie solche Anpassungen intern?"}
{"ts": "186:00", "speaker": "E", "text": "Über unseren wöchentlichen Architecture-Sync und ein Confluence-Page-Update. Kritische Punkte gehen zusätzlich als Change-Notice in den Slack-Channel #helios-ops, damit alle on-call Engineers sofort informiert sind."}
{"ts": "189:42", "speaker": "I", "text": "Sie hatten vorhin kurz die SLOs erwähnt – können Sie im Kontext der Scale-Phase des Helios Datalake noch einmal konkret sagen, welche Zielwerte jetzt gelten und wie Sie diese technisch absichern?"}
{"ts": "189:55", "speaker": "E", "text": "Ja, die Kern-SLOs sind Latenz < 5 Minuten für Streaming-Streams via Kafka und < 45 Minuten für Batch-Loads. Wir sichern das über ein kombiniertes Monitoring mit Prometheus-Alerts und Snowflake Query-Profiler. Zusätzlich haben wir im Runbook RB-MON-015 Schwellenwerte definiert, die automatisch ein Incident-Ticket im System ITSM-HEL-queue erzeugen."}
{"ts": "190:18", "speaker": "I", "text": "Und wie verhält sich das zu den SLAs, die Sie mit den Fachbereichen vereinbart haben?"}
{"ts": "190:28", "speaker": "E", "text": "Die SLAs sind etwas großzügiger – 10 Minuten für Streams und eine Stunde für Batch. Das gibt uns Puffer. Wir haben auch ein internen Leitfaden, der besagt: wenn SLO verfehlt, sofort Ursachenanalyse starten, auch wenn SLA noch nicht verletzt ist."}
{"ts": "190:46", "speaker": "I", "text": "Verstanden. Jetzt zum Zusammenspiel: Wie greifen bei Ihnen Kafka-Ingestion und Batch-Partitionierung ineinander, gerade mit Blick auf RFC-1287?"}
{"ts": "190:59", "speaker": "E", "text": "RFC-1287 beschreibt, dass wir eingehende Kafka-Themen in Mikro-Batches von 15 Minuten aggregieren, die dann als Parquet in S3 landen. Von dort übernimmt der Batch-Loader diese Files und partitioniert nach event_time und source_system. Das bedeutet: Streaming- und Batch-Pfade teilen sich den ersten Stage-Bucket, wodurch wir konsistente Partition Keys haben."}
{"ts": "191:25", "speaker": "I", "text": "Das klingt nach einer engen Kopplung. Hat das Auswirkungen auf die dbt-Modellierung?"}
{"ts": "191:36", "speaker": "E", "text": "Ja, genau. Die dbt-Modelle erwarten diese Partition Keys. Unsere staging-Modelle nutzen Macros aus dem internen Package 'helios_utils', um die Partitionierung konsistent zu interpretieren. Änderungen daran müssen wir über das CI/CD-Deploy-Flow testen, sonst riskieren wir inkonsistente History-Tables."}
{"ts": "191:57", "speaker": "I", "text": "Wie testen Sie diese Änderungen vor dem Rollout in Produktion?"}
{"ts": "192:06", "speaker": "E", "text": "Wir nutzen eine isolierte Snowflake-Dev-Database und Replay-Sets aus S3 (gesampelte Kafka-Batches). Das Test-Runbook RB-DBT-021 beschreibt, wie wir diese Sätze in die Dev-Pipeline einspeisen, um sowohl Performance als auch Korrektheit zu validieren."}
{"ts": "192:26", "speaker": "I", "text": "Lassen Sie uns auf die Skalierung kommen: welche Maßnahmen haben Sie zuletzt implementiert, um Performance bei wachsendem Datenvolumen zu sichern?"}
{"ts": "192:38", "speaker": "E", "text": "Wir haben die Snowflake Virtual Warehouses für Transformations-Jobs auf 'Medium' skaliert und Auto-suspend auf 60 s reduziert, um Kosten zu sparen. Außerdem wurden in dbt inkrementelle Modelle optimiert, indem wir nur geänderte Partitionen seit dem letzten Run laden."}
{"ts": "192:58", "speaker": "I", "text": "Apropos Kosten: POL-FIN-007 hatten Sie mal erwähnt – wie wirkt sich das konkret auf Ihre Entscheidungen aus?"}
{"ts": "193:09", "speaker": "E", "text": "POL-FIN-007 ist unsere interne Cloud-Kostenrichtlinie. Sie zwingt uns, jede Skalierungsmaßnahme gegen monatliche Budgetgrenzen zu bewerten. Beispiel: wir haben ein Ticket COST-HEL-442 dokumentiert, in dem wir einen Warehouse-Upsize ablehnten, weil die geplante Query-Optimierung den gleichen Effekt billiger erbrachte."}
{"ts": "193:33", "speaker": "I", "text": "Und in puncto Ausfallsicherheit – welche Runbooks wie RB-ING-042 kommen da zum Einsatz?"}
{"ts": "193:44", "speaker": "E", "text": "RB-ING-042 ist unser Failover-Runbook für den Ingestion Layer. Wenn die primäre Kafka-Cluster-Region ausfällt, schalten wir per DNS den Connector auf die Backup-Region. Die Snowflake-Seite erkennt das automatisch, weil beide auf denselben External Stage-Name zeigen. Wir testen das halbjährlich im Rahmen von DR-Übungen laut RB-DR-001."}
{"ts": "197:42", "speaker": "I", "text": "Sie hatten vorhin die Snowflake-Performance angesprochen. Mich würde interessieren, wie Sie beim Helios Datalake aktuell den Ressourcenverbrauch im Auge behalten, insbesondere bei steigender Datenlast."}
{"ts": "197:55", "speaker": "E", "text": "Ja, wir nutzen dafür eine Kombination aus dem internen Monitoring-Stack auf Basis Prometheus/Grafana und den Snowflake Resource Monitors. Wir haben im Runbook RB-OPS-019 genau definiert, wie wir bei Überschreiten von 80 % des Credits-Limits reagieren, um die SLOs nicht zu gefährden."}
{"ts": "198:14", "speaker": "I", "text": "Gibt es da automatische Trigger oder ist das ein manueller Prozess?"}
{"ts": "198:20", "speaker": "E", "text": "Teilweise automatisiert: Ab 75 % gibt es Alerts im Slack-Channel #helios-ops, und ab 85 % triggert ein Airflow-DAG den Scale-Down weniger kritischer Virtual Warehouses, wie in POL-FIN-007 zur Kostenkontrolle festgelegt."}
{"ts": "198:39", "speaker": "I", "text": "Im Zusammenhang mit Multi-Region, wie stellen Sie sicher, dass ein Ausfall in einer Region nicht den gesamten Betrieb lahmlegt?"}
{"ts": "198:47", "speaker": "E", "text": "Wir fahren aktiv-active zwischen Frankfurt und Stockholm. Die Datenströme werden über Kafka MirrorMaker gespiegelt, und Snowflake-Replication ist mit 15 Minuten Intervall konfiguriert. Das minimiert den BLAST_RADIUS, wie in RB-DR-001 beschrieben."}
{"ts": "199:08", "speaker": "I", "text": "Und welche RTO- und RPO-Ziele sind dabei gesetzt?"}
{"ts": "199:14", "speaker": "E", "text": "RTO liegt bei 60 Minuten, RPO bei 10 Minuten. Getestet wird das quartalsweise über DR-Drills; der letzte Drill, Ticket HEL-DR-2024-03, konnte die Umschaltung in 42 Minuten abschließen."}
{"ts": "199:33", "speaker": "I", "text": "Beeindruckend. Gab es bei diesen Tests auch Erkenntnisse für Architekturentscheidungen?"}
{"ts": "199:40", "speaker": "E", "text": "Ja, wir haben festgestellt, dass die Initialisierung der dbt-Modelle nach Failover der Flaschenhals war. Daraufhin haben wir ein Pre-warmed Warehouse für kritische Modelle eingerichtet, was zwar etwas höhere Standby-Kosten bedeutet, aber die RTO deutlich verkürzt."}
{"ts": "199:59", "speaker": "I", "text": "Das klingt nach einem klaren Trade-off zwischen Kosten und Resilienz."}
{"ts": "200:04", "speaker": "E", "text": "Genau, und wir haben diesen Trade-off in RFC-1452 dokumentiert. Dort sind Monitoring-Daten aus den DR-Drills als Evidenz angehängt, um die Entscheidung nachvollziehbar zu machen."}
{"ts": "200:18", "speaker": "I", "text": "Wie kommunizieren Sie solche Entscheidungen im Team?"}
{"ts": "200:23", "speaker": "E", "text": "Über unser Confluence-Portal mit einem dedizierten 'Architecture Decisions'-Bereich. Zusätzlich gibt es ein wöchentliches Engineering-Forum, in dem RFCs wie 1452 vorgestellt und diskutiert werden."}
{"ts": "200:39", "speaker": "I", "text": "Gab es schon mal Fälle, wo Sie eine solche Entscheidung wieder zurückgerollt haben?"}
{"ts": "200:45", "speaker": "E", "text": "Ja, in RFC-1390 hatten wir eine aggressive Snowflake-Auto-Suspend-Policy eingeführt, um Kosten zu sparen. Nach zwei Monaten zeigten die SLO-Reports aber, dass die Query-Latenz signifikant anstieg. Wir haben das dann auf Basis dieser Evidenz wieder revidiert."}
{"ts": "205:42", "speaker": "I", "text": "Können Sie mir bitte genauer schildern, wie die Ausfallsicherheitsarchitektur im Helios Datalake aktuell umgesetzt ist, vor allem im Multi-Region-Betrieb?"}
{"ts": "205:55", "speaker": "E", "text": "Ja, aktuell fahren wir ein aktives Active-Standby-Setup über zwei AWS-Regionen. Die primäre Region hostet die produktiven Snowflake-Cluster und die Kafka-Broker, während die sekundäre Region synchronisierte Metadaten und warmgehaltene Compute-Cluster vorhält."}
{"ts": "206:14", "speaker": "E", "text": "Für den Failover nutzen wir das Runbook RB-DR-001 aus der Titan-DR-Dokumentation. Das beschreibt exakt die Schritte, um den Traffic innerhalb von maximal 15 Minuten umzuswitchen, was unserem RTO von 20 Minuten entspricht."}
{"ts": "206:34", "speaker": "I", "text": "Und wie stellen Sie sicher, dass das RPO-Ziel eingehalten wird?"}
{"ts": "206:39", "speaker": "E", "text": "Das RPO ist auf 5 Minuten festgelegt. Wir erreichen das, indem wir die Kafka-Topics mit 3-facher Replikation und Cross-Region-Mirroring betreiben und die Snowflake-Staging-Tabellen via Continuous Data Protection replizieren."}
{"ts": "206:58", "speaker": "I", "text": "Gab es in den letzten Monaten Tests oder Simulationen, um diese Werte zu verifizieren?"}
{"ts": "207:03", "speaker": "E", "text": "Ja, im letzten Quartal haben wir ein geplantes DR-Drill-Event durchgeführt. Ticket DR-TEST-042 dokumentiert, dass wir in 14 Minuten umgestellt hatten und die maximale Datenlücke bei 3 Minuten lag."}
{"ts": "207:22", "speaker": "I", "text": "Wie beeinflussen solche DR-Übungen Ihre operativen Kosten und Performance-Metriken?"}
{"ts": "207:27", "speaker": "E", "text": "Kurzfristig steigen die Kosten, weil wir während der Übung beide Regionen voll aktiv fahren. Langfristig sparen wir aber, da wir dadurch Schwachstellen früh erkennen und z.B. die Cluster-Größen optimieren konnten, was gemäß POL-FIN-007 zu einer Reduktion von 12% bei den Compute-Kosten führte."}
{"ts": "207:48", "speaker": "I", "text": "Gab es Trade-offs zwischen Kostenoptimierung und Resilienz, die besonders schwierig waren?"}
{"ts": "207:53", "speaker": "E", "text": "Definitiv. Ein Beispiel: Wir hatten überlegt, auf asynchrone Replikation mit längeren Intervallen zu gehen, um Kosten zu sparen. Die Monitoring-Daten aus unserem Snowflake Query Profiling (siehe Report QP-2023-11) zeigten aber, dass selbst kurze Verzögerungen zu SLA-Verletzungen bei Echtzeit-Dashboards führen würden."}
{"ts": "208:15", "speaker": "E", "text": "Wir haben daher beschlossen, bei der synchronen Variante zu bleiben und stattdessen die Transformationen im dbt so zu optimieren, dass weniger Compute-Zeit verbraucht wird."}
{"ts": "208:28", "speaker": "I", "text": "Wie werden solche Entscheidungen intern dokumentiert und kommuniziert?"}
{"ts": "208:33", "speaker": "E", "text": "Das läuft bei uns über RFCs, in diesem Fall RFC-1332. Darin sind die Kostenanalysen, Performance-Messungen und die Ergebnisse der DR-Übung verlinkt, sodass jeder im Engineering-Team die Entscheidungsgrundlage nachvollziehen kann."}
{"ts": "208:50", "speaker": "I", "text": "Gibt es Lessons Learned aus diesen Entscheidungen, die Sie für die nächste Scale-Phase einplanen?"}
{"ts": "208:55", "speaker": "E", "text": "Ja, wir wollen zukünftig die DR-Tests noch enger mit Lasttests koppeln, um gleichzeitig Resilienz und Performance zu validieren. Außerdem planen wir, RB-ING-042 für Failover-Szenarien zu erweitern, damit auch die Ingestion-Pipelines automatisch umschalten können."}
{"ts": "214:42", "speaker": "I", "text": "Lassen Sie uns jetzt den Bogen zu den Performance-Optimierungen schlagen – welche konkreten Maßnahmen haben Sie in der Scale-Phase zusätzlich eingeführt?"}
{"ts": "214:54", "speaker": "E", "text": "Wir haben im Zuge der Scale-Phase ein mehrstufiges Caching auf Query-Ebene aktiviert, kombiniert mit dem Einsatz von Snowflake-Query-Acceleration-Services. Dazu kam eine Anpassung der Warehouse-Größen nach dem in POL-FIN-007 definierten Kostenrahmen."}
{"ts": "215:10", "speaker": "I", "text": "Gab es dabei Herausforderungen, gerade im Hinblick auf die Einhaltung der SLOs?"}
{"ts": "215:17", "speaker": "E", "text": "Ja, vor allem beim Umschalten der Warehouse-Größen mussten wir sicherstellen, dass die Latenz im Rahmen der 150 ms SLO für kritische Reports blieb. Das haben wir durch einen gestaffelten Rollout gemäß Runbook RB-ING-042 abgesichert."}
{"ts": "215:33", "speaker": "I", "text": "Könnten Sie erläutern, wie RB-ING-042 konkret bei einem Failover Szenario zum Einsatz kommt?"}
{"ts": "215:41", "speaker": "E", "text": "RB-ING-042 beschreibt die Umschaltung der Ingestion-Jobs von der Primärregion auf die Sekundärregion. Wir haben darin Checklisten für Kafka-Topic-Replikation und Snowflake-Stage-Switches, inklusive Testqueries, um Datenkonsistenz zu verifizieren."}
{"ts": "215:59", "speaker": "I", "text": "Wie spielt das mit den Titan DR Runbooks RB-DR-001 zusammen?"}
{"ts": "216:07", "speaker": "E", "text": "RB-DR-001 deckt den vollständigen Disaster Recovery ab, also nicht nur Ingestion, sondern auch dbt-Modelle und Metadaten-Replikation. Im Ernstfall wird RB-ING-042 als Modul innerhalb von RB-DR-001 aufgerufen, um den BLAST_RADIUS zu minimieren."}
{"ts": "216:25", "speaker": "I", "text": "Sie sprachen den BLAST_RADIUS an – welche konkreten Strategien setzen Sie zur Reduktion ein?"}
{"ts": "216:33", "speaker": "E", "text": "Segmentierung der Pipelines nach kritischen und nicht-kritischen Themen, isolierte Kafka-Cluster für Hochrisiko-Feeds, und wir nutzen für Snowflake separate Virtual Warehouses, um Lastspitzen in einem Bereich nicht auf alle Abfragen wirken zu lassen."}
{"ts": "216:51", "speaker": "I", "text": "Können Sie ein Beispiel geben, wo Sie Monitoring-Daten als Entscheidungsgrundlage für einen Trade-off verwendet haben?"}
{"ts": "217:00", "speaker": "E", "text": "Ja, Ticket OPS-472 zeigte, dass ein geplanter Merge-Job 40 % der Warehouse-Kapazität blockierte. Auf Basis der Metriken aus unserem Prometheus-Cluster entschieden wir, den Job in kleinere Batches zu splitten, auch wenn das geringfügig längere Gesamtlaufzeiten bedeutete."}
{"ts": "217:18", "speaker": "I", "text": "Wie dokumentieren Sie solche Entscheidungen intern?"}
{"ts": "217:25", "speaker": "E", "text": "Wir erstellen dafür RFCs, in diesem Fall RFC-1310, mit Abschnitt 'Evidence' für die Monitoring-Daten, den getesteten Alternativen und den finalen Beschluss. Diese werden im Confluence der Helios Datalake-Unit veröffentlicht."}
{"ts": "217:41", "speaker": "I", "text": "Und wie testen Sie die RTO/RPO Ziele praktisch?"}
{"ts": "217:48", "speaker": "E", "text": "Wir fahren vierteljährlich DR-Drills, bei denen wir simuliert die Primärregion abschalten. Dann messen wir die Zeit bis zur Wiederherstellung (RTO, derzeit 25 Minuten Ziel) und die Datenlücke (RPO, Ziel < 5 Sekunden) anhand der Kafka-Offsets und Snowflake-Timestamps."}
{"ts": "222:42", "speaker": "I", "text": "Könnten Sie bitte näher ausführen, wie Sie in der Scale-Phase des Helios Datalake speziell die Multi-Region-Anforderungen umgesetzt haben?"}
{"ts": "223:01", "speaker": "E", "text": "Ja, gern. Wir haben ab Q2 ein Dual-Region-Setup implementiert, basierend auf der Snowflake-Replication-Engine und einem asynchronen Kafka-Mirror. Das Ziel war, gemäß RB-DR-001 einen BLAST_RADIUS kleiner 20% der Gesamt-Workloads zu erzielen."}
{"ts": "223:24", "speaker": "I", "text": "Und wie stellen Sie sicher, dass diese Replikation auch im Fehlerfall wirklich greift?"}
{"ts": "223:39", "speaker": "E", "text": "Wir fahren vierteljährliche DR-Drills. Dabei triggern wir gezielt Failover-Playbooks aus dem Runbook RB-DR-001. Ein Test im März, Ticket HEL-DR-77, zeigte, dass wir unser RTO von 45 Minuten und RPO von 10 Minuten einhalten konnten."}
{"ts": "224:02", "speaker": "I", "text": "Gab es bei diesen Drills unerwartete Nebeneffekte?"}
{"ts": "224:15", "speaker": "E", "text": "Ja, beim ersten Durchlauf haben wir festgestellt, dass dbt-Modelle im Read-Replica-Kontext nicht sofort auf den neuesten Stand waren. Das lag an einer verzögerten CI/CD-Pipeline im GitOps-Flow, siehe RFC-1324 als Nachtrag zu RFC-1287."}
{"ts": "224:39", "speaker": "I", "text": "Interessant. Wie haben Sie diese Verzögerung adressiert?"}
{"ts": "224:53", "speaker": "E", "text": "Wir haben ein Hook-Skript in der Deployment-Pipeline ergänzt, das bei Failover-Events automatisch ein `dbt run --full-refresh` in der Zielregion triggert. Das ist jetzt Bestandteil von RB-ING-042, Abschnitt 'Cross-Region Recovery'."}
{"ts": "225:16", "speaker": "I", "text": "Stichwort RB-ING-042: Nutzen Sie dieses Runbook auch für Performance-Optimierungen oder ausschließlich für Failover?"}
{"ts": "225:30", "speaker": "E", "text": "Primär für Ingestion- und Failover-Szenarien, aber wir haben ergänzende Playbooks für Performance-Tuning erstellt. Zum Beispiel LB-OPT-009, das auf Basis der SLO-Metriken aus POL-FIN-007 entscheidet, ob wir Clusternodes temporär hochskalieren."}
{"ts": "225:54", "speaker": "I", "text": "Das klingt nach einem bewussten Trade-off zwischen Kosten und Performance."}
{"ts": "226:05", "speaker": "E", "text": "Genau. Wir haben in Februar eine Entscheidungsmatrix entwickelt (siehe DEC-HEL-5) mit drei Stufen: 'Cost-Save', 'Balanced', 'Performance-Boost'. Monitoring-Daten aus Snowflake Query History und Kafka Lag Metrics fließen automatisch in diese Matrix ein."}
{"ts": "226:32", "speaker": "I", "text": "Gab es ein konkretes Beispiel, wo diese Matrix den Ausschlag gegeben hat?"}
{"ts": "226:45", "speaker": "E", "text": "Ja, im April hatten wir an einem Montagmorgen eine anomale Latenzspitze in Batch-Loads aus der Region West. Die Matrix identifizierte sofort 'Performance-Boost' als Modus, wir skalierten um +3 Nodes für 6 Stunden, Ticket HEL-PERF-221 dokumentiert das."}
{"ts": "227:10", "speaker": "I", "text": "Wie kommunizieren Sie solche kurzfristigen Entscheidungen im Team?"}
{"ts": "227:23", "speaker": "E", "text": "Über unseren internen RFC-Kanal im ChatOps. Wir erzeugen eine Kurzform-RFC, z.B. RFC-1411, die den Auslöser, getroffene Maßnahme, erwartete Auswirkung und Nachreview enthält. Das ist inzwischen fester Bestandteil unserer Governance."}
{"ts": "230:42", "speaker": "I", "text": "Lassen Sie uns noch etwas tiefer in die Dokumentationsprozesse eintauchen – wie halten Sie die Lessons Learned aus den letzten DR-Tests fest?"}
{"ts": "231:05", "speaker": "E", "text": "Wir nutzen dafür ein internes Confluence-Space namens DR-Knowledgebase. Nach jedem Test, der zumeist über RB-DR-001 orchestriert wird, erstellen wir ein Post-Mortem-Dokument mit Referenz zur Ticket-ID, z.B. HEL-DR-2024-03. Darin stehen die Root Causes, die Recovery-Steps und die Anpassungen an Runbooks."}
{"ts": "231:36", "speaker": "I", "text": "Und diese Post-Mortems fließen dann auch in die Architekturentscheidungen zurück?"}
{"ts": "231:50", "speaker": "E", "text": "Ja, genau. Beispiel: Beim letzten Multi-Region-Failover haben wir festgestellt, dass die Kafka-Replikation zwischen FRA und DUB knapp 40 Sekunden hinterherhing. Diese Erkenntnis führte zu einer Erweiterung von RFC-1322, wo wir die Replikations-Buffer und die Netzwerk-QoS-Parameter angepasst haben."}
{"ts": "232:20", "speaker": "I", "text": "Interessant. Gab es bei der Umsetzung dieser Anpassungen Kostenimplikationen?"}
{"ts": "232:35", "speaker": "E", "text": "Ja, definitiv. Wir mussten mehr Throughput bei unserem Cloud-Provider buchen, was laut POL-FIN-007 eine Budgetprüfung auslöste. Wir haben das abgewogen gegen die SLA-Anforderung von maximal 60 Sekunden Latenz im Failover-Fall."}
{"ts": "233:02", "speaker": "I", "text": "Wie dokumentieren Sie solche Trade-offs formell?"}
{"ts": "233:15", "speaker": "E", "text": "Das landet in einem Architecture Decision Record (ADR), der an das ursprüngliche RFC angehängt wird. Wir referenzieren darin Metriken aus unserem Monitoring-Stack, z.B. Snowflake Query Latency und Kafka Lag, um die Entscheidung zu untermauern."}
{"ts": "233:44", "speaker": "I", "text": "Gab es auch Szenarien, in denen Sie bewusst Performance zugunsten von Kosten oder Resilienz reduziert haben?"}
{"ts": "233:58", "speaker": "E", "text": "Ja, bei den nächtlichen Batch-Loads. Laut RFC-1287 haben wir die Parallelität von 12 auf 8 Streams reduziert, um Compute Credits in Snowflake zu sparen. Das verlängert den Load um ca. 4 Minuten, liegt aber noch innerhalb des vereinbarten SLA-Fensters."}
{"ts": "234:26", "speaker": "I", "text": "Wie reagieren die Stakeholder auf solche Anpassungen?"}
{"ts": "234:40", "speaker": "E", "text": "Wir machen vorab ein Stakeholder-Review, bei dem wir mit Diagrammen aus dem Data Observability Tool zeigen, dass die Daten morgens trotzdem pünktlich für die BI-Reports verfügbar sind. Transparenz hilft, Akzeptanz zu schaffen."}
{"ts": "235:05", "speaker": "I", "text": "Letzte Frage: Gibt es aktuell Risiken, die Sie im Helios Datalake besonders im Blick behalten?"}
{"ts": "235:18", "speaker": "E", "text": "Ja, zwei Hauptpunkte: Zum einen die steigende Event-Rate im Kafka-Cluster, die uns perspektivisch zu einer Repartitionierung zwingen könnte, zum anderen regulatorische Anforderungen an die Datenaufbewahrung, die zusätzliche Storage-Kosten und Compliance-Checks nach sich ziehen."}
{"ts": "235:45", "speaker": "I", "text": "Und wie mitigieren Sie diese Risiken?"}
{"ts": "236:02", "speaker": "E", "text": "Für die Event-Rate haben wir einen Proof-of-Concept in Staging laufen, der dynamisch Partitions auf Basis der Lag-Metriken anpasst. Für Compliance arbeiten wir eng mit unserem Governance-Team zusammen und haben RB-COM-015 eingeführt, ein Runbook zur revisionssicheren Archivierung in S3-kompatiblen Buckets."}
{"ts": "238:42", "speaker": "I", "text": "Lassen Sie uns noch einmal konkret auf die Kostenoptimierung zurückkommen – wie haben Sie POL-FIN-007 in der Praxis umgesetzt, ohne die Performance zu beeinträchtigen?"}
{"ts": "238:55", "speaker": "E", "text": "Wir haben im Rahmen von POL-FIN-007 die Query-Kostenanalyse in Snowflake aktiviert und Alerts gesetzt, die ab 80% des geplanten Budgets anschlagen. Parallel dazu haben wir die dbt-Modelle so angepasst, dass aggregierte Tabellen nur bei Bedarf neu berechnet werden."}
{"ts": "239:18", "speaker": "I", "text": "Gab es dabei Konflikte mit den definierten SLOs, zum Beispiel bei der Latenz der ELT-Pipelines?"}
{"ts": "239:28", "speaker": "E", "text": "Ja, teilweise. In vier Fällen mussten wir laut Ticket HEL-ING-547 kurzfristig Indexierungen zurücknehmen, was die Latenz um etwa 90 Sekunden erhöht hat. Wir haben das durch optimierte Micro-Batches in Kafka kompensiert."}
{"ts": "239:52", "speaker": "I", "text": "Interessant. Können Sie das Zusammenspiel dieser Micro-Batches mit der Batch-Partitionierung aus RFC-1287 erläutern?"}
{"ts": "240:07", "speaker": "E", "text": "Klar. Wir haben die in RFC-1287 empfohlene Tages-Partitionierung beibehalten, aber die Kafka-Ingestion so konfiguriert, dass innerhalb eines Tagesfensters 15-Minuten-Segmente gebildet werden. So konnten wir die Latenz reduzieren, ohne das Snowflake-Cluster zu überlasten."}
{"ts": "240:33", "speaker": "I", "text": "Und wie wird das in Ihrer Testumgebung abgesichert, bevor es in Produktion geht?"}
{"ts": "240:44", "speaker": "E", "text": "Wir nutzen eine Staging-Umgebung mit identischem dbt-Deployment-Prozess. Jede Änderung durchläuft automatisierte Tests anhand von Runbook RB-ING-042, inklusive Failover-Simulationen und Lasttests."}
{"ts": "241:06", "speaker": "I", "text": "Apropos Failover – wie binden Sie die Titan DR Runbooks wie RB-DR-001 in diese Tests ein?"}
{"ts": "241:17", "speaker": "E", "text": "RB-DR-001 definiert die Sequenz für einen Multi-Region-Failover. In unseren monatlichen DR-Drills simulieren wir den Ausfall der Primärregion und prüfen, ob die Kafka-Lags und Snowflake-Replicas innerhalb der RTO von 15 Minuten auf Stand sind."}
{"ts": "241:43", "speaker": "I", "text": "Und die RPO-Ziele – werden die im gleichen Drill überprüft?"}
{"ts": "241:53", "speaker": "E", "text": "Ja, wir validieren, dass maximal 30 Sekunden an Daten verloren gehen dürfen. Das erreichen wir durch synchrone Topic-Replikation zwischen den Regionen und zeitgleichen Batch-Loads."}
{"ts": "242:15", "speaker": "I", "text": "Gab es schon einmal eine Situation, in der Sie diese Ziele nicht einhalten konnten?"}
{"ts": "242:26", "speaker": "E", "text": "Einmal, im März, als eine Netzwerk-Latenz auftrat, die zu 75 Sekunden Datenverlust führte. Wir haben daraufhin gemäß Incident-Report HEL-INC-209 die Replikationspuffer vergrößert und das Monitoring verschärft."}
{"ts": "242:50", "speaker": "I", "text": "Wenn Sie auf die gesamte Skalierungsphase blicken – welche Trade-offs haben Sie am meisten beschäftigt?"}
{"ts": "243:02", "speaker": "E", "text": "Definitiv das Gleichgewicht zwischen Kosten und Resilienz. Evidence aus Monitoring-Dashboards (z.B. HEL-MON-88) zeigte, dass kleinere Cluster günstiger waren, aber in Peak-Zeiten drohten Timeouts. Letztlich haben wir uns für eine moderate Überprovisionierung entschieden, um die SLAs einzuhalten."}
{"ts": "246:02", "speaker": "I", "text": "Sie hatten vorhin erwähnt, dass die Multi-Region-Failover-Strategie auf RB-DR-001 basiert. Können Sie das noch einmal im praktischen Ablauf schildern?"}
{"ts": "246:17", "speaker": "E", "text": "Ja, also im Ernstfall folgen wir einem klar definierten Ablauf im Titan DR Runbook RB-DR-001. Zuerst wird der BLAST_RADIUS evaluiert, basierend auf den letzten Heartbeat-Metriken aus Region West und East. Wenn der Ausfall in einer Region über 3 Minuten Persistenz zeigt, triggern wir ein kontrolliertes Failover via unserem Orchestrator, der die Snowflake Reader Accounts in der sekundären Region aktiviert."}
{"ts": "246:44", "speaker": "I", "text": "Welche Rolle spielt dabei die Kafka-Ingestion? Muss die umkonfiguriert werden?"}
{"ts": "246:56", "speaker": "E", "text": "Teilweise, ja. Wir haben in RFC-1452 dokumentiert, dass Consumer-Gruppen für kritische Topics dual-registriert sind. Das heißt, beim Failover übernehmen automatisch die Consumer in der Zielregion. Die Batch-Ingestion verschiebt sich entsprechend, wir passen die Offsets an, um keine Duplikate in den Snowflake Staging Tables zu erzeugen."}
{"ts": "247:22", "speaker": "I", "text": "Und wie testen Sie diese Szenarien? Gibt es dafür festgelegte Intervalle?"}
{"ts": "247:35", "speaker": "E", "text": "Ja, wir führen vierteljährlich DR-Drills durch, die in unserem Monitoring als Ticketserie DR-TEST-0xx angelegt sind. Dabei simulieren wir den Ausfall einer Region und messen die RTO- und RPO-Ziele. Unser Ziel ist RTO < 10 Minuten und RPO < 1 Minute, das haben wir in den letzten drei Tests eingehalten."}
{"ts": "247:58", "speaker": "I", "text": "Gab es bei diesen Tests unerwartete Probleme, die zu Anpassungen geführt haben?"}
{"ts": "248:10", "speaker": "E", "text": "Beim Test DR-TEST-014 hatten wir z.B. eine Verzögerung in der dbt-Transformation, weil ein Modell auf eine UDF in Region West referenzierte, die noch nicht in East deployt war. Das hat uns veranlasst, in RFC-1520 festzuhalten, dass alle UDFs synchron in beiden Regionen bereitgestellt werden müssen."}
