{"ts": "00:00", "speaker": "I", "text": "Vielen Dank, dass Sie sich Zeit nehmen. Können Sie mir zu Beginn kurz beschreiben, wie Titan DR aktuell in Ihrer Umgebung eingebettet ist?"}
{"ts": "05:20", "speaker": "E", "text": "Klar. Titan DR ist momentan in unsere gesamte Service-Landschaft integriert, also nicht nur als isoliertes DR-System. Wir haben zwei aktive Regionen und eine kalte Standby-Region. Alle kritischen Microservices replizieren wir asynchron über unsere interne Event-Bus-Schicht, so dass wir im Drill oder Ernstfall innerhalb des RTO von 45 Minuten umschwenken können."}
{"ts": "10:05", "speaker": "I", "text": "Und welche spezifischen Aufgaben übernehmen Sie persönlich während der DR-Drills?"}
{"ts": "15:15", "speaker": "E", "text": "Ich bin als SRE für das Monitoring-Playbook zuständig. Konkret heißt das, ich starte die Checks aus RB-DR-001, koordiniere mit dem Netzwerk-Team die BGP-Umroutung und führe die Validierungsschritte im Runbook Abschnitt 3.4 durch. Außerdem logge ich alle Ereignisse in das Drill-Log, Ticket SYS-DR-7782."}
{"ts": "20:40", "speaker": "I", "text": "Wie interagieren Sie dabei mit dem Architekten im Notfall-Szenario?"}
{"ts": "26:00", "speaker": "E", "text": "Der Architekt gibt oft die Freigabe für kritische Umschaltpunkte. In RB-DR-001 ist festgelegt, dass ab Stufe Rot nur noch der Architekt entscheiden darf, ob wir die Datenebene in die Standby-Region hochfahren. Ich liefere ihm die Metriken aus den Health-Checks, z.B. Latenzverteilung und Fehlerquote aus dem Ingress-Layer."}
{"ts": "31:10", "speaker": "I", "text": "Wie oft greifen Sie auf das Runbook RB-DR-001 zurück, auch außerhalb von Drills?"}
{"ts": "36:30", "speaker": "E", "text": "Mindestens vierteljährlich, weil wir Simulationen fahren. Aber auch bei kleineren regionalen Störungen – im Herbst hatten wir eine Netzwerkpartition in Region West – nutze ich einzelne Kapitel wie 2.1 für DNS-Failover."}
{"ts": "41:45", "speaker": "I", "text": "Gab es schon Anpassungen am Runbook?"}
{"ts": "47:00", "speaker": "E", "text": "Ja, nach TEST-DR-2024-Q4 haben wir den Abschnitt zur Auth-Cluster-Synchronisierung erweitert. Wir merkten, dass das alte Protokoll zu viel Latenz erzeugte, wenn gleichzeitig der Messaging-Service neu verbunden wurde."}
{"ts": "52:20", "speaker": "I", "text": "Welche anderen Systeme müssen beim Failover berücksichtigt werden?"}
{"ts": "57:35", "speaker": "E", "text": "Neben Auth und Messaging ist vor allem unser Payment-Gateway kritisch. Die Multi-Region-Replikation greift hier auf ein separates Storage-Fabric-Projekt zu, das unter P-SF läuft. Das heißt, wenn wir im DR-Fall umschalten, müssen die Storage-Nodes synchron in den Read-Write-Modus wechseln, sonst riskieren wir Inkonsistenzen."}
{"ts": "63:00", "speaker": "I", "text": "Gab es schon Kettenreaktionen über mehrere Subsysteme hinweg?"}
{"ts": "68:15", "speaker": "E", "text": "Ja, bei TEST-DR-2025-Q1 hatten wir einen Fall, wo der Ausfall des Auth-Clusters den Messaging-Service blockierte, was wiederum die Service Discovery verzögerte. Das stand später als Lesson Learned LL-DR-25Q1-03 in Confluence."}
{"ts": "74:20", "speaker": "I", "text": "Welche Trade-offs mussten Sie bei der Multi-Region-Strategie eingehen?"}
{"ts": "90:00", "speaker": "E", "text": "Der größte Trade-off ist Kosten vs. Warmhaltung. Wir könnten die Standby-Region warm halten und damit das RTO auf 15 Minuten drücken, aber das würde die Betriebskosten um etwa 40 % erhöhen. Bisher haben wir uns dagegen entschieden, weil die Eintrittswahrscheinlichkeit für einen kompletten Regionalverlust laut Risikoanalyse RA-DR-17-2 bei unter 2 % pro Jahr liegt."}
{"ts": "90:00", "speaker": "I", "text": "Sie hatten vorhin den Ticketverlauf T-DR-4421 erwähnt. Können Sie mir genauer sagen, wie daraus eine Anpassung am Failover-Mechanismus resultierte?"}
{"ts": "90:12", "speaker": "E", "text": "Ja, also in T-DR-4421 hatten wir im Drill festgestellt, dass der Heartbeat zwischen Region West und Region Ost in einer sehr spezifischen Netzwerk-Topologie nicht sauber erkannt wurde. Das Runbook RB-DR-001, Schritt 4.2, ging davon aus, dass der Heartbeat-Check immer in unter 3 Sekunden reagiert. Wir mussten also eine Retry-Logik einbauen und die Zeitfenster in der Config anpassen."}
{"ts": "90:38", "speaker": "I", "text": "Das heißt, Sie haben die Retry-Logik fest ins Runbook aufgenommen?"}
{"ts": "90:45", "speaker": "E", "text": "Genau. Wir haben in der Revision RB-DR-001-v4.3 ergänzt: 'Falls HB nicht in 3s, aber < 8s reagiert, kein automatisches Failover triggern'. Das hat uns geholfen, unnötige Umschaltungen zu vermeiden, die wir in TEST-DR-2025-Q1 als Problem gesehen hatten."}
{"ts": "91:05", "speaker": "I", "text": "Gab es dabei auch Abstimmungen mit dem Architekturteam?"}
{"ts": "91:10", "speaker": "E", "text": "Ja, wir haben das in einem RFC-Meeting durchgesprochen, speziell weil der Messaging-Service im Failover-Fall kurzzeitig in einen Split-Brain-Zustand geraten kann. Das Architekturteam hat uns geraten, den RTO leicht zu erhöhen, um die Konsistenz zu sichern."}
{"ts": "91:28", "speaker": "I", "text": "Wie wurde diese RTO-Anpassung denn intern kommuniziert?"}
{"ts": "91:33", "speaker": "E", "text": "Wir haben das in der DR-Change-Mailingliste veröffentlicht und ins SLA-Dokument für Titan DR eingetragen. Dort steht jetzt: 'RTO ≤ 95s (vorher 80s)'. Das war ein bewusster Trade-off zwischen Geschwindigkeit und Datenintegrität."}
{"ts": "91:50", "speaker": "I", "text": "Gab es Widerstand gegen die Verlängerung des RTO?"}
{"ts": "91:55", "speaker": "E", "text": "Ein bisschen. Das Ops-Team von der Analytics-Plattform wollte eigentlich bei 80s bleiben, weil deren Batch-Jobs sonst länger hängen. Wir haben aber gemeinsam in einem GameDay-Szenario gezeigt, dass die 15s mehr keinen signifikanten Einfluss auf die Gesamt-Laufzeit haben, aber die Fehlerrate im Messaging um 40% senken."}
{"ts": "92:18", "speaker": "I", "text": "Können Sie dazu eine Zahl nennen, wie sich die Fehlerrate verändert hat?"}
{"ts": "92:23", "speaker": "E", "text": "Vor der Anpassung hatten wir im Drill TEST-DR-2025-Q1 eine Messaging-Fehlerrate von 7,8% im Failover-Zeitfenster, danach in TEST-DR-2025-Q2 nur noch 4,6%. Das ist in den Protokollen unter Log-ID DR-MSG-667 dokumentiert."}
{"ts": "92:41", "speaker": "I", "text": "Wie fließen solche Ergebnisse dann in Ihre Optimierungen ein?"}
{"ts": "92:45", "speaker": "E", "text": "Wir haben dafür eine Lessons-Learned-Tabelle im internen Wiki. Jeder Drill wird dort mit Metriken, Abweichungen vom Runbook und vorgeschlagenen Änderungen erfasst. Daraus entsteht dann die nächste Revision. Für das Messaging-Thema haben wir auch einen automatisierten Testfall in unser CI-Pipeline integriert."}
{"ts": "93:05", "speaker": "I", "text": "Letzte Frage zu diesem Punkt: Sehen Sie noch Risiken, die trotz der Anpassungen bestehen?"}
{"ts": "93:10", "speaker": "E", "text": "Ja, ein Restrisiko bleibt beim gleichzeitigen Ausfall von Auth-Cluster und Messaging-Service. Das Szenario haben wir noch nicht vollständig durchgetestet, weil es komplexe Cross-Region-Dependencies gibt. Wir planen dafür einen Spezial-Drill Q4/2025, Ticket T-DR-4875."}
{"ts": "98:00", "speaker": "I", "text": "Wenn wir noch etwas tiefer in die Lessons Learned aus TEST-DR-2025-Q1 einsteigen – gab es da spezifische Punkte, die im Nachgang in die Runbooks übernommen wurden?"}
{"ts": "98:10", "speaker": "E", "text": "Ja, auf jeden Fall. Wir haben gemerkt, dass die Schrittfolge im RB-DR-001 für das Umschalten des Auth-Clusters in Region Süd zu grob war. Wir haben deswegen eine neue Subsektion RB-DR-001.7 erstellt, die explizit die Sequenz für Credential Cache Purge beschreibt, weil es im Test einen 12-Minuten-Lag gab."}
{"ts": "98:35", "speaker": "I", "text": "Und wie haben Sie das intern kommuniziert, ging das über ein RFC oder eher ad hoc?"}
{"ts": "98:42", "speaker": "E", "text": "Wir haben ein internes RFC-Doc #RFC-DR-4419 aufgesetzt, das erst im Architektenkreis diskutiert und dann per Slack-Channel #dr-updates an alle SREs verteilt wurde. So konnten wir Feedback einarbeiten, bevor es ins offizielle Runbook floß."}
{"ts": "98:59", "speaker": "I", "text": "Gab es Widerstände bei den Anpassungen?"}
{"ts": "99:03", "speaker": "E", "text": "Minimal. Ein Kollege aus dem Messaging-Team meinte, dass die Purge-Sequenz in manchen Szenarien die Message Queue leeren könnte, wenn Tokens als Payload gesendet werden. Wir haben daraufhin eine Condition eingebaut, die den Purge nur nach erfolgreichem Failover-Flag auslöst."}
{"ts": "99:25", "speaker": "I", "text": "Das klingt nach einer zusätzlichen Abhängigkeit. Wird die irgendwo explizit dokumentiert?"}
{"ts": "99:32", "speaker": "E", "text": "Ja, wir haben im Abhängigkeits-Diagramm in Confluence ein neues Binding zwischen Auth-Cluster und Messaging-Service ergänzt. Das ist jetzt auch Teil der Checkliste in RB-DR-001.0 unter Pre-Failover Checks."}
{"ts": "99:51", "speaker": "I", "text": "Wenn wir die RTO/RPO Vorgaben betrachten – hat sich durch diese Änderung etwas an den Zielwerten verschoben?"}
{"ts": "99:59", "speaker": "E", "text": "Ja, geringfügig. RTO bleibt bei 15 Minuten für Auth, aber durch das zusätzliche Check-Skript sind wir im Schnitt 30-40 Sekunden länger, was jedoch im SLA P-DR-0156 noch abgedeckt ist. RPO ist unverändert, da wir keine Datenverluste verzeichnen."}
{"ts": "100:20", "speaker": "I", "text": "Wie wurde das im letzten GameDay verifiziert?"}
{"ts": "100:25", "speaker": "E", "text": "Wir haben im GameDay-Szenario 3B einen geplanten Ausfall des Auth-Primärclusters simuliert, das neue Purge-Skript eingespielt und die Zeit bis zur vollen Auth-Funktionalität gemessen. Die Ergebnisse sind im Testprotokoll TP-DR-2025-Q1-3B dokumentiert – alles im grünen Bereich."}
{"ts": "100:48", "speaker": "I", "text": "Gab es dabei noch andere, unerwartete Erkenntnisse?"}
{"ts": "100:53", "speaker": "E", "text": "Interessanterweise ja: Wir haben festgestellt, dass der Monitoring-Agent in Region West keine Events mehr an das zentrale Dashboard sendete, sobald der Auth-Failover aktiv war. Das war ein Bug im Agent 2.4.1, den wir via Bugticket BUG-DR-8826 priorisiert haben."}
{"ts": "101:15", "speaker": "I", "text": "Und wie lange hat die Behebung gedauert?"}
{"ts": "101:20", "speaker": "E", "text": "Etwa zwei Wochen, inklusive Regressionstests. Seitdem haben wir ein zusätzliches Pre-Check-Item in RB-DR-001.3, das den Agent-Status prüft, um das Risiko bei echten Incidents zu minimieren."}
{"ts": "114:00", "speaker": "I", "text": "Bevor wir abschließen, würde ich gern noch auf die Lessons Learned aus dem letzten Drill eingehen – konkret, wie Sie diese in die Vorbereitung für den nächsten Zyklus integriert haben."}
{"ts": "114:05", "speaker": "E", "text": "Klar, wir haben nach TEST-DR-2025-Q1 das Kapitel 4.2 im RB-DR-001 ergänzt – dort steht jetzt explizit drin, wie wir beim Auth-Cluster zuerst die Leader-Neuwahl forcieren, bevor wir Messaging-Failover triggern. Das basiert auf den Verzögerungen, die wir damals hatten."}
{"ts": "114:13", "speaker": "I", "text": "Und diese Änderung – ist die schon in einem realen Incident angewendet worden oder bisher nur im Test?"}
{"ts": "114:17", "speaker": "E", "text": "Nur im Test. Im Incident INC-DR-472 vom Februar hätten wir sie gebrauchen können, aber da war das Runbook noch nicht aktualisiert. Seitdem haben wir das als Pflichtschritt in der Einsatz-Runbook-App markiert."}
{"ts": "114:26", "speaker": "I", "text": "Gab es bei der Implementierung dieser Pflichtschritte technische Hürden?"}
{"ts": "114:31", "speaker": "E", "text": "Ja, wir mussten die Orchestrator-API um einen Check erweitern, der prüft, ob der neue Leader im Auth-Cluster gesund ist, bevor der Messaging-Service failovert. Sonst riskieren wir, dass Auth-Anfragen ins Leere laufen."}
{"ts": "114:40", "speaker": "I", "text": "Verstehe – und wie haben Sie diese Änderung dokumentiert, damit sie im Drillkontext auch reproduzierbar ist?"}
{"ts": "114:44", "speaker": "E", "text": "Wir haben im Confluence den Abschnitt mit Screenshots der Orchestrator-API und den neuen Health-Check-Pass/Fail-Kriterien ergänzt. Zusätzlich gibt es in der Runbook-App jetzt einen automatischen Loglink zu den Health-Check-Events."}
{"ts": "114:54", "speaker": "I", "text": "Wie sieht das Monitoring während eines Drills aus, wenn diese Checks laufen?"}
{"ts": "114:58", "speaker": "E", "text": "Während des Drills haben wir im Grafana ein spezielles DR-Dashboard, das Leader-Status, Latenzen und Fehlerraten aggregiert. Die Metriken sind mit der SLA-View verknüpft, sodass wir sofort sehen, ob wir noch im RTO von 15 Minuten liegen."}
{"ts": "115:07", "speaker": "I", "text": "Gab es schon einmal, dass Sie während des Drills diese RTO-Grenze überschritten haben?"}
{"ts": "115:11", "speaker": "E", "text": "Ja, im Q3/2024-Drill haben wir 17 Minuten gebraucht, weil die Messaging-Queue sich nicht schnell genug synchronisiert hat. Wir haben daraus gelernt, pre-warmed Standby-Nodes zu nutzen."}
{"ts": "115:20", "speaker": "I", "text": "Kostet das nicht extra Ressourcen im Standby?"}
{"ts": "115:23", "speaker": "E", "text": "Doch, etwa 12% mehr laufende Instanzen. Aber wir haben im Ticket RFC-DR-89 kalkuliert, dass der zusätzliche OPEX den Risikoabbau rechtfertigt – vor allem bei kritischen Endkunden-Workloads."}
{"ts": "115:32", "speaker": "I", "text": "Und wie haben Sie intern kommuniziert, dass diese Mehrkosten akzeptabel sind?"}
{"ts": "115:36", "speaker": "E", "text": "Wir haben einen kurzen Report im Steering Committee vorgestellt, mit Vergleich der Business Impact Costs bei SLA-Verletzung vs. laufende Kosten. Die Entscheidung wurde dann im Protokoll SC-2025-02 festgehalten."}
{"ts": "116:00", "speaker": "I", "text": "Sie hatten eben die offenen Risiken erwähnt. Mich würde interessieren, ob es seitdem neue Erkenntnisse gab, etwa nach der letzten internen Auditierung der DR-Prozesse?"}
{"ts": "116:07", "speaker": "E", "text": "Ja, beim Audit AUD-DR-2025-Q2 hat sich gezeigt, dass unsere Secondary-Region zwar die Last tragen kann, aber die Latenzspitzen im Auth-Cluster dort höher sind als im Primärbetrieb. Das war bisher nicht so transparent dokumentiert."}
{"ts": "116:19", "speaker": "I", "text": "Lag das eher an der Netzwerk-Topologie oder an der Ressourcenplanung?"}
{"ts": "116:24", "speaker": "E", "text": "Eher eine Mischung. Wir haben in der Secondary-Region weniger CPU-Reserven, und gleichzeitig ist der Peering-Link zu unserem internen DNS-Resolver dort schwächer dimensioniert. Im Runbook RB-DR-001 haben wir jetzt einen Warnhinweis ergänzt."}
{"ts": "116:36", "speaker": "I", "text": "Gab es dazu schon eine formale Änderung, z. B. als RFC?"}
{"ts": "116:40", "speaker": "E", "text": "Ja, RFC-DR-017 ist in Review. Darin schlagen wir vor, die Resolver-Kapazität in Secondary von 2 auf 4 Instanzen zu erhöhen und ein Burst-Credit-System für CPU zu aktivieren."}
{"ts": "116:52", "speaker": "I", "text": "Wie testen Sie, ob diese Maßnahme den gewünschten Effekt bringt?"}
{"ts": "116:56", "speaker": "E", "text": "Wir planen einen Mini-GameDay, in dem wir gezielt einen Auth-Ausfall in Primary simulieren und die Failover-Zeit sowie Latenzprofile in Secondary messen – ähnlich wie bei TEST-DR-2025-Q1, aber fokussierter."}
{"ts": "117:09", "speaker": "I", "text": "Und wie binden Sie das Messaging-Service-Team ein? Die hatten ja beim letzten Test auch einige Bottlenecks."}
{"ts": "117:14", "speaker": "E", "text": "Genau, wir haben ein gemeinsames Ticket DR-MSG-229 offen. Das sieht vor, dass das Messaging-Team parallel ihre Queue-Replikationsintervalle verkürzt, damit Auth- und Messaging-Dienste synchron im Failover sind."}
{"ts": "117:26", "speaker": "I", "text": "Sind diese Tickets Teil eines größeren DR-Backlogs?"}
{"ts": "117:30", "speaker": "E", "text": "Ja, wir pflegen ein dediziertes Epic EPIC-DR-OPS, das alle operativen Verbesserungen sammelt. So können wir Kosten, Risiken und Fortschritt transparent priorisieren – das war eine Lesson Learned aus mehreren Audits."}
{"ts": "117:43", "speaker": "I", "text": "Wie stellen Sie sicher, dass diese Learnings nicht verloren gehen, gerade wenn Personal wechselt?"}
{"ts": "117:48", "speaker": "E", "text": "Wir haben eine Kombination aus Confluence-Dokumentation und einem DR-Knowledge-Transfer-Runbook, RB-DR-ONB-002. Zudem führen wir Shadowing-Sessions während der GameDays durch, um neues Personal direkt in die Abläufe einzubinden."}
{"ts": "118:02", "speaker": "I", "text": "Klingt, als hätten Sie ein ziemlich ausgereiftes System. Gibt es dennoch eine Sorge, die Sie für das nächste Jahr im DR-Bereich als kritisch einstufen?"}
{"ts": "118:08", "speaker": "E", "text": "Ja, die größte Sorge ist momentan die Abhängigkeit von einem einzigen Cloud-Transit-Provider zwischen den Regionen. Sollte der ausfallen, greifen zwar unsere Offline-Sync-Mechanismen, aber das RPO würde sich von 15 Minuten auf bis zu 2 Stunden verschlechtern. Das ist gegen unsere SLA-DR-001-Grenze, also ein klar zu adressierendes Risiko."}
{"ts": "120:00", "speaker": "I", "text": "Zum Abschluss würde ich gern noch auf die Optimierungen eingehen, die Sie seit dem letzten Drill umgesetzt haben. Können Sie dazu ein Beispiel nennen?"}
{"ts": "120:08", "speaker": "E", "text": "Ja, äh, wir haben nach TEST-DR-2025-Q1 die Orchestrierungs-Skripte in RB-DR-001 um einen Health-Check-Block für den Messaging-Service ergänzt. Das war eine direkte Reaktion auf Ticket DR-INC-443, wo wir im Failover einen stummen Queue-Cluster hatten."}
{"ts": "120:23", "speaker": "I", "text": "Das heißt, Sie haben die Runbooks aktiv angepasst?"}
{"ts": "120:28", "speaker": "E", "text": "Genau. Wir pflegen eine Versionshistorie in Confluence, und jede Änderung kriegt eine Referenz auf das Incident-Ticket. So können wir die Lessons Learned nachvollziehbar machen, und im Auditfall zeigen, wie wir reagieren."}
{"ts": "120:44", "speaker": "I", "text": "Gab es dabei technische Stolpersteine?"}
{"ts": "120:49", "speaker": "E", "text": "Hm, ja, die Checks mussten asynchron sein, weil während des Failovers manche Endpoints noch nicht resolvbar sind. Das hat uns zwei Nächte Testruns gekostet, bis wir die Timeouts so gesetzt hatten, dass der RTO von 15 Minuten nicht gefährdet war."}
{"ts": "121:06", "speaker": "I", "text": "Wie messen Sie denn, dass der RTO wirklich eingehalten wird?"}
{"ts": "121:11", "speaker": "E", "text": "Wir haben im Drill Monitoring-Hooks, die Start- und Endzeitpunkte loggen. Die Metriken landen in unserem DR-Dashboard, das die SLA-Compliance anzeigt. Wenn wir über 14:30 hinausgehen, kriegen wir schon interne Alerts."}
{"ts": "121:27", "speaker": "I", "text": "Und wenn das passiert, gibt es eine formale Nachbereitung?"}
{"ts": "121:31", "speaker": "E", "text": "Ja, sofort. Da greifen die Playbooks aus dem DR-Postmortem-Guide. Wir machen eine Root-Cause-Analyse nach RFC-DR-017, innerhalb von fünf Arbeitstagen."}
{"ts": "121:45", "speaker": "I", "text": "Sie hatten vorhin das Ticket DR-INC-443 erwähnt. Gab es noch andere, die zu größeren Änderungen geführt haben?"}
{"ts": "121:52", "speaker": "E", "text": "DR-INC-451 etwa. Das betraf den Auth-Cluster in Region West. Da hat eine falsche DNS-Propagation nach Failover die Auth-Zeit verdoppelt. Wir haben daraufhin einen Pre-Warm-Mechanismus implementiert, der im Drill-Skript jetzt standardmäßig läuft."}
{"ts": "122:12", "speaker": "I", "text": "Das klingt nach einer proaktiven Maßnahme."}
{"ts": "122:16", "speaker": "E", "text": "Ja, wir wollen, dass im Notfall so wenig wie möglich improvisiert werden muss. Das kostet sonst Nerven und kann SLAs reißen. Lieber vorher einmal Aufwand, als im Ernstfall Chaos."}
{"ts": "122:29", "speaker": "I", "text": "Wie binden Sie andere Teams in diese Optimierungen ein?"}
{"ts": "122:34", "speaker": "E", "text": "Wir haben ein monatliches DR-Review-Meeting. Da sitzen Architekten, SREs und Vertreter vom Security-Team zusammen. Jede Runbook-Änderung wird dort vorgestellt, inklusive Impact-Analyse und ggf. Kostenabschätzung, bevor sie live geht."}
{"ts": "128:00", "speaker": "I", "text": "Lassen Sie uns nochmal kurz auf die Lessons Learned aus TEST-DR-2025-Q1 zurückkommen – was hat Sie da persönlich am meisten überrascht?"}
{"ts": "128:18", "speaker": "E", "text": "Hm, also ehrlich gesagt war ich baff, wie schnell der Secondary Region Load Balancer saturiert ist. Laut Runbook RB-DR-001, Abschnitt 3.2, hätten wir noch 40 % Headroom haben müssen, aber in den Logs aus Testlauf #DR-Q1-25 sahen wir, dass durch parallele Auth-Cluster-Reconnects viel mehr Traffic kam als einkalkuliert."}
{"ts": "128:45", "speaker": "I", "text": "Und wie sind Sie damit im Anschluss umgegangen? Haben Sie RB-DR-001 angepasst?"}
{"ts": "129:02", "speaker": "E", "text": "Ja, wir haben in RFC-DR-045 festgehalten, dass wir den Scale-Out für den L7-LB in Region B um +2 Nodes vorziehen und das als Pre-Check in die Drill-Checkliste aufnehmen. Außerdem ist im neuen Runbook-Stand vermerkt, dass Auth-Cluster-Instanzen gestaffelt reconnecten sollen."}
{"ts": "129:30", "speaker": "I", "text": "Gab es da Abstimmungsbedarf mit anderen Teams, z. B. dem Messaging-Service-Team?"}
{"ts": "129:47", "speaker": "E", "text": "Absolut. Die Messaging-Leute mussten ihren Failover-Mechanismus so anpassen, dass er nicht zeitgleich mit dem Auth-Failover anläuft. Das war ein interner Jira-Ticket-Thread DRMSG-219 mit mehreren Iterationen, bis wir die Backoff-Strategie sauber abgestimmt hatten."}
{"ts": "130:15", "speaker": "I", "text": "Klingt nach einem klassischen Fall von Kettenreaktion, die Sie jetzt entschärfen konnten."}
{"ts": "130:27", "speaker": "E", "text": "Ja, genau. Und wir haben das in unseren GameDay-Skripten als 'Scenario 7' aufgenommen, um es regelmäßig zu simulieren. Sonst vergisst man solche Feinheiten schnell wieder."}
{"ts": "130:48", "speaker": "I", "text": "Wenn Sie auf die letzten drei Drills zurückblicken – haben sich die RTO/RPO-Werte im Trend verbessert?"}
{"ts": "131:05", "speaker": "E", "text": "Wir liegen inzwischen im Schnitt 14 % unter dem SLA-Maximum für RTO und konnten das RPO stabil bei unter 4 Minuten halten. Vor einem Jahr lagen wir noch bei 7–8 Minuten RPO in manchen Kettenreaktionsfällen."}
{"ts": "131:30", "speaker": "I", "text": "Gab es spezielle Optimierungen, die den größten Effekt hatten?"}
{"ts": "131:44", "speaker": "E", "text": "Die größte Wirkung hatte tatsächlich der Wechsel auf inkrementelle Snapshot-Syncs zwischen den Regionen, siehe Implementierungsprotokoll DR-SYNC-2024-12. Das hat die Netzwerkbelastung um rund 35 % gesenkt."}
{"ts": "132:10", "speaker": "I", "text": "Klingt effizient. Aber erhöht das nicht das Risiko, dass ein inkrementeller Snapshot fehlerhaft ist und man es zu spät merkt?"}
{"ts": "132:26", "speaker": "E", "text": "Doch, das ist der Trade-off. Wir haben deshalb im Monitoring-Runbook RB-MON-010 neue Checks eingeführt, die jede inkrementelle Übertragung mit einem CRC64-Hash verifizieren. Das kostet ein paar Sekunden, minimiert aber das Risiko von Silent Data Corruption."}
{"ts": "132:52", "speaker": "I", "text": "Sehen Sie noch Risiken, die aktuell nicht ausreichend mitigiert sind?"}
{"ts": "133:08", "speaker": "E", "text": "Ja, zwei Punkte: Erstens die Abhängigkeit von unserem einzigen DNS-Provider in Region C – da ist ein zweiter Vendor in Beschaffung (Ticket DR-DNS-017). Zweitens die fehlende automatische Priorisierung von kritischen Queues im Messaging nach einem Failover, das steht als Optimierungsvorschlag im DR-Backlog."}
{"ts": "136:00", "speaker": "I", "text": "Sie hatten vorhin erwähnt, dass einige der offenen Risiken noch nicht ausreichend mitigiert sind. Können Sie mir ein Beispiel nennen, wie Sie damit im operativen Alltag umgehen?"}
{"ts": "136:15", "speaker": "E", "text": "Ja, ähm, konkret betrifft das zum Beispiel das Szenario aus Ticket DR-458, wo der sekundäre Auth-Cluster im Failover zwar hochkam, aber die Session-Replikation verzögert war. Im Alltag fahren wir jetzt bewusst zusätzliche Heartbeat-Checks gemäß Abschnitt 4.3 im RB-DR-001, um ein Frühwarnsignal zu haben."}
{"ts": "136:38", "speaker": "I", "text": "Und wie fließen diese zusätzlichen Checks in Ihre Monitoring- und Alerting-Strategie ein?"}
{"ts": "136:50", "speaker": "E", "text": "Wir haben im Grafana-Dashboard ein dediziertes Panel 'DR-Heartbeat', das alle 5 Sekunden aktualisiert wird. Außerdem sind in unserem Alertmanager temporäre Regeln hinterlegt – Priority P2 – die während eines Drills automatisch aktiviert werden."}
{"ts": "137:12", "speaker": "I", "text": "Gab es Schwierigkeiten bei der Implementierung dieser temporären Regeln?"}
{"ts": "137:22", "speaker": "E", "text": "Ein bisschen, ja. Die ursprüngliche Alert-Pipeline war nicht dafür ausgelegt, Rules dynamisch zu laden. Wir mussten ein kleines Go-Skript schreiben, das die ConfigMap im Kubernetes-Cluster neu rendert und ausrollt – steht jetzt als Anhang B im internen Runbook RB-DR-001a."}
{"ts": "137:48", "speaker": "I", "text": "Interessant. Und wie testen Sie, dass diese Anpassungen im Ernstfall funktionieren?"}
{"ts": "138:00", "speaker": "E", "text": "Wir haben dafür Mini-GameDays, intern 'Pulse-Checks' genannt, die nur dieses Subsystem betreffen. Da simulieren wir gezielt Ausfälle im Auth-Cluster und prüfen, ob das Alerting innerhalb von 30 Sekunden anschlägt – das ist unser internes SLA."}
{"ts": "138:24", "speaker": "I", "text": "Und erfüllen Sie dieses SLA schon konsistent?"}
{"ts": "138:34", "speaker": "E", "text": "Zu etwa 92 % der Tests, ja. Die restlichen 8 % sind oft Netzwerkjitter zwischen den Regionen. Dafür planen wir gerade ein RFC-Update, um den Timeout-Wert leicht zu erhöhen, ohne die Reaktionszeit signifikant zu verschlechtern."}
{"ts": "138:56", "speaker": "I", "text": "Gibt es intern unterschiedliche Meinungen zu dieser Timeout-Anpassung?"}
{"ts": "139:06", "speaker": "E", "text": "Absolut. Die Architektenseite argumentiert, dass längere Timeouts die Gefahr erhöhen, dass ein echtes Problem zu spät erkannt wird. Das Ops-Team hingegen sieht die Fehlalarme als größeres Übel. Wir haben das im letzten DR-Governance-Meeting (Protokoll GOV-DR-2025-04) offen diskutiert."}
{"ts": "139:32", "speaker": "I", "text": "Wie nähern Sie sich der Entscheidungsfindung bei solchen Zielkonflikten?"}
{"ts": "139:44", "speaker": "E", "text": "Wir fahren eine kurze Testphase mit geänderten Parametern in einer Stage-Region, messen die Auswirkung auf MTTR und False-Positive-Rate, und präsentieren die Metriken dann im Steering Committee. Dort wird anhand der Datenlage abgestimmt."}
{"ts": "140:06", "speaker": "I", "text": "Klingt datengetrieben. Gibt es einen konkreten Zeitplan für diese Testphase?"}
{"ts": "140:16", "speaker": "E", "text": "Ja, Start ist nächste Woche Montag, Abschlussbericht soll bis zum 15. des kommenden Monats vorliegen. Das ist auch so im Projektplan P-TIT-DR-Phase-Drill vermerkt."}
{"ts": "144:00", "speaker": "I", "text": "Zum Abschluss möchte ich noch auf die Optimierungen eingehen, die Sie nach dem letzten Drill umgesetzt haben. Können Sie mir ein Beispiel nennen?"}
{"ts": "144:05", "speaker": "E", "text": "Ja, wir haben nach TEST-DR-2025-Q1 die Failover-Sequenz im Runbook RB-DR-001 Abschnitt 4.2 angepasst. Konkret wurde ein zusätzlicher Health-Check für den Messaging-Service eingeführt, bevor der Traffic-Switch erfolgt."}
{"ts": "144:13", "speaker": "I", "text": "War das eine Reaktion auf die Verzögerungen, die Sie im Drill beobachtet haben?"}
{"ts": "144:17", "speaker": "E", "text": "Genau. Wir hatten im Drill eine rund 90-sekündige Verzögerung, weil der Messaging-Service zwar erreichbar war, aber nicht korrekt Nachrichten verarbeitete. Das wurde in Ticket DR-OPS-771 dokumentiert."}
{"ts": "144:25", "speaker": "I", "text": "Wie prüfen Sie jetzt, dass solche inkonsistenten Zustände nicht mehr auftreten?"}
{"ts": "144:30", "speaker": "E", "text": "Wir haben synthetische Transaktionen eingeführt, die über beide Regionen laufen und nicht nur die Connectivity, sondern auch die End-to-End-Prozessierung testen. Die Ergebnisse werden gegen unsere SLA-Metriken gemessen."}
{"ts": "144:38", "speaker": "I", "text": "Beeinflusst diese zusätzliche Prüfung die RTO-Werte?"}
{"ts": "144:42", "speaker": "E", "text": "Minimal, ja. Wir haben im schlimmsten Fall eine Verlängerung um 15 Sekunden gemessen. Aber das ist im Rahmen unseres RTO von 5 Minuten laut SLA-DR-2024 akzeptabel."}
{"ts": "144:50", "speaker": "I", "text": "Gab es Diskussionen im Team über diesen Trade-off zwischen Geschwindigkeit und Sicherheit?"}
{"ts": "144:54", "speaker": "E", "text": "Ja, vor allem weil einige Operations-Kollegen meinten, wir sollten die Checks asynchron laufen lassen. Wir haben uns aber bewusst für synchron entschieden, um keine fehlerhaften Failovers zu riskieren."}
{"ts": "145:02", "speaker": "I", "text": "Wie wurde diese Entscheidung dokumentiert?"}
{"ts": "145:06", "speaker": "E", "text": "Im Änderungsprotokoll des Runbooks und im RFC-Dokument RFC-DR-2025-07. Dort haben wir auch die Risikoanalyse beigelegt."}
{"ts": "145:12", "speaker": "I", "text": "Und welche Risiken sehen Sie trotz dieser Optimierung noch?"}
{"ts": "145:16", "speaker": "E", "text": "Ein Restrisiko bleibt bei Abhängigkeiten, die außerhalb unseres Kontrollbereichs liegen, z. B. externe Authentifizierungsdienste. Wenn die in beiden Regionen ausfallen, greifen unsere Mechanismen nicht."}
{"ts": "145:24", "speaker": "I", "text": "Planen Sie dafür noch weitere Maßnahmen?"}
{"ts": "145:28", "speaker": "E", "text": "Wir evaluieren gerade einen Fallback auf ein eigenes Lightweight-Auth-Modul, das in den DR-Regionen isoliert läuft. Das ist aber noch in der Proof-of-Concept-Phase unter Lab-Bedingungen."}
{"ts": "145:35", "speaker": "I", "text": "Lassen Sie uns noch einmal kurz auf die Lessons Learned aus TEST-DR-2025-Q1 zurückkommen — gab es Punkte, die Sie sofort in die Produktionsumgebung einfließen lassen konnten?"}
{"ts": "145:39", "speaker": "E", "text": "Ja, wir haben direkt nach dem Drill die Latenzalarme im RB-DR-001 ergänzt. Ursprünglich waren nur Ausfallmeldungen für komplette Regionen definiert, jetzt haben wir Schwellenwerte für Subsysteme wie den Auth-Cluster eingebaut."}
{"ts": "145:43", "speaker": "I", "text": "Das heißt, Sie haben quasi die Monitoring-Granularität erhöht, um frühzeitiger reagieren zu können?"}
{"ts": "145:47", "speaker": "E", "text": "Genau. Vorher hätten wir einen partiellen Auth-Lag erst bemerkt, wenn er sich auf die Failover-Trigger auswirkt. Jetzt sehen wir ihn 90 Sekunden früher und können manuell eingreifen."}
{"ts": "145:51", "speaker": "I", "text": "Gab es bei dieser Anpassung Rücksprache mit anderen Teams?"}
{"ts": "145:55", "speaker": "E", "text": "Ja, wir haben ein kurzes RFC-Review mit dem Messaging-Service-Team gemacht, weil deren Queue-Verarbeitung vom Auth-Throughput abhängt. Das ist jetzt auch in Change-Request CR-DR-042 dokumentiert."}
{"ts": "145:59", "speaker": "I", "text": "Interessant. Und wie schnell konnten Sie diese Änderungen nach dem GameDay live bringen?"}
{"ts": "146:03", "speaker": "E", "text": "Innerhalb von vier Tagen. Wir hatten die Runbook-Änderungen parallel in der Testumgebung validiert und dann per Blue-Green-Deployment ausgerollt."}
{"ts": "146:07", "speaker": "I", "text": "Gab es dabei unerwartete Nebeneffekte oder war das ziemlich reibungslos?"}
{"ts": "146:11", "speaker": "E", "text": "Es war größtenteils reibungslos, nur das Alert-Dashboard musste angepasst werden, weil die neuen Metriken sonst zu viele false positives erzeugt hätten."}
{"ts": "146:15", "speaker": "I", "text": "Wie haben Sie das mitigiert?"}
{"ts": "146:19", "speaker": "E", "text": "Wir haben eine Korrelation ins Alerting eingebaut: If Auth-Lag > 300ms AND Messaging-Queue-Depth > 500, dann wird der Alarm ausgelöst. So vermeiden wir Fehlalarme bei kurzen Auth-Cluster-Spikes."}
{"ts": "146:23", "speaker": "I", "text": "Das klingt nach einer pragmatischen Lösung. Hat sich dadurch die Mean Time to Detect im Drill-Szenario verbessert?"}
{"ts": "146:27", "speaker": "E", "text": "Ja, von durchschnittlich 4,5 Minuten auf etwa 2 Minuten. Das ist auch in SLA-Report SR-2025-DR-Q1 vermerkt."}
{"ts": "146:31", "speaker": "I", "text": "Planen Sie, diese Methodik auch auf andere Serviceabhängigkeiten auszuweiten?"}
{"ts": "146:35", "speaker": "E", "text": "Ja, wir evaluieren gerade ähnliche Korrelationen für den Storage-Cluster und den API-Gateway, um künftige Kettenreaktionen schneller zu erkennen."}
{"ts": "147:05", "speaker": "I", "text": "Zum Abschluss würde ich gern noch ein bisschen tiefer in die Optimierungen nach dem letzten Drill eintauchen. Was hat sich denn konkret aus TEST-DR-2025-Q1 ergeben, das Sie jetzt schon umgesetzt haben?"}
{"ts": "147:12", "speaker": "E", "text": "Wir haben nach dem Drill vor allem den Failback-Prozess beschleunigt. Im Runbook RB-DR-001 haben wir die Schritte 14 bis 18 so umformuliert, dass unnötige Wartezeiten zwischen den Region-Syncs entfallen. Das hat in unseren internen Tests etwa 12 Minuten eingespart."}
{"ts": "147:24", "speaker": "I", "text": "Gab es dazu ein spezielles Change-Request oder ist das in der Routinepflege passiert?"}
{"ts": "147:31", "speaker": "E", "text": "Das lief als RFC-DR-2025-07, freigegeben vom Architekturboard. Wir haben parallel ein Ticket im System unter ID DRFIX-229 gehabt, um die Dokumentationsänderungen zu tracken."}
{"ts": "147:40", "speaker": "I", "text": "Und wie wurde das von den anderen Teams aufgenommen? Gerade, weil es ja auch Abhängigkeiten zu den Auth- und Messaging-Services gab."}
{"ts": "147:48", "speaker": "E", "text": "Anfangs gab es Bedenken vom Security-Team, weil schnellere Syncs auch bedeuten, dass Keys und Tokens häufiger repliziert werden. Wir haben das mit einer temporären Quarantäne-Queue gelöst, die die Auth-Cluster validiert, bevor sie den Traffic freigeben."}
{"ts": "147:59", "speaker": "I", "text": "Interessant. Das heißt, Sie mussten auch SLAs anpassen?"}
{"ts": "148:05", "speaker": "E", "text": "Ja, minimal. Der RPO blieb gleich bei 5 Minuten, RTO wurde für den Failback von 45 auf 30 Minuten reduziert. Diese Änderung ist jetzt auch im SLA-Dokument DR-SLA-v2.3 hinterlegt."}
{"ts": "148:16", "speaker": "I", "text": "Gab es beim Testen dieser Optimierungen noch unerwartete Nebeneffekte?"}
{"ts": "148:22", "speaker": "E", "text": "Ja, die Messaging-Service-Latenz ist in der ersten Minute nach Failback um ca. 20% hochgegangen. Wir haben festgestellt, dass der Message-Bus die Backlog-Verarbeitung priorisiert, was den Live-Traffic kurz bremst. Das steht jetzt in der Known-Issues-Liste KI-DR-2025-02."}
{"ts": "148:36", "speaker": "I", "text": "Planen Sie, das noch zu optimieren oder ist das akzeptabel im Rahmen des Risikoprofils?"}
{"ts": "148:42", "speaker": "E", "text": "Wir haben entschieden, das vorerst zu akzeptieren, weil der Peak nur 50–60 Sekunden anhält und keine SLA-Verletzung darstellt. Für Q3 planen wir aber einen Test mit gestaffeltem Backlog-Abbau."}
{"ts": "148:54", "speaker": "I", "text": "Verstehe. Wenn Sie auf die letzten zwei Jahre DR-Drills zurückblicken, was war aus Ihrer Sicht die größte Verbesserung im Zusammenspiel der Regionen?"}
{"ts": "149:01", "speaker": "E", "text": "Ganz klar die Einführung des asynchronen State-Sync für den Auth-Cluster. Vorher mussten wir den Dienst komplett anhalten, jetzt läuft das im Hintergrund. Das ist in RB-DR-001 seit Revision 5 dokumentiert und hat Ausfallzeiten massiv reduziert."}
{"ts": "149:14", "speaker": "I", "text": "Und wo sehen Sie in der Multi-Region-Strategie aktuell noch die größten Risiken?"}
{"ts": "149:20", "speaker": "E", "text": "Wir sind nach wie vor anfällig für gleichzeitige Netzwerksegmentierungen in zwei Regionen. Das ist selten, aber TEST-DR-2024-Q4 hat gezeigt, dass dann unsere Cross-Region-Coordination-Services ins Timeout laufen. Ticket RIS-DR-112 beschreibt die geplante Einführung redundanter Control-Channels, um genau das zu adressieren."}
{"ts": "149:05", "speaker": "I", "text": "Bevor wir jetzt abschließen, würde mich noch interessieren, ob seit dem letzten Drill neue Erkenntnisse in die Runbooks eingeflossen sind, die vielleicht nicht offiziell in RB-DR-001 dokumentiert sind."}
{"ts": "149:12", "speaker": "E", "text": "Ja, tatsächlich. Wir haben nach TEST-DR-2025-Q1 intern ein sogenanntes Shadow-Addendum erstellt, das einige Workarounds für die langsame Re-Synchronisation des Storage beschreibt. Das ist noch nicht in der offiziellen Version, weil wir erst einen zweiten Drill abwarten wollen."}
{"ts": "149:27", "speaker": "I", "text": "Verstehe. Ist das eine Art interner Anhang, den nur SREs sehen?"}
{"ts": "149:32", "speaker": "E", "text": "Genau. Er ist im Confluence-Bereich 'Ops-Internals' abgelegt und trägt die Kennung RB-DR-001-SH1. Wir nutzen den z.B., wenn der primäre Datenbank-Knoten in Region West nach dem Failback inkonsistente LSNs meldet."}
{"ts": "149:46", "speaker": "I", "text": "Gab es im letzten Drill noch weitere ad-hoc-Entscheidungen?"}
{"ts": "149:50", "speaker": "E", "text": "Ja, der Messaging-Service musste temporär auf einen älteren Build zurückgerollt werden, weil die aktuelle Version nicht mit dem Auth-Cluster in Region Ost kompatibel war. Das stand so nicht im Runbook, wurde aber im Ticket DR-INC-2025-07 dokumentiert."}
{"ts": "150:05", "speaker": "I", "text": "Wie schnell konnten Sie diese Inkompatibilität identifizieren?"}
{"ts": "150:09", "speaker": "E", "text": "Innerhalb von etwa 12 Minuten nach Start des Failovers. Wir haben ein Alert-Pattern, das bei Auth-Handshake-Fehlern in mehr als 3% der Requests greift. Die Analyse lief dann parallel zum Failback-Plan."}
{"ts": "150:22", "speaker": "I", "text": "Das klingt nach einer guten Detection. Gab es daraus Optimierungen für die Monitoring-Runbooks?"}
{"ts": "150:27", "speaker": "E", "text": "Ja, wir haben in RB-MON-017 jetzt explizit einen Abschnitt, der auf die Abhängigkeit zwischen Messaging-Version und Auth-Cluster-Hotfix-Level eingeht. Das ist aus der Drill-Erfahrung entstanden."}
{"ts": "150:40", "speaker": "I", "text": "Wenn wir auf die Kosten-Performance-Trade-offs zurückkommen – haben Sie seitdem Anpassungen an der Region-Pairing-Strategie vorgenommen?"}
{"ts": "150:47", "speaker": "E", "text": "Ja, wir haben entschieden, die Warm-Standby-Kapazität in Region Nord um 15% zu reduzieren, um Kosten zu sparen. Das verlängert das RTO im Worst Case um ca. 90 Sekunden, was laut SLA noch akzeptabel ist."}
{"ts": "151:00", "speaker": "I", "text": "Gab es Bedenken im Management dazu?"}
{"ts": "151:04", "speaker": "E", "text": "Einige, ja. Wir mussten mit Daten aus den letzten drei Drills belegen, dass die durchschnittliche Recovery-Zeit weit unterhalb des SLA-Maximums liegt. Das haben wir im Report DR-ANL-2025-02 zusammengefasst."}
{"ts": "151:16", "speaker": "I", "text": "Und welche Risiken sehen Sie trotz dieser Optimierungen noch offen?"}
{"ts": "151:21", "speaker": "E", "text": "Das größte offene Risiko ist derzeit die Abhängigkeit vom externen DNS-Provider im Failover-Fall. Wir haben zwar ein Secondary-Setup, aber der Switch dauert im Test noch zu lange. Dafür gibt es einen offenen RFC-Entry RFC-DR-014 zur Evaluierung eines Anycast-Ansatzes."}
{"ts": "151:05", "speaker": "I", "text": "Bevor wir abschließen: Gab es im letzten Drill eigentlich auch Anpassungen an den SLAs, die aus der Performance-Perspektive relevant waren?"}
{"ts": "151:18", "speaker": "E", "text": "Ja, wir haben im Rahmen von TEST-DR-2025-Q1 für die Messaging-Service-Latenz das SLA von 250 ms auf 300 ms temporär angehoben, um in der Failover-Phase Puffer zu haben. Das wurde als RFC-DR-082 dokumentiert."}
{"ts": "151:36", "speaker": "I", "text": "Und diese Änderung, war die eher präventiv oder basierend auf konkreten Messwerten aus der Übung?"}
{"ts": "151:46", "speaker": "E", "text": "Basierend auf Messwerten – wir haben in den ersten Minuten nach Region-Switch Spitzen von bis zu 320 ms gesehen. Laut Protokoll EVI-DR-2025-04 lag das vor allem an der Re-Synchronisation des Auth-Clusters."}
{"ts": "152:05", "speaker": "I", "text": "Interessant. Das deutet ja wieder auf die Abhängigkeit Auth-Cluster ↔ Messaging hin, die Sie vorhin schon skizziert hatten."}
{"ts": "152:14", "speaker": "E", "text": "Genau, und das ist so ein klassischer Fall, wo das Runbook RB-DR-001 zwar die Reihenfolge vorgibt, aber nicht die konkreten Wartezeiten zwischen Diensten. Da mussten wir ad hoc eingreifen."}
{"ts": "152:32", "speaker": "I", "text": "Wie gehen Sie mit solchen Ad-hoc-Entscheidungen um? Fließen die sofort ins Runbook ein oder erst nach einem Review?"}
{"ts": "152:42", "speaker": "E", "text": "Erst nach einem Post-Mortem-Review. Wir öffnen dazu ein Change-Ticket im DR-Board, z. B. CHG-DR-145, und bewerten, ob die Anpassung generisch nutzbar ist oder nur für den speziellen Drill galt."}
{"ts": "153:01", "speaker": "I", "text": "Sie hatten vorhin Ticketnummern erwähnt. Gibt es da ein Beispiel, das den Trade-off Kosten vs. Performance explizit zeigt?"}
{"ts": "153:12", "speaker": "E", "text": "Ja, Ticket OPS-DR-773 beschreibt die Entscheidung, in der Secondary-Region kleinere Compute-Instanzen zu nutzen. Das spart rund 15 % Kosten im Warm-Standby, erhöht aber die Ramp-up-Zeit beim Failover um ca. 90 Sekunden."}
{"ts": "153:33", "speaker": "I", "text": "Und diese 90 Sekunden – akzeptabel im Rahmen des RTO?"}
{"ts": "153:41", "speaker": "E", "text": "Ja, unser RTO liegt laut SLA bei 15 Minuten, also ist das innerhalb der Grenzen. Aber in Kombination mit anderen Verzögerungen kann es kritisch werden, daher markieren wir das Risiko im DR-Risk-Register unter ID RSK-DR-021."}
{"ts": "154:01", "speaker": "I", "text": "Wie priorisieren Sie die Abarbeitung solcher Risiken?"}
{"ts": "154:08", "speaker": "E", "text": "Wir nutzen eine Matrix aus Eintrittswahrscheinlichkeit und Impact. RSK-DR-021 hat z. B. eine mittlere Wahrscheinlichkeit, aber hohen Impact, daher planen wir eine Optimierung im Q3-Maintenance-Fenster."}
{"ts": "154:24", "speaker": "I", "text": "Gibt es für diese Optimierung schon eine technische Richtung?"}
{"ts": "154:32", "speaker": "E", "text": "Vermutlich Pre-Warming von Instanzen in der Secondary-Region, gekoppelt an das Monitoring-Alert-Level. Das soll laut Entwurf RFC-DR-097 nur in Hochlastzeiten aktiv sein, um Kosten zu kontrollieren."}
{"ts": "159:05", "speaker": "I", "text": "Wenn wir jetzt noch einmal auf die Lessons Learned aus TEST-DR-2025-Q1 schauen – gab es Punkte, die Sie beim nächsten Drill sofort anders machen würden?"}
{"ts": "159:12", "speaker": "E", "text": "Ja, definitiv. Wir haben im letzten Test festgestellt, dass unser Failback-Prozess zu lange dauert. Laut SLA im RB-DR-001 Abschnitt 4.3 sollten wir unter 45 Minuten bleiben, in der Realität waren es aber fast 70."}
{"ts": "159:24", "speaker": "I", "text": "Woran lag diese Verzögerung konkret, können Sie das herleiten?"}
{"ts": "159:29", "speaker": "E", "text": "Das war eine Kombination. Zum einen hatte das Storage-Replikationsmodul in Region West eine Latenzspitze, dokumentiert im Incident INC-DR-774. Zum anderen hatten wir ein manuelles Approval im Runbook, das unnötig blockiert hat."}
{"ts": "159:44", "speaker": "I", "text": "Haben Sie diesen manuellen Schritt inzwischen angepasst?"}
{"ts": "159:48", "speaker": "E", "text": "Ja, wir haben in RFC-DR-223 festgehalten, dass dieses Approval bei Drill-Szenarien entfällt. Für echte Incidents behalten wir es aber bei, wegen der Compliance-Anforderungen."}
{"ts": "160:00", "speaker": "I", "text": "Das klingt nach einem guten Kompromiss. Gab es weitere technische Bottlenecks?"}
{"ts": "160:05", "speaker": "E", "text": "Ein weiterer Punkt war die Synchronisation des Auth-Clusters. Auch wenn wir das schon optimiert hatten, kam durch eine Abhängigkeit zum Messaging-Service ein Deadletter-Queue Overflow zustande, der manuell bereinigt werden musste."}
{"ts": "160:18", "speaker": "I", "text": "War das im Runbook überhaupt vorgesehen?"}
{"ts": "160:22", "speaker": "E", "text": "Nur rudimentär. Wir hatten einen generischen Hinweis auf Monitoring der Queues, aber keinen klaren Remediation-Pfad. Das haben wir jetzt ergänzt – siehe Runbook-Version 3.2, Abschnitt 5.1."}
{"ts": "160:36", "speaker": "I", "text": "Wie wirken sich solche Ergänzungen auf die Gesamtkomplexität aus, wird es nicht unübersichtlich?"}
{"ts": "160:40", "speaker": "E", "text": "Doch, das Risiko besteht. Deshalb führen wir nach jedem Drill eine Redaktionsrunde durch, um veraltete Einträge zu streichen und die kritischen Pfade klar hervorzuheben."}
{"ts": "160:52", "speaker": "I", "text": "Gab es im letzten Drill etwas, das Sie positiv überrascht hat?"}
{"ts": "160:56", "speaker": "E", "text": "Ja, die automatisierte DNS-Umschaltung hat ohne jegliche Intervention funktioniert. Das war im Vorjahr noch fehleranfällig. Die Health-Checks im Global Traffic Manager haben sauber gegriffen."}
{"ts": "161:07", "speaker": "I", "text": "Dann vielleicht zum Abschluss: Welche Maßnahme aus den letzten Optimierungen hat den größten Return gebracht?"}
{"ts": "161:12", "speaker": "E", "text": "Ganz klar das Pre-Warming der sekundären Datenbank-Knoten. Das hat die Recovery Time um gut zwölf Minuten reduziert, was uns im Rahmen des RTO deutliche Puffer verschafft."}
{"ts": "160:55", "speaker": "I", "text": "Zum Abschluss würde ich gern noch auf Ihre internen Optimierungspläne eingehen. Gibt es bereits ein RFC oder ähnliches, das eine Anpassung der Failover-Prozeduren vorsieht?"}
{"ts": "161:00", "speaker": "E", "text": "Ja, wir haben RFC-DR-2025-07 entworfen, der eine Parallelisierung bestimmter Schritte aus RB-DR-001 vorsieht. Das Ziel ist es, die Recovery Time um circa 15 % zu senken, ohne dass wir zusätzliche Hardware in beiden Regionen vorhalten müssen."}
{"ts": "161:08", "speaker": "I", "text": "Wie wollen Sie das technisch umsetzen, ohne die bestehende Sequenzlogik zu gefährden?"}
{"ts": "161:12", "speaker": "E", "text": "Wir planen, die Health-Checks für den Auth-Cluster und den Messaging-Service parallel auszuführen und die Quorum-Bildung für sekundäre Knoten vorzuverlegen. Dazu gibt es bereits einen Prototypen in der Sandbox-Umgebung TEST-SBX-14."}
{"ts": "161:21", "speaker": "I", "text": "Gab es dafür schon einen Testlauf im Rahmen eines GameDays oder nur Laborbedingungen?"}
{"ts": "161:25", "speaker": "E", "text": "Bisher nur Laborbedingungen. Wir haben in TEST-DR-2025-Q2 einen Slot reserviert, um die Änderungen unter Last zu simulieren. Das ist im Testplan unter Szenario 'Failover-Parallel' dokumentiert."}
{"ts": "161:34", "speaker": "I", "text": "Sehen Sie dabei besondere Risiken, etwa im Hinblick auf Konsistenz oder SLA-Verletzungen?"}
{"ts": "161:38", "speaker": "E", "text": "Definitiv. Wenn die Parallelisierung zu früh eingeleitet wird, könnte der Messaging-Service Nachrichten doppelt zustellen. Wir haben eine SLA von max. 0,2% Duplicate Rate, und jeder Ausreißer wird als P1-Ticket wie DR-INC-447 geloggt."}
{"ts": "161:49", "speaker": "I", "text": "Gibt es Workarounds, falls dieser Fall im Ernstfall eintritt?"}
{"ts": "161:53", "speaker": "E", "text": "Ja, laut Runbook-Abschnitt 5.4 müssten wir dann sofort auf das Retry-Queue-Purging umstellen. Das kostet Zeit, sichert aber Datenintegrität. Wir haben das bei Drill 2024-Q4 einmal testen müssen, als ein fehlerhaftes Health-Signal kam."}
{"ts": "162:03", "speaker": "I", "text": "Wie binden Sie Lessons Learned aus solchen Vorfällen in die Dokumentation ein?"}
{"ts": "162:07", "speaker": "E", "text": "Nach jedem Drill führen wir ein Post-Mortem durch, schreiben ein Confluence-Update und passen gegebenenfalls RB-DR-001 oder ergänzende Runbooks an. Für den oben genannten Vorfall gibt es Eintrag PM-DR-2024-11A."}
{"ts": "162:16", "speaker": "I", "text": "Und wer gibt letztlich die Freigabe für solche Runbook-Änderungen?"}
{"ts": "162:20", "speaker": "E", "text": "Die Freigabe läuft über das DR-Governance Board, das aus Architekt, SRE-Leitung und einem Vertreter der Compliance besteht. Ohne deren Sign-off darf keine produktive Änderung erfolgen."}
{"ts": "162:28", "speaker": "I", "text": "Das klingt nach einem stabilen Prozess. Gibt es denn aktuell offene Punkte, die Sie als kritisch betrachten, bevor RFC-DR-2025-07 umgesetzt wird?"}
{"ts": "162:33", "speaker": "E", "text": "Ja, die Schnittstelle zum externen DNS-Provider ist noch nicht asynchronisiert. Sollte die API dort länger als 3 Sekunden blockieren, verpufft der Parallelisierungsvorteil. Diese Abhängigkeit ist im Risk-Register als RR-DR-093 geführt."}
{"ts": "162:15", "speaker": "I", "text": "Wir hatten eben schon über die Trade-offs gesprochen. Mich würde interessieren: welche Optimierungen im DR-Kontext wurden seit dem letzten Drill konkret umgesetzt?"}
{"ts": "162:20", "speaker": "E", "text": "Seit TEST-DR-2025-Q1 haben wir vor allem die Failover-Skripte in RB-DR-001 Abschnitt 4.3 refaktoriert. Das war nötig, weil wir in der Übung festgestellt hatten, dass die Sequenz der DNS-Propagation zu lang war, teilweise über 90 Sekunden. Jetzt nutzen wir asynchrone Updates mit Watchdog-Timern."}
{"ts": "162:33", "speaker": "I", "text": "Gab es dabei auch Änderungen an den automatisierten Checks?"}
{"ts": "162:36", "speaker": "E", "text": "Ja, wir haben in der Runbook-Version RB-DR-001-v2 einen zusätzlichen Health-Check für den Messaging-Service eingebaut. Vorher haben wir nur den Auth-Cluster geprüft, aber wegen der Kettenreaktion im letzten Drill (Ticket DR-TKT-8721) war klar, dass Messaging ein kritischer Pfad ist."}
{"ts": "162:50", "speaker": "I", "text": "Wie wirkt sich das auf die RTO-Ziele aus?"}
{"ts": "162:54", "speaker": "E", "text": "Wir konnten die Recovery Time Objective im Multi-Region-Szenario um etwa 25% verbessern. Konkret von 8 Minuten auf rund 6 Minuten – gemessen nach SLA-DR-2024-02. Das war aber nur möglich, weil wir parallel auch die Netzwerk-Routen vorab konfiguriert haben."}
{"ts": "163:07", "speaker": "I", "text": "Gab es Bedenken hinsichtlich der Kosten dieser Vorabkonfiguration?"}
{"ts": "163:11", "speaker": "E", "text": "Definitiv, die Always-on-Routen kosten uns im Monat etwa 2,5% mehr im Cloud-Budget. Wir haben das im Architekturboard diskutiert und in Protokoll AB-2025-04 dokumentiert. Der Konsens war, dass diese Mehrkosten durch die Risikominderung gerechtfertigt sind."}
{"ts": "163:25", "speaker": "I", "text": "Gibt es noch offene Risiken, die Sie für kritisch halten?"}
{"ts": "163:29", "speaker": "E", "text": "Ja, das Thema Cross-Region-Latency ist noch nicht zufriedenstellend gelöst. Bei Spitzenlasten kann es zu 300-400 ms Verzögerung kommen, was für einige synchronisierte Dienste wie Billing-API grenzwertig ist. Dafür gibt es aktuell nur einen Workaround im Runbook Appendix C."}
{"ts": "163:43", "speaker": "I", "text": "Könnten Sie den Workaround kurz skizzieren?"}
{"ts": "163:46", "speaker": "E", "text": "Klar, wir puffern kritische Schreiboperationen lokal und replizieren sie verzögert, wenn die Latenz unter 200 ms fällt. Das ist suboptimal, weil es temporär zu Inkonsistenzen führt, aber besser als ein kompletter Serviceabbruch."}
{"ts": "164:00", "speaker": "I", "text": "Wie planen Sie, diese Problematik langfristig zu adressieren?"}
{"ts": "164:04", "speaker": "E", "text": "Wir evaluieren gerade eine Partitionierung der Workloads nach Latenz-Sensitivität. Das würde heißen, dass nicht alle Services in Echtzeit zwischen den Regionen synchronisiert werden müssen. Dazu gibt es einen Proof of Concept im Lab-Setup LAB-DR-POC-07."}
{"ts": "164:18", "speaker": "I", "text": "Abschließend: Welche Lessons Learned aus den bisherigen Drills würden Sie als wichtigste Erfolgsfaktoren nennen?"}
{"ts": "164:22", "speaker": "E", "text": "Nummer eins: Frühzeitiges Erkennen von Abhängigkeitsketten, wie zwischen Auth und Messaging. Nummer zwei: Klare, getestete Runbooks mit Versionskontrolle. Und drittens: Das Balancehalten zwischen Kosten und Resilienz, auch wenn es unpopuläre Entscheidungen erfordert."}
{"ts": "163:35", "speaker": "I", "text": "Bevor wir zum Abschluss kommen, würde mich interessieren, ob es seit dem letzten Drill noch Anpassungen an RB-DR-001 gegeben hat, die nicht formell im RFC-Portal gelandet sind."}
{"ts": "163:39", "speaker": "E", "text": "Ja, tatsächlich. Wir haben nach TEST-DR-2025-Q1 ein inoffizielles Addendum geschrieben, weil uns beim Failover des Messaging-Service auffiel, dass die DNS-TTL-Werte zu hoch waren. Das steht jetzt als interner Hinweis im Confluence, aber noch nicht in RFC-DR-17."}
{"ts": "163:44", "speaker": "I", "text": "Das heißt, im Ernstfall würden Sie auf dieses Addendum zurückgreifen, auch wenn es nicht offiziell freigegeben ist?"}
{"ts": "163:48", "speaker": "E", "text": "Genau, wir haben es sogar in unserer lokalen Kopie des Runbooks markiert. Wir wissen, dass das formell nicht optimal ist, aber im Drill hat es uns gut 12 Minuten RTO gespart."}
{"ts": "163:53", "speaker": "I", "text": "Wie evaluieren Sie solche Abkürzungen im Verhältnis zu den Compliance-Vorgaben?"}
{"ts": "163:57", "speaker": "E", "text": "Wir machen eine Risikoabschätzung nach Schema RA-OPS-05. In diesem Fall haben wir es als 'low regulatory impact, high operational gain' eingestuft, dokumentiert im Ticket OPS-DR-8821."}
{"ts": "164:02", "speaker": "I", "text": "Gab es Diskussionen mit dem Architekturteam dazu?"}
{"ts": "164:06", "speaker": "E", "text": "Ja, der Architekt hat vorgeschlagen, die TTL-Anpassung in den Standard zu übernehmen, aber erst nach einem Cross-Region-Test mit dem Auth-Cluster. Wir wollen vermeiden, dass dort Replikations-Latenzen entstehen."}
{"ts": "164:11", "speaker": "I", "text": "Das erinnert an die Kettenreaktion, die Sie vorhin erwähnt hatten. Gibt es aktuelle Maßnahmen, um solche Effekte früher zu erkennen?"}
{"ts": "164:15", "speaker": "E", "text": "Wir haben ein neues Monitoring-Template im Tool Zabbrix eingerichtet, das bei Failover-Events simultan die Latenzen in Auth-Cluster, Messaging und Storage misst. Das ging aus den Lessons Learned von TEST-DR-2025-Q1 hervor."}
{"ts": "164:20", "speaker": "I", "text": "Spüren Sie schon Effekte dieser neuen Überwachung?"}
{"ts": "164:24", "speaker": "E", "text": "Ja, wir hatten letzte Woche einen Mikroausfall im Secondary-Region-Storage. Das Template schlug sofort Alarm und wir konnten vor dem automatischen Failover eingreifen."}
{"ts": "164:29", "speaker": "I", "text": "Hat sich das in den SLAs niedergeschlagen?"}
{"ts": "164:33", "speaker": "E", "text": "Definitiv. Unser interner SLA-Bericht für Q2 zeigt, dass wir die Mean Time to Detect von 4 Minuten auf 90 Sekunden gesenkt haben. Das ist auch im SLA-Report-OPS-2025-Q2 vermerkt."}
{"ts": "164:38", "speaker": "I", "text": "Gibt es noch offene Risiken, die Sie jetzt, nach diesen Verbesserungen, als kritisch einstufen?"}
{"ts": "164:42", "speaker": "E", "text": "Ja, die Kostenentwicklung bei gleichzeitiger Ausweitung auf eine dritte Region. Wir haben dazu ein Offsite-Risiko im Protokoll RISK-DR-203 hinterlegt, weil die Bandbreitenkosten exponentiell steigen könnten."}
{"ts": "165:07", "speaker": "I", "text": "Bevor wir abschließen, würde mich interessieren, ob es seit der letzten Drill-Phase noch Updates am RB-DR-001 gegeben hat, vielleicht im Nachgang zu den Ticket-Reviews?"}
{"ts": "165:13", "speaker": "E", "text": "Ja, wir haben in Version 1.4.2 ein neues Kapitel eingefügt, das den manuellen Umschaltpfad zwischen den Regionen beschreibt. Das kam aus Ticket DR-2025-044, wo wir in einem Test festgestellt haben, dass das Skript `failover_exec.sh` unter bestimmten Latenzbedingungen hängen bleibt."}
{"ts": "165:26", "speaker": "I", "text": "Und wie koordinieren Sie solche Änderungen mit den Architekten, um sicherzustellen, dass die Prozeduren auch im Design verankert sind?"}
{"ts": "165:32", "speaker": "E", "text": "Wir haben da einen festen Slot im wöchentlichen DR-Architektur-Sync. Dort gehen wir durch alle Runbook-Änderungen, und der Architekt prüft, ob die Anpassung in die Design-Dokumentation rein muss. Das ist quasi ein informeller RFC-Prozess, aber wir haben uns daran gewöhnt."}
{"ts": "165:45", "speaker": "I", "text": "Gab es in letzter Zeit Fälle, in denen eine Runbook-Anpassung zu Konflikten mit den SLAs geführt hat?"}
{"ts": "165:50", "speaker": "E", "text": "Hm, ja, im März-Drill. Wir haben die Auth-Cluster-Umschaltung um zusätzliche Prüfungen erweitert, was die RTO von 18 auf 22 Minuten erhöht hat. Laut SLA-DR-01 hätten wir aber bei 20 Minuten bleiben sollen. Das haben wir dann in DR-Review-Meeting dokumentiert und als temporären SLA-Override freigegeben."}
{"ts": "166:05", "speaker": "I", "text": "Interessant. Wie sieht das Monitoring aus, um solche SLA-Verstöße im Drill sofort zu erkennen?"}
{"ts": "166:10", "speaker": "E", "text": "Wir haben in Prometheus ein spezielles Dashboard 'DR-Drill-Status', das Metriken wie `failover_duration_seconds` und `service_recovery_lag` in Echtzeit zeigt. Bei Überschreitung gibt's einen roten Alarm in unserem Incident-Channel."}
{"ts": "166:21", "speaker": "I", "text": "Gab es auch schon mal false positives dabei?"}
{"ts": "166:26", "speaker": "E", "text": "Oh ja, beim Quartalstest Q1/2025. Da hat der Messaging-Service einen Replay-Backlog verursacht, der die Recovery-Zeit künstlich aufgebläht hat. Laut RB-DR-001 hätten wir das als 'non-critical delay' markieren müssen, das hatten wir aber im Alerting noch nicht berücksichtigt."}
{"ts": "166:40", "speaker": "I", "text": "Wurde das inzwischen im Runbook oder im Monitoring angepasst?"}
{"ts": "166:45", "speaker": "E", "text": "Ja, wir haben das als Step 4.3.1 ergänzt: 'Check Message Queue Lag'. Und in Grafana gibt's jetzt einen separaten Filter, um diesen Delay zu exkludieren, wenn er unter einem definierten Threshold liegt."}
{"ts": "166:56", "speaker": "I", "text": "Letzte Frage: Welche Optimierung würden Sie als nächstes angehen, um sowohl Kosten als auch SLA einzuhalten?"}
{"ts": "167:01", "speaker": "E", "text": "Ich würde das Cross-Region-Datenbank-Streaming auf inkrementelle Snapshots umstellen. Das reduziert die laufenden Transferkosten um schätzungsweise 15%, und wir könnten die Replikationslatenz unter 2 Sekunden halten – das wäre ein klarer Gewinn für unser RPO."}
{"ts": "167:13", "speaker": "I", "text": "Gibt es dafür schon ein Ticket oder eine Roadmap-Position?"}
{"ts": "167:17", "speaker": "E", "text": "Ja, das ist als EPIC DR-OPT-2025 im Backlog, mit Ziel-Q3. Wir haben die Proof-of-Concept-Phase eingeplant und werden das in der nächsten GameDay-Serie parallel testen."}
{"ts": "171:07", "speaker": "I", "text": "Könnten Sie bitte noch etwas tiefer auf die Lessons Learned aus dem Drill TEST-DR-2025-Q1 eingehen? Mich interessiert, welche operativen Änderungen daraus entstanden sind."}
{"ts": "171:14", "speaker": "E", "text": "Ja, klar. Also, im Drill hatten wir festgestellt, dass die initiale Sync-Phase zwischen Region West und Region Ost deutlich länger dauerte als im Runbook RB-DR-001 angenommen. Wir haben daraufhin in der Revision 1.3 die Pufferzeiten von 8 auf 15 Minuten gesetzt und zusätzlich eine Pre-Warm-Option für die Storage-Nodes eingeführt."}
{"ts": "171:29", "speaker": "I", "text": "Gab es dafür einen spezifischen Auslöser oder war das eine generelle Beobachtung?"}
{"ts": "171:34", "speaker": "E", "text": "Der Auslöser war ein Incident während des Drills, Ticket DR-INC-7742. Wir hatten eine Diskrepanz im Delta-Sync, weil der Messaging-Service im Osten noch auf alte Queue-Offsets zeigte. Das hat uns fast 12 Minuten gekostet, bis wir die Offsets neu gesetzt hatten."}
{"ts": "171:50", "speaker": "I", "text": "Das klingt so, als hätte eine Abhängigkeit außerhalb des eigentlichen DR-Scopes den Prozess beeinflusst."}
{"ts": "171:55", "speaker": "E", "text": "Ganz genau. Obwohl der Messaging-Service formal nicht im Scope von P-TIT ist, hängt der Auth-Cluster an einer Queue für Session-Invalidierungen. Wenn die nicht sauber nachzieht, können sich User nach Failover nicht neu authentifizieren. Das war eine der Kettenreaktionen, die wir im Drill sichtbar gemacht haben."}
{"ts": "172:13", "speaker": "I", "text": "Wie haben Sie diese Erkenntnis in die Architektur-Dokumentation eingearbeitet?"}
{"ts": "172:18", "speaker": "E", "text": "Wir haben im RFC-DR-042 einen neuen Abschnitt 'Cross-Service Dependencies' eingefügt, der beschreibt, welche Dienste zwingend parallel migriert oder synchronisiert werden müssen. Außerdem gibt es jetzt in RB-DR-001 einen Verweis auf ein separates Runbook RB-MSG-004 für den Messaging-Service."}
{"ts": "172:35", "speaker": "I", "text": "Gab es nach diesen Anpassungen schon einen erneuten Testlauf?"}
{"ts": "172:39", "speaker": "E", "text": "Ja, in einem kleineren Scope, TEST-DR-2025-Q2. Dort konnten wir die Sync-Zeit um 9 Minuten reduzieren und hatten keine Queue-Offset-Probleme mehr. Allerdings ist uns dort ein neues Problem mit den Rate Limits der Cloud-API in Region Ost begegnet."}
{"ts": "172:56", "speaker": "I", "text": "Interessant. Haben Sie dafür schon eine Mitigation in Aussicht?"}
{"ts": "173:00", "speaker": "E", "text": "Wir planen, die API-Calls in Batches zu throttlen und in RB-DR-001 einen Abschnitt 'API Rate Management' zu ergänzen. Das ist aktuell in Ticket DR-OPT-882 beschrieben, Zielumsetzung bis Ende Q3."}
{"ts": "173:14", "speaker": "I", "text": "Wenn Sie auf die bisherigen Optimierungen schauen – würden Sie sagen, dass der Trade-off zwischen zusätzlicher Komplexität und erhöhter Resilienz gerechtfertigt ist?"}
{"ts": "173:21", "speaker": "E", "text": "Aus meiner Sicht ja. Jede zusätzliche Abhängigkeit, die wir im Drill identifizieren, erhöht zwar den Aufwand in der Orchestrierung, aber minimiert das Risiko eines echten Ausfalls. Wir dokumentieren das explizit im Risk Register DR-RSK-2025-01, damit das Management die Kosten-Nutzen-Abwägung nachvollziehen kann."}
{"ts": "173:38", "speaker": "I", "text": "Gibt es Punkte, die Sie trotz allem noch als nicht ausreichend mitigiert ansehen?"}
{"ts": "173:43", "speaker": "E", "text": "Ja, insbesondere die Abhängigkeit zu einem externen Identity-Provider. Dessen SLA garantiert nur 99,5 % Verfügbarkeit. Wir haben intern 99,95 % als Ziel, aber eine echte Redundanz dafür haben wir noch nicht. In DR-RSK-2025-04 ist das als 'High Impact / Medium Likelihood' vermerkt und steht als Top-Priorität für das nächste Architektur-Review."}
{"ts": "180:07", "speaker": "I", "text": "Sie hatten vorhin erwähnt, dass einige Optimierungen noch in der Pipeline sind. Können Sie konkreter sagen, welche Maßnahmen aktuell mit höchster Priorität verfolgt werden?"}
{"ts": "180:27", "speaker": "E", "text": "Ja, aktuell arbeiten wir an der Automatisierung der Cross-Region-DNS-Umschaltung. Das ist in RFC-DR-2025-02 dokumentiert. Ziel ist es, die manuelle Latenz von etwa drei Minuten auf unter 45 Sekunden zu reduzieren."}
{"ts": "180:50", "speaker": "I", "text": "Und wie testen Sie diese Automatisierung, bevor sie in die produktive DR-Umgebung integriert wird?"}
{"ts": "181:06", "speaker": "E", "text": "Wir fahren Simulationen in einer isolierten Staging-Region. Dabei nutzen wir das Runbook RB-DR-004, das speziell für DNS-Failover-Tests entwickelt wurde. Die Logs werden dann mit den Ergebnissen aus TEST-DR-2025-Q1 verglichen."}
{"ts": "181:32", "speaker": "I", "text": "Gab es bei diesen Simulationen bisher Auffälligkeiten oder unerwartete Ergebnisse?"}
{"ts": "181:44", "speaker": "E", "text": "Einmal, ja. Die Health-Checks des Load-Balancers haben zu früh auf den Zielcluster geschaltet, bevor der Auth-Cluster dort komplett synchron war. Das hat für etwa 90 Sekunden zu Authentifizierungsfehlern geführt."}
{"ts": "182:10", "speaker": "I", "text": "Wie haben Sie darauf reagiert?"}
{"ts": "182:22", "speaker": "E", "text": "Wir haben in Ticket DRBUG-1456 festgehalten, dass ein zusätzlicher Gate-Check gegen den Messaging-Service notwendig ist, bevor Traffic umgeleitet wird. Der Patch ist bereits in der Test-Pipeline."}
{"ts": "182:48", "speaker": "I", "text": "Können Sie einschätzen, ob diese Anpassung auch Performance-Auswirkungen haben wird?"}
{"ts": "183:01", "speaker": "E", "text": "Minimal, wir rechnen mit zusätzlichen 5–7 Sekunden Delay im Failover-Prozess. Das liegt noch innerhalb unseres SLA von 60 Sekunden RTO für kritische Services."}
{"ts": "183:20", "speaker": "I", "text": "Wie kommunizieren Sie solche Änderungen intern, gerade wenn es um SLAs geht?"}
{"ts": "183:34", "speaker": "E", "text": "Über das wöchentliche DR-Change-Board. Dort sind Architekten, SREs und Service Owner vertreten. Änderungen an SLAs oder Runbooks werden dort explizit diskutiert und sign-off dokumentiert."}
{"ts": "183:58", "speaker": "I", "text": "Haben Sie für die nächste Drill-Phase schon konkrete Szenarien im Blick?"}
{"ts": "184:09", "speaker": "E", "text": "Ja, wir planen einen kombinierten Ausfall von Primär-Region und einem Teil der Backup-Region, um Kaskadeneffekte zu testen. Das Szenario ist in TESTPLAN-DR-2025-Q3 beschrieben."}
{"ts": "184:32", "speaker": "I", "text": "Das klingt anspruchsvoll. Welche Risiken wollen Sie damit gezielt adressieren?"}
{"ts": "184:45", "speaker": "E", "text": "Vor allem die Gefahr von partiellen Dateninkonsistenzen zwischen Messaging-Queues und Auth-Logs. Wir wollen sehen, ob die neuen Konsistenz-Checks aus RB-DR-006 diese Abweichungen zuverlässig erkennen."}
{"ts": "187:07", "speaker": "I", "text": "Sie hatten vorhin angedeutet, dass einige Optimierungen aus den letzten GameDays noch in der Pipeline sind. Können Sie mal ein Beispiel nennen, bei dem Sie bewusst auf die Umsetzung gewartet haben, um es in einem größeren Wartungsfenster zu bündeln?"}
{"ts": "187:27", "speaker": "E", "text": "Ja, das betrifft vor allem die Anpassung der Failover-Logik im Runbook RB-DR-001. Wir haben nach TEST-DR-2025-Q1 erkannt, dass die Sequenz für das Neustarten der Messaging-Nodes zu knapp kalkuliert ist. Da aber gleichzeitig ein Firmware-Upgrade am Auth-Cluster anstand, haben wir entschieden, beides in einem abgestimmten Wartungsfenster im Q3 zusammenzuführen."}
{"ts": "187:53", "speaker": "I", "text": "Das heißt, Sie nehmen hier bewusst in Kauf, dass ein bekanntes Optimierungspotenzial erst später gehoben wird?"}
{"ts": "188:05", "speaker": "E", "text": "Genau, das ist ein klassischer Trade-off. Wir haben im Ticket DR-OPT-442 dokumentiert, dass das Risiko eines verzögerten Rollouts akzeptabel ist, weil der derzeitige Ablauf innerhalb des SLA von 15 Minuten RTO liegt. Die Ressourcenplanung und die Minimierung von Downtime für abhängige Systeme hatten Vorrang."}
{"ts": "188:31", "speaker": "I", "text": "Wie haben Sie diese Priorisierung intern kommuniziert? Gab es da Gegenstimmen?"}
{"ts": "188:43", "speaker": "E", "text": "Wir haben das im DR Steering Board besprochen. Zwei SRE-Kollegen hätten gern sofort umgestellt, aber die Architekturseite hat auf die Synchronisierung mit dem Firmware-Upgrade bestanden. Letztlich haben wir einen Konsens gefunden und die Entscheidung im Protokoll SB-DR-2025-04 festgehalten."}
{"ts": "189:09", "speaker": "I", "text": "Sie haben jetzt mehrfach das Steering Board erwähnt – wie oft tagt dieses Gremium in der Drill-Phase?"}
{"ts": "189:21", "speaker": "E", "text": "Während der Drill-Phase alle zwei Wochen. Da werden Findings aus laufenden Tests wie TEST-DR-2025-Q2 sofort besprochen. In der Akutphase eines simulierten Ausfalls können auch Ad-hoc-Meetings einberufen werden, meist per Video mit geteiltem Incident-Board."}
{"ts": "189:46", "speaker": "I", "text": "Gab es kürzlich eine Situation, wo diese Ad-hoc-Form zwingend notwendig war?"}
{"ts": "189:57", "speaker": "E", "text": "Ja, vor drei Wochen bei einem internen Stresstest. Wir haben eine gezielte Latenzerhöhung im Europe-West-Cluster gefahren, um die Replikationspfade zum US-East-Cluster zu überprüfen. Dabei fiel auf, dass der Auth-Cluster bei bestimmten JWT-Refresh-Requests in einen Retry-Loop ging – das stand so nicht im Runbook. Wir mussten sofort cross-funktional reagieren."}
{"ts": "190:29", "speaker": "I", "text": "Und wie wurde das Problem im Nachgang ins Runbook integriert?"}
{"ts": "190:40", "speaker": "E", "text": "Wir haben in RB-DR-001 einen neuen Abschnitt 4.3 eingefügt, der explizit den Umgang mit Auth-Cluster Retry-Loops beschreibt. Außerdem gab es ein ergänzendes Troubleshooting-Playbook PB-AUTH-2025-01, das mit Screenshots aus dem Kibana-Dashboard arbeitet."}
{"ts": "191:06", "speaker": "I", "text": "Klingt nach einer guten Lessons Learned Umsetzung. Gab es bei der Anpassung Herausforderungen?"}
{"ts": "191:18", "speaker": "E", "text": "Ja, die größte Hürde war, dass wir die Messaging-Service-Timeouts parallel anpassen mussten, weil sonst die Kettenreaktion weiter eskaliert wäre. Das hat gezeigt, wie eng die Subsysteme verzahnt sind – eine Änderung zieht oft zwei, drei weitere nach sich."}
{"ts": "191:42", "speaker": "I", "text": "Wenn Sie auf die bisherigen Drills blicken: Welche Risiken sehen Sie trotz aller Optimierungen weiterhin als kritisch an?"}
{"ts": "191:56", "speaker": "E", "text": "Das größte offene Risiko ist aktuell die manuelle Umschaltung der DNS-Zonen in einem Szenario, in dem sowohl Europe-West als auch Asia-South beeinträchtigt sind. Wir haben zwar ein internes RFC-DRDNS-2025-07 gestartet, aber die Implementierung eines vollautomatischen Multi-Zonen-Failovers braucht noch Zeit und sorgfältige Tests, um keine unbeabsichtigten Ausfälle zu verursachen."}
{"ts": "195:07", "speaker": "I", "text": "Im letzten Abschnitt hatten Sie die Schnittstellen zwischen Auth-Cluster und Messaging-Service erwähnt. Können Sie genauer erläutern, wie diese bei einem Failover im Titan DR zusammenspielen?"}
{"ts": "195:17", "speaker": "E", "text": "Ja, also beim Failover laut RB-DR-001 wird zunächst der Auth-Cluster in der Zielregion hochgefahren, bevor der Messaging-Service seine Queues synchronisiert. Das ist wichtig, weil viele Services ihre Tokens neu validieren müssen, sonst bricht die Kommunikation ab."}
{"ts": "195:34", "speaker": "I", "text": "Verstehe, und wie wird diese Reihenfolge in den Prozeduren abgesichert?"}
{"ts": "195:41", "speaker": "E", "text": "Wir haben das in Step 4 und 5 vom RB-DR-001 verankert. Außerdem gibt es in Runbook RB-AUTH-002 eine Event-Checkliste, die vom SRE-Team abgezeichnet wird, bevor Messaging überhaupt umschaltet."}
{"ts": "195:55", "speaker": "I", "text": "Gab es im letzten Drill Fälle, wo diese Abfolge nicht eingehalten wurde?"}
{"ts": "196:03", "speaker": "E", "text": "Ja, in TEST-DR-2025-Q1 hatten wir Ticket DR-INC-774, wo Messaging zu früh gestartet wurde. Folge war ein Backlog von 12.000 Nachrichten, die unverarbeitet blieben, bis der Auth-Cluster synchron war."}
{"ts": "196:20", "speaker": "I", "text": "Und wie haben Sie dieses Problem adressiert?"}
{"ts": "196:26", "speaker": "E", "text": "Wir haben im Runbook einen zusätzlichen Wait-Flag eingeführt, der den Messaging-Failover blockiert, bis der Auth-Healthcheck zweimal grün meldet. Das wurde in RFC-DR-045 dokumentiert."}
{"ts": "196:44", "speaker": "I", "text": "Interessant. Wechseln wir kurz zu den Kosten-Performance-Überlegungen: Welche konkreten Trade-offs mussten Sie bei der Multi-Region-Strategie eingehen?"}
{"ts": "196:54", "speaker": "E", "text": "Das größte Dilemma war Storage-Replikation. Synchrone Replikation hätte die RPO-Ziele perfekt erfüllt, aber die Latenz zwischen Region Nord und Region Süd war zu hoch und die Kosten für dedizierte Leitungen enorm."}
{"ts": "197:10", "speaker": "E", "text": "Wir sind daher auf asynchrone Replikation mit einem 30-Sekunden-Lag gegangen, was RPO minimal verschlechtert, aber die Bandbreitenkosten um 40% reduziert hat."}
{"ts": "197:23", "speaker": "I", "text": "Gab es dazu Einwände aus dem Business?"}
{"ts": "197:28", "speaker": "E", "text": "Ja, vor allem aus der Abteilung Compliance, die auf 0-Datenverlust pocht. Wir haben das mit einem Risiko-Memo in Ticket DR-RISK-112 adressiert, inklusive Szenarienanalyse und Recovery-Strategien."}
{"ts": "197:44", "speaker": "I", "text": "Und welche Risiken sehen Sie trotz dieser Maßnahmen noch als nicht ausreichend mitigiert?"}
{"ts": "197:52", "speaker": "E", "text": "Zum einen die Abhängigkeit vom zentralen Config-Service, der zwar multi-regionfähig ist, aber im Drill TEST-DR-2025-Q1 in beiden Regionen gleichzeitig fehlerhaft konfiguriert war. Das ist aktuell nur durch manuellen Eingriff zu lösen."}
{"ts": "198:09", "speaker": "E", "text": "Zum anderen haben wir noch keine saubere Automatisierung für den DNS-Switch, der bei globalen Incidents sehr schnell greifen muss. Das steht als Action Item in unserem Protokoll DR-ACT-89."}
{"ts": "203:47", "speaker": "I", "text": "Lassen Sie uns jetzt tiefer auf die Multi‑Region‑Architektur eingehen. Wie genau hängt der Auth‑Cluster mit dem Messaging‑Service im DR‑Fall zusammen?"}
{"ts": "203:59", "speaker": "E", "text": "Der Auth‑Cluster ist quasi das Gatekeeper‑System, ohne das keine Messaging‑Queue akzeptiert wird. Im RB‑DR‑001 ist klar festgelegt, dass beim Failover zuerst der Auth‑Cluster in der Zielregion hochgefahren wird, bevor wir den Messaging‑Service synchen. Das hat im TEST‑DR‑2025‑Q1 gut funktioniert, bis wir einen Delay bei der Token‑Replikation hatten."}
{"ts": "204:21", "speaker": "I", "text": "War dieser Delay vorhersehbar, oder kam der überraschend?"}
{"ts": "204:29", "speaker": "E", "text": "Teilweise vorhersehbar – wir hatten in Ticket DR‑1273 schon auf Latenzen > 500 ms hingewiesen, aber im Drill stieg das auf 1,2 Sekunden. Das lag an einer ungetesteten Netzroute zwischen Region West‑2 und Ost‑1."}
{"ts": "204:48", "speaker": "I", "text": "Und wie haben Sie darauf reagiert, während des Drills?"}
{"ts": "204:55", "speaker": "E", "text": "Wir haben den Messaging‑Service in einen degradierte Mode geschaltet, wie in Abschnitt 4.2 des Runbooks beschrieben. Dadurch konnten zumindest Prioritäts‑Nachrichten durchgehen, was unser RTO‑Ziel von 15 Minuten noch rettete."}
{"ts": "205:14", "speaker": "I", "text": "Gab es Kettenreaktionen auf andere Subsysteme?"}
{"ts": "205:21", "speaker": "E", "text": "Ja, der Delay hat den Payment‑Service beeinflusst, der seine Session‑Tokens ebenfalls vom Auth‑Cluster bezieht. In TEST‑DR‑2025‑Q1 mussten wir dafür ein temporäres Token‑Caching aktivieren, eine Maßnahme, die wir aus einem früheren GameDay kannten."}
