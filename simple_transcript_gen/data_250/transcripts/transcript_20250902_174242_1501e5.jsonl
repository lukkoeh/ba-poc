{"ts": "00:00", "speaker": "I", "text": "Können Sie mir bitte einmal den aktuellen Stand des Orion Edge Gateway Projekts schildern, gerade im Hinblick auf die Build-Phase und Ihre Verantwortlichkeiten?"}
{"ts": "03:15", "speaker": "E", "text": "Klar, also wir sind im Build-Phase-Midpoint, API Gateway Core ist implementiert, die Rate-Limiting-Engine hat gerade den ersten Integrationstest bestanden. Als SRE kümmere ich mich um Observability, Oncall-Playbooks und das Automatisieren der mTLS-Zertifikatsrotation. DevOps-seitig übernimmt mein Kollege das CI/CD für die Auth-Komponenten."}
{"ts": "07:02", "speaker": "I", "text": "Das heißt, Sie haben auch direkten Einfluss auf die Auth-Integration?"}
{"ts": "10:21", "speaker": "E", "text": "Ja, wir arbeiten eng mit dem Aegis IAM Team zusammen, um Just-in-Time Access Tokens in die Gateway Pipeline zu injizieren. Wir halten uns strikt an POL-SEC-001, unser Policy für Least Privilege, und prüfen bei jedem Merge, ob irgendwelche Rollen überprivilegiert sind."}
{"ts": "14:11", "speaker": "I", "text": "Wie stellen Sie in der Praxis sicher, dass dieses Least Privilege tatsächlich umgesetzt wird?"}
{"ts": "18:44", "speaker": "E", "text": "Wir haben im Build-Stage einen automatischen Policy-Linter, der YAML-basiert die Roles gegen erlaubte Scope-Definitionen prüft. Lessons Learned aus Incident GW-4821 – damals hatten wir eine zu breite ServiceAccount-Berechtigung – sind direkt in diese Pipeline eingeflossen."}
{"ts": "23:10", "speaker": "I", "text": "GW-4821, das war der Vorfall mit den zu weit gefassten API Keys, richtig?"}
{"ts": "27:55", "speaker": "E", "text": "Genau. Damals fehlte uns eine Pre-Deployment-Prüfung. Jetzt haben wir in RB-GW-011 einen Schritt für Blue/Green Rollouts ergänzt, bei dem parallel ein Security Smoke Test läuft, der auch AuthZ-Pfade testet."}
{"ts": "32:30", "speaker": "I", "text": "Apropos RB-GW-011 – können Sie mir den Ablauf eines Blue/Green Rollouts nach diesem Runbook skizzieren?"}
{"ts": "36:48", "speaker": "E", "text": "Klar: Wir deployen parallel eine 'Blue'-Instanz mit neuem Build, routen 5% Traffic dorthin, messen p95 Latenz und Error Rate. Wenn SLA-ORI-02 im 15-Minuten-Fenster erfüllt ist, erhöhen wir stufenweise. Bei Abweichung triggern wir ein automatisches Rollback per Jenkins Pipeline."}
{"ts": "41:12", "speaker": "I", "text": "Und welche KPIs überwachen Sie dafür konkret?"}
{"ts": "45:30", "speaker": "E", "text": "Primär p95 Latenz < 120ms, AuthN Success Rate > 99,95%, und mTLS Handshake Failure Rate < 0,1%. Diese Metriken sind in Prometheus hinterlegt und via Grafana-Dashboards im Oncall-Runbook verlinkt."}
{"ts": "50:05", "speaker": "I", "text": "Wie interagiert das Gateway denn in diesem Setup mit Poseidon Networking für mTLS?"}
{"ts": "54:20", "speaker": "E", "text": "Das Gateway ruft über Poseidons SecureChannel API die Zertifikate ab und validiert sie lokal. Änderungen bei Poseidon – etwa Cipher Suite Updates – müssen wir mit Aegis IAM abstimmen, weil die Auth Tokens auch die TLS Layer beeinflussen können."}
{"ts": "59:44", "speaker": "I", "text": "Sehen Sie Risiken bei aggressivem Rate Limiting, z.B. bei legitimen Lastspitzen?"}
{"ts": "90:00", "speaker": "E", "text": "Ja, absolut. Wir hatten ein Test-Szenario, Ticket ORI-TEST-219, wo ein legitimer Partner-Loadtest geblockt wurde. Wir haben daraufhin adaptive Rate Limiting konfiguriert – basierend auf Consumer-Trust-Level – um Availability nicht zu opfern, während wir Security hoch halten."}
{"ts": "90:00", "speaker": "I", "text": "Wir haben ja vorhin die Abhängigkeiten zu Aegis IAM und Poseidon Networking beleuchtet. Können Sie jetzt noch einmal konkret darauf eingehen, wie sich die Lessons Learned aus Incident GW-4821 in Ihre aktuellen Build-Pipelines übertragen haben?"}
{"ts": "90:08", "speaker": "E", "text": "Ja, sicher. Aus GW-4821 haben wir gelernt, dass unsere AuthZ Tests nicht nur synthetisch, sondern auch unter realer Last gefahren werden müssen. In der Build-Pipeline haben wir daher ein Stage eingebaut, die mit Daten aus dem Poseidon Testnetz arbeitet. Dadurch sehen wir schon vor Deploy, ob JIT Access via Aegis IAM unter mTLS auch bei 80% Peak Load noch funktioniert."}
{"ts": "90:20", "speaker": "I", "text": "Und diese Stage ist fest in RB-GW-011 verankert oder eher eine Ad-hoc-Ergänzung?"}
{"ts": "90:26", "speaker": "E", "text": "RB-GW-011 hatte ursprünglich nur die Blue/Green Rollout-Sequenz drin, aber wir haben in Revision 3.2 eine Pre-Prod Smoke-Testphase ergänzt. Das ist jetzt verbindlich, weil SLA-ORI-02 sonst gefährdet wäre."}
{"ts": "90:38", "speaker": "I", "text": "Verbindlich klingt gut, aber wie stellen Sie die Einhaltung sicher?"}
{"ts": "90:43", "speaker": "E", "text": "Wir loggen jede Stage als Build-Artifact und archivieren sie im internen Compliance-Repo. Außerdem gibt es einen Jenkins-Job, der prüft, ob das Stage-Log die Kennung 'AUTHZ-PREPROD-OK' enthält, bevor ein Green-Deployment freigegeben wird."}
{"ts": "90:55", "speaker": "I", "text": "Kommen wir noch mal zur p95 Latenz < 120ms. Haben Sie aktuell Abweichungen festgestellt?"}
{"ts": "91:01", "speaker": "E", "text": "In den letzten zwei Wochen gab es zwei Spikes auf 135ms, jeweils während simultaner mTLS Handshakes und IAM Token Refreshes. Wir haben das in Ticket ORI-PRF-219 dokumentiert und arbeiten an einem Session-Reuse-Mechanismus für mTLS."}
{"ts": "91:15", "speaker": "I", "text": "Das klingt nach einem typischen Performance-vs-Security Trade-off. Wie priorisieren Sie das?"}
{"ts": "91:20", "speaker": "E", "text": "Wir nutzen eine Matrix aus Risiko- und Impact-Bewertung. Für ORI-PRF-219 hat die Security-Prio leicht höheres Gewicht, aber wir implementieren eben Optimierungen wie TLS Session Tickets, um die Latenz zu senken ohne mTLS aufzugeben."}
{"ts": "91:34", "speaker": "I", "text": "Wie schnell können Sie solche Optimierungen in den Build bringen, ohne die Abnahmezyklen zu sprengen?"}
{"ts": "91:39", "speaker": "E", "text": "Dank unserer Canary-Branch-Policy können wir Änderungen in isolierten Nodes deployen und innerhalb von 48 Stunden valide Metriken gegen SLA-ORI-02 sammeln. Erst danach gehen sie in den regulären Blue/Green Flow."}
{"ts": "91:52", "speaker": "I", "text": "Und wenn die Metriken nicht passen?"}
{"ts": "91:55", "speaker": "E", "text": "Dann greift ein automatischer Rollback, ebenfalls in RB-GW-011 definiert. Wir dokumentieren das im Change-Log, und es geht in die nächste Sprint-Planung als Tech Debt Item."}
{"ts": "92:06", "speaker": "I", "text": "Letzte Frage: Gibt es derzeit ein Risiko, das Sie bewusst akzeptieren, um den Go-Live nicht zu verzögern?"}
{"ts": "92:12", "speaker": "E", "text": "Ja, wir haben die Rate-Limit-Buckets für interne Services etwas großzügiger gesetzt. Das Risiko von Abuse ist intern gering, aber es sichert, dass Batch-Jobs des Data Teams nicht ins Timeout laufen. Das ist in Risk-Register ORI-RSK-07 erfasst und für Post-Go-Live-Härtung vorgesehen."}
{"ts": "96:00", "speaker": "I", "text": "Lassen Sie uns jetzt noch einmal konkret auf RB-GW-011 eingehen. Wie strikt halten Sie sich im Deployment daran, insbesondere bei Blue/Green Rollouts?"}
{"ts": "96:15", "speaker": "E", "text": "Wir halten uns sehr strikt daran. RB-GW-011 definiert ja nicht nur die Umschaltsequenz, sondern auch die Health-Check-Intervalle. Wir haben z. B. im letzten Rollout mit Ticket CHG-ORI-245 gesehen, dass ein zusätzliches Delay von 5 Sekunden zwischen Green-Cluster-Verifikation und DNS-Switch die p95 Latenz während des Umschaltens stabil gehalten hat."}
{"ts": "96:45", "speaker": "I", "text": "Und wie messen Sie diese Stabilität dann in Bezug auf SLA-ORI-02?"}
{"ts": "97:00", "speaker": "E", "text": "Wir erfassen Metriken wie 'gateway_request_latency_p95' und 'auth_token_exchange_time'. SLA-ORI-02 fordert p95 < 120 ms, und wir triggern automatisierte Slack-Alerts, wenn drei aufeinanderfolgende 1‑Minuten‑Samples >110 ms liegen. Das ist in unserem Monitoring-Runbook RB-GW-009 dokumentiert."}
{"ts": "97:30", "speaker": "I", "text": "Gab es in der letzten Iteration Abweichungen?"}
{"ts": "97:42", "speaker": "E", "text": "Ja, kurzzeitig. Während einer erhöhten Last durch Integrationstests aus dem Poseidon-Team stieg die Latenz auf 135 ms. Wir haben das als INC-ORI-512 erfasst und in der Post-Mortem-Analyse festgestellt, dass das mTLS-Handshake-Caching nicht aktiv war."}
{"ts": "98:10", "speaker": "I", "text": "Das führt mich zu einer anderen Frage: Wie verknüpfen Sie Änderungen im Poseidon Networking mit dem Gateway, um solche Effekte zu vermeiden?"}
{"ts": "98:25", "speaker": "E", "text": "Wir haben eine wöchentliche Schnittstellenrunde mit dem Poseidon-Team. Änderungen, die mTLS-Parameter betreffen, müssen laut RFC-POSEI-007 mindestens 48 Stunden vor Deployment in unserem Staging getestet werden. In diesem Fall war es ein Hotfix, der an uns vorbeiging – seither gibt es einen automatisierten Test-Job, der Poseidon-Builds gegen unsere Gateway-Staging-Umgebung fährt."}
{"ts": "98:58", "speaker": "I", "text": "Könnte man argumentieren, dass diese zusätzlichen Tests die Time-to-Market verlangsamen?"}
{"ts": "99:10", "speaker": "E", "text": "Ja, aber die Erfahrung aus INC-ORI-512 hat uns gezeigt, dass ein halber Tag mehr Testzeit günstiger ist als ein Produktionsvorfall. Wir priorisieren in solchen Cross-Team-Abhängigkeiten die Stabilität vor Geschwindigkeit."}
{"ts": "99:32", "speaker": "I", "text": "Wie sieht es bei Aegis IAM aus? Gibt es dort ähnliche Vorabprüfungen?"}
{"ts": "99:45", "speaker": "E", "text": "Definitiv. Für JIT Access wird jeder IAM-Release-Kandidat in einer isolierten Umgebung mit simulierten Gateway-Requests verprobt. Wir nutzen dafür Szenarien aus unserem Security-Testplan SEC-TSP-019, die auch mTLS-Break und Rate-Limit-Breach simulieren."}
{"ts": "100:15", "speaker": "I", "text": "Sie hatten eingangs erwähnt, dass Sie Rate Limiting aggressiv fahren. Gab es Fälle, wo legitime Lastspitzen darunter litten?"}
{"ts": "100:28", "speaker": "E", "text": "Ja, beim Launch eines Partner-Features im Januar. Wir hatten das Burst-Limit zu tief gesetzt, wodurch 8 % legitimer Requests gedrosselt wurden. Das ist in Risk-Log RSK-ORI-014 dokumentiert. Seitdem setzen wir adaptive Limits, die anhand von historischen Trafficmustern justieren."}
{"ts": "100:55", "speaker": "I", "text": "Und die Balance zwischen mTLS und Latenz – gab es da bewusste Abstriche?"}
{"ts": "101:00", "speaker": "E", "text": "Wir haben entschieden, bei internen Microservice-zu-Microservice-Calls innerhalb desselben Trust-Domain-Clusters auf mTLS zu verzichten und stattdessen HMAC-Signaturen zu nutzen. Diese Entscheidung ist in Architektur-Entscheidungs-Record ADR-ORI-07 festgehalten, um die Latenz pro Call um ~15 ms zu senken."}
{"ts": "112:00", "speaker": "I", "text": "Lassen Sie uns nochmal konkret zu RB-GW-011 kommen: Wie läuft aus Ihrer Sicht der Blue/Green Rollout im Orion Edge Gateway praktisch ab?"}
{"ts": "112:15", "speaker": "E", "text": "Gemäß RB-GW-011 starten wir eine neue Green-Umgebung parallel zur bestehenden Blue. Wir deployen das Build-Artefakt dort, fahren synthetische Smoke Tests und dann den AuthN/AuthZ-Test mit dem Aegis IAM Staging-Cluster. Erst wenn alle Checks – inklusive mTLS-Handshake mit Poseidon Testnetz – durch sind, schalten wir über den Load Balancer schrittweise den Traffic um."}
{"ts": "112:46", "speaker": "I", "text": "Und wie verifizieren Sie, dass SLA-ORI-02, insbesondere p95 < 120ms, eingehalten wird?"}
{"ts": "113:00", "speaker": "E", "text": "Wir haben Prometheus-Alerts auf p95-Latenzen gesetzt, die direkt in unser Grafana-Dashboard ORI-Perf einfließen. Während des Green-Rollouts messen wir p95 per Endpoint, also /auth, /data, /metrics. Bei Abweichungen größer 10 ms brechen wir den Switch ab und analysieren die Latenzpfade."}
{"ts": "113:28", "speaker": "I", "text": "Interessant. Gab es zuletzt Abweichungen?"}
{"ts": "113:35", "speaker": "E", "text": "Ja, im Ticket GW-4927 hatten wir im /auth-Endpoint eine p95 von 138 ms. Root Cause war eine neue mTLS-Zertifikatsprüfung in Poseidon 4.2, die bei nicht gecachten CRLs zusätzliche Roundtrips erzeugte. Wir haben das in Absprache mit dem Poseidon-Team optimiert, indem wir CRL-Caching auf Gateway-Seite für 60 Sekunden aktiviert haben."}
{"ts": "113:59", "speaker": "I", "text": "Wie dokumentieren Sie solche Abweichungen systematisch?"}
{"ts": "114:12", "speaker": "E", "text": "Wir pflegen ein Confluence-Log 'ORI-Deviations'. Jeder Eintrag enthält Ticket-ID, Metriken vor/nach Fix, betroffene Services, sowie Lessons Learned. Für GW-4927 haben wir ergänzt: 'Vor Änderung kein mTLS-CRL-Caching, Performanceeinbruch >15% auf /auth'."}
{"ts": "114:38", "speaker": "I", "text": "Zur Multi-Hop Abhängigkeit: Wie gehen Sie vor, wenn Aegis IAM eine API-Version ändert, die JIT Access betrifft?"}
{"ts": "114:50", "speaker": "E", "text": "Wir haben in CI eine Integration Stage, die den Orion Edge Gateway Build automatisch gegen die jeweils aktuelle Aegis IAM Beta testet. Beispiel: Aegis 3.1 hat den Claim 'x-jit-scope' eingeführt. Unser Auth-Adapter hat daraufhin eine Mapping-Tabelle implementiert, um alte und neue Claims parallel zu unterstützen, bis Green vollständig live ist."}
{"ts": "115:20", "speaker": "I", "text": "Und bei Poseidon Networking?"}
{"ts": "115:27", "speaker": "E", "text": "Ähnlich. Änderungen im Poseidon mTLS-Handshake-Protokoll werden in einem separaten Staging-VLAN getestet. Wir haben dafür einen Runbook-Abschnitt in RB-GW-011 Appendix B, der 'mTLS Proto-Change Simulation' beschreibt. Enthält Testmatrix mit Latenz, Error Rate, und Fallback-Strategien."}
{"ts": "115:54", "speaker": "I", "text": "Kommen wir zu den Trade-offs: Aggressives Rate Limiting kann ja legitime Lastspitzen treffen. Welche Risiken sehen Sie?"}
{"ts": "116:05", "speaker": "E", "text": "Das Hauptrisiko ist, dass bei synchronisierten Client-Retries – z. B. nach einem externen Ausfall – tausende Requests binnen Sekunden eintreffen und durch das Limit 429-Fehler erzeugen. Das kann wiederum zu Kaskadeneffekten führen. Wir mitigieren das mit Burst-Buffern und adaptiven Limits, die bei bekannten Kunden-IPs temporär anheben."}
{"ts": "116:33", "speaker": "I", "text": "Und wie haben Sie die Balance zwischen p95 < 120 ms und mTLS gefunden?"}
{"ts": "116:45", "speaker": "E", "text": "Wir haben gemessen, dass der mTLS-Handshake ~35 ms kostet. Um im Budget zu bleiben, mussten wir Server-TLS-Session-Resumption aktivieren und Keep-Alive zwischen Gateway und Backend forcieren. Zusätzlich haben wir die Cipher-Suite so gewählt, dass sie FIPS-konform und gleichzeitig hardwarebeschleunigt ist (AES-GCM mit SHA256)."}
{"ts": "120:00", "speaker": "I", "text": "Bevor wir den Wrap-up machen, würde mich interessieren: Gab es in den letzten zwei Sprints noch Änderungen an der Auth-Integration, die nicht im ursprünglichen RFC-ORI-17 standen?"}
{"ts": "120:15", "speaker": "E", "text": "Ja, wir haben tatsächlich einen zusätzlichen Schritt eingebaut, um die Token-Validierung gegen das Aegis IAM in einer Pre-Auth-Phase zu prüfen. Das kam nach einem internen Security Review hoch, Ticket SEC-245."}
{"ts": "120:36", "speaker": "I", "text": "Und wie haben Sie das in die bestehenden Pipelines integriert, ohne den Blue/Green Ablauf laut RB-GW-011 zu gefährden?"}
{"ts": "120:49", "speaker": "E", "text": "Wir haben einen Canary-Stage vor dem eigentlichen Green-Deployment eingefügt, der nur gegen die Sandbox-Instanz des Aegis IAM läuft. So haben wir keinen Einfluss auf die Live-Traffic-Phase."}
{"ts": "121:10", "speaker": "I", "text": "Klingt sauber. Gab es Auswirkungen auf die Latenz, speziell in Bezug auf SLA-ORI-02 p95 < 120ms?"}
{"ts": "121:24", "speaker": "E", "text": "Minimal. Wir haben im Canary-Test 3-5ms Overhead gemessen, was im Regelbetrieb durch Connection-Reuse kompensiert wird. Die p95-Linie bleibt stabil."}
{"ts": "121:44", "speaker": "I", "text": "Okay, und wie gehen Sie mit Abhängigkeiten zu Poseidon Networking um, wenn dort mTLS Protokolle geändert werden?"}
{"ts": "121:57", "speaker": "E", "text": "Wir haben eine wöchentliche Schnittstellen-Review-Session mit dem Poseidon-Team. Änderungen werden über das gemeinsame Repo PN-INT dokumentiert und via Integrationstests im Gateway validiert."}
{"ts": "122:18", "speaker": "I", "text": "Gab es einen Fall, wo eine Änderung dort ein Rollback erzwang?"}
{"ts": "122:28", "speaker": "E", "text": "Ja, bei PN-Update 3.4 hatten wir eine inkompatible Cipher-Suite. Wir mussten das Green auf Blue zurückschalten, nach Kapitel 5.3 in RB-GW-011."}
{"ts": "122:49", "speaker": "I", "text": "Wie bewerten Sie rückblickend das Risk/Benefit solcher kurzfristigen Rollbacks?"}
{"ts": "123:02", "speaker": "E", "text": "Sie sind manchmal unvermeidlich. Benefit: Verfügbarkeit bleibt hoch. Risk: Technische Schulden, weil Anpassungen ad hoc geschehen."}
{"ts": "123:18", "speaker": "I", "text": "Und haben Sie diese Lessons in die Runbooks eingepflegt?"}
{"ts": "123:29", "speaker": "E", "text": "Ja, RB-GW-011 hat nun eine Checkliste für mTLS-Kompatibilität vor Go-Live. Das ist direkt aus Incident GW-4821 und PN-Update 3.4 abgeleitet."}
{"ts": "123:46", "speaker": "I", "text": "Letzte Frage: Würden Sie ein Security-Feature jemals zurückstufen, wenn es die Latenz signifikant verschlechtert?"}
{"ts": "124:00", "speaker": "E", "text": "Nur temporär und mit Genehmigung laut POL-SEC-001-Override-Prozess. Beispiel: Temporäres Abschalten von OCSP-Checks bei Ausfall des Validation-Services, um SLA zu halten."}
{"ts": "129:00", "speaker": "I", "text": "Sie hatten vorhin angedeutet, dass Sie beim Rate Limiting bewusst einen konservativen Wert gewählt haben. Können Sie das bitte genauer ausführen?"}
{"ts": "129:05", "speaker": "E", "text": "Ja, also wir sind von 1.200 req/s pro Client ausgegangen, obwohl Lasttests Peaks bis 1.800 gezeigt haben. Hintergrund ist POL-SEC-001: wir priorisieren Missbrauchsschutz. Wir haben aber im Runbook RB-GW-011 einen Override dokumentiert, falls legitime Kampagnen diese Limits benötigen."}
{"ts": "129:15", "speaker": "I", "text": "Und wie wird dieser Override praktisch implementiert? Ist das ein manueller Eingriff?"}
{"ts": "129:21", "speaker": "E", "text": "Teilweise. Wir haben in der Config-Pipeline einen Feature-Flag 'rl_override', der nur im Oncall-Playbook freigegeben wird, Ticket-Referenz nötig, z.B. CHG-ORI-2024-118. Das Deployment erfolgt über Blue/Green, damit wir rollback-fähig bleiben."}
{"ts": "129:33", "speaker": "I", "text": "Sie erwähnten Blue/Green – wie binden Sie dort die mTLS-Handshake-Optimierungen aus dem Poseidon-Team ein?"}
{"ts": "129:39", "speaker": "E", "text": "Das war tricky. Wir mussten in der Staging-Zone beide Zertifikatsketten parallel halten. Poseidon Networking liefert per API ein neues Cert-Bundle, wir synchronisieren das mit dem Gateway-Cluster über unsere Deployment Hooks. Ohne diese Multi-Hop-Koordination hätten wir Downtime riskiert."}
{"ts": "129:52", "speaker": "I", "text": "Das klingt nach enger Abstimmung. Gibt es dafür feste SLAs zwischen den Teams?"}
{"ts": "129:57", "speaker": "E", "text": "Ja, intern haben wir SLA-POS-02 für die Cert-Bereitstellung, max. 10 Minuten Verzögerung. Kommt das Aegis IAM ins Spiel, müssen wir zusätzlich JIT Token Refresh berücksichtigen, was SLA-AEG-05 regelt."}
{"ts": "130:09", "speaker": "I", "text": "Also eine Kette aus drei SLAs, die im Ernstfall alle halten müssen. Was passiert, wenn einer davon verletzt wird?"}
{"ts": "130:15", "speaker": "E", "text": "Dann greift unser Incident Response Plan IRP-GW-04. Beispiel: im Testlauf von GW-4821 fiel das Poseidon Cert-API 15 Minuten aus, wir sind auf Caching mit Grace-Period umgestiegen, mTLS akzeptierte temporär vorherige Certs, dokumentiert in TKT-4821-INC."}
{"ts": "130:28", "speaker": "I", "text": "Ein bewusstes Sicherheitsdowngrade also. Wie wägen Sie ab, ob das vertretbar ist?"}
{"ts": "130:33", "speaker": "E", "text": "Wir haben eine Matrix im Security Risk Register SRR-ORI, die Impact vs. Likelihood abbildet. mTLS-Handshake mit altem Cert für max. 30 Min hat niedrigen Impact, hohes Availability-Gewicht. Entscheidung fällt der Oncall SRE mit Freigabe durch Security Duty Officer."}
{"ts": "130:45", "speaker": "I", "text": "Und dokumentieren Sie diese Entscheidung dann im gleichen Ticket?"}
{"ts": "130:49", "speaker": "E", "text": "Genau, im TKT wird der Timestamp, die verantwortliche Person und die Rückkehr zur Normalpolitik festgehalten. Lessons Learned gehen ins Post-Mortem, wir passen RB-GW-011 ggf. an."}
{"ts": "130:58", "speaker": "I", "text": "Sehen Sie hier langfristig technischen Schuldenaufbau, wenn solche Ausnahmen häufiger vorkommen?"}
{"ts": "131:04", "speaker": "E", "text": "Definitiv. Häufige Overrides deuten auf zu strenge Defaults hin. Wir tracken die Häufigkeit in den SLA-Reports und planen Quartalsweise Review. Ziel ist, Sicherheit und Performance im Gleichgewicht zu halten, ohne die Build-Phase zu verzögern."}
{"ts": "131:00", "speaker": "I", "text": "Sie hatten vorhin kurz den Stand des Orion Edge Gateway erwähnt. Können Sie mir bitte noch einmal klar umreißen, welche Verantwortlichkeiten Sie als SRE in dieser Build-Phase haben?"}
{"ts": "131:05", "speaker": "E", "text": "Klar, als SRE verantworte ich aktuell vor allem die Automatisierung der Deployments, die Einrichtung der Observability-Stacks und die Durchsetzung der Security-Policies wie POL-SEC-001 im CI/CD. Das umfasst auch die Oncall-Bereitschaft für unsere Test- und Staging-Umgebungen, falls während der Blue/Green Rollouts gem. RB-GW-011 Probleme auftauchen."}
{"ts": "131:15", "speaker": "I", "text": "Und wie greifen diese Aufgaben mit den DevOps-Verantwortlichkeiten zusammen, gerade bei der Auth-Integration?"}
{"ts": "131:20", "speaker": "E", "text": "Wir haben da eine enge Verzahnung: DevOps kümmert sich um das Packaging der Gateway-Services und die Anpassung der Helm Charts für die AuthN/AuthZ-Module. Ich als SRE prüfe dann, ob die Integration mit Aegis IAM für JIT Access unter Last standhält und ob unsere Rate-Limiting-Konfigurationen nicht im Konflikt mit den Auth-Token-Lifetimes stehen."}
{"ts": "131:31", "speaker": "I", "text": "Stichwort Rate Limiting: Wie stellen Sie sicher, dass das Prinzip Least Privilege im Gateway-Kontext tatsächlich umgesetzt wird?"}
{"ts": "131:38", "speaker": "E", "text": "Wir kombinieren mehrere Kontrollen: Zum einen rollenbasierte Policies aus Aegis IAM, die wir über mTLS-verifizierte Service-Identitäten enforced haben, zum anderen API-Keys mit minimalen Scopes. Zusätzlich haben wir nach GW-4821 gelernt, dass wir in den Pipelines negative Tests einbauen, um zu prüfen, dass keine überflüssigen Rechte gewährt werden."}
{"ts": "131:50", "speaker": "I", "text": "GW-4821 war der Incident mit der fehlerhaften Token-Validierung, richtig?"}
{"ts": "131:53", "speaker": "E", "text": "Genau, da hat ein fehlendes Claim-Check im Gateway dazu geführt, dass ein Service aus Poseidon Networking auf Admin-Routen zugreifen konnte. Wir haben daraufhin in RB-GW-011 explizit einen Pre-Go-Live Testschritt ergänzt, der Claims und mTLS-Handshake gegen bekannte Good/Bad-Cases prüft."}
{"ts": "132:04", "speaker": "I", "text": "Wie sieht denn so ein Blue/Green Rollout bei Ihnen praktisch aus?"}
{"ts": "132:10", "speaker": "E", "text": "Gemäß RB-GW-011 deployen wir die neue Gateway-Version parallel zur bestehenden in einer isolierten Namespace. Wir routen dann schrittweise 10%, 30%, 60% des Traffics über die neue Instanz, messen p95 Latenz, Error-Rate und mTLS-Handshake-Zeiten. Erst wenn SLA-ORI-02 in allen KPIs erfüllt ist, schalten wir auf 100%."}
{"ts": "132:24", "speaker": "I", "text": "Welche konkreten KPIs prüfen Sie neben der p95 Latenz?"}
{"ts": "132:28", "speaker": "E", "text": "Neben der p95 Latenz <120 ms schauen wir auf Auth-Response-Zeiten <50 ms, mTLS-Verbindungsaufbau <30 ms und auf eine Error-Rate <0,5%. Wir tracken das in Grafana-Dashboards, die direkt aus den Prometheus-Metriken des Gateways und der Aegis IAM API gespeist werden."}
{"ts": "132:40", "speaker": "I", "text": "Wie interagiert das Gateway denn genau mit Aegis IAM für den JIT Access?"}
{"ts": "132:45", "speaker": "E", "text": "Wenn ein Dienst aus Poseidon Networking eine Verbindung initiiert, stellt das Gateway eine mTLS-gesicherte Verbindung zu Aegis IAM her und fordert über den JIT-Endpunkt ein zeitlich begrenztes Access-Token an. Dieses Token wird dann im Gateway-Cache gespeichert, wobei wir Cache-Invaliderungs-Events aus Aegis IAM abonnieren, um Missbrauch zu verhindern."}
{"ts": "132:58", "speaker": "I", "text": "Und wenn sich bei Poseidon Networking etwas ändert, etwa an den Zertifikats-Authorities für mTLS?"}
{"ts": "133:03", "speaker": "E", "text": "Dann greifen unsere automatisierten Zert-Rollout-Skripte. Wir haben im Build-Pipeline einen Schritt, der die neuen Root- und Intermediate-CAs aus Poseidon Networking zieht und in den Gateway-Truststore einspielt. Dieser Schritt ist mit einem Canary-Test gekoppelt, der prüft, ob bestehende mTLS-Verbindungen intakt bleiben, bevor wir das in Produktion promoten."}
{"ts": "133:00", "speaker": "I", "text": "Sie sagten vorhin, dass die mTLS-Implementierung nicht nur im Gateway selbst, sondern auch in der Poseidon-Schicht verankert ist. Können Sie bitte genauer beschreiben, wie diese beiden Layers zusammenspielen?"}
{"ts": "133:10", "speaker": "E", "text": "Ja, klar. Also, das Orion Edge Gateway terminiert zunächst die eingehenden TLS-Verbindungen, prüft die Client-Zertifikate gegen unsere interne Trust-Store-Policy, und dann wird die Verbindung intern an Poseidon Networking weitergegeben, wo ein zweiter mTLS-Handshake stattfindet – das ist unser sogenannter 'inner ring'. Das ergibt doppelte Sicherheit, aber auch zusätzliche Latenz, wie wir in GW-4821 gesehen haben."}
{"ts": "133:33", "speaker": "I", "text": "Und wie wird in dieser Kette Aegis IAM für den JIT Access eingebunden?"}
{"ts": "133:40", "speaker": "E", "text": "Der Aegis IAM-Connector sitzt logisch zwischen Gateway und den Backend-Services. Sobald das Gateway ein gültiges mTLS-Setup erkennt, triggert es einen AuthN-Flow gegen Aegis, holt ein zeitlich limitiertes Access-Token, und gibt das zusammen mit der Anfrage in den Service-Mesh weiter. Das ist ein echter Multi-Hop: TLS-Check, Aegis AuthN/AuthZ, dann Service-Call."}
{"ts": "133:59", "speaker": "I", "text": "Gab es dabei Herausforderungen in der Automatisierung?"}
{"ts": "134:05", "speaker": "E", "text": "Auf jeden Fall. RB-GW-011 beschreibt den Blue/Green-Mechanismus, aber wir mussten einen zusätzlichen Hook einbauen, der mTLS-Zertifikate in beiden Rings validiert, bevor der Traffic geswitched wird. In der CI/CD-Pipeline haben wir dafür einen Pre-Deployment-Check eingebaut, der sowohl Poseidon- als auch Aegis-Endpunkte testet."}
{"ts": "134:26", "speaker": "I", "text": "Wie wird das in Ihren Metriken für SLA-ORI-02 abgebildet?"}
{"ts": "134:33", "speaker": "E", "text": "Wir messen die p95 Latenz end-to-end, also vom ersten SYN-Paket bis zur Response. Die mTLS- und Aegis-Hop-Zeiten sind separate Spans im Trace. SLA-ORI-02 erlaubt uns 120ms p95, wir liegen aktuell bei 108ms in Staging, was knapp, aber akzeptabel ist."}
{"ts": "134:50", "speaker": "I", "text": "Welche Lessons Learned aus GW-4821 haben konkret in diese Spans Einzug gehalten?"}
{"ts": "134:57", "speaker": "E", "text": "In GW-4821 hatten wir ein Race Condition zwischen Zertifikat-Validation und Token-Issuance. Wir haben daraufhin die Sequenz strikt serialisiert und Timeout-Werte angepasst. Außerdem loggen wir jetzt granularer pro Hop, um Bottlenecks schneller zu erkennen."}
{"ts": "135:15", "speaker": "I", "text": "Wie gehen Sie mit Änderungen in Poseidon Networking um, die den mTLS-Handshake beeinflussen könnten?"}
{"ts": "135:22", "speaker": "E", "text": "Wir haben einen wöchentlichen Sync mit dem Poseidon-Team. Jede Änderung an den Cipher Suites oder an der Trust-Anchor-Konfiguration wird in einem RFC dokumentiert, z.B. RFC-POS-027. Vor Merge läuft ein Integrationstest in einer isolierten Staging-Umgebung, die das Gateway mit Aegis und Poseidon verbindet."}
{"ts": "135:45", "speaker": "I", "text": "Und was passiert, wenn Aegis IAM selbst ein Update am Token-Format ausrollt?"}
{"ts": "135:51", "speaker": "E", "text": "Dann greifen unsere Contract-Tests. Wir validieren das neue Token gegen die Gateway-Parser. Sollte es nicht passen, blockt der Deployment-Job, und wir öffnen ein Ticket, z.B. ORI-AEG-112, um die Parser anzupassen. Das ist alles in RB-GW-011 als Ausnahmefall beschrieben."}
{"ts": "136:12", "speaker": "I", "text": "Können Sie ein Beispiel nennen, wo die Multi-Hop-Integration direkt einen Incident verhindert hat?"}
{"ts": "136:18", "speaker": "E", "text": "Ja, vor zwei Wochen hat Poseidon unbeabsichtigt eine alte Root-CA entfernt. Unser inner ring mTLS-Check hat das erkannt, bevor der Traffic ins Backend ging. Wir konnten so einen potenziellen AuthN-Fail im Produktionssystem verhindern und haben den Rollout sofort gestoppt."}
{"ts": "139:00", "speaker": "I", "text": "Wenn wir jetzt in die Trade-off-Diskussion gehen – welche konkreten Risiken sehen Sie denn beim aggressiven Rate Limiting? Ich meine, gerade in Bezug auf legitime Lastspitzen."}
{"ts": "139:05", "speaker": "E", "text": "Das Hauptproblem ist, dass unser Limit-Algorithmus in der aktuellen Konfiguration sehr starr ist. RB-GW-011 beschreibt zwar das adaptive Fensterverfahren, aber bei einem plötzlichen Anstieg legitimer Requests – etwa bei einem Partner-Release – schlägt das System zu früh zu. Wir haben das in Ticket OPS-ORI-332 dokumentiert."}
{"ts": "139:11", "speaker": "I", "text": "Und wie haben Sie das mitigiert? Gab es temporäre Ausnahmen oder kontextbasierte Whitelists?"}
{"ts": "139:16", "speaker": "E", "text": "Ja, wir haben in der Rollout-Checkliste einen Step ergänzt, der im Vorfeld erwartete Event-Spitzen in einer Whitelist hinterlegt, die dann im Gateway via API geladen wird. Das steht jetzt als Anhang C in RB-GW-011."}
{"ts": "139:22", "speaker": "I", "text": "Das klingt nach einem Kompromiss zwischen Sicherheit und Verfügbarkeit. Wie wirkt sich das auf die SLA-ORI-02 Werte aus, speziell p95 < 120ms?"}
{"ts": "139:28", "speaker": "E", "text": "Durch die Whitelist fällt der zusätzliche Rate-Limit-Check für diese Events weg, was die Latenz um ca. 8–10ms senkt. Wir haben die Messungen im Monitoring-Dashboard ORI-LAT-View, Report vom 13. Mai, hinterlegt."}
{"ts": "139:34", "speaker": "I", "text": "Und beim mTLS-Handshake? Da hatten Sie vorhin gesagt, dass dies auch einen Einfluss auf die Latenz hat."}
{"ts": "139:40", "speaker": "E", "text": "Richtig. Ein vollständiger mTLS-Handshake kostet uns im Schnitt 15–18ms. Um SLA-ORI-02 zu halten, nutzen wir Session Resumption, dokumentiert in RFC-DES-ORI-07. Das reduziert den Overhead auf 4–5ms."}
{"ts": "139:46", "speaker": "I", "text": "Gab es Überlegungen, mTLS nur zwischen bestimmten Subsystemen zu erzwingen?"}
{"ts": "139:52", "speaker": "E", "text": "Kurzzeitig, ja. Aber laut POL-SEC-001 ist es verpflichtend für alle internen Service-zu-Service-Kommunikationen. Wir haben nur bei asynchronen, signierten Events auf mTLS verzichtet, um die Throughput-Anforderungen zu erfüllen."}
{"ts": "139:58", "speaker": "I", "text": "Das war dann ein dokumentierter Ausnahmefall?"}
{"ts": "140:04", "speaker": "E", "text": "Genau. Ausnahme-Request SEC-ORI-21-EXC, genehmigt vom Security Board, gültig bis Phase 'Operate'. Eine Neubewertung ist für Q4 geplant."}
{"ts": "140:10", "speaker": "I", "text": "Sie haben jetzt mehrere Kompromisse genannt. Gab es ein Beispiel, wo Sie ein Sicherheitsfeature zugunsten der Verfügbarkeit komplett abgeschaltet haben?"}
{"ts": "140:16", "speaker": "E", "text": "Ja, temporär haben wir den Geo-IP-Fencing-Check deaktiviert, als die API-Nutzung in einer neuen Region testweise geöffnet wurde. Der Check verursachte Timeouts in 7% der Requests. In OPS-ORI-287 haben wir die Abschaltung und die Rücknahme dokumentiert."}
{"ts": "140:22", "speaker": "I", "text": "Und wie stellen Sie sicher, dass solche temporären Maßnahmen nicht dauerhaft im System verbleiben?"}
{"ts": "140:28", "speaker": "E", "text": "Wir haben einen wöchentlichen Security-Drift-Check, der unter anderem Ausnahmeregeln mit definiertem Enddatum prüft. Das ist seit Lessons Learned aus GW-4821 ein fester Bestandteil im Runbook RB-GW-011, Abschnitt 5.4."}
{"ts": "140:36", "speaker": "I", "text": "Lassen Sie uns noch einmal konkret auf die Risiken eingehen, die Sie beim aggressiven Rate Limiting identifiziert haben. Wie dokumentieren Sie diese im Projektkontext?"}
{"ts": "140:41", "speaker": "E", "text": "Wir führen für jedes identifizierte Risiko einen Eintrag im Risk-Log ORI-RL-2024 an, mit Severity, Impact und konkretem Runbook-Verweis. Zum Beispiel das Risiko RL-07, das legitime Lastspitzen fälschlich als DDoS markiert – da verweisen wir explizit auf RB-GW-011 Abschnitt 5.2 für das Umschalten in den 'lenient mode'."}
{"ts": "140:49", "speaker": "I", "text": "Und wie prüfen Sie, ob dieser lenient mode tatsächlich keine Sicherheitslücken reißt?"}
{"ts": "140:54", "speaker": "E", "text": "Da nutzen wir eine Kombination aus synthetischen Tests und realen Lastprofilen aus der Staging-Umgebung. Wir simulieren Burst-Traffic mit bekannten AuthN-Token-Mustern. Zusätzlich validieren wir über Aegis IAM Audit-Traces, ob keine unautorisierte Session durchrutscht."}
{"ts": "141:02", "speaker": "I", "text": "Sie hatten vorhin p95 Latenz < 120 ms erwähnt. Gab es Fälle, wo Sie zugunsten der Performance Sicherheitsfunktionen angepasst haben?"}
{"ts": "141:07", "speaker": "E", "text": "Ja, beim mTLS-Handshake haben wir das Session Resumption Feature von Poseidon Networking aktiviert. Das reduziert die Handshake-Zeit um durchschnittlich 18 ms. Trade-off war, dass wir die Session-Lifetime von 1h auf 4h erhöht haben, was ein leicht erhöhtes Risiko bei kompromittierten Sessions bedeutet."}
{"ts": "141:16", "speaker": "I", "text": "Wie wurde dieser Trade-off abgesegnet?"}
{"ts": "141:21", "speaker": "E", "text": "Das ging durch das Architecture Change Board, Referenz ACB-ORI-112. Wir haben die Entscheidung mit Messdaten aus dem Performance-Monitoring belegt und ein compensating control definiert: IP-Binding für alle resumed Sessions."}
{"ts": "141:29", "speaker": "I", "text": "Gab es Diskussionen mit dem Security-Team dazu?"}
{"ts": "141:34", "speaker": "E", "text": "Ja, die Kollegen von SecOps haben auf POL-SEC-001 verwiesen und darauf bestanden, dass wir die Lifetime-Erhöhung nur in Kombination mit erweitertem Logging aktivieren. Das Logging haben wir in den RB-GW-011 aufgenommen – Kapitel 7.4 beschreibt das."}
{"ts": "141:42", "speaker": "I", "text": "Wie fließen diese Änderungen in Ihre Oncall-Runbooks ein?"}
{"ts": "141:47", "speaker": "E", "text": "Wir führen Changelogs pro Runbook. Für RB-GW-011 gibt es eine Versionierung im internen Git. Oncall-Ingenieure bekommen bei jedem Merge eine Notification im Incident-Channel. So sind sie über neue lenient mode Triggers oder mTLS-Resumption Policies informiert."}
{"ts": "141:55", "speaker": "I", "text": "Und wenn ein Incident auftritt, der genau diese Trade-offs betrifft?"}
{"ts": "142:00", "speaker": "E", "text": "Dann greifen wir auf das Incident Template IT-GW-SEC-05 zurück. Das fordert uns auf, sofort die Session-Lifetimes zu invalidieren und das Rate Limiting auf den Standardwert zurückzusetzen. Parallel wird ein Post-Mortem initiiert, um zu prüfen, ob der Trade-off noch tragbar ist."}
{"ts": "142:08", "speaker": "I", "text": "Sehen Sie im Moment noch offene Risiken, die nicht adressiert sind?"}
{"ts": "142:13", "speaker": "E", "text": "Ein Punkt ist das Zusammenspiel von JIT Access aus Aegis IAM mit dem lenient mode. Wir testen gerade, ob sehr kurze JIT-Token im lenient mode fälschlich akzeptiert werden. Ticket ORI-SEC-782 ist dazu in Bearbeitung."}
{"ts": "142:06", "speaker": "I", "text": "Lassen Sie uns noch einmal konkret auf die Risikoabschätzung beim aggressiven Rate Limiting eingehen – welche Metriken haben Sie da im Blick, um Fehlalarme bei legitimen Lastspitzen zu vermeiden?"}
{"ts": "142:12", "speaker": "E", "text": "Wir haben im Monitoring zusätzlich zu den klassischen Requests-per-Second auch eine Burst-Analyse implementiert, die Sliding Windows mit 5s und 60s kombiniert. Das hilft uns, zwischen einem DDoS-Versuch und einem legitimen Traffic-Burst, wie er z.B. nach einer neuen Feature-Aktivierung auftritt, zu unterscheiden."}
{"ts": "142:20", "speaker": "I", "text": "Und wie wird das in Ihren Runbooks konkret abgebildet?"}
{"ts": "142:25", "speaker": "E", "text": "In RB-GW-011, Abschnitt 4.3, haben wir einen Entscheidungsbaum hinterlegt: Wenn das 5s-Fenster >150% des Medianwerts liegt, aber das 60s-Fenster stabil bleibt, wird nur eine Soft-Limit-Warnung generiert. Erst bei gleichzeitiger Überschreitung beider Fenster greifen die harten Limits."}
{"ts": "142:34", "speaker": "I", "text": "Gab es Situationen, in denen Sie dieses Vorgehen temporär ausgesetzt haben?"}
{"ts": "142:39", "speaker": "E", "text": "Ja, beim Rollout von Build 1.4 hatten wir eine geplante Marketingkampagne. Wir haben dafür in Ticket CHG-ORI-558 ein temporäres Override dokumentiert, das die Limits um 30% angehoben hat, um die erwarteten Peaks absorbieren zu können."}
{"ts": "142:47", "speaker": "I", "text": "Kommen wir zur Balance zwischen mTLS und Latenz – wie haben Sie das technisch optimiert?"}
{"ts": "142:52", "speaker": "E", "text": "Wir haben Session Resumption via TLS Tickets aktiviert, sodass der volle Handshake nur bei der ersten Verbindung durchgeführt wird. Laut unseren Messungen in PRE-ENV-03 bringt das im Schnitt eine Reduktion von ~40ms pro Request."}
{"ts": "143:00", "speaker": "I", "text": "Gab es Bedenken seitens Security dabei?"}
{"ts": "143:04", "speaker": "E", "text": "Ja, die Security-Abteilung wollte zunächst eine kürzere Lebensdauer für die Tickets, um Replay-Risiken zu minimieren. Wir haben uns auf 10 Minuten geeinigt, dokumentiert in SEC-RFC-212, und zusätzliche Nonce-Checks eingebaut."}
{"ts": "143:12", "speaker": "I", "text": "Wie wirkt sich das auf SLA-ORI-02 aus?"}
{"ts": "143:16", "speaker": "E", "text": "Sehr positiv: Unser p95 liegt aktuell bei 108ms über alle Endpunkte hinweg, deutlich unter der 120ms-Grenze aus SLA-ORI-02. Die Kombination aus Caching und Session Resumption ist hier der Schlüssel."}
{"ts": "143:23", "speaker": "I", "text": "Haben Sie ein Beispiel, wo Sie ein Sicherheitsfeature zugunsten der Verfügbarkeit angepasst haben?"}
{"ts": "143:28", "speaker": "E", "text": "Im Incident INC-ORI-774 hatten wir massive Zeitüberschreitungen bei der CRL-Prüfung externer Zertifikate. Wir haben kurzfristig auf OCSP Stapling umgeschaltet, was die Latenzprobleme behoben hat, aber ein geringfügig höheres Risiko bei Revocation-Delays bedeutet."}
{"ts": "143:37", "speaker": "I", "text": "Wie haben Sie dieses Risiko kommuniziert?"}
{"ts": "143:41", "speaker": "E", "text": "Wir haben einen Risk Acceptance Record RAR-ORI-119 erstellt, die Entscheidung im CAB protokolliert und einen Rückbauplan in RB-GW-011 ergänzt, falls die externe CRL-Infrastruktur stabilisiert wird."}
{"ts": "144:06", "speaker": "I", "text": "Lassen Sie uns jetzt konkret auf die Entscheidung zurückkommen, das Rate Limiting weniger strikt zu gestalten. Was war da der ausschlaggebende Punkt?"}
{"ts": "144:13", "speaker": "E", "text": "Der Hauptfaktor war, dass wir in den Lasttests gemäß RB-GW-011 gesehen haben, dass bei 300 req/s von legitimen Clients schon gedrosselt wurde. Das hat SLA-ORI-02 mehrfach verletzt, vor allem in Peak-Zeiten wie Monatsabschluss."}
{"ts": "144:24", "speaker": "I", "text": "Also haben Sie die Limits einfach erhöht?"}
{"ts": "144:28", "speaker": "E", "text": "Nicht nur. Wir haben von einem harten Limit auf ein Token-Bucket-Model mit Burst-Fenster gewechselt. So erreichen wir p95 Latenzen unter 120 ms auch bei Lastspitzen, ohne gleich mTLS-Handshake-Timeouts zu riskieren."}
{"ts": "144:39", "speaker": "I", "text": "Und was sagt POL-SEC-001 dazu? Gab es Konflikte?"}
{"ts": "144:44", "speaker": "E", "text": "POL-SEC-001 verlangt primär Schutz vor Abuse. Wir haben das mit zusätzlicher Anomalieerkennung aus GW-4821 kombiniert, die im Poseidon Networking Layer läuft. So konnten wir die Policy erfüllen und trotzdem Availability halten."}
{"ts": "144:56", "speaker": "I", "text": "Können Sie ein Beispiel für diese Anomalieerkennung nennen?"}
{"ts": "145:00", "speaker": "E", "text": "Ja, wir haben eine Heuristik, die JIT-Access-Tokens aus Aegis IAM überwacht. Wenn ein Client ungewöhnlich viele kurze Sessions aufbaut, greift ein Soft-Limit, bevor wir hart drosseln."}
{"ts": "145:11", "speaker": "I", "text": "Das bedeutet, Sie vertrauen stark auf die Integrität von Aegis IAM?"}
{"ts": "145:15", "speaker": "E", "text": "Genau. Wir haben in RB-GW-011 festgehalten, dass Änderungen an IAM-Schemas zuerst in einer isolierten Staging-Zone mit Poseidon-mTLS getestet werden. Ein Ticket wie GW-5932 dokumentiert den letzten Schemawechsel und dessen Gateway-Auswirkungen."}
{"ts": "145:28", "speaker": "I", "text": "Gab es bei diesen Tests auch Performance-Überraschungen?"}
{"ts": "145:32", "speaker": "E", "text": "Ja, die mTLS-Handshakes mit neuen Cipher Suites haben initial 20 ms mehr verursacht. Wir haben durch Session Resumption im Poseidon Layer kompensiert. Das war ein bewusster Trade-off zugunsten der Latenz."}
{"ts": "145:44", "speaker": "I", "text": "Und das Security-Team war einverstanden?"}
{"ts": "145:47", "speaker": "E", "text": "Nach Review der Threat Models ja. Die Runbook-Anpassung ist als Revision 3 in RB-GW-011 dokumentiert, mit Verweis auf die Freigabe durch SecOps."}
{"ts": "145:57", "speaker": "I", "text": "Wenn Sie jetzt auf diese Entscheidungen zurückblicken – würden Sie etwas anders machen?"}
{"ts": "146:01", "speaker": "E", "text": "Vielleicht frühzeitiger die Lasttests mit realen IAM- und mTLS-Parametern fahren. Das hätte uns die Anpassung des Rate Limiting schon Wochen früher erlaubt und die Iterationszyklen verkürzt."}
{"ts": "146:06", "speaker": "I", "text": "Kommen wir bitte nochmal konkret zurück auf den Punkt mit dem aggressiven Rate Limiting – Sie hatten vorhin angedeutet, dass es da in Verbindung mit den p95-Latenzen gewisse Spannungsfelder gab?"}
{"ts": "146:11", "speaker": "E", "text": "Genau, wir hatten in der Staging-Umgebung initial ein Limit von 500 RPS pro Tenant festgelegt. Das hat bei synthetischen Tests wunderbar funktioniert, aber unter realer Last – insbesondere bei legitimen Burst-Szenarien wie monatlichen Abrechnungen – haben wir SLA-ORI-02 mehrfach verfehlt."}
{"ts": "146:17", "speaker": "I", "text": "Und wie sind Sie dann vorgegangen? Haben Sie einfach das Limit erhöht oder etwas differenzierter gearbeitet?"}
{"ts": "146:22", "speaker": "E", "text": "Wir haben anhand der Runbook-Empfehlungen aus RB-GW-011 eine adaptive Variante implementiert. Das heißt: wir messen die aktuelle Latenz und erlauben bei p95 < 100 ms temporär 20 % mehr Requests, bevor wir hart drosseln."}
{"ts": "146:28", "speaker": "I", "text": "Gab es dafür ein formales Change-Ticket oder war das eher ein stillschweigender Patch?"}
{"ts": "146:33", "speaker": "E", "text": "Es lief über RFC-ORI-219, dokumentiert im internen Confluence mit Verweis auf Incident GW-4821. Wir wollten bewusst die Lessons Learned festhalten, weil dort ein zu starres Limit die Auth-Calls zu Aegis IAM blockiert hat."}
{"ts": "146:39", "speaker": "I", "text": "Das heißt, die Authentifizierungskette hing direkt mit der Rate-Limit-Policy zusammen?"}
{"ts": "146:44", "speaker": "E", "text": "Ja, genau. Die JIT Access Tokens werden vom Gateway geprüft. Wenn der Upstream zu Aegis IAM durch das Limit in die Queue läuft, steigt die End-to-End-Latenz, und das konnte im schlimmsten Fall den mTLS-Handshake zu Poseidon Networking verzögern."}
{"ts": "146:50", "speaker": "I", "text": "Wie haben Sie das mTLS-Thema in diesem Zusammenhang optimiert?"}
{"ts": "146:55", "speaker": "E", "text": "Wir haben Session Resumption aktiviert und die Zertifikatsprüfung asynchron gestaltet, soweit POL-SEC-001 das zulässt. Das reduziert die Handshake-Zeit um ca. 35 ms im Mittel, ohne die Security-Baseline zu verletzen."}
{"ts": "147:01", "speaker": "I", "text": "Gab es Bedenken aus dem Security-Team wegen der asynchronen Prüfungen?"}
{"ts": "147:06", "speaker": "E", "text": "Ja, natürlich. Wir mussten in SEC-Review-Meeting #17 belegen, dass durch zusätzliche Validierungs-Callbacks keine verwundbaren Fenster entstehen. Die Beweislage kam aus unseren Chaos-Tests im Pre-Prod-Cluster."}
{"ts": "147:12", "speaker": "I", "text": "Wie haben diese Anpassungen Ihre SLA-Compliance beeinflusst?"}
{"ts": "147:17", "speaker": "E", "text": "Seit Deployment von RFC-ORI-219 halten wir p95 stabil bei 108–112 ms, selbst bei Peak-Events. Die Error-Rate blieb unter 0,2 %, was unter dem Grenzwert von SLA-ORI-02 liegt."}
{"ts": "147:23", "speaker": "I", "text": "Und würden Sie sagen, dass hier Verfügbarkeit über Security priorisiert wurde?"}
{"ts": "147:28", "speaker": "E", "text": "Ich würde es eher als informierte Balance bezeichnen. Wir haben Sicherheitsfeatures nicht deaktiviert, sondern so angepasst, dass die User Experience nicht leidet. Die Dokumentation im Risk-Register RR-ORI-07 zeigt klar, welche Rest-Risiken akzeptiert wurden."}
{"ts": "147:42", "speaker": "I", "text": "Lassen Sie uns jetzt konkret auf die Entscheidung von letzter Woche eingehen – Sie hatten ja das Rate Limiting aggressiv eingestellt, um bösartigen Traffic zu blocken. Wie stark hat sich das auf die p95 Latenz ausgewirkt?"}
{"ts": "147:47", "speaker": "E", "text": "Ja, wir haben in der Tat kurzzeitig einen Anstieg auf 134 ms p95 gesehen, was über dem SLA-ORI-02 Schwellenwert liegt. Das war direkt nach Aktivierung der neuen Limiter-Regeln in Stage. Wir haben dann gemäß RB-GW-011 einen rollback auf die vorherige Konfiguration gemacht und mit Ticket OPS-4823 dokumentiert."}
{"ts": "147:53", "speaker": "I", "text": "Das heißt, Sie nutzen den Runbook-Pfad für Blue/Green Rollouts auch für Konfigurationsänderungen im Rate Limiter?"}
{"ts": "147:57", "speaker": "E", "text": "Genau, wir behandeln solche Änderungen wie ein Deployment, mit Pre-Checks, Traffic-Mirroring und Health-Endpoint-Monitoring. Damit können wir innerhalb von fünf Minuten auf die alte Revision zurückspringen – das ist im Abschnitt 4.2 von RB-GW-011 so festgelegt."}
{"ts": "148:03", "speaker": "I", "text": "Und wie hat sich die mTLS-Handshake-Optimierung ausgewirkt? Ich meine, Sie hatten da ja parallel Anpassungen laufen."}
{"ts": "148:08", "speaker": "E", "text": "Wir haben das Session-Resumption-Feature aus Poseidon Networking 2.4 aktiviert, sodass bei Wiederverbindungen kein kompletter TLS-Handshake nötig ist. Dadurch sparen wir im Schnitt 18 ms pro Request – was in Summe geholfen hat, auch bei restriktiveren Limits unter p95 < 120 ms zu bleiben."}
{"ts": "148:15", "speaker": "I", "text": "Gab es dafür eine Policy-Ausnahme, falls ältere Clients dieses Feature nicht unterstützen?"}
{"ts": "148:20", "speaker": "E", "text": "Ja, gemäß POL-SEC-001 dürfen wir für maximal 60 Tage ein Fallback ohne Session-Resumption anbieten, dokumentiert in Ausnahme-AZ-119. Nach dieser Frist müssen Clients aktualisiert sein oder sie werden geblockt."}
{"ts": "148:27", "speaker": "I", "text": "In Bezug auf Lessons Learned aus GW-4821 – wie hat dieses Incident Ihre jetzige Entscheidung beeinflusst?"}
{"ts": "148:32", "speaker": "E", "text": "Damals hatten wir keinen klaren Rollback-Plan für AuthN-Funktionen, was zu 42 Minuten Downtime führte. Jetzt integrieren wir AuthN/AuthZ-Checks in unser Staging-Canary, inklusive Aegis IAM JIT Access Simulation. So sehen wir sofort, ob ein Policy-Change legitimen Traffic ausschließt."}
{"ts": "148:39", "speaker": "I", "text": "Wie binden Sie Aegis IAM in diese Tests ein – nutzen Sie dort auch mTLS?"}
{"ts": "148:44", "speaker": "E", "text": "Ja, über eine dedizierte Test-CA in Poseidon Networking. Wir generieren temporäre Zertifikate im CI-Job, die Aegis IAM akzeptiert. Das erlaubt uns, die komplette AuthN-Kette mit mTLS-Handshake und Token-Validation zu simulieren."}
{"ts": "148:51", "speaker": "I", "text": "Welche Risiken sehen Sie aktuell noch bei der Balance zwischen Sicherheit und Performance?"}
{"ts": "148:56", "speaker": "E", "text": "Das größte Risiko ist, legitime Lastspitzen – etwa während Wartungsfenstern externer Partner – als Angriff zu werten. Deshalb hinterlegen wir in der Limiter-Config Whitelists mit temporären Ausnahmen, die automatisch nach dem Maintenance-Fenster auslaufen."}
{"ts": "149:03", "speaker": "I", "text": "Und das wird alles revisionssicher dokumentiert?"}
{"ts": "149:07", "speaker": "E", "text": "Ja, Änderungen an den Whitelists gehen in unser Change-Repo mit Ticket-ID, SLA-Referenz und Impact-Analyse. Das ist Teil der Compliance-Prüfung nach POL-SEC-001 und wird quartalsweise auditiert."}
{"ts": "149:02", "speaker": "I", "text": "Sie hatten vorhin erwähnt, dass Sie beim Orion Edge Gateway in der Build-Phase sehr eng mit den Security-Policies arbeiten. Können Sie das nochmal konkretisieren?"}
{"ts": "149:07", "speaker": "E", "text": "Ja, also wir setzen POL-SEC-001 schon in der CI/CD-Pipeline durch, indem wir ein Pre-Deploy-Script haben, das sowohl mTLS-Zertifikate als auch Role Bindings überprüft. Das ist im Runbook RB-GW-011 Schritt 3 beschrieben."}
{"ts": "149:15", "speaker": "I", "text": "Und wie sieht Ihre Rolle da genau aus? Oncall, Automationen – was fällt in Ihren Bereich?"}
{"ts": "149:20", "speaker": "E", "text": "Ich bin primär für das Oncall der Gateway-Cluster verantwortlich, also PagerDuty-Alerts bei AuthN-Fehlern oder Latenzspitzen über p95. Automationen betreffen vor allem das Blue/Green-Rollout gemäß RB-GW-011, um Downtime zu vermeiden."}
{"ts": "149:29", "speaker": "I", "text": "Wie greifen in diesem Kontext Ihre SRE- und DevOps-Verantwortlichkeiten ineinander?"}
{"ts": "149:34", "speaker": "E", "text": "DevOps sorgt für die Pipeline und Infrastruktur als Code, SRE überwacht und optimiert die Laufzeitumgebung. Bei der Auth-Integration mit Aegis IAM arbeiten wir Hand in Hand – DevOps integriert die JIT Access API, SRE testet unter Last."}
{"ts": "149:44", "speaker": "I", "text": "Gibt es konkrete Lessons Learned aus dem Incident GW-4821, die Sie hier anwenden?"}
{"ts": "149:49", "speaker": "E", "text": "Ja, GW-4821 war ein AuthZ-Bypass durch fehlerhafte Token-Validierung. Wir haben daraus gelernt, dass wir Token gegen zwei unabhängige Validatoren prüfen. Das ist jetzt Teil der Pre-Prod-Tests und im Ticket SEC-TST-77 dokumentiert."}
{"ts": "149:59", "speaker": "I", "text": "Wie verifizieren Sie SLA-ORI-02 im laufenden Betrieb?"}
{"ts": "150:04", "speaker": "E", "text": "Wir messen p95 Latenz und Error Rate kontinuierlich mit Prometheus und triggern einen wöchentlichen SLA-Report. Wenn p95 > 120ms über 15 Minuten, wird ein Incident nach IR-GW-04 eröffnet."}
{"ts": "150:14", "speaker": "I", "text": "Und die Integration zu Poseidon Networking für mTLS – wie läuft das technisch ab?"}
{"ts": "150:19", "speaker": "E", "text": "Poseidon stellt uns per API die Zertifikatsketten bereit. Wir holen die per Sidecar-Container, rotieren sie alle 24h. Änderungen in Poseidon triggern via Webhook einen Reload des Gateway-Listeners."}
{"ts": "150:29", "speaker": "I", "text": "Kommen wir zu den Trade-offs: aggressives Rate Limiting versus legitime Lastspitzen – wie haben Sie das gelöst?"}
{"ts": "150:34", "speaker": "E", "text": "Wir haben adaptive Limits implementiert: baseline 500 RPS pro Client, aber mit Burst-Toleranz auf Basis historischer Traffic-Profile. Das reduziert das Risiko, legitime Spitzen abzuschneiden, ohne die Policy zu verletzen."}
{"ts": "150:44", "speaker": "I", "text": "Und beim mTLS-Handshake versus Latenz – konkrete Anpassungen?"}
{"ts": "150:49", "speaker": "E", "text": "Wir cachen Session Tickets serverseitig, reduzieren damit die Full Handshakes um ca. 70%. Das wurde als Ausnahme zu POL-SEC-001 im RFC-ORI-22 genehmigt, um SLA-ORI-02 zu halten."}
{"ts": "151:02", "speaker": "I", "text": "Bevor wir in die Details gehen – können Sie kurz aufzeigen, wie Sie im Orion Edge Gateway Projekt aktuell den Build-Stand einschätzen?"}
{"ts": "151:08", "speaker": "E", "text": "Ja, wir sind im Build bei etwa 85 % der geplanten Features, die API-Gateway-Basics – Routing, initiale Rate-Limits – laufen stabil in Staging. Die Auth-Integration mit Aegis IAM ist funktional, aber wir optimieren gerade noch die mTLS-Handshake-Latenz."}
{"ts": "151:18", "speaker": "I", "text": "Und Ihre Rolle – mehr SRE oder mehr DevOps in diesem Setup?"}
{"ts": "151:23", "speaker": "E", "text": "Es ist ein Hybrid. Als SRE bin ich für Oncall und Incident-Response nach RB-GW-011 zuständig, als DevOps-Engineer steuere ich die CI/CD-Pipelines und automatisiere das Testing von AuthN/AuthZ."}
{"ts": "151:32", "speaker": "I", "text": "Wie setzen Sie in der Build-Phase konkret POL-SEC-001 um?"}
{"ts": "151:37", "speaker": "E", "text": "Wir haben in den Pipelines einen Policy-Check integriert, der sicherstellt, dass Least Privilege für alle Gateway-Service-Accounts gilt. Lessons Learned aus Ticket GW-4821 – fehlende Scope-Überprüfung – sind darin fest verdrahtet."}
{"ts": "151:46", "speaker": "I", "text": "Können Sie den Blue/Green Ablauf nach RB-GW-011 skizzieren?"}
{"ts": "151:51", "speaker": "E", "text": "Klar: Wir deployen die neue Version parallel, verifizieren Health-Checks und mTLS-Funktion, dann schalten wir den Traffic schrittweise um. Jeder Schritt wird im Runbook-Log mit Zeitstempel und p95-Latenz dokumentiert."}
{"ts": "152:01", "speaker": "I", "text": "Sie erwähnten die Integration mit Aegis IAM. Wie läuft JIT Access durch das Gateway?"}
{"ts": "152:06", "speaker": "E", "text": "Das Gateway validiert eingehende Tokens gegen Aegis IAM in Echtzeit, nutzt dabei Poseidon Networking für den mTLS-Tunnel. Änderungen im IAM-Schema triggern automatisch ein Kompatibilitäts-Checkjob in unserer Pipeline."}
{"ts": "152:16", "speaker": "I", "text": "Und was passiert, wenn Poseidon einen Patch fährt?"}
{"ts": "152:20", "speaker": "E", "text": "Dann laufen unsere mTLS-Regressionstests. Wenn SLA-ORI-02 – p95 unter 120 ms – gefährdet ist, erstellen wir ein temporäres Policy-Bypass-Ticket, abgestimmt mit Security."}
{"ts": "152:30", "speaker": "I", "text": "Risiken bei aggressivem Rate Limiting – wie gehen Sie damit um?"}
{"ts": "152:34", "speaker": "E", "text": "Wir loggen Dropped Requests separat und korrelieren sie mit legitimen Traffic-Spikes, z. B. bei Partner-APIs. Falls wir fälschlich drosseln, gibt es im Runbook einen Override-Prozess, der temporär 20 % mehr Durchsatz erlaubt."}
{"ts": "152:44", "speaker": "I", "text": "Gab es einen Fall, wo Sie Sicherheit zugunsten der Verfügbarkeit angepasst haben?"}
{"ts": "152:49", "speaker": "E", "text": "Ja, im Loadtest OEG-LT-09 haben wir den mTLS-Zwang für interne Microservices für 30 Minuten gelockert, um einen Durchsatz-Bottleneck zu identifizieren. Dokumentiert als Ausnahmefall in SEC-EXC-17 und nach Abstimmung mit dem CISO-Team wieder zurückgenommen."}
{"ts": "153:02", "speaker": "I", "text": "Sie hatten vorhin erwähnt, dass Sie im Rahmen von RB-GW-011 beim Blue/Green Rollout gewisse Schritte angepasst haben. Können Sie das bitte noch konkretisieren?"}
{"ts": "153:08", "speaker": "E", "text": "Ja, sicher. Ursprünglich sah RB-GW-011 vor, dass wir den Traffic in 50/50 Schritten umschalten. Nach GW-4821 haben wir das geändert auf 10/90, 30/70, 50/50, um schrittweiser Latenzspitzen zu erkennen. Das ist zwar mehr Aufwand, hat aber die SLA-ORI-02 Compliance verbessert."}
{"ts": "153:18", "speaker": "I", "text": "Und wie überprüfen Sie in diesen Phasen, ob die Auth-Integration mit Aegis IAM stabil läuft?"}
{"ts": "153:23", "speaker": "E", "text": "Wir haben ein automatisiertes Smoke-Test-Skript, das JIT Access Tokens erzeugt und validiert. Dieses läuft vor jedem Umschalt-Schritt und prüft auch mTLS Handshakes via Poseidon Networking."}
{"ts": "153:33", "speaker": "I", "text": "Gab es Situationen, in denen diese Tests Fehlalarme produziert haben?"}
{"ts": "153:38", "speaker": "E", "text": "Ja, einmal hat ein Poseidon Zertifikats-Rollover den Test fehlschlagen lassen, obwohl der Live-Traffic ok war. Wir haben daraufhin in den Runbooks eine Ausnahme für geplante Rollover aufgenommen."}
{"ts": "153:48", "speaker": "I", "text": "Interessant. Wie dokumentieren Sie solche Ausnahmen im Hinblick auf POL-SEC-001?"}
{"ts": "153:54", "speaker": "E", "text": "Wir führen ein Confluence-Register der Policy-Ausnahmen mit Ticket-IDs, z.B. SEC-EXC-2023-14. Jede Ausnahme muss vom Security Board genehmigt werden, und wir vermerken auch die Dauer der Gültigkeit."}
{"ts": "154:04", "speaker": "I", "text": "Kommen wir noch mal auf das Thema Latenz zurück: Wie messen Sie konkret die p95 im Build-Phase-Test?"}
{"ts": "154:09", "speaker": "E", "text": "Wir nutzen ein synthetisches Lastprofil über Gatling mit 500 RPS, wobei wir p95 und p99 Latenzen über 15 Minuten aggregieren. Werte werden direkt gegen SLA-ORI-02 geprüft."}
{"ts": "154:19", "speaker": "I", "text": "Und falls p95 knapp drüber liegt, was passiert dann?"}
{"ts": "154:23", "speaker": "E", "text": "Dann treten die in RB-GW-011 definierten Retry-Optimierungen in Kraft. Falls keine Besserung, wird der Rollout gestoppt und ein Incident nach IR-GW-005 eröffnet."}
{"ts": "154:33", "speaker": "I", "text": "Haben Sie auch schon mal entschieden, ein Sicherheitsfeature temporär runterzufahren, um die Verfügbarkeit zu sichern?"}
{"ts": "154:38", "speaker": "E", "text": "Ja, bei einem externen Partner-Event haben wir mTLS Session-Resumption aktiviert, obwohl es laut POL-SEC-001 nur für interne Services freigegeben ist. Das reduzierte Handshake-Overhead und half, die p95 unter 120ms zu halten."}
{"ts": "154:48", "speaker": "I", "text": "Wie wurde diese Abweichung abgesichert?"}
{"ts": "154:53", "speaker": "E", "text": "Über ein temporäres Override-Flag im Gateway-Config, dokumentiert im Ticket SEC-EXC-2023-21 und mit einer Post-Mortem-Analyse, um Risiken für die Zukunft zu bewerten."}
{"ts": "155:22", "speaker": "I", "text": "Lassen Sie uns noch einmal konkret auf das Incident-Response-Playbook eingehen: Wie haben Sie RB-GW-011 im letzten Blue/Green Rollout angewendet?"}
{"ts": "155:27", "speaker": "E", "text": "Wir sind streng nach den Schritten in RB-GW-011 vorgegangen – also zuerst Pre-Deployment Checks, dann Traffic-Shifting in 10 %-Inkrementen. Währenddessen haben wir mit den Metriken aus SLA-ORI-02, insbesondere der p95 Latenz, kontinuierlich überwacht. Any deviation over 5 ms triggered automatic rollback scripts."}
{"ts": "155:33", "speaker": "I", "text": "Gab es bei diesem Rollout spezielle Herausforderungen im Zusammenhang mit der Auth-Integration?"}
{"ts": "155:39", "speaker": "E", "text": "Ja, beim Umschalten auf den neuen Blue-Cluster gab es eine Race Condition mit dem JIT Access Token Refresh aus Aegis IAM. Wir mussten für diesen Fall ein temporäres Throttling auf der Auth-API einbauen, um Token-Bounces zu vermeiden, documented under ticket ORI-DEP-442."}
{"ts": "155:46", "speaker": "I", "text": "Und wie haben Sie solche temporären Maßnahmen nach dem Rollout wieder zurückgenommen?"}
{"ts": "155:51", "speaker": "E", "text": "Nach der Stabilitätsphase (48 h) haben wir per Runbook-Step 9 das Throttling entfernt und in der Lessons-Learned-Sektion vermerkt, dass wir künftig Pre-Warm-Up Tokens einsetzen wollen."}
{"ts": "155:56", "speaker": "I", "text": "Sie hatten vorhin die mTLS-Handshake-Anpassungen erwähnt. Können Sie erläutern, wie sich das konkret auf die Verbindung zu Poseidon Networking auswirkt?"}
{"ts": "156:02", "speaker": "E", "text": "Sure. Wir haben die TLS Session Resumption aktiviert und auf Poseidon Networking abgestimmt. Das reduziert die Handshake-Latenz um ca. 18 ms im Schnitt. Dafür mussten wir allerdings in der Poseidon Config die Session Ticket Lifetime verlängern, was aus Security-Sicht als Ausnahme in POL-SEC-001 dokumentiert ist."}
{"ts": "156:09", "speaker": "I", "text": "War die Security-Abteilung mit dieser Ausnahme sofort einverstanden?"}
{"ts": "156:15", "speaker": "E", "text": "Nicht sofort, es gab mehrere Review-Runden. Wir mussten beweisen, dass die Tickets regelmäßig rotiert werden und dass das Risiko von Replay Attacks durch zusätzliche Nonces mitigiert ist. Das ist alles in RPT-SEC-77 festgehalten."}
{"ts": "156:22", "speaker": "I", "text": "Wie überwachen Sie jetzt in der Build-Phase, dass diese Konfigurationen nicht versehentlich überschrieben werden?"}
{"ts": "156:28", "speaker": "E", "text": "Wir haben in der CI/CD-Pipeline einen Compliance-Check integriert, der die Poseidon TLS Settings gegen ein YAML-Policy-File validiert. Any drift triggers a fail in the pipeline."}
{"ts": "156:34", "speaker": "I", "text": "Gab es bislang Fälle, in denen dieser Compliance-Check angesprungen ist?"}
{"ts": "156:39", "speaker": "E", "text": "Ja, zweimal. Einmal, als ein Entwickler versehentlich die Session Ticket Lifetime im Feature Branch auf 5 Minuten gesetzt hatte – das hätte die Performance verbessert, aber gegen die agreed-upon security posture verstoßen."}
{"ts": "156:45", "speaker": "I", "text": "Abschließend: Welche Risiken sehen Sie aktuell noch offen in Bezug auf aggressive Rate-Limits?"}
{"ts": "156:50", "speaker": "E", "text": "Das Haupt­risiko ist, dass bei legitimen Lastspitzen kritische Clients wie das Monitoring selbst geblockt werden. Wir mitigieren das durch eine Whitelist in der Rate-Limiting-Engine und durch dynamische Limits, die sich an der p95 Latenz orientieren, as described in ORI-RL-Policy-v3."}
{"ts": "156:58", "speaker": "I", "text": "Lassen Sie uns bitte noch einmal konkret durchgehen, wie RB-GW-011 beim letzten Blue/Green Deployment angewendet wurde."}
{"ts": "157:05", "speaker": "E", "text": "Ja, also wir haben exakt die Schritte aus RB-GW-011, Kapitel 4.2, befolgt: Erst komplettes Pre-Flight Checkscripte gegen die Staging-Umgebung, dann paralleles Hochfahren der Green-Instanzgruppe. Die Traffic-Shift-Phase haben wir in 10%-Inkrementen über 15 Minuten gefahren, um Latenzspitzen zu beobachten."}
{"ts": "157:21", "speaker": "I", "text": "Gab es Abweichungen vom Runbook?"}
{"ts": "157:24", "speaker": "E", "text": "Nur eine kleine: Wir haben aufgrund eines Poseidon-Updates vorab einen zusätzlichen mTLS-Handshake-Test eingefügt, der nicht in RB-GW-011 steht, aber durch Ticket CHG-ORI-229 dokumentiert ist."}
{"ts": "157:37", "speaker": "I", "text": "Wie fließen solche Anpassungen wieder zurück in die offiziellen Runbooks?"}
{"ts": "157:42", "speaker": "E", "text": "Wir erstellen nach dem Deployment einen 'Post-Mortem Light' Bericht, der im Confluence-Bereich ORI-OPS landet. Die Änderung wird dann in der nächsten Runbook-Review-Session mit SRE und DevOps validiert und ggf. als Revision eingetragen."}
{"ts": "157:56", "speaker": "I", "text": "Und im Hinblick auf SLA-ORI-02 – welche KPIs wurden während des Shifts überwacht?"}
{"ts": "158:01", "speaker": "E", "text": "Wir haben p95 Latenz, Error Rate > 0.1%, und AuthN-Failure Ratio als primäre KPIs gehabt. Zusätzlich haben wir für diesen Rollout ein Live-Dashboard mit Poseidon TLS Negotiation Time eingeblendet, um Handshake-Verluste zu erkennen."}
{"ts": "158:16", "speaker": "I", "text": "Gab es irgendwelche SLA-Verletzungen?"}
{"ts": "158:19", "speaker": "E", "text": "Kurzzeitig, ja – in Minute 7 des Traffic-Shifts lag die p95 Latenz bei 128 ms. Das war über dem Zielwert, aber unter dem Toleranzwert von SLA-ORI-02 Annex B, daher kein Breach, nur ein gelber Alert."}
{"ts": "158:34", "speaker": "I", "text": "Wie dokumentieren Sie diese gelben Alerts?"}
{"ts": "158:37", "speaker": "E", "text": "Wir taggen sie in unserem Alertmanager mit 'SLA-WARN', verlinken auf das Grafana-Panel und hängen den Screenshot an das Deployment-Ticket. In diesem Fall ORI-DEP-584."}
{"ts": "158:49", "speaker": "I", "text": "Im Zusammenhang mit Aegis IAM – gab es dort während des Deployments Abhängigkeiten, die Probleme machten?"}
{"ts": "158:55", "speaker": "E", "text": "Ja, Aegis hat zeitgleich ein Minor Release der JIT Access Engine gefahren. Die neue Token-Cache-Strategie hat die Handshake-Zeit um ca. 3 ms erhöht. Wir haben das live kompensiert, indem wir in der Gateway-Config den Keep-Alive Timeout angepasst haben."}
{"ts": "159:10", "speaker": "I", "text": "Das klingt nach einer kleinen, aber kritischen Anpassung – war das vorab genehmigt?"}
{"ts": "159:14", "speaker": "E", "text": "Ja, wir hatten eine generische Exception in POL-SEC-001 für temporäre Timeout-Tunings, dokumentiert unter SEC-EXC-014. Die Genehmigung kam per ChatOps-Approval von unserem Security Lead, 4 Minuten vor Anwendung."}
{"ts": "159:58", "speaker": "I", "text": "Bevor wir zu den letzten Punkten kommen, möchte ich noch verstehen, wie sich die Erkenntnisse aus GW-4821 konkret in die Build-Phase von Orion Edge Gateway eingefügt haben."}
{"ts": "160:03", "speaker": "E", "text": "Klar, GW-4821 war ja dieser Zwischenfall mit dem unzureichend geprüften AuthZ-Bypass. Wir haben daraus in den Build-Pipelines zusätzliche statische Checks und mTLS-Validierungen direkt im Staging eingeführt, gemäß POL-SEC-001."}
{"ts": "160:12", "speaker": "I", "text": "Und diese Checks laufen automatisch bei jedem Merge in main?"}
{"ts": "160:15", "speaker": "E", "text": "Genau. Zusätzlich gibt es einen manuellen Gatekeeper-Job, der die JIT Access Policies gegen Aegis IAM prüft. Das ist in Runbook RB-GW-011 unter Abschnitt 4.2 dokumentiert."}
{"ts": "160:24", "speaker": "I", "text": "Wie wirkt sich das auf eure p95-Latenztests aus?"}
{"ts": "160:27", "speaker": "E", "text": "Minimal, weil wir die Tests parallelisieren. mTLS-Handshake-Overhead messen wir separat und kalkulieren ihn in SLA-ORI-02 ein. Unser Zielwert bleibt < 120ms p95."}
{"ts": "160:35", "speaker": "I", "text": "Gab es Situationen, wo ihr den Overhead bewusst in Kauf genommen habt?"}
{"ts": "160:38", "speaker": "E", "text": "Ja, etwa bei internen Admin-APIs, wo wir doppelte mTLS-Layer fahren. Da akzeptieren wir 10–15ms mehr, weil das Risiko eines Missbrauchs dort höher bewertet wird."}
{"ts": "160:46", "speaker": "I", "text": "Interessant. Und bei Blue/Green Rollouts – wie läuft das genau nach RB-GW-011?"}
{"ts": "160:50", "speaker": "E", "text": "Wir deployen die neue Green-Umgebung, fahren synthetische Lasttests, prüfen Metriken wie AuthZ-Durchsatz und mTLS-Fehlerquote. Erst wenn p95-Latenz und Error-Rate im grünen Bereich sind, schalten wir um."}
{"ts": "160:59", "speaker": "I", "text": "Wie dokumentieren Sie Abweichungen?"}
{"ts": "161:02", "speaker": "E", "text": "Über Tickets im internen Tracker, z.B. ORI-QA-217. Die enthalten Metriken und eine Root-Cause-Analyse. Verbesserungen fließen dann ins Runbook oder in die CI/CD-Templates."}
{"ts": "161:10", "speaker": "I", "text": "Gab es bei aggressivem Rate Limiting schon mal Probleme mit legitimen Lastspitzen?"}
{"ts": "161:14", "speaker": "E", "text": "Ja, bei einem Partner-Launch. Wir mussten den Burst-Wert kurzfristig verdoppeln, Risiko wurde als akzeptabel dokumentiert unter RISK-ORI-09."}
{"ts": "161:21", "speaker": "I", "text": "Das heißt, Sie passen Policies situativ an?"}
{"ts": "161:24", "speaker": "E", "text": "Genau, aber nur nach Freigabe durch SecOps und mit einer festgelegten Rückrollstrategie, wie sie in RB-GW-011, Abschnitt 6.3, beschrieben ist."}
{"ts": "161:26", "speaker": "I", "text": "Sie hatten vorhin mTLS-Overhead erwähnt – können Sie genauer ausführen, wie sich das auf den Build-Stand im Orion Edge Gateway auswirkt?"}
{"ts": "161:30", "speaker": "E", "text": "Ja, aktuell sehen wir im internen Staging etwa +18 ms bei p95, wenn wir mTLS gemäss POL-SEC-001 strikt erzwingen. Laut unseren Messungen (Ticket PERF-231) fällt das vor allem beim Handshake ins Gewicht, weniger bei den Folgetransfers."}
{"ts": "161:38", "speaker": "I", "text": "Und diese Messungen laufen automatisiert oder müssen Sie die manuell anstoßen?"}
{"ts": "161:42", "speaker": "E", "text": "Teilautomatisiert. Wir haben im CI-Job \"gw_latency_check\" Hooks, die die Latenzen vor und nach dem mTLS-Enable messen und gegen SLA-ORI-02 prüfen. Manuelle Verifikation machen wir nur, wenn ein Threshold > 20 ms überschritten wird."}
{"ts": "161:51", "speaker": "I", "text": "Gab es denn schon so eine Überschreitung im Build?"}
{"ts": "161:54", "speaker": "E", "text": "Einmal, beim Build 2024.05-rc2, da hat ein misconfigured Poseidon Networking Node ein Downgrade von TLS 1.3 auf 1.2 erzwungen, was den Handshake verlangsamt hat. Wir haben das in INC-4482 dokumentiert."}
{"ts": "162:03", "speaker": "I", "text": "Wie schnell konnten Sie da reagieren?"}
{"ts": "162:06", "speaker": "E", "text": "Innerhalb von 45 Minuten. RB-GW-011 sieht vor, bei Handshake-Anomalien sofort auf das Blue-Cluster zu wechseln und gleichzeitig die Poseidon Configs zu replizieren."}
{"ts": "162:14", "speaker": "I", "text": "Könnten solche Anomalien nicht auch aus Aegis IAM kommen, also beim JIT Access?"}
{"ts": "162:18", "speaker": "E", "text": "Ja, theoretisch. Aegis IAM nutzt für JIT Access ein kurzlebiges Zertifikat, das bei Clock Skew > 2 Sekunden schon fehlschlägt. Deshalb synchronisieren wir via Poseidon Time Sync Service jede 60 Sekunden – das ist so eine implizite Policy, die nirgends im Runbook steht."}
{"ts": "162:28", "speaker": "I", "text": "Das klingt nach einem potenziellen Single Point of Failure. Haben Sie das im Risikoregister?"}
{"ts": "162:32", "speaker": "E", "text": "Ja, im RISK-REG-ORI unter Eintrag R-17. Wir stufen es als 'mittel' ein, weil wir einen Fallback auf NTP Public Peers haben. Aber wir monitoren die Drift mit dem KPI 'cert_issue_latency'."}
{"ts": "162:41", "speaker": "I", "text": "Wie balancieren Sie das mit der Forderung nach p95 < 120 ms?"}
{"ts": "162:45", "speaker": "E", "text": "Durch Pre-Warming der TLS Sessions und Session Resumption Tickets. Das haben wir ab Build 2024.06 aktiviert, was laut Benchmark-Report (BENCH-042) den Overhead im Median um 40 % gesenkt hat."}
{"ts": "162:54", "speaker": "I", "text": "Gab es einen Fall, wo Sie ein Security Feature zugunsten der Verfügbarkeit temporär zurückgefahren haben?"}
{"ts": "162:58", "speaker": "E", "text": "Ja, im Stress-Test ST-029 haben wir Rate Limiting von 200rps auf 400rps hochgesetzt, um legitime Batch-Loads durchzulassen. Das war abgesegnet durch Change RFC-ORI-112 und ist in der Lessons Learned Sektion mit Bezug auf GW-4821 vermerkt."}
{"ts": "162:46", "speaker": "I", "text": "Sie hatten vorhin den mTLS-Overhead erwähnt – können Sie mir bitte etwas genauer erklären, wie Sie diesen in der Build-Phase gemessen haben und welche Tools Sie verwenden?"}
{"ts": "162:51", "speaker": "E", "text": "Ja, klar. Wir haben in der Stage-Umgebung synthetische Lasttests gefahren, mit k6 und einer modifizierten Version unseres internen Gateway-Benchmarkers. Dabei messen wir nicht nur die Latenz zwischen Client und Gateway, sondern auch die Handshake-Dauer direkt aus den TLS-Logs der Poseidon Networking Nodes. Die Werte schreiben wir in Prometheus und vergleichen sie gegen die Zielwerte aus SLA-ORI-02."}
{"ts": "162:59", "speaker": "I", "text": "Und diese Zielwerte, die sind im SLA-ORI-02 fixiert?"}
{"ts": "163:03", "speaker": "E", "text": "Genau, dort steht p95 Latenz ≤ 120 ms unter Normallast. Für mTLS haben wir eine Ausnahmeformel dokumentiert – Runbook RB-GW-011 verweist auf Anhang B, wo zulässige Peaks bis 140 ms unter bestimmten Zertifikatswechsel-Szenarien akzeptiert werden."}
{"ts": "163:12", "speaker": "I", "text": "Sie sprachen auch von Lessons Learned aus GW-4821. Ist da etwas in die AuthN/AuthZ-Tests eingeflossen, das besonders wichtig war?"}
{"ts": "163:17", "speaker": "E", "text": "Ja, aus GW-4821 haben wir gelernt, dass Race Conditions bei parallelen Token-Refreshs auftreten können. Deshalb haben wir in der Pipeline jetzt einen Chaos-Test, der gezielt Refresh-Wellen auslöst, während Aegis IAM neue Schlüsselpaare publiziert. Der Gateway-Code muss damit umgehen, ohne 401-Fehler zu generieren."}
{"ts": "163:28", "speaker": "I", "text": "Wie fließt das in Ihre täglichen Oncall-Aufgaben ein?"}
{"ts": "163:31", "speaker": "E", "text": "Wir haben im Oncall-Runbook eine Sektion, die die Logs der Aegis-IAM-Bridge in Echtzeit überwacht. Wenn dort ein plötzlicher Anstieg an TokenValidierungsfehlern auftaucht, wird automatisch ein PagerDuty-Alert ausgelöst. Das ist eine direkte Übersetzung der Lessons Learned in Betriebspraxis."}
{"ts": "163:41", "speaker": "I", "text": "Gab es denn bei den Blue/Green Rollouts konkrete Probleme, die Sie dokumentieren mussten?"}
{"ts": "163:45", "speaker": "E", "text": "Ja, in Ticket ORI-DEP-227 haben wir festgehalten, dass bei einem Green-Switch die mTLS-Zertifikate der neuen Nodes noch nicht vollständig propagiert waren. Laut RB-GW-011 haben wir jetzt eine zusätzliche Health-Check-Phase eingeführt, die explizit den mTLS-Handshake prüft, bevor Traffic fließt."}
{"ts": "163:57", "speaker": "I", "text": "Und wie gehen Sie mit Änderungen in Poseidon Networking um, wenn diese die Handshake-Mechanik betreffen?"}
{"ts": "164:01", "speaker": "E", "text": "Wir haben eine Cross-Team RFC-Prozedur, RFC-POS-074, die vorschreibt, dass jede mTLS-Änderung zuerst in einer isolierten Orion-Gateway-Instanz getestet werden muss. Wir setzen dann ein Canary mit 5% des Traffics, bevor wir den Rest umschalten."}
{"ts": "164:12", "speaker": "I", "text": "Gab es bei aggressivem Rate Limiting schon Fälle, in denen legitime Lastspitzen gebremst wurden?"}
{"ts": "164:16", "speaker": "E", "text": "Ja, bei einem Partner-Launch im März. Die API-Ratenlimits, konfiguriert nach POL-SEC-001, haben kurzzeitig legitime Bulk-Requests blockiert. Wir haben daraufhin ein adaptives Window eingeführt, das bei whitelisted Clients während Launch-Zeitfenstern höhere Limits zulässt."}
{"ts": "164:27", "speaker": "I", "text": "Das klingt nach einem Trade-off zwischen Sicherheit und Verfügbarkeit. Wie haben Sie das dokumentiert?"}
{"ts": "164:31", "speaker": "E", "text": "Im Risiko-Register ORI-RISK-014 ist das als kontrolliertes Risiko geführt. Wir verweisen dort auf die Monitoring-Regeln, die bei Überschreiten der p95-Latenz oder Anstieg der Error-Rate den adaptive Mode automatisch deaktivieren, um Missbrauch zu verhindern."}
{"ts": "164:10", "speaker": "I", "text": "Wir hatten vorhin Incident-Response kurz angerissen. Können Sie mir bitte schildern, wie RB-GW-011 in Ihrem Team tatsächlich umgesetzt wird, gerade wenn ein Blue/Green-Rollout schiefgeht?"}
{"ts": "164:16", "speaker": "E", "text": "Ja, also gemäß RB-GW-011 haben wir einen zweistufigen Rollback-Plan. Schritt eins ist das automatische Zurückschalten des Traffics per Canary-Flip innerhalb von 90 Sekunden. Schritt zwei, falls Persistenzdaten betroffen sind, ist ein koordinierter Restore aus dem letzten Snap der Config-DB. Wir haben das im letzten Quartal zweimal geübt, inklusive der mTLS-Session Resets."}
{"ts": "164:27", "speaker": "I", "text": "Und wie messen Sie dabei, ob die p95-Latenz innerhalb des SLA-ORI-02 bleibt?"}
{"ts": "164:32", "speaker": "E", "text": "Wir haben Prometheus-Alerts, die direkt den p95-Wert aus dem Ingress-Log des Gateways ziehen. Die Quellmetriken werden mit einer 1-Minuten-Granularität berechnet. Sobald wir über 120 ms liegen, gibt es eine Slack-Notification und der Oncall zieht das Runbookkapitel 4.2 heran."}
{"ts": "164:44", "speaker": "I", "text": "Gab es bei den letzten Incidents Abweichungen, die Sie dokumentiert haben?"}
{"ts": "164:48", "speaker": "E", "text": "Ja, Ticket INC-ORI-223 war so ein Fall. Während eines Updates des Aegis IAM änderte sich der Token-Endpoint, und unsere AuthZ-Checks blockierten. Latenz ging auf p95=350 ms hoch. Wir haben daraus abgeleitet, dass wir einen Fallback-Cache für JIT Access Tokens brauchen."}
{"ts": "164:59", "speaker": "I", "text": "Das klingt nach einem Multi-Hop-Problem. Können Sie die Abhängigkeiten Aegis IAM und Poseidon Networking hier nochmal darstellen?"}
{"ts": "165:04", "speaker": "E", "text": "Klar. Das Gateway fragt Aegis IAM für JIT Access an; das IAM validiert gegen sein eigenes Policy-Backend. Parallel verhandeln wir mit Poseidon Networking die mTLS-Session. Wenn Poseidon ein neues Root-Cert ausrollt und Aegis gerade Tokens rotiert, potenziert sich die Latenz. Deshalb haben wir in RFC-ORI-032 eine Sequenzierung vorgeschrieben: erst IAM-Update, dann Poseidon-Cert-Rollout."}
