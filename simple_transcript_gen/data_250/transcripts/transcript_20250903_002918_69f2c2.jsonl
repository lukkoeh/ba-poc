{"ts": "00:00", "speaker": "I", "text": "Können Sie mir bitte den aktuellen Stand des Orion Edge Gateway Projekts kurz zusammenfassen, damit wir ein gemeinsames Bild haben?"}
{"ts": "05:45", "speaker": "E", "text": "Ja, klar. Wir befinden uns in der Mitte der Build-Phase. Die Kernfunktionen – API-Gateway Routing, konfigurierbare Rate-Limits und die Integration mit unserem Aegis IAM – sind im internen Testbetrieb. Wir haben seit letzter Woche erste p95-Latenzmessungen im Staging, und die liegen stabil unter 110 ms."}
{"ts": "10:30", "speaker": "I", "text": "Welche Hauptziele hatten Sie zu Beginn der Build-Phase definiert?"}
{"ts": "15:00", "speaker": "E", "text": "Primär drei: Erstens die Einhaltung von SLA-ORI-02, sprich p95 < 120 ms unter Peak-Load. Zweitens eine Zero-Downtime-Deployment-Strategie, basierend auf unserem Runbook RB-GW-011. Und drittens eine nahtlose Authentifizierung über Aegis IAM, ohne dass Clients Anpassungen brauchen."}
{"ts": "20:15", "speaker": "I", "text": "Wie messen Sie momentan den Fortschritt gegenüber diesen Zielen?"}
{"ts": "25:00", "speaker": "E", "text": "Wir tracken in Jira alle Epics gegen Milestone M2. Für die Latenz haben wir in Grafana Dashboards mit p95- und p99-Kurven, die aus Prometheus-Daten gespeist werden. Zero-Downtime überprüfen wir mit synthetischen Canary-Requests während Deployments."}
{"ts": "30:10", "speaker": "I", "text": "Sie sprachen SLA-ORI-02 an. Wie stellen Sie sicher, dass diese Anforderung kontinuierlich eingehalten wird?"}
{"ts": "35:40", "speaker": "E", "text": "Wir haben in Runbook RB-MON-021 festgelegt, dass jeder Merge in main automatisch einen Lasttest in der CI auslöst. Bei >115 ms p95 gibt es einen Blocker-Status. Zusätzlich laufen in der Produktionsumgebung kontinuierliche Probes, die Alerts in OpsGenie generieren, falls der 5-Minuten-Median über 100 ms steigt."}
{"ts": "40:00", "speaker": "I", "text": "Gab es bislang Abweichungen?"}
{"ts": "45:20", "speaker": "E", "text": "Einmal, ja – im März, Ticket INC-ORI-174. Da hatten wir einen Spike auf 140 ms p95 wegen einer unerwarteten CPU-Drosselung im Kubernetes-Cluster. Wir haben daraufhin die HPA-Grenzen in deployment.yaml angepasst und das Limit höher gesetzt."}
{"ts": "50:00", "speaker": "I", "text": "Welche externen Systeme sind für das Orion Edge Gateway kritisch?"}
{"ts": "55:30", "speaker": "E", "text": "Neben Aegis IAM ist das vor allem unser zentrales Logging-System VelaLog für Audit Trails, sowie das interne Service Mesh HeliosNet, das für mTLS zwischen den Microservices sorgt. Fällt eine dieser Komponenten aus, können wir weder authentifizieren noch sauber routen."}
{"ts": "60:00", "speaker": "I", "text": "Wie koordinieren Sie Änderungen mit dem Aegis IAM Team?"}
{"ts": "65:15", "speaker": "E", "text": "Wir haben ein wöchentliches Sync-Meeting und nutzen die RFC-Queue im Confluence-Bereich 'IAM-EDGE'. Änderungen wie neue JWT-Claim-Strukturen werden mindestens zwei Sprints vorher angekündigt. Für Breaking Changes gibt es eine dedizierte Staging-Bridge."}
{"ts": "70:00", "speaker": "I", "text": "Gab es schon Schnittstellenänderungen, die zu Verzögerungen geführt haben?"}
{"ts": "75:00", "speaker": "E", "text": "Ja, im Januar hatte Aegis IAM die Token-Lifetime von 15 auf 5 Minuten reduziert, ohne unsere Load-Test-Szenarien anzupassen. Das führte zu vermehrten Reauthentifizierungen und einer Woche Verzögerung, bis wir die Gateway-Caches entsprechend erweitert hatten."}
{"ts": "90:00", "speaker": "I", "text": "Kommen wir nun zum Risikomanagement: Welche Haupt-Risiken sehen Sie aktuell für das Projekt Orion Edge Gateway, gerade im Hinblick auf die bevorstehenden Integrations-Tests?"}
{"ts": "90:08", "speaker": "E", "text": "Also, eines der größten Risiken ist tatsächlich die enge Kopplung an das Aegis IAM. Wenn das Team dort kurzfristig Änderungen am JWT-Schema vornimmt, könnten unsere Auth-Flows brechen. Zweitens: unsere Rate-Limiting-Implementierung nutzt Redis-Cluster mit festem Timeout, und bei hoher Last droht Latenz-Spitzen über die 120 ms hinaus."}
{"ts": "90:25", "speaker": "I", "text": "Gab es schon Vorfälle, bei denen Sie die Runbook-Prozedur RB-GW-011, also das Rolling Deployment Blue/Green, anwenden mussten?"}
{"ts": "90:33", "speaker": "E", "text": "Ja, im Test-Cluster vor zwei Wochen. Wir hatten einen Memory-Leak im neuen Rate-Limiter-Modul. Über das Runbook RB-GW-011 haben wir den grünen Slot vorbereitet, Traffic schrittweise umgeleitet und nach 15 Minuten den blauen Slot komplett dekativiert. Das Ganze war in Ticket ORI-INC-438 dokumentiert."}
{"ts": "90:50", "speaker": "I", "text": "Wie stellen Sie sicher, dass Lessons Learned aus solchen Vorfällen in zukünftige Deployments einfließen?"}
{"ts": "90:58", "speaker": "E", "text": "Nach jedem Incident führen wir ein Post-Mortem nach Template PM-TPL-02 durch. Die Action Items wandern in das technische Backlog. Zum Beispiel haben wir nach ORI-INC-438 ein neues Monitoring-Alert definiert, das Heap-Nutzung >85% als Warnung markiert."}
{"ts": "91:15", "speaker": "I", "text": "Gab es Entscheidungen, bei denen Performance gegen Feature-Umfang abgewogen wurde?"}
{"ts": "91:21", "speaker": "E", "text": "Ja, wir haben die geplante Geo-IP-Blocking-Funktion in Sprint 14 verschoben, um mehr Zeit für die Optimierung der Request-Pipeline zu gewinnen. Das war nötig, um SLA-ORI-02 p95<120 ms einzuhalten."}
{"ts": "91:34", "speaker": "I", "text": "Und wie gehen Sie mit Konflikten zwischen Kundenerwartungen und technischen Limitierungen um?"}
{"ts": "91:40", "speaker": "E", "text": "Wir kommunizieren transparent über Release Notes und in den Steering Committees. Bei zu hohen Erwartungen setzen wir Proof-of-Concept Builds ein, um die Machbarkeit zu validieren, bevor wir Commitments geben."}
{"ts": "91:55", "speaker": "I", "text": "Welche Kriterien nutzen Sie, um Backlog-Prioritäten anzupassen?"}
{"ts": "92:00", "speaker": "E", "text": "Primär Kundenimpact, Compliance-Relevanz und technische Abhängigkeiten. Ein Beispiel: Ein Security-Fix für das Token-Parsing hatte Vorrang vor einem UI-Feature, obwohl letzteres mehr Sichtbarkeit gehabt hätte."}
{"ts": "92:15", "speaker": "I", "text": "Was sind die nächsten Meilensteine für Orion Edge Gateway?"}
{"ts": "92:20", "speaker": "E", "text": "In zwei Wochen starten wir die End-to-End Tests mit dem Beta-Kundenkreis. Danach folgt das Penetration Testing mit externen Auditoren. Go-Live ist für Ende des nächsten Quartals geplant."}
{"ts": "92:34", "speaker": "I", "text": "Welche langfristigen Verbesserungen sind bereits geplant?"}
{"ts": "92:40", "speaker": "E", "text": "Langfristig wollen wir ein adaptives Rate-Limiting basierend auf Machine Learning einführen und die Auth-Integration so umbauen, dass auch OAuth 2.1 Flows unterstützt werden. Das senkt zukünftige Abhängigkeiten und Risiken."}
{"ts": "98:00", "speaker": "I", "text": "Lassen Sie uns bitte noch einmal konkret auf Entscheidungen eingehen, bei denen Sie Performance gegen den Feature-Umfang abwägen mussten."}
{"ts": "98:08", "speaker": "E", "text": "Ja, ein Beispiel ist Ticket ORI-412. Wir hatten die Option, einen erweiterten Payload-Transformations-Filter einzubauen, der für einige Kunden spannend gewesen wäre. Allerdings hätte dieser Filter die p95 Latenz um etwa 25 ms erhöht. Aufgrund von SLA-ORI-02 mussten wir das Feature hinten anstellen."}
{"ts": "98:32", "speaker": "I", "text": "Gab es da interne Diskussionen, oder war das klar?"}
{"ts": "98:36", "speaker": "E", "text": "Es gab eine intensive Diskussion im Architektur-Review. Wir haben die Metriken aus den Grafana-Dashboards herangezogen und gesehen, dass wir in Spitzenlastzeiten ohnehin knapp unter 120 ms liegen. Deshalb war die Entscheidung relativ klar: Performance first."}
{"ts": "98:58", "speaker": "I", "text": "Und wie reagieren Sie, wenn Kunden explizit nach solchen Features fragen?"}
{"ts": "99:04", "speaker": "E", "text": "Wir dokumentieren das in unserem RFC-Backlog und kommunizieren transparent die technischen Grenzen. Manchmal bieten wir einen Workaround an, z. B. via Preprocessing im Kundenkonnektor, um die Gateway-Latenz nicht zu belasten."}
{"ts": "99:22", "speaker": "I", "text": "Wie fließen solche Entscheidungen in die Backlog-Priorisierung ein?"}
{"ts": "99:26", "speaker": "E", "text": "Wir nutzen eine Kombination aus Business Value Scoring und Technical Risk Rating. Ein Feature mit hohem Nutzen, aber hohem Risiko für SLA-Verletzungen bekommt eine niedrigere Priorität, bis wir Kapazitäten für Optimierung haben."}
{"ts": "99:46", "speaker": "I", "text": "Gibt es ein Beispiel, wo Sie technische Limitierungen aktiv kommunizieren mussten, um Erwartungen zu managen?"}
{"ts": "99:52", "speaker": "E", "text": "Ja, bei einem Kunden aus dem IoT-Bereich. Er wollte WebSocket-Unterstützung mit bidirektionalem Push. Wir mussten klarstellen, dass unser aktueller Reverse Proxy nur HTTP/2 voll unterstützt und WebSockets im Edge Layer noch experimentell sind. Das wurde im Ticket ORI-377 festgehalten."}
{"ts": "100:16", "speaker": "I", "text": "Wie binden Sie Lessons Learned aus solchen Fällen in künftige Deployments ein?"}
{"ts": "100:20", "speaker": "E", "text": "Wir erweitern Runbook RB-GW-011 um eine Sektion 'Feature Impact Assessment'. Darin steht, welche SLA-relevanten KPIs vor Go-Live erneut zu prüfen sind. Außerdem führen wir ein Mini-Post-Mortem für abgelehnte Features durch."}
{"ts": "100:44", "speaker": "I", "text": "Kommen wir zum Abschluss: Was sind die nächsten Meilensteine im Orion Edge Gateway?"}
{"ts": "100:50", "speaker": "E", "text": "Wir planen für Q4 den Abschluss der Auth-Integration mit Aegis IAM v2, dann einen Stresstest mit 1,5 Mio RPS und schließlich das Go-Live in drei Pilotregionen. Parallel evaluieren wir adaptive Rate Limiting."}
{"ts": "101:12", "speaker": "I", "text": "Und welche langfristigen Verbesserungen stehen bereits auf der Roadmap?"}
{"ts": "101:18", "speaker": "E", "text": "Langfristig wollen wir eine Multi-Region-Deployment-Strategie mit automatischem Failover, eine feinere Mandanten-Isolation auf API-Ebene und ein selbstlernendes Caching-Modul implementieren, um Latenzen weiter zu optimieren."}
{"ts": "114:00", "speaker": "I", "text": "Kommen wir nun zu den Trade-offs, die Sie in der Build-Phase treffen mussten. Gab es konkrete Situationen, in denen Sie Performance gegen Feature-Umfang abwägen mussten?"}
{"ts": "114:06", "speaker": "E", "text": "Ja, definitiv. Ein Beispiel: Wir hatten eine geplante Data-Transformation-Engine im Gateway, die sehr komplexe Payload-Manipulationen erlaubt hätte. Das hätte aber in unseren Tests die p95 Latenz von SLA-ORI-02 um ca. 35 ms erhöht. Wir haben uns dann entschieden, diese Engine in eine nachgelagerte Service-Chain auszulagern, um die Gateway-Latenz niedrig zu halten."}
{"ts": "114:16", "speaker": "I", "text": "Das klingt nach einer bewussten Entscheidung zugunsten der Performance. Gab es bei dieser Entscheidung Diskussion mit den Stakeholdern?"}
{"ts": "114:22", "speaker": "E", "text": "Ja, die Business-Seite wollte die Engine sofort, um ein Premium-Feature zu vermarkten. Wir haben anhand von Load-Test-Berichten (Testlauf ID LT-ORI-17) aus JMeter gezeigt, dass die User Experience sonst deutlich leiden würde. Am Ende haben wir einen RFC (RFC-ORI-023) verabschiedet, der das Feature ins Q4 verschiebt."}
{"ts": "114:35", "speaker": "I", "text": "Wie gehen Sie generell damit um, wenn Kundenerwartungen nicht mit technischen Limitierungen übereinstimmen?"}
{"ts": "114:41", "speaker": "E", "text": "Wir arbeiten mit sogenannten Expectation Alignment Workshops. Dort zeigen wir Kunden anhand von Monitoring-Dashboards, was technisch realistisch ist. Außerdem dokumentieren wir Constraints in unserem Confluence-Bereich 'ORI-Limits', damit keine falschen Annahmen entstehen."}
{"ts": "114:52", "speaker": "I", "text": "Gab es jüngst einen konkreten Vorfall, bei dem Sie RB-GW-011 Rolling Deployments anwenden mussten?"}
{"ts": "114:58", "speaker": "E", "text": "Ja, vor drei Wochen. Wir hatten einen Incident (Ticket INC-ORI-4421), bei dem ein fehlerhafter Rate-Limiting-Algorithmus in Produktion ging. Mit RB-GW-011 haben wir eine Blue/Green-Strategie genutzt, um die fehlerhafte Green-Instanz sofort zurückzunehmen, ohne Downtime. Das hat uns geholfen, SLA-ORI-02 weiter einzuhalten."}
{"ts": "115:12", "speaker": "I", "text": "Und wie fließen solche Lessons Learned in zukünftige Deployments ein?"}
{"ts": "115:17", "speaker": "E", "text": "Wir haben nach dem Incident ein Post-Mortem durchgeführt, die Ursachen analysiert und RB-GW-011 um einen Pre-Deployment Canary-Testschritt erweitert. Dieser Schritt ist nun fester Bestandteil des Runbooks ORI-DEP-05."}
{"ts": "115:26", "speaker": "I", "text": "Welche Kriterien nutzen Sie, um die Prioritäten im Backlog anzupassen, wenn solche Vorfälle auftreten?"}
{"ts": "115:31", "speaker": "E", "text": "Wir gewichten nach Impact auf SLA-ORI-02, Sicherheitsrelevanz und Kundenvertragsverpflichtungen. Ein Incident mit SLA-Gefahr rutscht automatisch in die höchste Prioritätsklasse P1."}
{"ts": "115:40", "speaker": "I", "text": "Zum Abschluss: Was sind die nächsten Meilensteine für das Orion Edge Gateway?"}
{"ts": "115:45", "speaker": "E", "text": "Nächste Woche starten wir mit dem Integrationstest gegen die neue Aegis IAM API v2. Danach planen wir einen Performance-Benchmark-Sprint, um sicherzustellen, dass wir für die Beta im September die p95-Latenz sogar unter 100 ms drücken können."}
{"ts": "115:55", "speaker": "I", "text": "Und langfristig?"}
{"ts": "116:00", "speaker": "E", "text": "Langfristig wollen wir eine adaptive Rate-Limiting-Komponente entwickeln, die Machine Learning nutzt, um Traffic-Spitzen vorherzusagen. Erfolg messen wir aus Kundensicht über NPS und API-Response-Zeiten im Live-Betrieb."}
{"ts": "116:00", "speaker": "I", "text": "Können Sie bitte noch einmal konkret schildern, wie sich die letzten Latenzoptimierungen auf den geplanten Funktionsumfang ausgewirkt haben?"}
{"ts": "116:05", "speaker": "E", "text": "Ja, gern. Wir hatten im Ticket ORI-PERF-145 zwei neue Caching-Strategien vorgesehen, mussten aber eines davon streichen, um die p95-Latenz unter 120 ms zu halten. Das hat bedeutet, dass der Geo-IP-Filter erst im nächsten Release kommt."}
{"ts": "116:15", "speaker": "I", "text": "Das heißt, SLA-ORI-02 war in dieser Entscheidung maßgeblich?"}
{"ts": "116:18", "speaker": "E", "text": "Genau. Laut unserem Runbook RB-SLA-KEEP-01 priorisieren wir Maßnahmen, die direkt auf SLA-ORI-02 einzahlen, auch wenn dafür Features verschoben werden müssen."}
{"ts": "116:28", "speaker": "I", "text": "Gab es dabei Gegenstimmen aus dem Stakeholder-Kreis?"}
{"ts": "116:31", "speaker": "E", "text": "Ja, das Vertriebsteam wollte den Geo-IP-Filter unbedingt zum Messeauftritt, aber wir haben anhand der Metriken im Observability-Dashboard gezeigt, dass jede zusätzliche Lookup-Operation ca. 15 ms hinzugefügt hätte."}
{"ts": "116:43", "speaker": "I", "text": "Sie hatten vorhin den Incident erwähnt, bei dem RB-GW-011 zum Einsatz kam – können Sie den Ablauf kurz skizzieren?"}
{"ts": "116:47", "speaker": "E", "text": "Klar. Am 03. Mai hatten wir einen Auth-Service-Timeout mit Aegis IAM, was den Gateway-Throughput halbierte. Wir haben ein Blue/Green-Deployment gefahren, um auf die vorherige stabile Auth-Library zu wechseln. Das stand so in RB-GW-011 unter Abschnitt 3.2 beschrieben."}
{"ts": "116:59", "speaker": "I", "text": "Wie lange hat die Recovery gedauert?"}
{"ts": "117:01", "speaker": "E", "text": "Vom Start der Analyse bis zur vollständigen Wiederherstellung: etwa 22 Minuten. Das war innerhalb unseres internen Incident-SLO von 30 Minuten für kritische Pfade."}
{"ts": "117:10", "speaker": "I", "text": "Welche Lessons Learned haben Sie daraus abgeleitet?"}
{"ts": "117:13", "speaker": "E", "text": "Wir haben beschlossen, künftig Canary-Deployments auch für Library-Updates zu nutzen, nicht nur für Core-Services. Außerdem wurde eine Schnittstellentest-Suite mit dem Aegis-Team vereinbart."}
{"ts": "117:24", "speaker": "I", "text": "Blicken wir kurz nach vorn: Was sind die nächsten Meilensteine im Projektplan?"}
{"ts": "117:27", "speaker": "E", "text": "Nächster Halt ist das Feature-Freeze im Juli, gefolgt von einer vierwöchigen Performance-Härtung. Danach planen wir das Go-Live im September für ausgewählte Pilotkunden."}
{"ts": "117:36", "speaker": "I", "text": "Und langfristig, welche Verbesserungen sind schon auf der Roadmap?"}
{"ts": "117:40", "speaker": "E", "text": "Langfristig wollen wir die Policy Engine modularisieren, um flexibel neue Auth-Mechanismen einbinden zu können, und eine adaptive Rate Limiting-Komponente entwickeln, die Lastspitzen ohne starre Limits abfängt."}
{"ts": "124:00", "speaker": "I", "text": "Lassen Sie uns nun etwas tiefer in die Abwägung zwischen Performance und Feature-Umfang gehen. Wie haben Sie konkret entschieden, welche Features in diesem Sprint zurückgestellt werden, um SLA-ORI-02 einzuhalten?"}
{"ts": "124:15", "speaker": "E", "text": "Wir haben im Sprint Planning die p95 Latenzmetriken aus dem letzten Canary Release analysiert. Da wir bei 118 ms lagen, haben wir entschieden, das geplante Payload-Transformation-Feature zu verschieben. Das ist in Jira-Ticket ORI-324 dokumentiert, mit Verweis auf SLA-ORI-02."}
{"ts": "124:38", "speaker": "I", "text": "Gab es dazu Diskussionen mit dem Vertrieb oder dem Kundenmanagement?"}
{"ts": "124:44", "speaker": "E", "text": "Ja, der Vertrieb wollte die Transformation drin haben, weil ein Pilotkunde darauf wartet. Aber wir haben anhand der Daten aus dem Observability-Dashboard gezeigt, dass zusätzliche Processing-Schritte in der Gateway-Pipeline uns wahrscheinlich über 120 ms p95 gebracht hätten."}
{"ts": "125:05", "speaker": "I", "text": "Und wie haben Sie das kommuniziert?"}
{"ts": "125:10", "speaker": "E", "text": "Über ein kurzes Tech-Briefing im Management-Call und schriftlich im Change-Log. Wir haben auch die Runbook-Referenz RB-GW-011 erwähnt, um zu erklären, wie ein Blue/Green Deployment uns erlaubt hätte, das Feature später gezielt einzuschleusen."}
{"ts": "125:30", "speaker": "I", "text": "Gab es in letzter Zeit einen Vorfall, bei dem RB-GW-011 tatsächlich Anwendung fand?"}
{"ts": "125:37", "speaker": "E", "text": "Ja, vor drei Wochen. Beim Rollout der neuen Auth-Integration mit Aegis IAM hat ein fehlerhaftes Token-Expiry-Handling zu 401er Fehlern geführt. Wir haben sofort auf die vorherige Green-Umgebung zurückgeschaltet, wie in Abschnitt 4.2 des Runbooks beschrieben."}
{"ts": "125:58", "speaker": "I", "text": "Wie lange hat die Wiederherstellung gedauert?"}
{"ts": "126:02", "speaker": "E", "text": "Unter zehn Minuten bis zur vollen Funktionsfähigkeit. Danach haben wir einen Hotfix in der Blue-Umgebung getestet und um 02:00 Uhr nachts erneut umgeschaltet."}
{"ts": "126:18", "speaker": "I", "text": "Und welche Lessons Learned sind daraus hervorgegangen?"}
{"ts": "126:23", "speaker": "E", "text": "Wir haben eine Pre-Deployment-Checkliste für Auth-Änderungen ergänzt, und das Staging-Environment jetzt mit einem realistischeren Token-Lifetime-Parameter versehen. Das ist in Confluence-Seite QA-GW-Auth-Checks dokumentiert."}
{"ts": "126:42", "speaker": "I", "text": "Blicken wir auf die nächsten Meilensteine: was steht als Nächstes an?"}
{"ts": "126:48", "speaker": "E", "text": "Ende des Monats der Abschluss der Rate-Limiting-Optimierung, dann im nächsten Quartal das gestaffelte Ausrollen des Payload-Transformation-Features – mit zusätzlichem Latenz-Benchmarking parallel."}
{"ts": "127:04", "speaker": "I", "text": "Und langfristig, welche Verbesserungen haben Sie im Blick?"}
{"ts": "127:10", "speaker": "E", "text": "Langfristig wollen wir Edge-Caching einführen, um die Latenz zu reduzieren, und eine engere CI/CD-Integration mit automatischen SLA-Gates. Aus Kundensicht wird der Erfolg an der stabilen Einhaltung von SLA-ORI-02 und der schnelleren Time-to-Feature gemessen."}
{"ts": "132:00", "speaker": "I", "text": "Zum Abschluss würde ich gerne noch einmal auf die Lessons Learned aus dem letzten Deployment zurückkommen, speziell im Hinblick auf die Blue/Green-Strategie aus RB-GW-011."}
{"ts": "132:10", "speaker": "E", "text": "Ja, klar. Wir haben festgestellt, dass der Umschaltzeitpunkt zwischen den Umgebungen zu knapp kalkuliert war. Dadurch hatten wir in der Green-Phase kurzzeitig p95-Latenzen von über 125 ms, was leicht über SLA-ORI-02 lag."}
{"ts": "132:20", "speaker": "I", "text": "Wie haben Sie diese Abweichung in der Praxis dokumentiert und kommuniziert?"}
{"ts": "132:32", "speaker": "E", "text": "Wir haben sofort ein Ticket im Incident-Board erstellt, ID INC-ORI-448, und die Metriken aus Prometheus angehängt. Zusätzlich gab es ein kurzes Post-Mortem im Team mit dem Link zum Runbook-Abschnitt 4.2, wo der Umschaltpuffer definiert ist."}
{"ts": "132:44", "speaker": "I", "text": "Gab es daraufhin Anpassungen am Runbook selbst?"}
{"ts": "132:55", "speaker": "E", "text": "Ja, wir haben den Puffer von bisher 30 Sekunden auf 90 Sekunden erhöht und einen zusätzlichen Health-Check-Endpoint integriert, der sowohl die Auth-Integration mit Aegis IAM prüft als auch die Rate-Limiting-Logik."}
{"ts": "133:06", "speaker": "I", "text": "Das heißt, Sie haben die Abhängigkeit zu Aegis IAM stärker in den Deployment-Flow eingebunden?"}
{"ts": "133:17", "speaker": "E", "text": "Genau. Früher haben wir nur nach Verfügbarkeit der Orion-Services geschaut, jetzt prüfen wir zusätzlich, ob die Auth-Flows mit IAM stabil laufen. Das kam aus einer Kombination von Feedback der DevOps und einem Hinweis aus Ticket DEP-ORI-119."}
{"ts": "133:29", "speaker": "I", "text": "Sehen Sie hier noch Risiken, wenn man die Checks erweitert?"}
{"ts": "133:40", "speaker": "E", "text": "Ein kleines Risiko ist, dass wir beim Deployment länger in der Blue-Phase bleiben, wenn externe Systeme gerade instabil sind. Das kann geplante Releases verzögern, aber wir nehmen das in Kauf, um SLA-Verstöße zu vermeiden."}
{"ts": "133:52", "speaker": "I", "text": "Wie wird das Team intern auf solche Verzögerungen vorbereitet?"}
{"ts": "134:04", "speaker": "E", "text": "Wir haben im internen Confluence eine Checkliste ergänzt, die die Kommunikationspfade beschreibt: Slack-Kanal #deploy-alerts pingt automatisch das Release-Management an, wenn ein Check fehlschlägt."}
{"ts": "134:15", "speaker": "I", "text": "Und wie gehen Sie mit Kundenkommunikation um, falls sich dadurch Features verzögern?"}
{"ts": "134:27", "speaker": "E", "text": "Wir haben eine vereinbarte Regel mit dem Account Management, dass bei Verzögerungen >24h ein proaktives Update an betroffene Kunden gesendet wird, inklusive einer kurzen Erklärung und neuem ETA."}
{"ts": "134:39", "speaker": "I", "text": "Könnte man argumentieren, dass diese Vorsicht zu Lasten der Time-to-Market geht?"}
{"ts": "134:50", "speaker": "E", "text": "Ja, das ist der Trade-off. Aber wir haben gelernt, dass ein Verstoß gegen SLA-ORI-02 und ein incident-bedingtes Hotfix-Release mehr Zeit und Vertrauen kostet als ein Tag Verzögerung bei einem geplanten Feature."}
{"ts": "136:00", "speaker": "I", "text": "Bevor wir abschließen, wollte ich noch kurz nachhaken: Gab es in den letzten zwei Wochen konkrete Anpassungen am Deployment-Plan, die aus Lessons Learned resultierten?"}
{"ts": "136:05", "speaker": "E", "text": "Ja, wir haben nach dem Incident vom März, Ticket INC-ORI-447, in unserem Runbook RB-GW-011 den Schritt zur Pre-Deployment-Latenzprüfung erweitert. Das heißt, wir führen jetzt vor jedem Blue/Green-Switch eine synthetische Traffic-Simulation durch, um p95-Latenzwerte zu verifizieren."}
{"ts": "136:15", "speaker": "I", "text": "Und diese Simulation – läuft die automatisiert oder manuell?"}
{"ts": "136:18", "speaker": "E", "text": "Automatisiert, direkt in der CI/CD-Pipeline. Wir haben das als Stage 'latency-gate' hinterlegt. Falls der Wert über 115ms liegt, wird der Deploy gestoppt und das Build-Team benachrichtigt."}
{"ts": "136:28", "speaker": "I", "text": "Gab es schon einen Fall, wo dieses Gate tatsächlich den Rollout verhindert hat?"}
{"ts": "136:32", "speaker": "E", "text": "Einmal letzte Woche. Wir hatten durch eine unoptimierte JWT-Validierung, die vom Aegis IAM kam, plötzlich bei Peak-Load eine p95 von 128ms. Das latency-gate hat gegriffen, wir haben die Abhängigkeit gefixt, bevor es live ging."}
{"ts": "136:45", "speaker": "I", "text": "Interessant – da sieht man die Verzahnung zu externen Systemen. Wie schnell konntet ihr das mit dem Aegis-Team klären?"}
{"ts": "136:50", "speaker": "E", "text": "Innerhalb von vier Stunden. Wir haben in unserem Schnittstellen-Channel eine Eskalation gemäß RFC-INT-07 gemacht. Da steht genau drin, welche Logs und Traces wir mitliefern müssen, damit sie effizient debuggen können."}
{"ts": "137:00", "speaker": "I", "text": "Das klingt nach einem gut gelebten Prozess. Gibt es aus deiner Sicht noch Lücken im Risikomanagement?"}
{"ts": "137:05", "speaker": "E", "text": "Hm, ja… Was wir noch nicht abdecken, sind simultane Incidents in mehreren Upstream-APIs. Unser aktuelles Playbook geht eher von einem isolierten Problem aus. Das wollen wir in RB-GW-015 als Multi-Failure-Scenario ergänzen."}
{"ts": "137:16", "speaker": "I", "text": "Also quasi eine Art Chaos-Engineering-Light für Schnittstellen?"}
{"ts": "137:20", "speaker": "E", "text": "Genau. Wir wollen gezielt Last und Fehler injizieren, um zu sehen, ob unsere Fallback-Mechanismen SLA-ORI-02 noch halten können, auch wenn zum Beispiel Auth und Rate Limiting gleichzeitig verzögern."}
{"ts": "137:32", "speaker": "I", "text": "Letzte Frage dazu: Wie kommuniziert ihr solche geplanten Tests an Stakeholder, damit es nicht zu Missverständnissen kommt?"}
{"ts": "137:36", "speaker": "E", "text": "Wir schicken vorab eine Testankündigung über das Change Advisory Board-Portal und markieren die Sessions als 'simuliert'. Außerdem setzen wir im Monitoring einen Flag 'test-mode', damit keine falschen Alarmierungen ausgelöst werden."}
{"ts": "137:48", "speaker": "I", "text": "Gut, das ist sauber. Vielleicht noch ein Wort zu den nächsten Meilensteinen?"}
{"ts": "137:52", "speaker": "E", "text": "Kurzfristig: Abschluss der Auth-Caching-Optimierung, um 10ms Latenz einzusparen. Mittel- bis langfristig planen wir die Einführung von Adaptive Rate Limits pro Mandant, das ist schon im Backlog als EPIC ORI-57 beschrieben."}
{"ts": "137:36", "speaker": "I", "text": "Bevor wir zum Abschluss kommen, wollte ich noch einmal auf die Lessons Learned eingehen – wie stellen Sie sicher, dass diese nicht nur dokumentiert, sondern auch gelebt werden?"}
{"ts": "137:41", "speaker": "E", "text": "Wir haben einen Abschnitt in unserem Runbook RB-GW-011 erweitert, in dem wir konkrete Checklisten für Blue/Green Deploys aufnehmen. Zusätzlich führen wir nach jedem Sprint ein kurzes Review der letzten Incidents durch, um die Erkenntnisse ins Backlog zu übertragen."}
{"ts": "137:48", "speaker": "I", "text": "Das heißt, diese Reviews sind fest im Prozess verankert?"}
{"ts": "137:51", "speaker": "E", "text": "Genau, sie sind Teil unseres Definition-of-Done. Ohne ein abgeschlossenes Lessons-Learned-Item schließen wir kein Epic, selbst wenn alle Stories technisch fertig sind."}
{"ts": "137:57", "speaker": "I", "text": "Und wie wird das vom Team angenommen? Es kostet ja Zeit."}
{"ts": "138:01", "speaker": "E", "text": "Es gab anfangs Skepsis, aber der Incident im Mai, Ticket ORI-INC-2023-05-14, hat gezeigt, dass die Dokumentation uns bei der Root-Cause-Analyse viel Zeit gespart hat. Das war ein echter Augenöffner."}
{"ts": "138:09", "speaker": "I", "text": "Können Sie zu diesem Incident noch kurz erläutern, wie die Reaktionszeit war im Vergleich zur SLA-Vorgabe?"}
{"ts": "138:14", "speaker": "E", "text": "Ja, unsere SLA-ORI-02 definiert p95 Latenz unter 120 ms, und wir hatten in der Peak-Phase Werte bis 180 ms. Durch die schnelle Rollback-Strategie aus RB-GW-011 waren wir aber innerhalb von 14 Minuten wieder compliant, also deutlich unter dem maximal erlaubten 30-Minuten-Window für SLA-Verletzungen."}
{"ts": "138:25", "speaker": "I", "text": "Beeindruckend. Hat das auch Einfluss auf die Priorisierung künftiger Features gehabt?"}
{"ts": "138:29", "speaker": "E", "text": "Ja, wir haben zwei geplante Features – Geo-IP Rate Limiting und erweiterte Auth-Provider – um einen Sprint nach hinten verschoben, um die Latenzpfade zu optimieren. Das war ein klarer Trade-off zugunsten Stabilität."}
{"ts": "138:38", "speaker": "I", "text": "Wie kommunizieren Sie solche Verschiebungen an Stakeholder, die vielleicht eher auf die neuen Features warten?"}
{"ts": "138:43", "speaker": "E", "text": "Wir nutzen einen monatlichen Release-Newsletter und verweisen dort transparent auf die SLA-Compliance-Zahlen. Wenn wir verdeutlichen, dass 99,97 % Uptime für alle Kunden relevanter ist als ein einzelnes Feature, verstehen es die meisten sehr gut."}
{"ts": "138:51", "speaker": "I", "text": "Gibt es dafür auch einen KPI im Reporting?"}
{"ts": "138:54", "speaker": "E", "text": "Ja, wir tracken 'SLA Breach Minutes' und 'Customer Impacted Sessions'. Diese KPIs sind direkt mit den OKRs des Teams verknüpft."}
{"ts": "138:59", "speaker": "I", "text": "Gut, und abschließend: worauf bereiten Sie sich im nächsten Quartal besonders vor?"}
{"ts": "139:03", "speaker": "E", "text": "Wir werden die Schnittstellen zum Aegis IAM refactoren, um JWT-Parsing in Go direkt im Gateway zu implementieren, was uns voraussichtlich 15–20 ms Latenz spart. Parallel designen wir ein Canary-Release-Pattern, das RB-GW-011 ergänzt, um Risiken bei Feature-Toggles zu minimieren."}
{"ts": "138:06", "speaker": "I", "text": "Bevor wir abschließen, würde ich gerne noch wissen, wie Sie konkret die Lessons Learned aus dem letzten Blue/Green Deployment dokumentiert haben."}
{"ts": "138:12", "speaker": "E", "text": "Wir haben intern einen Eintrag im Runbook RB-GW-011 erweitert, äh, mit einem Abschnitt zu den Latenzspitzen, die während des Traffic-Switch auftraten. Zusätzlich gibt es im Confluence die Ticket-Referenz INC-ORI-77 mit einer detaillierten Timeline und Screenshots der Metriken."}
{"ts": "138:24", "speaker": "I", "text": "Gab es nach diesem Incident sofortige Anpassungen an den Deployment-Skripten?"}
{"ts": "138:29", "speaker": "E", "text": "Ja, wir haben in unserem Ansible-Playbook einen zusätzlichen Healthcheck-Step eingefügt, der vor dem Umschalten jetzt drei aufeinanderfolgende p95-Messungen unter 120 ms verlangt, um SLA-ORI-02 sicherzustellen."}
{"ts": "138:41", "speaker": "I", "text": "Haben Sie diesen Healthcheck auch in die CI/CD-Pipeline integriert?"}
{"ts": "138:46", "speaker": "E", "text": "Genau, in Jenkins Stage 'pre-prod-verify'. Das war eine der schnellen Wins aus der Post-Mortem-Analyse."}
{"ts": "138:54", "speaker": "I", "text": "Wie reagieren Sie, wenn dieser Healthcheck fehlschlägt?"}
{"ts": "138:59", "speaker": "E", "text": "Dann wird der Deploy automatisch abgebrochen und ein Rollback auf die vorherige Blue-Instanz ausgelöst. Das ist auch im Runbook Schritt 5.4 beschrieben."}
{"ts": "139:08", "speaker": "I", "text": "Gab es seither einen Fall, wo dieser Mechanismus ausgelöst wurde?"}
{"ts": "139:13", "speaker": "E", "text": "Einmal, vor zwei Wochen. Wir hatten ein Auth-Timeout mit dem Aegis IAM, wodurch die Latenz im Gateway angestiegen ist. Das Monitoring hat's erkannt, Deployment gestoppt, und wir konnten die Ursache mit dem IAM-Team binnen 40 Minuten beheben."}
{"ts": "139:27", "speaker": "I", "text": "Das klingt nach einer funktionierenden Schnittstellenkoordination."}
{"ts": "139:31", "speaker": "E", "text": "Ja, wir haben dafür den Channel #orion-aegis-sync und ein festes wöchentliches Change-Alignment Meeting, um solche Abhängigkeiten proaktiv zu managen."}
{"ts": "139:40", "speaker": "I", "text": "Zum Abschluss: Welche Metriken werden Sie nutzen, um den Projekterfolg aus Kundensicht in den nächsten sechs Monaten zu messen?"}
{"ts": "139:46", "speaker": "E", "text": "Neben der SLA-ORI-02 Latenz schauen wir auf Error Rate <0,2 %, API Gateway Throughput und NPS-Umfragen nach Go-Live. Diese KPIs sind im SLO-Dashboard hinterlegt."}
{"ts": "139:57", "speaker": "I", "text": "Und langfristig, gibt es schon Überlegungen zu weiteren Optimierungen?"}
{"ts": "140:02", "speaker": "E", "text": "Ja, wir planen ein Upgrade des Rate Limiting Moduls auf eine adaptive Variante, die Lastspitzen dynamischer abfedern kann, und die Integration eines erweiterten Observability-Stacks mit verteiltem Tracing, um künftige Latenzursachen noch schneller zu isolieren."}
{"ts": "141:06", "speaker": "I", "text": "Bevor wir zum Abschluss kommen, würde mich noch interessieren, wie Sie Lessons Learned aus dem letzten Blue/Green Deployment dokumentiert haben."}
{"ts": "141:12", "speaker": "E", "text": "Wir haben nach dem Incident ORI-INC-77 einen Post-Mortem Report erstellt, der im internen Confluence-Space abgelegt wurde. Darin sind Root Cause, Timeline und empfohlene Maßnahmen gemäß Runbook RB-GW-011 erfasst."}
{"ts": "141:20", "speaker": "I", "text": "Gab es dabei spezielle Anpassungen am Runbook selbst?"}
{"ts": "141:24", "speaker": "E", "text": "Ja, wir haben den Schritt zur Pre-Switch Latenzmessung erweitert. Vorher haben wir nur die p95-Latenz 5 Minuten nach Umschaltung gemessen. Jetzt prüfen wir auch direkt davor, um Anomalien früh zu erkennen."}
{"ts": "141:33", "speaker": "I", "text": "Inwiefern hat das Einfluss auf die Einhaltung von SLA-ORI-02?"}
{"ts": "141:38", "speaker": "E", "text": "Direkt, weil wir damit potenzielle Überschreitungen von 120 ms schon in der Staging-Umgebung sehen. Wir hatten z. B. im Test am 14. März einen Peak von 138 ms, den wir vor Live-Schaltung korrigieren konnten."}
{"ts": "141:48", "speaker": "I", "text": "Das heißt, Sie betreiben für Orion Edge Gateway auch ein dediziertes Staging mit identischer Lastsimulation?"}
{"ts": "141:54", "speaker": "E", "text": "Genau, mit synthetischen Requests über unser Tool 'LoadHammer'. Die Szenarien spiegeln reale API-Aufrufe, inkl. Auth gegen Aegis IAM, wider."}
{"ts": "142:02", "speaker": "I", "text": "Apropos IAM – gab es seit unserem letzten Gespräch Änderungen an der Schnittstelle, die Sie berücksichtigen mussten?"}
{"ts": "142:07", "speaker": "E", "text": "Ja, das Aegis-Team hat im RFC-AEG-214 die Token-Lebensdauer verkürzt. Das mussten wir im Gateway-Cache anpassen, sonst wären vermehrt Token-Refreshes in kritischen Pfaden aufgetreten."}
{"ts": "142:16", "speaker": "I", "text": "War das ein Risiko in Bezug auf Latenz?"}
{"ts": "142:19", "speaker": "E", "text": "Definitiv, jeder unnötige Refresh kostet uns etwa 15–20 ms. Deswegen haben wir einen proaktiven Refresh-Mechanismus implementiert, der Tokens asynchron erneuert."}
{"ts": "142:27", "speaker": "I", "text": "Wie gehen Sie dabei mit der Balance zwischen Komplexität und Performance um?"}
{"ts": "142:31", "speaker": "E", "text": "Wir haben bewusst nur in den High-Traffic-Services diesen Mechanismus eingeführt. Für selten genutzte Endpunkte wäre der Entwicklungsaufwand zu hoch im Verhältnis zur Latenzverbesserung."}
{"ts": "142:39", "speaker": "I", "text": "Klingt nach einem gezielten Trade-off. Haben Sie das auch im Backlog dokumentiert?"}
{"ts": "142:44", "speaker": "E", "text": "Ja, als Decision Record DR-ORI-005, inklusive Begründung, Alternativen und Bewertung nach den Kriterien 'Impact auf SLA' und 'Entwicklungsaufwand'."}
{"ts": "145:06", "speaker": "I", "text": "Bevor wir abschließen, würde mich interessieren, wie Sie Lessons Learned aus dem RB-GW-011 Incident konkret in die Runbooks einpflegen."}
{"ts": "145:10", "speaker": "E", "text": "Wir haben direkt nach dem Incident ein Post-Mortem gemäß unserem internen Leitfaden PM-EDGE erstellt, darin die Root Cause Analysis und die Abfolge der Ereignisse dokumentiert. Anschließend wurde Runbook RB-GW-DEP-03 um einen zusätzlichen Checkschritt erweitert, der vor dem Umschalten der Blue/Green-Umgebung die p95-Latenzmessung gegen SLA-ORI-02 validiert."}
{"ts": "145:16", "speaker": "I", "text": "Gab es dabei Diskussionen im Team, ob das den Deploy-Prozess verlangsamt?"}
{"ts": "145:20", "speaker": "E", "text": "Ja, durchaus. Einige Entwickler fanden, dass der zusätzliche 15‑Sekunden-Testzyklus nicht nötig sei. Aber aus Sicht des SLA-Risikos war klar, dass wir lieber minimal später live gehen, als eine Latenzverletzung zu riskieren."}
{"ts": "145:26", "speaker": "I", "text": "Und wie hat der Kunde auf diese Prozessänderung reagiert?"}
{"ts": "145:30", "speaker": "E", "text": "Überraschend positiv. In der letzten Review mit dem Key Account haben wir das transparent erklärt, inklusive der Metrik-Grafen aus unserem Grafana‑Board. Der Kunde sah das als Zeichen von Reife im Release-Management."}
{"ts": "145:36", "speaker": "I", "text": "Können Sie ein Beispiel geben, wie die Änderungen an den Schnittstellen mit Aegis IAM jetzt besser koordiniert werden?"}
{"ts": "145:40", "speaker": "E", "text": "Klar. Wir haben ein gemeinsames Confluence‑Board mit Release-Kalender eingeführt und ein wöchentliches Sync-Meeting. Außerdem meldet Aegis IAM jetzt RFCs mit mindestens zwei Sprints Vorlauf, sodass wir API-Änderungen im Gateway frühzeitig mocken und Lasttests fahren können."}
{"ts": "145:46", "speaker": "I", "text": "Haben die zusätzlichen Koordinationsschritte Einfluss auf die Time-to-Market?"}
{"ts": "145:50", "speaker": "E", "text": "Minimal, aber der Gewinn an Stabilität und SLA‑Konformität ist deutlich höher. Wir haben in den letzten drei Monaten keine ungeplanten Downtimes durch Auth-Änderungen mehr gehabt."}
{"ts": "145:56", "speaker": "I", "text": "Wie messen Sie künftig den Erfolg des Projekts aus Kundensicht?"}
{"ts": "146:00", "speaker": "E", "text": "Neben der SLA‑Erfüllung binden wir NPS‑Umfragen ins Kundenportal ein und erfassen Feature‑Adoption‑Rates. Ein Beispiel: Das neue Rate‑Limiting‑Dashboard, das wir in Sprint 34 eingeführt haben, wird bereits von 68 % der Pilotkunden genutzt."}
{"ts": "146:06", "speaker": "I", "text": "Gibt es schon einen Fahrplan für langfristige Verbesserungen?"}
{"ts": "146:10", "speaker": "E", "text": "Ja, wir planen für Q4 eine Migration auf das neue interne Observability‑Framework NovaTrace, um Latenzspitzen noch granularer zu analysieren. Außerdem wollen wir das Gateway modularisieren, damit künftige Feature-Toggles ohne Downtime möglich sind."}
{"ts": "146:16", "speaker": "I", "text": "Abschließend: Welcher Meilenstein steht als nächstes an?"}
{"ts": "146:20", "speaker": "E", "text": "Der nächste große Meilenstein ist die GA‑Freigabe von Orion Edge Gateway Version 1.0 in sechs Wochen. Bis dahin finalisieren wir die Auth‑Integration, führen End‑to‑End‑Tests durch und bereiten das Kunden-Onboarding-Material vor."}
{"ts": "146:30", "speaker": "I", "text": "Zum Thema Lessons Learned – wie fließen denn die Erfahrungen aus dem Incident mit RB-GW-011 konkret in Ihre künftigen Deployments ein?"}
{"ts": "146:35", "speaker": "E", "text": "Wir haben daraus drei Maßnahmen abgeleitet: Erstens ist im Runbook RB-GW-DEP-07 jetzt ein zusätzlicher Health-Check-Schritt vor dem Traffic-Switch dokumentiert. Zweitens haben wir im Staging eine Lastspitze simuliert, um das Verhalten bei p95 Latenzen über 120ms (SLA-ORI-02) vorab zu sehen. Drittens ist ein wöchentlicher Review mit dem Operations-Team eingeführt."}
{"ts": "146:45", "speaker": "I", "text": "Gab es dabei technische Anpassungen am Deployment-Tooling selbst?"}
{"ts": "146:50", "speaker": "E", "text": "Ja, wir haben das Blue/Green-Skript so erweitert, dass es automatisiert die SLO-Metriken aus unserem Observability-Stack (Prometheus + custom Exporter) prüft, bevor es die Green-Instanz produktiv schaltet."}
{"ts": "146:56", "speaker": "I", "text": "Interessant, und wie gehen Sie mit Feature Requests um, die potenziell die Latenz negativ beeinflussen könnten?"}
{"ts": "147:01", "speaker": "E", "text": "Wir priorisieren erst nach einem Impact Assessment. Ein Beispiel: Für Feature F-ORI-221 (komplexe Auth-Rule aus Aegis IAM) haben wir einen Proof of Concept gebaut, der zeigte, dass die p95 Latenz um ca. 15ms steigt. Da wir noch Puffer zu SLA-ORI-02 hatten, haben wir es freigegeben, aber mit einem Feature-Flag versehen."}
{"ts": "147:12", "speaker": "I", "text": "Kommt es vor, dass Kundenwünsche dennoch abgelehnt werden?"}
{"ts": "147:16", "speaker": "E", "text": "Ja, wenn sie sowohl den Latenz-Puffer als auch die CPU-Quota überschreiten würden. Wir hatten ein Ticket ORI-REQ-145, das einen sehr aufwändigen Geo-IP-Filter vorsah – der hätte unsere Gateway-Threads blockiert. Den haben wir zurückgestellt und Alternativen angeboten."}
{"ts": "147:25", "speaker": "I", "text": "Wie kommunizieren Sie solche Entscheidungen an die Stakeholder?"}
{"ts": "147:29", "speaker": "E", "text": "Wir nutzen ein Decision Log im Confluence-Workspace. Darin dokumentieren wir die technischen Analysen, die Abwägungen und die Referenzen zu SLAs und Runbooks. Zusätzlich gibt es ein monatliches Steering-Meeting, wo wir das transparent vorstellen."}
{"ts": "147:38", "speaker": "I", "text": "Und welche nächsten Meilensteine stehen jetzt konkret an?"}
{"ts": "147:42", "speaker": "E", "text": "Bis Ende des Quartals wollen wir das neue Rate-Limiting-Modul RLM-3.2 ausrollen. Parallel läuft ein Pilot mit Aegis IAM v2 für OAuth2 mTLS, um Auth-Speed zu optimieren."}
{"ts": "147:50", "speaker": "I", "text": "Langfristig – gibt es schon Verbesserungen in Planung, die über die SLAs hinausgehen?"}
{"ts": "147:54", "speaker": "E", "text": "Ja, wir evaluieren eine adaptive Routing-Logik, die Requests basierend auf aktueller Latenz dynamisch verteilt. Das ist nicht Teil des SLA, würde aber die Kundenerfahrung verbessern und uns Wettbewerbsvorteile bringen."}
{"ts": "148:02", "speaker": "I", "text": "Wie messen Sie am Ende den Erfolg des Projekts aus Kundensicht?"}
{"ts": "148:06", "speaker": "E", "text": "Neben der SLA-Einhaltung werten wir Net Promoter Scores und API-Usability-Feedback aus. Wir haben auch Metriken wie Error-Rate <0,1% und Onboarding-Zeit für neue Clients, die wir monatlich tracken."}
{"ts": "148:06", "speaker": "I", "text": "Bevor wir zum Abschluss kommen, würde mich noch interessieren, ob Sie aktuelle Optimierungen bei der Authentifizierungs-Integration mit Aegis IAM vornehmen."}
{"ts": "148:12", "speaker": "E", "text": "Ja, wir haben gerade ein Mini-Refactoring in der Token-Validierungsschicht angestoßen. Hintergrund war ein Finding aus Ticket ORI-DEV-447, das im letzten Sprint-Review diskutiert wurde."}
{"ts": "148:20", "speaker": "I", "text": "Können Sie kurz beschreiben, was genau in ORI-DEV-447 adressiert wurde?"}
{"ts": "148:24", "speaker": "E", "text": "Klar, es ging um die Cache-Invalidierung bei abgelaufenen JWTs. Wir hatten einen Edge Case, bei dem p95 Latenz unter Last leicht über 120 ms rutschte, weil der Refresh-Flow zu oft durchlief."}
{"ts": "148:34", "speaker": "I", "text": "Und wie haben Sie das gelöst, ohne die SLA-ORI-02 zu verletzen?"}
{"ts": "148:38", "speaker": "E", "text": "Wir haben einen Async-Preload der neuen Tokens eingeführt, basierend auf einem Heuristik-Wert aus dem letzten Profiling-Report. Das ist in Runbook RB-AUTH-005 dokumentiert."}
{"ts": "148:48", "speaker": "I", "text": "Gibt es da Berührungspunkte zu anderen Subsystemen?"}
{"ts": "148:52", "speaker": "E", "text": "Ja, der Async-Preload triggert im API-Gateway ein prefetch event, das wiederum die Rate-Limiter-Module aufwärmt. Das war ein Cross-Team-Change mit dem Netzwerkteam, damit die Warmup-Pfade nicht in die Quota zählen."}
{"ts": "149:04", "speaker": "I", "text": "Interessant, also eine Optimierung, die gleichzeitig Auth und Rate-Limiting betrifft."}
{"ts": "149:08", "speaker": "E", "text": "Genau, und diese Art von Multi-Hop-Optimierungen planen wir künftig in einer eigenen RFC-Reihe, damit die Abhängigkeiten klar sichtbar sind."}
{"ts": "149:16", "speaker": "I", "text": "Gab es im Zuge dessen Risiken, die Sie berücksichtigen mussten?"}
{"ts": "149:20", "speaker": "E", "text": "Ja, die größte Sorge war, dass wir durch das Preloading unnötig viele Tokens generieren und so das IAM-System belasten. Wir haben deshalb ein Limit von maximal fünf parallel preload requests eingeführt."}
{"ts": "149:30", "speaker": "I", "text": "War das eine schnelle Entscheidung oder eher Ergebnis längerer Abwägungen?"}
{"ts": "149:34", "speaker": "E", "text": "Das war nach zwei Tagen Spike-Testing klar. Wir haben Metriken aus ORI-PERF-LOG202 und historische IAM-Auslastungsdaten kombiniert, um die Belastungsgrenzen zu bestimmen."}
{"ts": "149:44", "speaker": "I", "text": "Wird diese Optimierung noch vor dem nächsten Meilenstein ausgerollt?"}
{"ts": "149:50", "speaker": "E", "text": "Ja, wir planen das Rollout in der nächsten Blue/Green-Deployment-Woche, dokumentiert in RB-GW-011, damit wir im Fall von Regressionen schnell auf die alte Version zurückschwenken können."}
{"ts": "152:06", "speaker": "I", "text": "Können wir noch einmal kurz auf die Lessons Learned aus dem letzten Blue/Green Deployment zurückkommen? Mich interessiert, ob daraus auch Anpassungen an den Runbooks erfolgt sind."}
{"ts": "152:12", "speaker": "E", "text": "Ja, wir haben tatsächlich nach dem Incident ORI-INC-202 mit RB-GW-011 den Abschnitt zur Rollback-Strategie im Runbook RBK-EDGE-DEP-04 erweitert. Neu ist, dass wir jetzt explizit einen 5-Minuten Smoke-Test-Checkpoint vornehmen, bevor der Green-Cluster den Traffic komplett übernimmt."}
{"ts": "152:24", "speaker": "I", "text": "Gab es dazu spezifische Metriken, die Sie als Gate verwenden?"}
{"ts": "152:27", "speaker": "E", "text": "Wir nutzen p95 Latenz, Error Rate und Auth-Handshake-Dauer als drei harte Gates. Bei Latenz > 120ms (SLA-ORI-02) oder Fehlerquote > 0,5% stoppen wir den Switch. Die Auth-Dauer kommt aus dem Aegis IAM Monitoring-Feed."}
{"ts": "152:38", "speaker": "I", "text": "Das heißt, die Schnittstelle zu Aegis IAM ist inzwischen auch für die Deployment-Entscheidungen kritisch?"}
{"ts": "152:42", "speaker": "E", "text": "Genau. Früher haben wir nur API-Response-Timings intern gemessen. Jetzt fließt der externe Auth-Handshake mit ein, weil wir festgestellt haben, dass der größte Teil der Latenz manchmal dort entsteht."}
{"ts": "152:51", "speaker": "I", "text": "Wie koordinieren Sie in so einem Fall kurzfristige Änderungen mit dem IAM-Team?"}
{"ts": "152:54", "speaker": "E", "text": "Wir haben seit Q2 ein 'Change Alignment Call' zweimal pro Woche. Außerdem gibt es einen gemeinsamen Slack-Channel #proj-ori-aegis, in dem wir geplante Deployments mit mindestens 48h Vorlauf ankündigen."}
{"ts": "153:04", "speaker": "I", "text": "Klingt formalisiert. Gab es dennoch ungeplante Störungen?"}
{"ts": "153:07", "speaker": "E", "text": "Ja, z.B. Ticket ORI-INT-045: Das Aegis-Team hat ein Minor-Update eingespielt, das den JWT-Signaturalgorithmus geändert hat. Unser Gateway hat dies nicht akzeptiert, was zu 401-Fehlern führte. Wir haben daraufhin einen Fallback-Parser aktiviert."}
{"ts": "153:18", "speaker": "I", "text": "War der Fallback-Parser schon vorher dokumentiert?"}
{"ts": "153:21", "speaker": "E", "text": "Er war in der Codebasis vorhanden, aber nicht im Runbook. Nach dem Incident wurde er im Abschnitt 'Auth Error Recovery' ergänzt, inklusive Schritt-für-Schritt-Anleitung und Test-Command."}
{"ts": "153:31", "speaker": "I", "text": "Wie stellen Sie sicher, dass solche Änderungen im Runbook auch wirklich bei allen Engineers ankommen?"}
{"ts": "153:34", "speaker": "E", "text": "Wir haben ein wöchentliches Sync-Meeting, das 'Runbook Review'. Dort werden Änderungen live vorgestellt. Zusätzlich gibt es einen Newsletter im Confluence-Space, der die Changelog-Einträge zusammenfasst."}
{"ts": "153:44", "speaker": "I", "text": "Abschließend: Gibt es auf Basis dieser Erfahrungen schon Planungen für proaktive Risiko-Scans vor Deployments?"}
{"ts": "153:48", "speaker": "E", "text": "Ja, wir evaluieren gerade ein Pre-Deploy QA-Skript, das wichtige Endpunkte mit synthetischen Requests gegen Staging und Pre-Prod testet. Das soll automatisiert in der CI/CD-Pipeline laufen und bei Abweichungen automatisch ein Blocker-Flag setzen."}
{"ts": "153:30", "speaker": "I", "text": "Bevor wir abschließen, würde ich gern noch auf die Lessons Learned aus dem letzten Blue/Green Deployment zurückkommen. Gab es konkrete Anpassungen im Runbook RB-GW-011 danach?"}
{"ts": "153:36", "speaker": "E", "text": "Ja, wir haben nach dem Incident-Ticket INC-ORI-458 zwei Punkte ergänzt: Erstens eine explizite Latenz-Messung im Pre-Switch-Check, zweitens einen manuellen Gate-Stop, falls p95 über 110ms liegt."}
{"ts": "153:44", "speaker": "I", "text": "Das klingt wie eine Verschärfung der SLA-Vorgaben um einen Safety-Margin. War das leicht durchzusetzen?"}
{"ts": "153:49", "speaker": "E", "text": "Intern ja, weil das AIOps-Dashboard schon die entsprechenden Metriken liefert. Wir mussten nur in der CI/CD-Pipeline den Hook ergänzen; das war im RFC-ORI-72 dokumentiert."}
{"ts": "153:56", "speaker": "I", "text": "Und wie wirkt sich das auf die Abstimmung mit dem Aegis IAM Team aus, wenn deren Auth-API die Latenz beeinflusst?"}
{"ts": "154:02", "speaker": "E", "text": "Wir haben ein gemeinsames SLO-Dokument erstellt, SLO-EDGE-AEG-01. Das definiert, dass die Auth-Response in unter 40ms liegen muss, sonst wird ein Pre-Warm-Mechanismus aktiviert."}
{"ts": "154:10", "speaker": "I", "text": "Gab es schon einen Fall, wo dieser Pre-Warm nötig war?"}
{"ts": "154:14", "speaker": "E", "text": "Einmal, bei einem Lasttest im März. Wir haben das in LoadTestReport-ORI-MAR23 dokumentiert. Der Pre-Warm hat die p95 von 145ms auf 105ms gedrückt."}
{"ts": "154:22", "speaker": "I", "text": "Spannend. Das heißt, ihr habt jetzt eine Art Fail-Safe-Layer zwischen Gateway und IAM."}
{"ts": "154:26", "speaker": "E", "text": "Genau, und das ist im Architekturschaubild ARD-ORI-04 vermerkt. Es war ein Multi-Team-Workshop nötig, um das zu designen."}
{"ts": "154:33", "speaker": "I", "text": "Wie beeinflusst diese Architekturentscheidung die Roadmap? Müssen Features nach hinten geschoben werden?"}
{"ts": "154:38", "speaker": "E", "text": "Ja, zwei Integrationsfeatures (Bulk Token Refresh, GeoIP-Blocking) sind um ein Sprint verschoben, um die Stabilitätsschicht sauber zu testen."}
{"ts": "154:45", "speaker": "I", "text": "Und wie kommunizieren Sie diese Verschiebungen an Stakeholder?"}
{"ts": "154:49", "speaker": "E", "text": "Über das monatliche Release Committee Meeting und ein Changelog in Confluence. Wir verlinken dort auch die JIRA-Tickets, z.B. ORI-234 für die Latenzoptimierung."}
{"ts": "154:56", "speaker": "I", "text": "Gibt es noch offene Risiken aus Ihrer Sicht, die wir nicht besprochen haben?"}
{"ts": "155:01", "speaker": "E", "text": "Ein Risiko ist die Abhängigkeit vom neuen CDN-Anbieter für Edge-Caching. Wir haben noch kein vollständiges Failover getestet, das ist als Task ORI-278 im Backlog."}
{"ts": "155:06", "speaker": "I", "text": "Wir hatten vorhin den Incident mit RB-GW-011 angesprochen. Können Sie schildern, wie genau der Blue/Green-Ansatz in der Situation umgesetzt wurde?"}
{"ts": "155:11", "speaker": "E", "text": "Ja, klar. Wir haben uns strikt an Runbook RB-GW-011 gehalten: Erst die Green-Umgebung mit der neuen API-Version hochfahren, Health-Checks nach dem Schema in Abschnitt 4.2 durchführen – das sind 15 Sekunden Liveness, 30 Sekunden Readiness – und erst dann den Traffic via Load-Balancer-Shift umleiten."}
{"ts": "155:18", "speaker": "I", "text": "Gab es in diesem Fall besondere Herausforderungen bei den Health-Checks?"}
{"ts": "155:22", "speaker": "E", "text": "Ja, wir hatten ein Edge-Case, bei dem die Auth-Integration mit Aegis IAM in Green langsamer reagierte, wodurch der p95 knapp über 120ms ging. Wir haben das erkannt, indem wir das Latenz-Dashboard im Grafana-Cluster ORI-MON-1 beobachtet haben."}
{"ts": "155:30", "speaker": "I", "text": "Und wie sind Sie dann vorgegangen, um innerhalb der SLA zu bleiben?"}
{"ts": "155:34", "speaker": "E", "text": "Wir haben sofort das Rate-Limiting-Profil RL-ORI-03 temporär verschärft, um Last zu reduzieren, und parallel das IAM-Team gebeten, den Token-Cache zu erhöhen. Nach 7 Minuten waren wir wieder unter den 120ms."}
{"ts": "155:42", "speaker": "I", "text": "Das klingt nach guter Koordination. Fließen solche Learnings in die Deployment-Strategie ein?"}
{"ts": "155:46", "speaker": "E", "text": "Absolut, wir haben nach dem Incident-Ticket INC-ORI-2024-07-14 in Confluence eine Sektion ergänzt, in der wir speziell für Auth-Latenzen einen Vorab-Lasttest in Staging vorschreiben."}
{"ts": "155:53", "speaker": "I", "text": "Kommen wir zu den Priorisierungen im Backlog: Gab es zuletzt einen Fall, bei dem Sie ein Feature zurückgestellt haben, um Performance zu optimieren?"}
{"ts": "155:57", "speaker": "E", "text": "Ja, das interne Metrics-Export-Feature für Partner-APIs wurde von Sprint 22 auf Sprint 25 verschoben, weil wir die HTTP Handler in Go refaktorieren mussten, um Garbage-Collection-Peaks zu vermeiden. Das hatte direkten Einfluss auf SLA-ORI-02."}
{"ts": "156:04", "speaker": "I", "text": "Wie reagieren Stakeholder, wenn Features verschoben werden?"}
{"ts": "156:08", "speaker": "E", "text": "Wir kommunizieren transparent im Steering Committee und verweisen auf die Risiko-Matrix RM-ORI-05. Meistens gibt es Verständnis, wenn wir mit Daten aus den p95-Reports untermauern können, dass eine Verschiebung nötig ist."}
{"ts": "156:15", "speaker": "I", "text": "Welche Risiken stehen aktuell ganz oben auf Ihrer Liste?"}
{"ts": "156:19", "speaker": "E", "text": "Hauptsächlich externe API-Änderungen von Aegis IAM ohne Vorankündigung. Außerdem ein potenzieller Engpass bei der TLS-Termination, falls der geplante Hardwaretausch sich verzögert."}
{"ts": "156:25", "speaker": "I", "text": "Und wie mitigieren Sie diese?"}
{"ts": "156:29", "speaker": "E", "text": "Für die API-Änderungen haben wir einen wöchentlichen Schema-Diff-Check automatisiert. Für die TLS-Hardware halten wir eine virtuelle Fallback-Instanz in Reserve, die wir in 15 Minuten provisionieren können."}
{"ts": "156:30", "speaker": "I", "text": "Lassen Sie uns noch etwas tiefer auf das Thema Risikomanagement eingehen. Welche Risiken stehen derzeit ganz oben auf Ihrer Liste?"}
{"ts": "156:35", "speaker": "E", "text": "Aktuell sind es vor allem zwei: Erstens, potenzielle Engpässe bei der Authentifizierung, falls das Aegis IAM Update vom nächsten Monat Verzögerung hat. Zweitens, das Risiko, dass wir bei hohen Lastspitzen die p95-Latenz von 120 ms aus SLA-ORI-02 überschreiten, wenn die Rate-Limiting-Strategie nicht wie im Runbook RB-RL-004 greift."}
{"ts": "156:44", "speaker": "I", "text": "Könnten Sie ein Beispiel geben, wie Sie diese Risiken konkret monitoren?"}
{"ts": "156:48", "speaker": "E", "text": "Ja, wir haben in Grafana Dashboards, die direkt aus den Prometheus-Metriken gespeist werden. Für Auth-Latenz gibt es einen Alertmanager-Alarm AL-ORI-17, der bei >80 ms p95 über 5 Minuten anschlägt. Für Rate-Limit-Fehler nutzen wir eine synthetische Traffic-Quelle im Staging, die täglich gegen die API läuft."}
{"ts": "156:59", "speaker": "I", "text": "Und falls ein Alarm ausgelöst wird, wie sieht der Prozess aus?"}
{"ts": "157:03", "speaker": "E", "text": "Da folgen wir Runbook RB-IR-002: Innerhalb von 15 Minuten nimmt der On-Call-Engineer das Ticket im System JIRA-OPS auf, klassifiziert es nach Schweregrad und startet, falls nötig, einen Hotfix-Deploy via Blue/Green, wie bei RB-GW-011 beschrieben."}
{"ts": "157:12", "speaker": "I", "text": "Gab es kürzlich einen Fall, der so eskaliert ist?"}
{"ts": "157:15", "speaker": "E", "text": "Vor drei Wochen, Ticket INC-ORI-142. Da hat ein ungetestetes IAM-Policy-Update zu Auth-Zeitouts geführt. Wir haben binnen 8 Minuten reagiert, die alte Policy wieder eingespielt und das Aegis-Team involviert. Post-Mortem-Dokumentation ist jetzt Teil unseres Wissensarchivs."}
{"ts": "157:26", "speaker": "I", "text": "Wie stellen Sie sicher, dass diese Lessons Learned auch wirklich in die nächsten Releases einfließen?"}
{"ts": "157:30", "speaker": "E", "text": "Wir haben ein monatliches Release-Retrospective-Meeting. Dort werden die Post-Mortems vorgestellt, und wir leiten verbindliche Actions ab, die in das Backlog wandern. Zum Beispiel haben wir nach INC-ORI-142 ein Pre-Deploy-IAM-Policy-Check-Skript hinzugefügt."}
{"ts": "157:40", "speaker": "I", "text": "Das klingt sehr prozessgetrieben. Gab es auch Situationen, in denen Sie abwägen mussten zwischen schneller Feature-Einführung und Performance-Stabilität?"}
{"ts": "157:45", "speaker": "E", "text": "Ja, im Februar stand Feature F-GW-Search sehr hoch auf der Kundenwunschliste. Aber die Volltextsuche hätte initial unsere Latenz um ~15 ms erhöht. Wir haben entschieden, nur eine vereinfachte Version in Sprint 12 zu liefern und die optimierte Variante in Sprint 14, nachdem wir Caching-Strategien für Suchindizes implementiert hatten."}
{"ts": "157:56", "speaker": "I", "text": "Welche Kriterien waren da ausschlaggebend für die Priorisierung?"}
{"ts": "158:00", "speaker": "E", "text": "Die Hauptkriterien waren SLA-ORI-02-Compliance, Kundenimpact, und der Entwicklungsaufwand. Wir nutzen dafür eine einfache Scorecard: 1–5 Punkte pro Kriterium, und Features mit <10 Punkten werden verschoben."}
{"ts": "158:09", "speaker": "I", "text": "Zum Abschluss: Was sind die nächsten Meilensteine im Projekt?"}
{"ts": "158:13", "speaker": "E", "text": "Nächster großer Meilenstein ist der Public Beta Launch in T-6 Wochen. Vorher müssen wir noch den Auth-Fallback-Mechanismus fertigstellen, Lasttests bis 2 Mio. Requests/Tag durchführen und die Dokumentation für den Self-Service-API-Key-Flow abschließen."}
{"ts": "158:06", "speaker": "I", "text": "Können Sie bitte noch einmal konkret auf die Lessons Learned aus dem letzten Deployment eingehen? Mich interessiert, wie diese in den Runbook-Prozess eingeflossen sind."}
{"ts": "158:12", "speaker": "E", "text": "Ja, gern. Wir haben nach dem Incident vom Typ RB-GW-011 im Runbook Abschnitt 4.3.2 ergänzt, dass vor jedem Traffic-Switch im Blue/Green Setup ein zusätzlicher Canary-Test gegen die Auth-API von Aegis IAM gefahren werden muss. Das war vorher implizit, jetzt ist es ein formaler Check mit Protokollpflicht."}
{"ts": "158:26", "speaker": "I", "text": "Gab es dazu auch Anpassungen in den Monitoring-Dashboards oder Alerts?"}
{"ts": "158:31", "speaker": "E", "text": "Ja, wir haben im Grafana-Board für Orion Edge Gateway ein Panel ergänzt, das die p95 Latenz aus SLA-ORI-02 in Echtzeit gegen den Canary-Test spiegelt. Alert-Threshold ist jetzt bei 110 ms gesetzt, damit wir noch einen Puffer zur 120 ms-Grenze haben."}
{"ts": "158:44", "speaker": "I", "text": "Und wie reagieren Sie, wenn dieser Puffer überschritten wird?"}
{"ts": "158:49", "speaker": "E", "text": "Dann greift unser Playbook ORI-PL-07: Wir frieren alle nicht-kritischen Deployments ein, schwenken gegebenenfalls auf die letzte stabile Blue-Umgebung zurück und benachrichtigen das Aegis IAM Team, falls die Ursache dort liegen könnte."}
{"ts": "158:59", "speaker": "I", "text": "Hatten Sie kürzlich so einen Fall, bei dem das Playbook ausgeführt wurde?"}
{"ts": "159:04", "speaker": "E", "text": "Vor drei Wochen, ja. Ticket ORI-INC-229. Die Latenz ist in der Canary-Phase plötzlich auf 138 ms gesprungen, weil Aegis IAM ein Schema-Update gefahren hat, das bei uns ungeplante Validierungen ausgelöst hat."}
{"ts": "159:18", "speaker": "I", "text": "Wie haben Sie daraus Rückschlüsse für das Schnittstellenmanagement gezogen?"}
{"ts": "159:23", "speaker": "E", "text": "Wir haben mit dem Aegis-Team eine neue Change-Notification-Policy vereinbart: Schema-Änderungen müssen jetzt mindestens 48 Stunden vorher angekündigt werden, und wir fahren vorab eine Lastprobe in der Staging-Umgebung mit den neuen Schemas."}
{"ts": "159:37", "speaker": "I", "text": "Das klingt nach einer klaren Verbesserung. Gab es intern Diskussionen, ob diese Policy eventuell zu Release-Verzögerungen führen könnte?"}
{"ts": "159:42", "speaker": "E", "text": "Ja, im Steering Committee war das ein Thema. Wir haben abgewogen zwischen Time-to-Market und der Stabilität. Die Entscheidung fiel zugunsten der Stabilität, da wir aus den letzten zwei Incidents gelernt haben, dass Downtime-Kosten die Verzögerungskosten deutlich übersteigen."}
{"ts": "159:55", "speaker": "I", "text": "Wie wird diese Entscheidung dokumentiert und kommuniziert?"}
{"ts": "160:00", "speaker": "E", "text": "In RFC-Dokument RFC-ORI-023. Wir pflegen alle wesentlichen Trade-off-Entscheidungen dort und referenzieren die dazugehörigen Tickets und Metriken. So können wir in Retrospektiven nachvollziehen, warum wir welchen Weg gewählt haben."}
{"ts": "160:12", "speaker": "I", "text": "Planen Sie weitere Änderungen an Runbooks oder Policies aus Lessons Learned?"}
{"ts": "160:17", "speaker": "E", "text": "Ja, wir wollen im nächsten Sprint ein automatisches Staging-Deployment nach jedem Merge in den Main-Branch verankern. Damit verringern wir die Lücke zwischen Entwicklung und Integrationstest und erkennen p95-Ausreißer früher."}
{"ts": "160:06", "speaker": "I", "text": "Bevor wir zum Abschluss kommen, würde ich gern noch tiefer auf die Lessons Learned aus dem letzten Blue/Green Deployment eingehen. Gab es da spezifische Punkte, die Sie in die Runbooks aufgenommen haben?"}
{"ts": "160:11", "speaker": "E", "text": "Ja, definitiv. Wir haben zum Beispiel im Runbook RB-GW-DEP-07 jetzt einen zusätzlichen Health-Check für die Auth-Integration verankert, weil beim letzten Deployment das Secondary Cluster einen Timeout bei Aegis IAM hatte. Früher war das nur implizit durch den End-to-End-Test abgedeckt."}
{"ts": "160:19", "speaker": "I", "text": "Und dieser Health-Check, läuft der automatisiert oder muss jemand manuell eingreifen?"}
{"ts": "160:23", "speaker": "E", "text": "Der ist voll automatisiert. Wir haben ihn in unser Observability-Tool Eclipso integriert, sodass er vor dem Traffic Switch ausgeführt wird. Falls er fehlschlägt, bleibt das alte Cluster aktiv und es wird ein Incident Ticket vom Typ PRE-DEP erzeugt."}
{"ts": "160:31", "speaker": "I", "text": "Können Sie ein Beispiel geben, wie so ein PRE-DEP Ticket aussieht?"}
{"ts": "160:36", "speaker": "E", "text": "Klar, PRE-DEP-451 war der Code beim letzten Mal. Es enthielt die Fehlermeldung aus dem Auth-Service, den betroffenen Namespace und eine verlinkte Chat-Thread-ID für das War Room Meeting. Das ist inzwischen Standardprozedere."}
{"ts": "160:44", "speaker": "I", "text": "Hat das auch Auswirkungen auf die SLA-Überwachung, gerade in Bezug auf p95 Latenz?"}
{"ts": "160:49", "speaker": "E", "text": "Indirekt ja. Wenn wir vor dem Umschalten Latenzpeaks sehen, wird der Rollout gestoppt. Das sichert, dass wir SLA-ORI-02 nicht verletzen. Es ist ein Trade-off: wir verzögern das Go-Live, behalten aber die Performance stabil."}
{"ts": "160:57", "speaker": "I", "text": "Gab es Überlegungen, diesen Trade-off anders zu gewichten, zum Beispiel Features schneller auszuliefern, auch wenn die Latenz kurzfristig steigt?"}
{"ts": "161:02", "speaker": "E", "text": "Ja, im Sprint 48 hatten wir eine Diskussion dazu. Letztlich haben wir uns dagegen entschieden, weil selbst kurze Latenzspitzen die Upstream-Partner irritieren könnten. Wir priorisieren hier die Stabilität über den Funktionsumfang."}
{"ts": "161:10", "speaker": "I", "text": "Wie fließen solche Entscheidungen eigentlich zurück ins Backlog-Management?"}
{"ts": "161:14", "speaker": "E", "text": "Wir markieren die Stories mit einem Flag `perf-critical`. Das beeinflusst die Priorisierung in Jira so, dass sie nur nach bestandenen Lasttests in den Release-Kandidaten gelangen. Das ist inzwischen auch in unserem internen Priorisierungsleitfaden DOK-PRI-02 dokumentiert."}
{"ts": "161:23", "speaker": "I", "text": "Das klingt nach einem klaren Prozess. Gibt es Risiken, dass dadurch Innovation ausgebremst wird?"}
{"ts": "161:28", "speaker": "E", "text": "Ja, das Risiko ist da. Wir versuchen gegenzusteuern, indem wir experimentelle Features in einer separaten Canary-Umgebung testen, die nicht SLA-relevant ist. So können wir Innovation und Stabilität parallel treiben."}
{"ts": "161:36", "speaker": "I", "text": "Wie wird aus Kundensicht der Erfolg gemessen, wenn diese Canary-Features später in Produktion gehen?"}
{"ts": "161:41", "speaker": "E", "text": "Wir setzen auf ein Mix aus quantitativen KPIs wie Error Rate < 0,2% und qualitativen Feedbacks aus dem Customer Advisory Board. Erst wenn beide Werte im grünen Bereich sind, erfolgt der Merge ins Hauptprodukt."}
{"ts": "161:30", "speaker": "I", "text": "Zum Abschluss unseres Gesprächs möchte ich noch auf die nächsten Meilensteine eingehen. Welche konkreten Deliverables stehen für Sie in den kommenden vier Wochen im Rahmen von Orion Edge Gateway an?"}
{"ts": "161:35", "speaker": "E", "text": "In den nächsten vier Wochen planen wir den Abschluss der API-Quota-Module, also das Rate-Limiting-Feature, das direkt aus den Lessons Learned vom RB-GW-011-Incident hervorgegangen ist. Außerdem soll die Integration in das neue Auth-Token-Format aus dem Aegis IAM Release 1.4 abgeschlossen werden."}
{"ts": "161:42", "speaker": "I", "text": "Sie sagten, das kommt aus den Lessons Learned – können Sie ein Beispiel nennen, wie genau diese Erkenntnisse in die aktuelle Planung einfließen?"}
{"ts": "161:47", "speaker": "E", "text": "Ja, beim Incident TCK-ORI-88 haben wir festgestellt, dass unser bisheriger Rate-Limiter unter Blue/Green Switches nicht sauber den State synchronisiert hat. Deshalb haben wir im Runbook RB-GW-011 einen Kapitelzusatz eingeführt, der explizit die Warm-up-Phase für Cache und Limiter beschreibt."}
{"ts": "161:54", "speaker": "I", "text": "Verstehe. Und diese Warm-up-Phase – wie messen Sie, ob sie erfolgreich war, bevor Sie den Traffic komplett umschalten?"}
{"ts": "161:59", "speaker": "E", "text": "Wir haben eine Metrik namens RL_SYNC_OK, die mindestens 95% erreichen muss, bevor der Loadbalancer die Route freigibt. Diese Metrik wird im Observability-Dashboard mit SLA-ORI-02 korreliert, um sicherzustellen, dass die p95-Latenz nicht überschritten wird."}
{"ts": "162:06", "speaker": "I", "text": "Gab es seit der Einführung dieser Metrik schon Fälle, in denen der Threshold nicht erreicht wurde?"}
{"ts": "162:10", "speaker": "E", "text": "Einmal, beim Staging-Deploy letzte Woche. Da hat das IAM-Team gleichzeitig ein Token-Parsing-Update ausgerollt, was den Limiter-Handshake um einige Sekunden verzögert hat. Wir haben daraus gelernt, Deploy-Fenster enger mit dem IAM-Team abzustimmen."}
{"ts": "162:18", "speaker": "I", "text": "Das klingt nach einer klassischen Abhängigkeitsfalle. Gibt es dafür jetzt einen formalen Prozess?"}
{"ts": "162:22", "speaker": "E", "text": "Ja, wir haben ein RFC-basiertes Change-Kalender-System eingeführt. Jede Schnittstellenänderung, die potenziell auf das Gateway wirkt, wird als RFC-Edge markiert und muss mindestens 72 Stunden vor Rollout im gemeinsamen Board freigegeben werden."}
{"ts": "162:29", "speaker": "I", "text": "Wie beeinflusst das Ihre Backlog-Priorisierung, wenn kurzfristige RFC-Edges auftauchen?"}
{"ts": "162:33", "speaker": "E", "text": "Wir halten 15% der Sprint-Kapazität als Puffer für solche Fälle frei. Das ist eine Abwägung: weniger geplante Features pro Sprint, aber deutlich geringeres Risiko von SLA-Verletzungen."}
{"ts": "162:39", "speaker": "I", "text": "Das ist dann sozusagen ein Trade-off zwischen Durchsatz und Stabilität. Wie bewerten Sie, ob sich dieser Puffer lohnt?"}
{"ts": "162:44", "speaker": "E", "text": "Wir tracken die Zahl der verhinderten Incidents im Post-Mortem-Log. Allein in Q2 haben wir drei potenzielle SLA-ORI-02-Breaches durch diesen Puffer vermeiden können. Das rechtfertigt den Verzicht auf ein bis zwei kleinere Features pro Release."}
{"ts": "162:51", "speaker": "I", "text": "Und aus Kundensicht – wie wird dieser Erfolg sichtbar?"}
{"ts": "162:55", "speaker": "E", "text": "Kundenseitig sehen wir das in den Support-Tickets: Die Zahl der Latenz-Beschwerden ist seit Einführung des Prozesses um 40% gesunken. Diese Kennzahl ist Teil unseres Customer-Success-Dashboards und wird auch im Quartalsbericht an das Management kommuniziert."}
{"ts": "163:00", "speaker": "I", "text": "Zum Thema Risikomanagement – Sie hatten vorhin kurz RB-GW-011 erwähnt. Können Sie ein Beispiel geben, wie Lessons Learned daraus konkret in den Deployment-Prozess eingeflossen sind?"}
{"ts": "163:08", "speaker": "E", "text": "Ja, klar. Aus dem Incident haben wir in unser internes Deployment-Runbook ORI-DEP-04 einen zusätzlichen Schritt aufgenommen: Vor dem Switch im Blue/Green Szenario wird jetzt ein p95-Latenz-Check über drei Minuten gefahren, um SLA-ORI-02 nochmals zu validieren."}
{"ts": "163:21", "speaker": "I", "text": "Also eine Art Pre-Production Gate?"}
{"ts": "163:24", "speaker": "E", "text": "Genau, und zusätzlich haben wir im Monitoring-Dashboard ein spezielles Widget integriert, das die Latenz gegen den Schwellenwert von 120 ms in Echtzeit visualisiert, damit das Deployment-Team sofort reagieren kann."}
{"ts": "163:36", "speaker": "I", "text": "Wie wirkt sich das auf die Bereitstellungszeit aus? Erhöht das nicht den Druck bei engen Release-Fenstern?"}
{"ts": "163:42", "speaker": "E", "text": "Ein wenig, ja. Statt 10 Minuten brauchen wir jetzt im Schnitt 13–14 Minuten pro Rollout. Aber das ist ein kalkulierter Trade-off – wir haben dadurch zwei potenzielle SLA-Verletzungen frühzeitig erkannt und abgebrochen."}
{"ts": "163:55", "speaker": "I", "text": "Sie hatten auch Aegis IAM als kritische Abhängigkeit genannt. Gab es seitdem weitere Schnittstellenänderungen, die Sie vor Herausforderungen gestellt haben?"}
{"ts": "164:02", "speaker": "E", "text": "Ja, im Ticket ORI-AEG-221 mussten wir im Build-Branch kurzfristig eine API-Version umstellen, weil das IAM-Team ein neues Tokenformat eingeführt hat. Wir haben das innerhalb von 48 Stunden gelöst, aber die Regressionstests waren dadurch zwei Tage blockiert."}
{"ts": "164:16", "speaker": "I", "text": "Wie haben Sie diese Verzögerung kompensiert, um die Meilensteine zu halten?"}
{"ts": "164:21", "speaker": "E", "text": "Wir haben ein 'Fast-Track'-Testprofil aus unserem Runbook ORI-QA-07 genutzt, das die kritischsten Endpunkte prüft. So konnten wir die API-Gateway-Builds trotz der Änderung rechtzeitig im Testsystem bereitstellen."}
{"ts": "164:34", "speaker": "I", "text": "Und das hat die Qualität nicht beeinträchtigt?"}
{"ts": "164:37", "speaker": "E", "text": "Nein, weil wir anschließend im regulären Testzyklus nachgezogen haben. Wichtig war, dass wir den Build-Flow nicht komplett stoppen mussten."}
{"ts": "164:46", "speaker": "I", "text": "Wenn Sie auf die Build-Phase schauen, wie balancieren Sie aktuell zwischen neuen Features und der Einhaltung der Performance-SLAs?"}
{"ts": "164:53", "speaker": "E", "text": "Wir priorisieren Features, die keinen negativen Einfluss auf die Latenz haben oder diese sogar verbessern. Bei riskanten Features setzen wir auf Feature Flags, um sie bei Bedarf schnell deaktivieren zu können."}
{"ts": "165:04", "speaker": "I", "text": "Das heißt, Sie nutzen auch im Livebetrieb kontrollierte Aktivierungen?"}
{"ts": "165:08", "speaker": "E", "text": "Richtig. Das ist in unserem RFC-Workflow ORI-RFC-15 festgeschrieben. So können wir bei Anzeichen von SLA-ORI-02-Risiken sofort reagieren, ohne ein komplettes Rollback fahren zu müssen."}
{"ts": "166:00", "speaker": "I", "text": "Sie hatten vorhin die Trade-offs zwischen Performance und Features erwähnt. Können Sie ein Beispiel geben, wo eine Entscheidung zu Gunsten der Latenz getroffen wurde?"}
{"ts": "166:05", "speaker": "E", "text": "Ja, im Sprint 18 haben wir die geplante JSON Schema Validation im Gateway verschoben, um optimierte Caching-Strategien einzuführen. Das hat direkt geholfen, SLA-ORI-02 p95 < 120 ms einzuhalten."}
{"ts": "166:11", "speaker": "I", "text": "Gab es dazu interne Dokumentation, auf die Sie sich stützen konnten?"}
{"ts": "166:15", "speaker": "E", "text": "Wir haben ein Decision Log im Confluence, Eintrag DL-ORI-045, der die Abwägung beschreibt: Feature Delay versus Performance-Gewinn, mit Messwerten aus unserem Prometheus Dashboard."}
{"ts": "166:22", "speaker": "I", "text": "Wie haben die Stakeholder reagiert, als Sie diese Verschiebung kommuniziert haben?"}
{"ts": "166:27", "speaker": "E", "text": "Überraschend positiv, weil wir transparent gezeigt haben, dass ohne den Schritt mehrere Endpunkte bei 150 ms lagen. Unser Runbook RB-GW-011 wurde als Referenz für den Rollout genutzt."}
{"ts": "166:34", "speaker": "I", "text": "Gibt es Risiken, dass diese verschobenen Features später Konflikte mit anderen Modulen verursachen?"}
{"ts": "166:38", "speaker": "E", "text": "Ja, zum Beispiel mit dem Schema Enforcement in Aegis IAM. Wenn wir unsere Validierungen zu spät bringen, könnten Auth-Fehler erst spät auffallen. Wir haben daher ein temporäres Monitoring-Rule-Set aktiviert."}
