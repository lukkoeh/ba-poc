{"ts": "00:00", "speaker": "I", "text": "Können Sie mir bitte einmal den aktuellen Status des Orion Edge Gateway skizzieren, damit wir ein gemeinsames Bild haben?"}
{"ts": "01:15", "speaker": "E", "text": "Ja, gerne. Wir befinden uns noch klar in der Build-Phase, etwa bei 70% der geplanten Implementierung. Die Kernfunktionen – API-Routing, mTLS-Handshake und das grundlegende Rate-Limiting – sind bereits im internen Staging-Cluster aktiv. Die Auth-Integration ist zu 60% umgesetzt, hier hängt noch die asynchrone Token-Validierung."}
{"ts": "04:20", "speaker": "I", "text": "Und welche strategischen Ziele verfolgen wir mit diesem Gateway in den nächsten 12 Monaten?"}
{"ts": "06:10", "speaker": "E", "text": "Das Hauptziel ist, allen mandantenfähigen Services einen einheitlichen Eintrittspunkt zu geben und damit sowohl Security als auch Observability zu konsolidieren. Wir wollen außerdem durch das zentrale Rate-Limiting Missbrauch vorbeugen und gleichzeitig die p95-Latenz unter 120 ms halten, wie im SLA-ORI-02 definiert."}
{"ts": "09:30", "speaker": "I", "text": "Wie fügt sich das Projekt in die Gesamtarchitektur von Novereon Systems ein?"}
{"ts": "12:00", "speaker": "E", "text": "Es wird vor dem bestehenden Service-Mesh platziert, quasi als Edge-Schicht. Dadurch können wir die Sicherheitslogik und Traffic-Shaping-Mechanismen aus den einzelnen Services herausziehen und zentralisieren. Das entlastet die Entwicklungsteams und reduziert redundante Implementierungen."}
{"ts": "16:40", "speaker": "I", "text": "Lassen Sie uns über SLA-ORI-02 sprechen. Welche Maßnahmen haben Sie implementiert, um die Latenz-Ziele konstant einzuhalten?"}
{"ts": "20:15", "speaker": "E", "text": "Wir haben mehrere Caching-Layer eingeführt, z.B. für Auth-Zertifikate, um den Handshake zu beschleunigen. Außerdem läuft ein Canary-Deployment, bei dem wir neue Filterregeln erst auf 5% des Traffics anwenden, um die Performance zu messen, bevor wir ausrollen. Und wir haben im Runbook RB-GW-011 klare Schwellenwerte für Alerting definiert: ab p95 > 100 ms wird ein Pre-Alert ausgelöst."}
{"ts": "24:50", "speaker": "I", "text": "Wie gehen Sie mit Incidents um, die das SLA verletzen könnten?"}
{"ts": "28:30", "speaker": "E", "text": "Wir haben ein dediziertes Incident-Playbook. Bei drohender Verletzung gibt es einen automatisierten Fallback auf statische Konfigurationsprofile, die z. B. komplexe Transformationsregeln deaktivieren. Parallel wird über das Incident-Ticket-System – zuletzt INC-ORI-221 – sofort eine Root-Cause-Analyse gestartet."}
{"ts": "33:00", "speaker": "I", "text": "Gibt es Abhängigkeiten zu anderen Plattformkomponenten, die das SLA beeinflussen?"}
{"ts": "37:15", "speaker": "E", "text": "Ja, insbesondere zur Auth-Service-API im ID-Cluster. Wenn deren Latenz ansteigt, wirkt sich das direkt auf das Gateway aus. Wir haben Monitoring-Hooks gesetzt, um Korrelationen festzustellen. Auch die Upstream-Datenbank-Abfragen bei komplexen Auth-Zuständen sind ein Risikofaktor."}
{"ts": "42:40", "speaker": "I", "text": "Wie ist die mTLS-Integration im Gateway technisch umgesetzt?"}
{"ts": "47:00", "speaker": "E", "text": "Wir nutzen eine Kombination aus Envoy-Proxies und einer internen PKI. Das Gateway validiert Clientzertifikate gegen den zentralen Cert-Store. Die Konfiguration ist in YAML-Templates hinterlegt und wird über unser CI/CD-System ausgerollt. Lessons Learned aus GW-4821 waren vor allem, dass das Hot-Reload von Zertifikaten zuverlässiger getestet werden muss."}
{"ts": "53:15", "speaker": "I", "text": "Wie oft haben Sie das Blue/Green Deployment Runbook RB-GW-011 tatsächlich genutzt?"}
{"ts": "90:00", "speaker": "E", "text": "Bisher dreimal in den letzten sechs Monaten, jeweils bei größeren Filter-Engine-Updates. In zwei Fällen lief es reibungslos, in einem Fall mussten wir wegen einer unerwarteten DNS-Propagation zurückrollen. Das ist auch im Runbook als Lernpunkt vermerkt."}
{"ts": "90:00", "speaker": "I", "text": "Könnten Sie bitte genauer erläutern, wie die mTLS-Integration im Gateway technisch gelöst ist, gerade im Zusammenspiel mit dem Auth-Cluster?"}
{"ts": "90:05", "speaker": "E", "text": "Ja, sicher. Wir nutzen auf der Ingress-Seite einen Envoy-basierten Proxy, der Client-Zertifikate gegen unsere interne CA prüft. Danach wird ein JWT vom Auth-Cluster angefordert, um Downstream-Services anzusprechen. Die Kette ist so konfiguriert, dass bei einem mTLS-Handshake-Timeout ein Fallback auf einen Retry-Mechanismus erfolgt, um die p95-Latenzgrenze nicht zu reißen."}
{"ts": "90:22", "speaker": "I", "text": "Und wie kam es im Ticket GW-4821 zur Abweichung?"}
{"ts": "90:27", "speaker": "E", "text": "Das war eine multi-hop Ursache: Ein Patch am Auth-Cluster hat die Zertifikatsvalidierungslatenz um ~35ms erhöht. In Kombination mit einem gleichzeitig laufenden Rate-Limiter-Test kam es dazu, dass die p95-Latenz in Peak-Zeiten knapp über 130ms stieg. Wir haben daraufhin im Runbook RB-GW-011 eine Notfallregel ergänzt, um den Rate-Limiter temporär zu drosseln."}
{"ts": "90:48", "speaker": "I", "text": "Das heißt, das Runbook wurde nach dem Incident angepasst?"}
{"ts": "90:53", "speaker": "E", "text": "Genau, wir pflegen die Runbooks in unserem internen Git-Repo, Versionierung via Tags wie 'RB-GW-011-v3'. Die Anpassung war in Commit-ID 7f2ab9 dokumentiert, inklusive einer Checkliste für das koordinierte Zurücksetzen des Limiters."}
{"ts": "91:10", "speaker": "I", "text": "Wie oft kam das Blue/Green Deployment Runbook in der Praxis zum Einsatz?"}
{"ts": "91:15", "speaker": "E", "text": "In den letzten sechs Monaten dreimal. Zweimal für geplante Releases, einmal im Incidentfall GW-4893, als wir aufgrund eines Memory-Leaks schnell zurückrollen mussten. Da gab es leichte Abweichungen vom Runbook, weil das Monitoring-Skript einen Threshold falsch interpretiert hat."}
{"ts": "91:34", "speaker": "I", "text": "Gibt es Abhängigkeiten zu externen Systemen, die das SLA derzeit kritisch beeinflussen könnten?"}
{"ts": "91:39", "speaker": "E", "text": "Ja, die größte Abhängigkeit ist das externe Geo-IP-Scoring, das wir vor dem eigentlichen Auth durchführen. Wenn dessen API langsamer wird, spüren wir das direkt in der Gateway-Latenz. Wir haben deshalb eine asynchrone Verarbeitung mit Caching eingebaut, aber das ist nur ein partieller Schutz."}
{"ts": "91:58", "speaker": "I", "text": "Wie überwachen Sie diese Abhängigkeit?"}
{"ts": "92:03", "speaker": "E", "text": "Mit einem dedizierten Synthetic Monitoring, das alle 30 Sekunden einen mTLS-Handshake plus Geo-IP-Lookup simuliert. Die Ergebnisse landen in unserem SLO-Dashboard, und wir haben Alert-Regeln, die bei Überschreiten von 100ms Geo-IP-Latenz anspringen."}
{"ts": "92:22", "speaker": "I", "text": "Gab es Lessons Learned aus diesen Monitoring-Ansätzen?"}
{"ts": "92:26", "speaker": "E", "text": "Definitiv. Eine Erkenntnis war, dass horizontales Skalieren des Proxies ohne gleichzeitiges Hochskalieren des Auth-Clusters kontraproduktiv ist. Das haben wir im Post-Mortem von GW-4821 klar festgehalten und in die Kapazitätsplanung aufgenommen."}
{"ts": "92:44", "speaker": "I", "text": "Das klingt nach einer wichtigen Vernetzung der Subsysteme."}
{"ts": "92:48", "speaker": "E", "text": "Ja, genau. Das Orion Edge Gateway ist eben kein isolierter Baustein – die Performance hängt von Auth, Rate Limiting, externen APIs und sogar unserem internen CA-Cluster ab. Diese Multi-Hop-Abhängigkeiten sind oft der Knackpunkt für SLA-ORI-02."}
{"ts": "98:00", "speaker": "I", "text": "Kommen wir jetzt zu den Risiken kurz vor der Inbetriebnahme – sehen Sie derzeit signifikante Blocker oder offene Punkte?"}
{"ts": "98:07", "speaker": "E", "text": "Ja, ein wesentlicher Punkt ist die Abhängigkeit von der externen Auth-API, die in den Peak-Zeiten schon mal >150 ms Antwortzeit hatte. Das gefährdet unser p95-Latenz-Ziel aus SLA-ORI-02."}
{"ts": "98:18", "speaker": "I", "text": "Gibt es dazu schon eine dokumentierte Risikoeinschätzung oder ist das eher eine Beobachtung aus den letzten Tests?"}
{"ts": "98:25", "speaker": "E", "text": "Beides. Wir haben im Testlog ORI-LAT-042 die Werte festgehalten, und in der Risiko-Matrix RSK-ORI-Q2/24 ist das als 'mittel-hoch' eingestuft."}
{"ts": "98:38", "speaker": "I", "text": "Wie gehen Sie mit diesem Risiko um – gibt es einen Runbook-Ansatz oder Workarounds?"}
{"ts": "98:45", "speaker": "E", "text": "Im Runbook RB-GW-014 ist ein Fallback-Mechanismus beschrieben: temporäres Caching der Auth-Tokens für bis zu 90 Sekunden, um die Antwortzeiten der Auth-API zu puffern."}
{"ts": "98:57", "speaker": "I", "text": "Aber das ist sicher ein Trade-off bei der Security, oder?"}
{"ts": "99:02", "speaker": "E", "text": "Genau, wir akzeptieren ein minimal erhöhtes Risiko bei Revocation-Events. Die Entscheidung wurde im Architekturrat Protokoll ARC-23-04-15 dokumentiert."}
{"ts": "99:14", "speaker": "I", "text": "Gab es weitere solche bewussten Trade-offs in der Architektur?"}
{"ts": "99:20", "speaker": "E", "text": "Ja, beim Rate Limiting setzen wir auf ein verteiltes Token-Bucket-Modell über Redis-Cluster. Das ist günstiger in der Latenz als ein zentraler Service, aber bei Cluster-Partitionen gibt’s Inkonsistenzen."}
{"ts": "99:33", "speaker": "I", "text": "Wurde das schon mal im Live-Betrieb simuliert?"}
{"ts": "99:39", "speaker": "E", "text": "Im Staging-Drill DR-ORI-07 haben wir eine Partition von 3 Minuten provoziert. Ergebnis: leichte Overruns bei Requests/min, aber SLA blieb knapp eingehalten."}
{"ts": "99:52", "speaker": "I", "text": "Welche evidenzbasierten Entscheidungen stehen für die nächsten drei Monate an?"}
{"ts": "99:58", "speaker": "E", "text": "Vor allem, ob wir mTLS-Handshakes asynchron vorberechnen (Pre-Warm) – das könnte 10 ms sparen, aber erfordert tiefere Eingriffe ins Gateway-Framework (RFC-ORI-09 in Review)."}
{"ts": "100:12", "speaker": "I", "text": "Und wie priorisieren Sie das im Vergleich zu anderen Backlog-Items?"}
{"ts": "100:20", "speaker": "E", "text": "Wir nutzen ein gewichtetes Scoring nach Incident-Wahrscheinlichkeit und SLA-Impact. Das Pre-Warm hat Score 8/10, während z. B. UI-Verbesserungen im Admin-Panel bei 3/10 liegen."}
{"ts": "106:00", "speaker": "I", "text": "Lassen Sie uns jetzt auf die Risiken für die Inbetriebnahme eingehen – welche sehen Sie derzeit als kritisch an?"}
{"ts": "106:10", "speaker": "E", "text": "Der größte Risikofaktor ist aktuell die Abhängigkeit vom externen Auth-Service 'NovaAuth', der bei Lastspitzen leicht über die 200 ms Antwortzeit geht und damit unser SLA-ORI-02 tangiert. Zudem ist die Testabdeckung für die Fallback-Logik laut Testreport TR-GW-19 nur bei 72 %, was uns bei einem Ausfall verwundbar macht."}
{"ts": "106:38", "speaker": "I", "text": "Gab es bewusst eingegangene Trade-offs bei der Architektur, um schneller in den Build zu kommen?"}
{"ts": "106:46", "speaker": "E", "text": "Ja, wir haben uns entschieden, das Rate Limiting zunächst rein in-memory im Gateway-Knoten zu fahren, ohne verteilte State-Synchronisation. Vorteil: geringere Latenz; Nachteil: bei Knoten-Neustarts verlieren wir den Zählerstand, was laut RFC-ORI-07 als akzeptiertes Risiko dokumentiert ist."}
{"ts": "107:11", "speaker": "I", "text": "Wie wird diese Entscheidung intern abgesichert?"}
{"ts": "107:17", "speaker": "E", "text": "Wir verweisen im Architekturentscheidungs-Record ADR-042 auf die Messungen aus Lasttest LT-GW-05, die zeigen, dass die p95 Latenz um 18 ms besser ist als mit Redis-basiertem State-Sharing. Zudem ist ein Epic EP-GW-112 zur Evaluierung von Redis-Cluster in Q4 geplant."}
{"ts": "107:43", "speaker": "I", "text": "Gibt es evidenzbasierte Entscheidungen, die in den nächsten drei Monaten anstehen?"}
{"ts": "107:49", "speaker": "E", "text": "Ja, die Frage, ob wir mTLS-Offloading in den Envoy-Proxies statt im Gateway durchführen. Wir sammeln dazu aktuell Metriken über den CPU-Impact (siehe Monitoring-Dashboard MD-GW-SEC) und werden auf Basis der Daten Ende August eine Entscheidung treffen."}
{"ts": "108:12", "speaker": "I", "text": "Welche Belege liegen für das mTLS-Offloading-Thema bisher vor?"}
{"ts": "108:17", "speaker": "E", "text": "In den letzten synthetischen Benchmarks (BM-SEC-08) sank die Gateway-CPU-Auslastung von 68 % auf 52 %, wenn mTLS im Envoy terminiert wurde. Allerdings stieg die Latenz zum internen Auth-Service um 7 ms – das müssen wir gegenrechnen."}
{"ts": "108:42", "speaker": "I", "text": "Wie gehen Sie mit der Mehrlatenz um, falls Sie sich fürs Offloading entscheiden?"}
{"ts": "108:49", "speaker": "E", "text": "Wir würden in diesem Fall die Keep-Alive-Zeiten zu NovaAuth erhöhen und im Runbook RB-GW-015 'Auth Connection Pooling' anpassen. Außerdem würden wir in der Canary-Phase enges Latenz-Monitoring mit Alert-Thresholds bei 110 ms p95 aktivieren."}
{"ts": "109:15", "speaker": "I", "text": "Sind die Runbooks allgemein auf solche Architekturänderungen vorbereitet?"}
{"ts": "109:22", "speaker": "E", "text": "Teilweise. RB-GW-011 für Blue/Green beschreibt zwar Rollbacks, aber Offloading-Szenarien sind noch nicht drin. Wir haben einen Pflegeprozess via Git-Repo 'gw-runbooks', Versionierung über Tags, und Change-Proposals laufen über das interne RFC-Board."}
{"ts": "109:47", "speaker": "I", "text": "Wie wird sichergestellt, dass Lessons Learned aus Incidents einfließen?"}
{"ts": "109:53", "speaker": "E", "text": "Nach jedem Incident-Review wird ein 'Runbook Delta'-Ticket erstellt, z. B. RB-DELTA-2024-06 nach Incident INC-GW-4412. Dieses muss innerhalb von zwei Sprints umgesetzt werden, sonst eskaliert es im Ops-Steering-Meeting."}
{"ts": "114:00", "speaker": "I", "text": "Lassen Sie uns bitte auf die identifizierten Risiken für die Inbetriebnahme eingehen – was steht ganz oben auf Ihrer Liste?"}
{"ts": "114:05", "speaker": "E", "text": "Aktuell sehe ich zwei Hauptpunkte: Erstens eine potenzielle Überlastung der Rate Limiter unter Peak Load, speziell wenn externe Auth-Systeme verzögert antworten. Zweitens, die noch nicht vollständig automatisierte Recovery bei mTLS-Handshake-Fehlern."}
{"ts": "114:14", "speaker": "I", "text": "Und wie quantifizieren wir diese Risiken? Gibt es Metriken oder historische Daten?"}
{"ts": "114:18", "speaker": "E", "text": "Ja, wir stützen uns auf die Logs aus unserem Synthetic Load Test vom 05.04., Run-ID LT-ORI-043. Dort haben wir bei 15% der Requests in Peak-Phase Latenzen > 140ms gesehen, was klar SLA-ORI-02 verletzt hätte."}
{"ts": "114:28", "speaker": "I", "text": "Gab es bewusst eingegangene Trade-offs in der Architektur, um kurzfristig voranzukommen?"}
{"ts": "114:32", "speaker": "E", "text": "Ja, wir haben entschieden, die globale Rate Limit Policy zunächst statisch zu definieren, um Implementierungsaufwand zu sparen. Das bedeutet aber, dass adaptive Limits noch fehlen, was sich in dynamischen Lastszenarien negativ auswirken kann."}
{"ts": "114:41", "speaker": "I", "text": "Okay. Gibt es Belege oder Tickets, die diese Entscheidung dokumentieren?"}
{"ts": "114:45", "speaker": "E", "text": "Das finden Sie in RFC-DOC-ORI-15, genehmigt am 22.03., sowie im Ticket ARC-4792, wo die Diskussion zu adaptiven Limits explizit verschoben wurde."}
{"ts": "114:54", "speaker": "I", "text": "Welche evidenzbasierten Entscheidungen stehen dann in den nächsten drei Monaten an?"}
{"ts": "114:58", "speaker": "E", "text": "Wir müssen entscheiden, ob wir vor Go-Live doch noch eine minimale adaptive Rate Limitierung implementieren. Die Entscheidung soll laut Roadmap bis Sprint 28 fallen, basierend auf den Messwerten aus dem aktuellen Canary-Testlauf."}
{"ts": "115:08", "speaker": "I", "text": "Hat dieser Canary-Testlauf schon erste Ergebnisse geliefert?"}
{"ts": "115:12", "speaker": "E", "text": "Vorläufig ja – die p95-Latenz liegt aktuell bei 112ms, aber in 2% der Zeitfenster sehen wir Ausreißer bis 180ms, meist korrelierend mit Auth-Backend-Zeitouts. Wir analysieren das zusammen mit dem Team von SecureAuthX."}
{"ts": "115:22", "speaker": "I", "text": "Welche Maßnahmen hätten wir laut Runbook RB-GW-011 ergreifen sollen, um diese Ausreißer zu vermeiden?"}
{"ts": "115:27", "speaker": "E", "text": "Das Runbook empfiehlt in Step 4 eine temporäre Erhöhung der Auth-Timeouts sowie ein Umschalten auf Blue-Cluster, falls mehr als 5% Error Rate auftreten. Im Incident letzte Woche haben wir das aber nicht gemacht, weil die Error Rate knapp darunter lag."}
{"ts": "115:38", "speaker": "I", "text": "Klingt nach einer Grauzone, wo Erfahrung wichtiger war als die feste Schwelle."}
{"ts": "115:42", "speaker": "E", "text": "Genau, das ist so ein Fall, wo die ungeschriebenen Heuristiken greifen: Wenn das Muster auf bekannte Backend-Lags hinweist, warten wir oft lieber ein paar Minuten, bevor wir den Cluster switchen, um unnötige Umschaltungen zu vermeiden."}
{"ts": "116:00", "speaker": "I", "text": "Lassen Sie uns noch einmal konkret auf die geplanten Lasttests vor der Inbetriebnahme eingehen. Wie realistisch sind die Szenarien im Vergleich zur tatsächlichen Produktionslast?"}
{"ts": "116:08", "speaker": "E", "text": "Wir haben die Lasttests so konzipiert, dass sie das 1,2‑fache der erwarteten Peak‑Load simulieren. Die Szenarien basieren auf den letzten 18 Monaten Traffic‑Analysen von vergleichbaren Gateways im Portfolio. Producer‑ und Consumer‑Patterns wurden aus den Log‑Aggregationen übernommen."}
{"ts": "116:21", "speaker": "I", "text": "Gibt es dafür eine dokumentierte Methodik oder ist das eher Erfahrungswissen?"}
{"ts": "116:26", "speaker": "E", "text": "Teilweise beides. Wir haben ein internes Dokument DOC‑PERF‑021, das die Methodik beschreibt, aber die Feinjustierung der Testprofile kommt aus dem Erfahrungswissen der SRE‑Teams. Das ist so eine ungeschriebene Regel: wir addieren immer einen Sicherheitsfaktor von mindestens 15 %, um unerwartete Peaks abzufangen."}
{"ts": "116:40", "speaker": "I", "text": "Wie gehen Sie im Test mit den mTLS‑Handshakes um, da diese ja zusätzliche Latenz verursachen können?"}
{"ts": "116:46", "speaker": "E", "text": "Wir simulieren die Handshakes mit echten Zertifikaten aus der Staging‑CA, um die CPU‑Last und Latenz realistisch abzubilden. Das war eine Lehre aus GW‑4821, dort hatten wir initial nur synthetische Handshakes und die Produktionslatenzen lagen dann 8 ms höher als geplant."}
{"ts": "116:59", "speaker": "I", "text": "Gab es schon Abnahmen oder Reviews dieser Testszenarien?"}
{"ts": "117:04", "speaker": "E", "text": "Ja, wir hatten vor zwei Wochen ein Review mit dem Architekturboard. Protokoll AB‑MIN‑079. Dort wurde vor allem die Abhängigkeit zum zentralen Auth‑Service thematisiert, weil dessen p95 aktuell bei 95 ms liegt und damit das SLA‑ORI‑02 schnell in Gefahr bringen kann."}
{"ts": "117:18", "speaker": "I", "text": "Das heißt, das Risiko liegt nicht nur im Gateway selbst, sondern auch in Upstream‑Services."}
{"ts": "117:23", "speaker": "E", "text": "Genau. Deshalb gibt es die Empfehlung, eine lokale Token‑Cache‑Schicht im Gateway zu aktivieren. Das ist ein Trade‑off zwischen Speicherverbrauch und Latenzstabilität, der noch final entschieden werden muss."}
{"ts": "117:35", "speaker": "I", "text": "Wie schnell könnte diese Cache‑Schicht implementiert werden, falls wir uns dafür entscheiden?"}
{"ts": "117:40", "speaker": "E", "text": "Der Code ist prototypisch in Branch feat/token‑cache vorhanden. Mit QA‑Tests und Runbook‑Anpassung RB‑GW‑015 wären das rund zwei Wochen Aufwand. Wichtig: Es müsste auch ein RFC erstellt werden, RFC‑GW‑073, weil es das AuthN‑Verhalten verändert."}
{"ts": "117:54", "speaker": "I", "text": "Würde das auch Einfluss auf bestehende Betriebsprozesse haben?"}
{"ts": "117:59", "speaker": "E", "text": "Ja, insbesondere auf das Incident‑Playbook für Auth‑Ausfälle. Der Fallback‑Pfad wäre dann anders zu dokumentieren, damit bei einem Ausfall des zentralen Auth‑Systems die Caches kontrolliert invalidiert werden. Sonst riskieren wir veraltete Token im System."}
{"ts": "118:12", "speaker": "I", "text": "Abschließend: Gibt es eine evidenzbasierte Empfehlung aus den bisherigen Tests, ob wir den Token‑Cache aktivieren sollten?"}
{"ts": "118:18", "speaker": "E", "text": "Die Messungen aus PERF‑RUN‑042 zeigen eine Reduktion der p95‑Latenz um 22 ms unter Peak‑Load mit aktiviertem Cache. Angesichts der aktuellen Auth‑Service‑Latenzen spricht vieles dafür, den Cache zu aktivieren, sofern das Security‑Team die Risiken als akzeptabel einstuft."}
{"ts": "120:00", "speaker": "I", "text": "Lassen Sie uns noch einmal auf die Lessons Learned aus GW-4821 eingehen – wie hat sich das konkret auf die Auth-Schnittstellen ausgewirkt?"}
{"ts": "120:15", "speaker": "E", "text": "Ja, also GW-4821 hat uns gezeigt, dass unser mTLS Handshake bei hoher Last über 85% CPU im Auth-Cluster erzeugt. Wir haben daraufhin in Revision 3.2 des Auth-Adapters ein Session Caching eingeführt, um das SLA-ORI-02 nicht zu reißen."}
{"ts": "120:38", "speaker": "I", "text": "Und das Caching – ist das im Runbook RB-GW-011 bereits dokumentiert, oder ist das noch ‚tribal knowledge‘?"}
{"ts": "120:50", "speaker": "E", "text": "Ähm, aktuell ist es nur im internen Confluence-Artikel TECH-AUTH-57 festgehalten. Wir haben den Change zwar im Deployment-Abschnitt ergänzt, aber das offizielle Runbook-Update ist noch offen, Ticket DOC-221."}
{"ts": "121:15", "speaker": "I", "text": "Verstanden. Gab es nach der Einführung des Session Cachings messbare Verbesserungen bei der p95 Latenz?"}
{"ts": "121:25", "speaker": "E", "text": "Ja, die p95 ist von 138ms auf im Schnitt 112ms gefallen, gemessen über 14 Tage via unserem Prometheus-SLA-Dashboard. Das hat uns wieder in den compliant Bereich zum SLA gebracht."}
{"ts": "121:44", "speaker": "I", "text": "Gibt es Abhängigkeiten, die diese Verbesserung noch gefährden könnten?"}
{"ts": "121:55", "speaker": "E", "text": "Nun, wenn der Token-Validator im externen IAM-System verzögert antwortet, dann nützt uns das Caching nur bedingt. Wir haben da eine Watchdog-Probe implementiert, die bei >500ms Response den Traffic auf einen Fallback-Validator umleitet."}
{"ts": "122:20", "speaker": "I", "text": "Wie wird dieser Fallback getestet? Im Live-Betrieb oder in einer Staging-Umgebung?"}
{"ts": "122:33", "speaker": "E", "text": "Wir testen den Failover alle zwei Wochen in der Staging-Umgebung mit synthetischen Delays. Das ist im Runbook RB-GW-014, Abschnitt 5.3, beschrieben."}
{"ts": "122:55", "speaker": "I", "text": "Okay, das klingt solide. Kommen wir zu Risiken: Gibt es technische Schulden, die wir vor Go-Live adressieren sollten?"}
{"ts": "123:07", "speaker": "E", "text": "Definitiv. Die Rate Limiting Engine basiert noch auf einer älteren Redis-Cluster-Version ohne Sentinel. Das birgt das Risiko von längeren Ausfällen bei Leader-Failover – wir haben das als Risk ID R-ORI-09 erfasst."}
{"ts": "123:28", "speaker": "I", "text": "Und ein Upgrade wäre vor Launch noch machbar?"}
{"ts": "123:39", "speaker": "E", "text": "Zeitlich eng. Ein Redis-Upgrade würde mindestens zwei volle Sprints kosten, plus Regressionstests. Wir müssten dann andere Features verschieben."}
{"ts": "124:00", "speaker": "I", "text": "Dann ist das ein klassischer Trade-off zwischen Stabilität und Feature-Umfang."}
{"ts": "124:10", "speaker": "E", "text": "Genau. Meine Empfehlung, basierend auf den letzten Chaos-Tests CT-ORI-05, wäre: Upgrade jetzt einplanen, um spätere SLA-Verletzungen und Incident-Kosten zu vermeiden."}
{"ts": "129:00", "speaker": "I", "text": "Könnten Sie bitte noch einmal auf die Abhängigkeit zur Auth-Service-Cluster-Revision eingehen, speziell im Hinblick auf die Stabilität des Gateways?"}
{"ts": "129:05", "speaker": "E", "text": "Ja, klar. Wir haben aktuell eine enge Kopplung zum Auth-Service in Revision 3.2. Diese nutzt einen eigenen Redis-Cluster für Token-Caching, und wenn dort Latenzspitzen auftreten, sehen wir sie unmittelbar im Gateway. In dem Sinne ist es ein klassischer single point of degradation."}
{"ts": "129:18", "speaker": "I", "text": "Und das beeinflusst dann direkt unsere p95-Latenz unter SLA-ORI-02?"}
{"ts": "129:22", "speaker": "E", "text": "Genau. Wir haben in der letzten Messwoche dreimal 140 ms p95 gesehen, laut Monitoring-Report MR-ORI-2024-05. Das waren alles Zeitfenster, in denen der Auth-Cluster Rebalancing gemacht hat."}
{"ts": "129:34", "speaker": "I", "text": "Haben wir dazu ein Runbook oder eine temporäre Mitigation?"}
{"ts": "129:38", "speaker": "E", "text": "Wir haben im RB-AUTH-007 eine Anweisung, wie man den Cache-Warmup manuell triggert. Das ist allerdings nicht im RB-GW-011 verlinkt, was wir als Lücke identifiziert haben."}
{"ts": "129:50", "speaker": "I", "text": "Verstehe. Das könnte in einem Review der Runbooks zusammengeführt werden. Apropos, wie oft mussten wir in den letzten Quartalen Blue/Green im Gateway wirklich nutzen?"}
{"ts": "129:57", "speaker": "E", "text": "Seit Q3 2023 nur zweimal produktiv. Einmal bei der Einführung der neuen Rate-Limiter-Engine (GW-4750) und einmal bei einem Patch für die mTLS-Library, der in PROD critical wurde."}
{"ts": "130:09", "speaker": "I", "text": "Gab es Unterschiede in der realen Durchführung im Vergleich zum Runbook RB-GW-011?"}
{"ts": "130:13", "speaker": "E", "text": "Ja, im zweiten Fall mussten wir den Traffic-Switch manuell verzögern, weil ein Downstream-System noch nicht ready war. Das steht so nicht in RB-GW-011, sondern in einer Slack-Notiz vom 14.02.2024."}
{"ts": "130:27", "speaker": "I", "text": "Das ist riskant, wenn diese Info nicht formalisiert wird. Gibt es Pläne dafür?"}
{"ts": "130:31", "speaker": "E", "text": "Ja, wir haben im RFC-GW-2024-06 aufgenommen, dass Runbooks mit Lessons Learned aus Incidents ergänzt werden müssen, inklusive Abhängigkeiten zu externen Services."}
{"ts": "130:43", "speaker": "I", "text": "Gut. Wenn wir auf die Inbetriebnahme schauen – welches ist aus Ihrer Sicht das größte Risiko?"}
{"ts": "130:48", "speaker": "E", "text": "Größtes Risiko ist derzeit die fehlende Lasttest-Abdeckung für kombinierte Auth- und Rate-Limiting-Spikes. Wir haben dafür noch keine vollständigen Szenarien in LT-ORI-04 definiert."}
{"ts": "131:00", "speaker": "I", "text": "Und welche Entscheidung müssen wir evidenzbasiert kurzfristig treffen?"}
{"ts": "131:05", "speaker": "E", "text": "Ob wir die Inbetriebnahme um zwei Wochen schieben, um den kombinierten Lasttest durchzuführen. Die Daten aus den letzten Incidents, inkl. GW-4821 und MR-ORI-2024-05, sprechen dafür, lieber zu testen, bevor wir live gehen."}
{"ts": "131:00", "speaker": "I", "text": "Lassen Sie uns nochmal konkret auf die mTLS-Integration zurückkommen. Gab es nach den Erkenntnissen aus GW-4821 Änderungen im Zertifikats-Rollover-Prozess?"}
{"ts": "131:15", "speaker": "E", "text": "Ja, wir haben den Prozess angepasst. Vorher hatten wir nur ein monatliches Rollover-Fenster, jetzt prüfen wir alle 14 Tage die Zertifikatsgültigkeit via automatisiertem Script. Das ist in Runbook RB-GW-014 dokumentiert."}
{"ts": "131:38", "speaker": "I", "text": "Und wie wirkt sich das auf die SLA-Überwachung, speziell p95 Latenz unter 120ms, aus?"}
{"ts": "131:53", "speaker": "E", "text": "Indirekt positiv. Durch die stabilere mTLS-Verbindung vermeiden wir die sporadischen Handshake-Retries, die im Incident von GW-4821 teilweise für Peaks bis 300ms verantwortlich waren."}
{"ts": "132:15", "speaker": "I", "text": "Gab es Abhängigkeiten, die Sie dabei berücksichtigen mussten, etwa zu den Auth-Systemen?"}
{"ts": "132:27", "speaker": "E", "text": "Genau, die Auth-Systeme AURUS-ID und das interne Token-Service hängen stark an der TLS-Termination. Wir haben einen Zwischentest in der Staging-Umgebung gefahren, um sicherzugehen, dass die neuen Zertifikate von beiden Systemen akzeptiert werden."}
{"ts": "132:50", "speaker": "I", "text": "Wie haben Sie diese Tests orchestriert?"}
{"ts": "133:02", "speaker": "E", "text": "Mit unserem CI/CD-Job 'gateway-cert-validate', der per API beide Systeme anspricht und den mTLS-Handshake simuliert. Das Ergebnis wird gegen die Runbook-Kriterien geprüft."}
{"ts": "133:25", "speaker": "I", "text": "Können Sie ein Beispiel für eine Abweichung von RB-GW-011 nennen, die nachträglich dokumentiert wurde?"}
{"ts": "133:39", "speaker": "E", "text": "Beim letzten Blue/Green-Deployment mussten wir wegen eines unvollständig synchronisierten Rate-Limiting-Clusters auf das Green-Target um 15 Minuten verzögert umschalten. Das Runbook geht von nahtloser Umschaltung aus, was hier nicht möglich war."}
{"ts": "134:05", "speaker": "I", "text": "Wie wird so ein Sonderfall dann festgehalten?"}
{"ts": "134:15", "speaker": "E", "text": "Wir haben in Confluence eine Rubrik 'Runbook Deviations', wo solche Fälle verlinkt mit dem zugehörigen Incident-Ticket eingetragen werden. In diesem Fall war das ORI-INC-233."}
{"ts": "134:38", "speaker": "I", "text": "Gibt es daraus abgeleitete Entscheidungen für die nächsten drei Monate?"}
{"ts": "134:49", "speaker": "E", "text": "Ja, wir planen ein Pre-Switch Health Check für alle abhängigen Cluster, bevor das Blue/Green-Skript durchläuft. Das soll Teil von RB-GW-011v3 werden."}
{"ts": "135:10", "speaker": "I", "text": "Sehen Sie dabei Risiken, etwa zusätzliche Latenz im Umschaltprozess?"}
{"ts": "135:20", "speaker": "E", "text": "Ein kleines Risiko, ja. Wenn der Health Check fehlschlägt, verzögert sich das Umschalten. Aber das ist ein bewusster Trade-off zugunsten der Stabilität, um SLA-ORI-02-Verletzungen zu vermeiden."}
{"ts": "139:00", "speaker": "I", "text": "Lassen Sie uns beim Thema Betriebsprozesse bleiben – wie oft wurde das Blue/Green Deployment Runbook in den letzten drei Monaten tatsächlich befolgt?"}
{"ts": "139:04", "speaker": "E", "text": "In dieser Zeit exakt vier Mal. Zwei Einsätze waren planmäßige Upgrades, die anderen zwei waren Hotfix-Deployments. Wir haben dabei laut RB-GW-011 alle Schritte eingehalten, bis auf die Wartezeit zwischen Traffic-Switch und Post-Deployment-Checks, die wir verkürzt haben."}
{"ts": "139:10", "speaker": "I", "text": "Gab es dafür einen bestimmten Grund, dass die Wartezeit reduziert wurde?"}
{"ts": "139:14", "speaker": "E", "text": "Ja, aufgrund eines dringenden Incident-Windows. Ticket INC-GW-5293 beschreibt das – wir mussten schneller zurück auf 100 % Traffic, um eine drohende SLA-ORI-02 Verletzung zu vermeiden."}
{"ts": "139:20", "speaker": "I", "text": "Verstehe. Wie wird denn sichergestellt, dass solche Abweichungen dokumentiert und in künftige Versionen des Runbooks einfließen?"}
{"ts": "139:24", "speaker": "E", "text": "Wir führen ein internes Changelog für alle Runbooks im Confluence-Workspace und verlinken die relevanten Tickets. RB-GW-011 v1.4 enthält bereits einen optionalen Pfad für verkürzte Wartezeiten in Ausnahmefällen."}
{"ts": "139:30", "speaker": "I", "text": "Gab es Fälle, wo dieser verkürzte Pfad negative Auswirkungen hatte?"}
{"ts": "139:34", "speaker": "E", "text": "Einmal, ja. Bei einem Blue-to-Green-Switch haben wir eine mTLS-Zertifikatsrotation zu spät bemerkt, weil der Post-Deployment-Check zu früh kam. Das erzeugte kurzfristig 401-Fehler auf Partner-APIs."}
{"ts": "139:40", "speaker": "I", "text": "Wie wurde das Problem behoben?"}
{"ts": "139:44", "speaker": "E", "text": "Wir haben sofort den mTLS-Handshake neu initialisiert und ein Grace-Period-Skript aus RB-GW-014 angewendet. Danach normalisierte sich die p95-Latenz wieder unter 120 ms."}
{"ts": "139:50", "speaker": "I", "text": "Interessant. Würden Sie sagen, dass RB-GW-011 in der Praxis robust genug ist, oder braucht es eine grundlegende Überarbeitung?"}
{"ts": "139:54", "speaker": "E", "text": "Grundsätzlich robust, aber wir überlegen, spezifische Checks für Abhängigkeiten wie das zentrale Auth-System einzubauen. Dort gab es laut GW-4821 Verzögerungen im Token-Issuing, die im Runbook aktuell nicht berücksichtigt sind."}
{"ts": "140:00", "speaker": "I", "text": "Also ein engeres Zusammenspiel zwischen Deployment-Checks und Auth-Komponente?"}
{"ts": "140:04", "speaker": "E", "text": "Genau. Das wäre ein Multi-Step-Check: erst Traffic-Switch, dann mTLS-Handshake prüfen, dann einen Test-Token anfordern und dessen Antwortzeit loggen."}
{"ts": "140:10", "speaker": "I", "text": "Und wann könnten diese Anpassungen produktiv gehen?"}
{"ts": "140:14", "speaker": "E", "text": "Wenn wir im nächsten Sprint (SP-27) Kapazität haben, könnte die aktualisierte Runbook-Version in KW 42 live sein, mit begleitendem Training für das Ops-Team."}
{"ts": "141:00", "speaker": "I", "text": "Kommen wir nun zu den konkreten Betriebsprozessen – wie laufen aktuell die Deployments für das Orion Edge Gateway im Build-Phase-Betrieb ab?"}
{"ts": "141:04", "speaker": "E", "text": "Der Standard ist aktuell ein wöchentliches Blue/Green Deployment, wie es in RB-GW-011 beschrieben ist. Allerdings haben wir in den letzten drei Zyklen ein adaptives Canary-Rollout eingefügt, um mTLS-Handshake-Fehler früh zu erkennen."}
{"ts": "141:10", "speaker": "I", "text": "Gab es in diesen Canary-Rollouts konkrete Findings, die zur Optimierung geführt haben?"}
{"ts": "141:14", "speaker": "E", "text": "Ja, bei Build 1.8.12 haben wir festgestellt, dass der Authenticator-Service bei etwa 3 % der Sessions ein Timeout >120ms hatte. Wir haben daraufhin das mTLS-Zertifikat-Cache-Intervall von 15 auf 45 Minuten erhöht, das hat die SLA-ORI-02-Compliance um 2 Prozentpunkte verbessert."}
{"ts": "141:22", "speaker": "I", "text": "Und wie wird diese Art von Anpassung dokumentiert? Steht das sofort im Runbook?"}
{"ts": "141:25", "speaker": "E", "text": "Nicht sofort. Wir führen zunächst eine Änderungsnotiz im internen RFC-Board ein – z.B. RFC-ORI-224 – und wenn die Änderung über zwei Releasezyklen stabil ist, pflegen wir sie in RB-GW-011 ein."}
{"ts": "141:32", "speaker": "I", "text": "Sie hatten vorhin Abweichungen vom Runbook erwähnt. Können Sie ein Beispiel nennen, wo bewusst davon abgewichen wurde?"}
{"ts": "141:36", "speaker": "E", "text": "Beim Incident INC-7754 aus Ticket GW-4821 sind wir direkt auf Green geswitched, obwohl das Runbook eine Canary-Phase von 30 Minuten vorsieht. Grund: Auth-Subsystem war komplett blockiert, jede Sekunde Verzögerung hätte Latenz-SLA-Verletzungen vergrößert."}
{"ts": "141:44", "speaker": "I", "text": "Verstehe. Wie stellen Sie sicher, dass solche Entscheidungen nicht zum neuen Standard werden, wenn sie eigentlich nur ein Notfall-Muster sind?"}
{"ts": "141:48", "speaker": "E", "text": "Wir markieren sie im Incident-Post-Mortem als 'Deviation', versehen sie mit einer Begründung und Review durch den Change Advisory Board. Ohne CAB-Bestätigung fließen sie nicht ins Regelwerk ein."}
{"ts": "141:55", "speaker": "I", "text": "Gibt es aktuelle Optimierungen in der Pipeline, die den Deployment-Fluss verbessern könnten?"}
{"ts": "141:59", "speaker": "E", "text": "Ja, wir testen gerade ein Pre-Warm-Skript für die mTLS-Session-Pools, damit die ersten Requests nach dem Switch keine kalten Handshakes haben. Testlauf im Staging hat die p95 Latenz von 118ms auf 104ms gedrückt."}
{"ts": "142:06", "speaker": "I", "text": "Wie wirkt sich das auf die Abhängigkeiten zu externen Systemen aus, gerade in Bezug auf Auth?"}
{"ts": "142:10", "speaker": "E", "text": "Das Pre-Warming erhöht kurzfristig die Last auf den Certificate Authority Endpoint, deshalb wird es nur in Wartungsfenstern getriggert. Wir nutzen Monitoring-Alerts aus dem CA-Subsystem, um den optimalen Zeitpunkt zu finden."}
{"ts": "142:16", "speaker": "I", "text": "Sehen Sie hier Risiken, die wir vor Go-Live noch adressieren müssen?"}
{"ts": "142:20", "speaker": "E", "text": "Ja, wenn die CA-Latenz unerwartet steigt, könnte das Pre-Warming kontraproduktiv werden. Wir planen daher eine Fallback-Logik, die bei CA-Response >80ms automatisch auf das alte Warmup-Schema zurückschaltet."}
{"ts": "142:36", "speaker": "I", "text": "Können Sie mir bitte den aktuellen Stand der Deployment-Pipeline für das Orion Edge Gateway schildern, speziell in Bezug auf die Build-Phase und die Übergabe an die Betriebsumgebung?"}
{"ts": "142:41", "speaker": "E", "text": "Ja, aktuell nutzen wir eine dreistufige Pipeline: Build in der isolierten CI-Umgebung, dann Staging mit simulierten Auth-Subsystemen und schließlich Production. Seit Build 0.9.14 haben wir auch die automatisierte mTLS-Zertifikatsprüfung direkt im Staging-Lauf verankert."}
{"ts": "142:49", "speaker": "I", "text": "Wie stellen Sie sicher, dass die mTLS-Integration im Auth-Subsystem konsistent bleibt, wenn wir Änderungen am Gateway-Core ausrollen?"}
{"ts": "142:54", "speaker": "E", "text": "Wir haben in Runbook RB-GW-011 einen zusätzlichen Validierungsschritt ergänzt, der vor dem Umschalten im Blue/Green prüft, ob die Auth-Handshake-Zeit unter 20ms bleibt. Das ist nicht im ursprünglichen Runbook von Februar enthalten, aber nach Incident GW-4821 aufgenommen."}
{"ts": "143:02", "speaker": "I", "text": "Incident GW-4821 war der mit der unerwarteten Latenzerhöhung, korrekt?"}
{"ts": "143:06", "speaker": "E", "text": "Genau, damals stieg die p95-Latenz auf 240ms, weil ein Zertifikats-Revocation-Check in der Auth-Kette hing. Wir mussten den Check asynchronisieren und seitdem überwachen wir diese Metrik separat im SLA-Dashboard SLA-ORI-02."}
{"ts": "143:15", "speaker": "I", "text": "Gibt es für SLA-ORI-02 automatisierte Alerts oder manuelle Checks?"}
{"ts": "143:19", "speaker": "E", "text": "Beides. Wir haben automatisierte Alerts bei Überschreiten von 110ms p95-Latenz und quartalsweise manuelle Reviews der Latency-Trends. Letzte Woche haben wir z.B. einen Spike im Zusammenhang mit einem Auth-Subsystem-Failover erkannt und proaktiv ein Failover-Tuning eingespielt."}
{"ts": "143:28", "speaker": "I", "text": "Wie gehen Sie bei einem Deployment konkret vor, um diese Risiken zu minimieren?"}
{"ts": "143:33", "speaker": "E", "text": "Wir fahren zuerst ein Blue-Deployment, testen mit synthetischen Requests aus drei Regionen, prüfen mTLS-Handshake und Rate Limiting. Erst wenn alle Checks grün sind, schalten wir den Traffic auf Green. Bei Abweichungen greifen wir auf die Rollback-Prozedur aus RB-GW-011 zurück."}
{"ts": "143:42", "speaker": "I", "text": "Gab es in letzter Zeit Abweichungen zwischen Runbook und tatsächlichem Vorgehen?"}
{"ts": "143:46", "speaker": "E", "text": "Ja, im April mussten wir beim Green-Switch einen manuellen Patch einspielen, der nicht im Runbook stand, weil eine neue Auth-API einen anderen Zertifikatspfad nutzte. Das haben wir danach in RB-GW-011 Version 1.7 ergänzt."}
{"ts": "143:54", "speaker": "I", "text": "Sind diese Runbook-Versionierungen allen beteiligten Teams transparent zugänglich?"}
{"ts": "143:58", "speaker": "E", "text": "Ja, wir pflegen sie im internen Git-basierten Ops-Repo, mit Changelog und Referenz auf die Tickets, z.B. GW-4821 oder GW-4975. Jede Änderung muss von Ops-Lead und Dev-Lead freigegeben werden."}
{"ts": "144:06", "speaker": "I", "text": "Haben die jüngsten Optimierungen auch die Build-Dauer beeinflusst?"}
{"ts": "144:10", "speaker": "E", "text": "Leicht, ja. Durch die zusätzlichen mTLS-Tests in Staging ist der Build-Lauf um ca. 3 Minuten länger, aber das reduziert das Risiko von SLA-Verletzungen massiv, was uns wichtiger ist als die paar Minuten längere Pipeline."}
{"ts": "144:00", "speaker": "I", "text": "Lassen Sie uns kurz zu den Lessons Learned aus Ticket GW-4821 zurückkommen. Was war aus Ihrer Sicht der Kern dieser Störung?"}
{"ts": "144:05", "speaker": "E", "text": "Der Kern war tatsächlich ein Race Condition im mTLS-Handshake zwischen Gateway und Auth-Backend. Wir hatten in der ursprünglichen Implementierung eine asynchrone Zertifikatsprüfung, die bei hoher Last zu Timeouts führte."}
{"ts": "144:15", "speaker": "I", "text": "Und wie hat das konkret das SLA-ORI-02 beeinflusst?"}
{"ts": "144:20", "speaker": "E", "text": "Wir hatten p95 Latenzen von über 300ms während der Spitzenlast, was deutlich über den 120ms lag. Das Monitoring schlug sofort Alarm, und wir sind nach Runbook RB-GW-011 auf die Blue-Umgebung zurückgefallen."}
{"ts": "144:33", "speaker": "I", "text": "Gab es beim Fallback Unterschiede zum beschriebenen Runbook?"}
{"ts": "144:37", "speaker": "E", "text": "Ja, im Runbook ist ein manueller DNS-Switch beschrieben, wir haben jedoch ein automatisiertes Failover via Consul-Redirect genutzt, weil das schneller war. Diese Abweichung ist aber noch nicht dokumentiert."}
{"ts": "144:50", "speaker": "I", "text": "Sie erwähnten vorhin Abhängigkeiten zu anderen Plattformkomponenten – können Sie das in Bezug auf die Auth-Integration ausführen?"}
{"ts": "144:55", "speaker": "E", "text": "Klar, das Gateway hängt stark vom AuthN/AuthZ-Service OrionAuth ab, der wiederum Tokens gegen unser zentrales IAM validiert. Fällt dort die Latenz ab, wirkt sich das 1:1 auf die Gateway-Response-Zeiten aus."}
{"ts": "145:08", "speaker": "I", "text": "Wie kontrollieren Sie das im laufenden Betrieb?"}
{"ts": "145:12", "speaker": "E", "text": "Wir haben synthetische mTLS-Transaktionen, die alle 30 Sekunden laufen, plus ein Latenz-Breakdown nach Hop. Das erlaubt uns, schnell zu sehen, ob die Verzögerung im Gateway oder im Auth-Backend liegt."}
{"ts": "145:24", "speaker": "I", "text": "Gab es seit GW-4821 weitere Anpassungen an der Latenzüberwachung?"}
{"ts": "145:28", "speaker": "E", "text": "Ja, wir haben Thresholds dynamisch gemacht. Früher war der Alert fix bei 100ms, jetzt orientiert er sich am 30-Tage-Median plus 20%. Das reduziert False Positives bei Lastspitzen."}
{"ts": "145:42", "speaker": "I", "text": "Das klingt sinnvoll. Wie fließt so etwas in die Runbook-Pflege ein?"}
{"ts": "145:46", "speaker": "E", "text": "Wir führen nach jedem Major-Incident ein Runbook-Review durch. Änderungen wie diese werden als Patch-Version RB-GW-011.x gepflegt, mit Change-Log im internen Wiki."}
{"ts": "145:58", "speaker": "I", "text": "Gibt es noch offene Punkte aus dem Review zu GW-4821?"}
{"ts": "146:02", "speaker": "E", "text": "Offen ist noch die Automatisierung des Zertifikats-Rollovers, um mTLS-Handshake-Probleme präventiv zu vermeiden. Das ist im RFC-GW-092 dokumentiert, aber noch nicht implementiert."}
{"ts": "146:00", "speaker": "I", "text": "Lassen Sie uns den Faden bei den Deployments aufnehmen. Wie genau haben Sie das mTLS-Auth-Subsystem in den Blue/Green-Flow integriert?"}
{"ts": "146:05", "speaker": "E", "text": "Wir haben das mTLS-Subsystem so eingebunden, dass während des Blue/Green-Wechsels beide Stacks parallel gültige Zertifikate aus dem internen PKI-Pool erhalten. Das minimiert Downtime, aber erfordert, dass der Cert-Renewal-Daemon doppelt läuft."}
{"ts": "146:14", "speaker": "I", "text": "Gab es dabei Latenzspitzen, die SLA-ORI-02 in Gefahr gebracht haben?"}
{"ts": "146:18", "speaker": "E", "text": "Ja, kurzzeitig. Insbesondere beim Umschalten der Upstream-Auth-Checks haben wir Peaks von 135 ms gesehen. Wir haben das über den Canary-Mode abgefangen und in der Metrik-Pipeline markiert, um keine SLA-Violations zu triggern."}
{"ts": "146:28", "speaker": "I", "text": "Und wie wird das in den Runbooks dokumentiert?"}
{"ts": "146:32", "speaker": "E", "text": "In RB-GW-011 gibt es seit Revision 4 ein Kapitel ‘Dual Cert Mode during B/G’. Das kam aus den Lessons Learned zu GW-4821, wo uns genau so ein Umschaltproblem eine Auth-Blockade verursachte."}
{"ts": "146:42", "speaker": "I", "text": "Können Sie das Ereignis GW-4821 noch mal kurz einordnen, bitte?"}
{"ts": "146:46", "speaker": "E", "text": "Sicher. Damals hat der neue Green-Stack ein mTLS-Handshake-Failure produziert, weil das Root-Cert nicht synchronisiert war. Die Load-Balancer haben dann reihenweise 502er geliefert, bis wir manuell auf Blue zurückgeschaltet haben."}
{"ts": "146:59", "speaker": "I", "text": "Das klingt nach einer klaren Cross-Komponenten-Kette von Problemen."}
{"ts": "147:03", "speaker": "E", "text": "Genau. Auth-Subsystem, LB-Config und das Deployment-Skript haben alle ineinander gegriffen. Wir haben daraus gelernt, dass Pre-Deployment Checks für Zertifikatsvalidität zwingend vor dem Traffic-Shift laufen müssen."}
{"ts": "147:15", "speaker": "I", "text": "Wie stellen Sie sicher, dass diese Checks auch bei Zeitdruck nicht übersprungen werden?"}
{"ts": "147:19", "speaker": "E", "text": "Wir haben im Jenkins-Pipeline-Template einen Gate-Step eingeführt, der ohne gültigen Pre-Check-Token den Deploy-Job nicht weiterführt. Das ist im Runbook festgehalten und hat seitdem keine Skip-Events mehr produziert."}
{"ts": "147:30", "speaker": "I", "text": "Wie wirkt sich diese zusätzliche Kontrolle auf die Time-to-Deploy aus?"}
{"ts": "147:34", "speaker": "E", "text": "Minimal, so etwa 40 Sekunden extra. Die SLA-Überwachung ist davon unbeeinträchtigt, da wir Deployments meist außerhalb der Peak-Zeiten fahren."}
{"ts": "147:42", "speaker": "I", "text": "Würden Sie sagen, dass die derzeitige Architektur resilient genug ist, um ähnliche Incidents abzufangen?"}
{"ts": "147:46", "speaker": "E", "text": "Für das Auth-Thema ja. Aber wir haben noch Lücken bei den Rate-Limitern, wenn diese mit hoher Last und gleichzeitigen Zertifikatswechseln umgehen müssen. Das wird einer der nächsten Optimierungsschwerpunkte."}
{"ts": "148:00", "speaker": "I", "text": "Lassen Sie uns direkt bei den Deployment-Abläufen einsteigen – wie läuft aktuell ein Blue/Green Deploy des Orion Edge Gateway ab, insbesondere mit Blick auf die mTLS-Integration?"}
{"ts": "148:05", "speaker": "E", "text": "Aktuell nutzen wir für Blue/Green ein zweistufiges Rollout-Schema. Erst wird die Green-Umgebung mit aktuellen Binaries und den neuen mTLS-Zertifikaten aus unserem internen PKI-Cluster ausgestattet. Danach führen wir einen Canary-Traffic-Shift von 5 % durch, um die p95-Latenz gegen SLA-ORI-02 zu messen."}
{"ts": "148:12", "speaker": "I", "text": "Wie verknüpfen Sie da konkret das Latenz-Monitoring mit dem Auth-Subsystem?"}
{"ts": "148:18", "speaker": "E", "text": "Wir haben im Monitoring-Stack ein zusammengesetztes Dashboard, das sowohl die Gateway-Latenzen als auch die Antwortzeiten des mTLS-Handshake-Endpunkts der Auth-API anzeigt. Das basiert auf Prometheus-Metriken und einem Alert-Rule-Set aus dem Runbook RB-GW-011, Abschnitt 4.2."}
{"ts": "148:26", "speaker": "I", "text": "Gab es beim letzten Deployment Abweichungen vom Runbook?"}
{"ts": "148:30", "speaker": "E", "text": "Ja, leicht. Im Runbook ist ein 15 Minuten Canary-Window vorgesehen. Beim letzten Mal haben wir aufgrund einer erhöhten Error-Rate im Auth-Subsystem das Window auf 30 Minuten verlängert, um auszuschließen, dass die SSL-Zertifikatsrotation, die aus Incident GW-4821 als Lesson Learned implementiert wurde, Einfluss hatte."}
{"ts": "148:39", "speaker": "I", "text": "Was genau war die Kernerkenntnis aus GW-4821?"}
{"ts": "148:44", "speaker": "E", "text": "Damals hatten wir ein abgelaufenes Zwischenzertifikat im Gateway-Truststore, das zu massiv erhöhten Handshake-Zeiten führte. Wir haben daraus gelernt, dass die Zertifikatsrotation im Deployment-Prozess stattfinden muss, nicht ad hoc während des Betriebs."}
{"ts": "148:52", "speaker": "I", "text": "Und wie spielt das mit anderen Plattformkomponenten zusammen?"}
{"ts": "148:57", "speaker": "E", "text": "Das hängt eng mit dem Rate Limiter zusammen: Wenn der Auth-Handshake länger dauert, puffert der Rate Limiter mehr Requests, was wiederum die Latenz erhöht. Diese Cross-Komponenten-Interaktion ist im Architektur-Diagramm unter ORI-ARCH-05 dokumentiert."}
{"ts": "149:05", "speaker": "I", "text": "Sind diese Diagramme im täglichen Betrieb im Einsatz?"}
{"ts": "149:10", "speaker": "E", "text": "Ja, vor allem im Incident-Channel. Wenn ein Alert auf SLA-ORI-02 triggert, posten wir automatisch einen Snapshot des Diagramms in unseren Chat, damit Ops und Dev das gleiche Bild haben."}
{"ts": "149:18", "speaker": "I", "text": "Das klingt pragmatisch. Welche Optimierungen sehen Sie noch?"}
{"ts": "149:23", "speaker": "E", "text": "Wir überlegen, das Canary-Window dynamisch anhand der realen Latenzverteilung zu steuern. Außerdem könnten wir mTLS-Zertifikatsprüfungen parallelisieren, um den Einfluss auf den Rate Limiter zu reduzieren."}
{"ts": "149:30", "speaker": "I", "text": "Würde das Änderungen am Runbook RB-GW-011 erfordern?"}
{"ts": "149:36", "speaker": "E", "text": "Definitiv. Abschnitt 5.1 müsste ergänzt werden, um dynamische Canary-Parameter zu berücksichtigen, und es bräuchte einen neuen Validierungsschritt vor dem Green-Cutover."}
{"ts": "149:36", "speaker": "I", "text": "Lassen Sie uns nochmal auf den aktuellen Rollout-Plan eingehen – wie ist der Stand beim Blue/Green Deployment des Gateways?"}
{"ts": "149:44", "speaker": "E", "text": "Wir haben derzeit das Green-Cluster live in der PreProd-Umgebung. Der Blue-Cluster ist bereits mit mTLS zum Auth-Subsystem verbunden, aber wir testen noch die Rate-Limiting-Regeln, um SLA-ORI-02 nicht zu gefährden."}
{"ts": "149:58", "speaker": "I", "text": "Gab es dabei schon messbare Werte im p95 Bereich, die kritisch werden könnten?"}
{"ts": "150:05", "speaker": "E", "text": "Ja, in zwei Nightly-Builds lag die p95 Latenz bei 128 ms, weil die Auth-API bei hoher Last Zertifikatsprüfungen sequenziell verarbeitet hat. Das haben wir aus GW-4821 gelernt und jetzt das Caching im mTLS Handshake verbessert."}
{"ts": "150:20", "speaker": "I", "text": "Interessant, also war die Zertifikatsrotation der Knackpunkt?"}
{"ts": "150:24", "speaker": "E", "text": "Genau, während der Rotation wurden neue Zertifikate erst nach vollständiger Verifizierung akzeptiert, was zu Blockaden führte. Laut Runbook RB-GW-011 hätten wir parallelisieren sollen, das war eine Abweichung."}
{"ts": "150:39", "speaker": "I", "text": "Wie gehen Sie jetzt sicher, dass solche Abweichungen nicht wieder passieren?"}
{"ts": "150:44", "speaker": "E", "text": "Wir haben eine Runbook-Versionierung eingeführt – jede Änderung bekommt eine ID wie RB-GW-011.v3 – und wir fahren Dry-Runs vor Zertifikatswechseln. Außerdem ist im Monitoring-Dashboard ein SLA-Alert direkt mit dem Deployment-Tool verknüpft."}
{"ts": "150:59", "speaker": "I", "text": "Welche Cross-Komponenten-Effekte haben Sie bei diesen Tests beobachtet?"}
{"ts": "151:04", "speaker": "E", "text": "Beim letzten Test hat ein aggressives Rate-Limiting im Gateway die Auth-API gedrosselt, was wiederum den Session-Cache im Downstream-Backend leerte. Dadurch stieg die Fehlerquote temporär auf 2 %, obwohl das Gateway selbst im SLA war."}
{"ts": "151:20", "speaker": "I", "text": "Das klingt nach einem klassischen Multi-Hop-Problem zwischen Subsystemen."}
{"ts": "151:24", "speaker": "E", "text": "Absolut. Deswegen haben wir jetzt ein kombiniertes SLO-Dashboard, das Gateway-Metriken und Auth- sowie Backend-Latenzen aggregiert. So sehen wir Korrelationen sofort."}
{"ts": "151:37", "speaker": "I", "text": "Wenn Sie auf die nächsten drei Monate schauen – welche Entscheidungen müssen wir treffen?"}
{"ts": "151:42", "speaker": "E", "text": "Wir müssen entscheiden, ob wir die Zertifikatsrotation asynchron gestalten, auch wenn das mehr Komplexität bringt. Außerdem steht an, ob wir das Rate-Limiting dynamisch anpassen, was Auswirkungen auf die Auth-API hätte."}
{"ts": "151:56", "speaker": "I", "text": "Sehen Sie dabei Risiken für die Inbetriebnahme?"}
{"ts": "152:00", "speaker": "E", "text": "Ja, bei dynamischem Rate-Limiting besteht die Gefahr von Unterschreitungen der Auth-API Cache-Hitrate, was SLA-ORI-02 verletzen könnte. Wir planen daher einen gestaffelten Rollout mit Shadow Traffic, um das Risiko zu minimieren."}
{"ts": "152:00", "speaker": "I", "text": "Lassen Sie uns direkt auf die Blue/Green-Strategie eingehen. Wie oft haben Sie in den letzten drei Monaten tatsächlich einen vollständigen Blue/Green-Cycle für das Gateway gefahren?"}
{"ts": "152:06", "speaker": "E", "text": "Also, wenn ich ehrlich bin, nur zweimal komplett. In den übrigen Fällen haben wir eher ein Canary-Style Rollout gemacht, um Latenzspitzen schneller zu erkennen. Das steht so nicht im RB-GW-011, ist aber in der Praxis oft effizienter."}
{"ts": "152:14", "speaker": "I", "text": "Das heißt, Sie weichen bewusst vom Runbook ab. Gab es dadurch Probleme mit der SLA-ORI-02 Überwachung?"}
{"ts": "152:21", "speaker": "E", "text": "Ja, beim ersten Mal hatten wir eine p95 Latenz von knapp 135 ms, weil die Canary-Instanz noch nicht alle mTLS-Keys preloaded hatte. Das hat uns dann eine Warnung im SLA-Dashboard eingebracht, kein harter Verstoß, aber knapp dran."}
{"ts": "152:30", "speaker": "I", "text": "War das im Kontext von GW-4821, dem Zertifikatsrotation-Incident?"}
{"ts": "152:36", "speaker": "E", "text": "Genau, das war kurz danach. GW-4821 hatte gezeigt, dass unser Key-Cache im Auth-Subsystem nicht synchron genug war. Wir mussten den Preload-Mechanismus erweitern, um während Blue/Green keine Cold Starts zu haben."}
{"ts": "152:46", "speaker": "I", "text": "Wie genau wurde der Preload implementiert?"}
{"ts": "152:51", "speaker": "E", "text": "Wir haben im Deployment-Skript eine Phase eingefügt, die die mTLS-Zertifikate aus dem zentralen Vault zieht und in beiden Stacks – Blue und Green – gleichzeitig lädt. Erst wenn die Latenz im Staging stabil ist, schalten wir um."}
{"ts": "153:02", "speaker": "I", "text": "Beeinflusst das Rate Limiting auf irgendeine Weise diesen Umschaltprozess?"}
{"ts": "153:07", "speaker": "E", "text": "Ja, indirekt. Während des Preloads simulieren wir Traffic, um die Auth-API zu wärmen. Das triggert aber die Ratenzähler. Deshalb haben wir im Runbook eine temporäre Whitelist-Regel eingeführt, die bei Deployments greift."}
{"ts": "153:18", "speaker": "I", "text": "Und diese Whitelist-Regel, ist die dauerhaft dokumentiert?"}
{"ts": "153:22", "speaker": "E", "text": "Nein, das ist genau einer der Punkte, wo wir vom dokumentierten RB-GW-011 abweichen. Wir haben es im internen Confluence vermerkt, aber die formale Runbook-Versionierung steht noch aus."}
{"ts": "153:30", "speaker": "I", "text": "Wie gehen Sie vor, um solche Abweichungen nachträglich in die offizielle Version zu bringen?"}
{"ts": "153:35", "speaker": "E", "text": "Wir sammeln die Änderungen während des Quartals, taggen sie mit einem Change-ID, z.B. CHG-GW-2024-07, und geben sie dann in den Runbook-Review-Workflow. Das dauert leider oft Wochen."}
{"ts": "153:44", "speaker": "I", "text": "Gab es in diesem Prozess schon Fälle, in denen eine nicht dokumentierte Abweichung später zu einem Incident geführt hat?"}
{"ts": "153:50", "speaker": "E", "text": "Ja, ein Beispiel war im Mai: Die Whitelist-Regel wurde vergessen zu aktivieren, weil sie nicht im offiziellen Ablauf stand. Folge: Bei der Umschaltung hat das Rate Limiting legitime Health-Checks geblockt, was zu einem 5-minütigen Ausfall führte."}
{"ts": "153:36", "speaker": "I", "text": "Zum Abschluss würde ich gerne noch verstehen, wie Sie aktuell Risiken für die Inbetriebnahme identifizieren und dokumentieren. Gibt es da einen festen Prozess?"}
{"ts": "153:45", "speaker": "E", "text": "Ja, wir nutzen ein kombiniertes Verfahren: monatliche Risk-Review-Meetings plus ein wöchentliches technisches Standup, in dem wir auch latent auftretende Latenzspikes gegen SLA-ORI-02 thematisieren. Alle Findings landen in unserem internen Confluence-Bereich, verlinkt mit den zugehörigen Jira-Tickets."}
{"ts": "153:59", "speaker": "I", "text": "Können Sie ein konkretes Beispiel nennen, das in den letzten Wochen aufgetreten ist?"}
{"ts": "154:04", "speaker": "E", "text": "Klar, Ticket RISK-224: Wir haben festgestellt, dass die Zertifikatsrotation im Auth-Subsystem, wie in GW-4821 beschrieben, immer noch einen kurzen Handshake-Delay produziert, der im ungünstigen Fall das p95-Limit überschreiten könnte, wenn gleichzeitig ein Rate-Limit-Reconfig läuft."}
{"ts": "154:17", "speaker": "I", "text": "Das klingt nach einer Abhängigkeit über mehrere Subsysteme hinweg, oder?"}
{"ts": "154:22", "speaker": "E", "text": "Genau. Das ist so ein klassischer Cross-Komponenten-Effekt: mTLS-Handshake hängt an der Auth-API, die wiederum bei Rate-Limit-Änderungen kurz CPU-Peaks hat. In der Summe kann das den Gateway-Threadpool ausreizen. Wir haben das im Runbook RB-GW-011 als neuen Check vor Deploy ergänzt."}
{"ts": "154:37", "speaker": "I", "text": "Gab es Diskussionen über mögliche Trade-offs, um dieses Risiko zu mitigieren?"}
{"ts": "154:42", "speaker": "E", "text": "Ja, zwei Varianten: a) Wir staffeln Zertifikatsrotation und Rate-Limit-Deploys strikt, was aber die CI/CD-Pipeline verlangsamt, oder b) wir investieren in eine asynchrone Handshake-Queue. Letzteres erfordert tiefe Eingriffe ins Gateway-Kernel-Modul."}
{"ts": "154:56", "speaker": "I", "text": "Wurde dazu schon eine Entscheidung getroffen?"}
{"ts": "155:00", "speaker": "E", "text": "Vorläufig ja – wir haben uns für Variante a) entschieden, um kurzfristig SLA-ORI-02 einzuhalten. Langfristig evaluieren wir b) im Rahmen von RFC-OG-145, das ist im Architekturboard für Q3 terminiert."}
{"ts": "155:12", "speaker": "I", "text": "Welche Evidenz hat das Gremium überzeugt, vorerst die konservative Variante zu wählen?"}
{"ts": "155:17", "speaker": "E", "text": "Die Monitoring-Daten aus den letzten vier Blue/Green Deployments, dokumentiert in den Metrics-Dumps MD-ORI-07 bis -10. Dort war klar ersichtlich, dass einfache zeitliche Entkopplung die Spikes unter 110ms hält. Keine SLA-Verletzung, daher geringeres Risiko."}
{"ts": "155:31", "speaker": "I", "text": "Und wie stellen Sie sicher, dass diese Lessons Learned nicht verloren gehen?"}
{"ts": "155:36", "speaker": "E", "text": "Wir haben seit diesem Jahr ein Runbook-Versionierungssystem in Git – jede Anpassung wie diese bekommt eine Commit-Message mit Ticket-Referenz und Link zum Monitoring-Snapshot. So ist die Historie jederzeit nachvollziehbar."}
{"ts": "155:47", "speaker": "I", "text": "Was wäre aus Ihrer Sicht der größte verbleibende Unsicherheitsfaktor für Go-Live?"}
{"ts": "155:52", "speaker": "E", "text": "Ehrlich gesagt: das Verhalten unter Lastspitzen, die aus gleichzeitigen Auth-Subsystem-Updates und externen API-Latenzen resultieren. Wir simulieren das im Staging, aber Production hat immer mehr Variablen – Stichwort Upstream-DNS-Lookups und unpredictable client bursts."}
{"ts": "156:00", "speaker": "I", "text": "Lassen Sie uns jetzt gezielt auf die ausstehenden Entscheidungen schauen: Welche konkreten Risiken sehen Sie aktuell für den Go-Live des Orion Edge Gateway?"}
{"ts": "156:05", "speaker": "E", "text": "Also, das größte Risiko ist derzeit die Stabilität der Auth-API unter Lastspitzen. Obwohl wir im Staging gute Werte hatten, gab es bei Lasttests mit simulierten 10k RPS leichte p95-Ausreißer über 130 ms, was formal SLA-ORI-02 verletzt."}
{"ts": "156:15", "speaker": "E", "text": "Hinzu kommt, ähm, dass die Rate-Limiting-Engine noch nicht alle edge cases aus Ticket GW-4972 berücksichtigt. Das betrifft vor allem Burst-Traffic im Millisekundenbereich."}
{"ts": "156:25", "speaker": "I", "text": "Verstehe. Gab es bewusst eingegangene Trade-offs, um die Build-Phase einzuhalten?"}
{"ts": "156:30", "speaker": "E", "text": "Ja, wir haben bei der mTLS-Handshake-Optimierung auf eine proprietäre Session-Reuse-Implementierung verzichtet, um die Kompatibilität mit unseren internen PKI-Richtlinien nicht zu gefährden. Das kostet uns ca. 5–7 ms pro Anfrage."}
{"ts": "156:40", "speaker": "E", "text": "Außerdem haben wir das Blue/Green Deployment laut RB-GW-011 so abgespeckt, dass nur alle zwei Wochen ein vollständiger Umschalt-Test läuft. Das spart Ressourcen, birgt aber das Risiko, dass seltene Umschaltfehler später auffallen."}
{"ts": "156:50", "speaker": "I", "text": "Welche evidenzbasierten Entscheidungen stehen in den nächsten drei Monaten an?"}
{"ts": "156:55", "speaker": "E", "text": "Wir müssen auf Basis der letzten drei Latenz-Reports (Report IDs LREP-042, -043, -044) entscheiden, ob wir die Auth-API vertikal skalieren oder den mTLS-Handshake asynchronisieren. Beide Wege haben Implikationen auf die Gesamtarchitektur."}
{"ts": "157:05", "speaker": "E", "text": "Zudem steht eine Entscheidung zur Versionierung der Runbooks an – aktuell liegt RB-GW-011 nur in Confluence, wir überlegen einen Git-basierten Workflow für Änderungen einzuführen."}
{"ts": "157:15", "speaker": "I", "text": "Gibt es für diese Entscheidungen bereits vorbereitete RFCs?"}
{"ts": "157:20", "speaker": "E", "text": "Ja, RFC-ORI-2024-07 deckt die mTLS-Optimierung ab, inklusive Impact-Analyse auf SLA-ORI-02. RFC-ORI-2024-08 behandelt die Runbook-Versionierung, inkl. Lessons Learned aus GW-4821."}
{"ts": "157:30", "speaker": "I", "text": "Wie wird entschieden, welchen Weg wir bei der Auth-API-Skalierung gehen?"}
{"ts": "157:35", "speaker": "E", "text": "Wir machen dazu ein Decision-Review-Meeting mit dem Architecture Board. Grundlage sind die Metriken der letzten Canary-Releases, die wir mit künstlich induzierten Burst-Patterns gefahren haben."}
{"ts": "157:45", "speaker": "E", "text": "Erst wenn wir nachweisen, dass die vertikale Skalierung keine signifikante Kostenspirale auslöst, werden wir diesen Weg freigeben – andernfalls priorisieren wir die asynchrone mTLS-Variante."}
{"ts": "157:55", "speaker": "I", "text": "Gibt es noch offene Risiken, die wir hier dokumentieren sollten?"}
{"ts": "158:00", "speaker": "E", "text": "Ja, ein unterschätztes Risiko ist die Abhängigkeit vom internen DNS-Cluster. Bei der Zertifikatsrotation in GW-4821 hatten wir einen 2‑Sekunden-DNS-Lag, der ungeplant durch die Gateway-Healthchecks ging. Das ist noch nicht voll adressiert."}
{"ts": "158:00", "speaker": "I", "text": "Lassen Sie uns noch einmal auf die Abhängigkeiten eingehen – gibt es seit unserem letzten Architektur-Review neue externe Schnittstellen, die das Gateway beeinflussen könnten?"}
{"ts": "158:05", "speaker": "E", "text": "Ja, tatsächlich. Seit dem letzten Review haben wir die Verbindung zum internen Audit-Log-Service v2 hinzugefügt. Der Service ist noch in der Beta, und wir mussten im Gateway einen zusätzlichen gRPC-Stub implementieren, um Audit-Events in Echtzeit zu streamen. Das bringt eine neue Latenzkomponente ins Spiel."}
{"ts": "158:15", "speaker": "I", "text": "Das klingt nach einem potenziellen SLA-Risiko. Gibt es Messwerte, wie stark diese neue Komponente die p95 Latenz beeinflusst?"}
{"ts": "158:20", "speaker": "E", "text": "Wir haben in den letzten zwei Wochen Messungen im Staging gemacht. Bei aktivierter Audit-Log-Anbindung stieg die p95 Latenz von 104 ms auf 112 ms. Das ist noch unter den 120 ms aus SLA-ORI-02, aber die Spanne ist kleiner geworden, weniger Puffer also."}
{"ts": "158:30", "speaker": "I", "text": "Wurde dafür ein Ticket erstellt, um die Optimierung frühzeitig anzugehen?"}
{"ts": "158:33", "speaker": "E", "text": "Ja, das ist Ticket GW-4972. Darin schlagen wir vor, den gRPC-Stream asynchron zu puffern, sodass Requests nicht direkt auf die Bestätigung des Audit-Services warten müssen."}
{"ts": "158:42", "speaker": "I", "text": "Wie wirkt sich dieser asynchrone Ansatz auf die Korrektheit der Audit-Daten aus?"}
{"ts": "158:45", "speaker": "E", "text": "Wir haben einen Mechanismus vorgesehen, der im Fehlerfall den Puffer auf die lokale Disk schreibt und beim Wiederverbinden nachsendet. Das minimiert Datenverlust, allerdings besteht ein minimales Risiko, dass bei einem gleichzeitigen Gateway- und Disk-Ausfall Events verloren gehen."}
{"ts": "158:56", "speaker": "I", "text": "Haben wir dazu schon eine Abweichung oder Ergänzung im Runbook RB-GW-011 hinterlegt?"}
{"ts": "159:00", "speaker": "E", "text": "Ja, im letzten Commit auf die Runbook-Repo-Branch 'gw-deploy' haben wir einen Abschnitt 'Audit-Stream Recovery' ergänzt. Darin wird beschrieben, wie der persistente Puffer manuell inspiziert und re-sent werden kann."}
{"ts": "159:09", "speaker": "I", "text": "Gut. Noch eine Frage: Wir hatten im letzten Incident-Drill festgestellt, dass die mTLS-Cert-Rotation mit laufenden Streams zu kurzzeitigen Abbrüchen führte. Wurde das adressiert?"}
{"ts": "159:15", "speaker": "E", "text": "Ja, das war Teil des Fixes aus GW-4821. Wir haben den Cert-Rotation-Handler so angepasst, dass er offene Streams über einen Graceful-Restart-Prozess migriert. Das ist jetzt auch im internen RFC-Doc RFC-GW-017 dokumentiert."}
{"ts": "159:26", "speaker": "I", "text": "Sehr gut. Abschließend: Welche kurzfristigen Risiken sehen Sie, wenn wir den Audit-Service v2 gleich zum Go-Live aktivieren?"}
{"ts": "159:31", "speaker": "E", "text": "Das größte Risiko ist tatsächlich die noch nicht ausgereifte Skalierung des Audit-Service. Wenn dieser unter Last ins Straucheln gerät, könnte er Backpressure ins Gateway geben. Unser Puffer hilft, aber wir haben im Stresstest gesehen, dass bei mehr als 500 gleichzeitigen Requests die Latenz deutlich schwankt."}
{"ts": "159:44", "speaker": "I", "text": "Wäre ein gestaffeltes Aktivieren, also Feature-Flag pro Service-Kunde, eine Option?"}
{"ts": "159:48", "speaker": "E", "text": "Ja, das ist sogar unser Favorit. Wir haben bereits einen Feature-Flag-Mechanismus im Config-Service. Damit könnten wir Audit-Logging für 10 % der Kunden aktivieren, Erfahrungen sammeln und dann sukzessive hochfahren. Das ist auch in der Entscheidungsvorlage DV-GW-2024-05 als empfohlene Maßnahme festgehalten."}
{"ts": "160:00", "speaker": "I", "text": "Lassen Sie uns bitte noch einmal genauer auf die anstehenden Entscheidungen eingehen, vor allem auf die, die Sie in den nächsten drei Monaten anstreben."}
{"ts": "160:05", "speaker": "E", "text": "Ja, also eine der wichtigsten Entscheidungen betrifft die finale Auswahl der Rate Limiting Engine. Wir haben aktuell einen Proof-of-Concept mit der internen Komponente RLX-4 laufen, aber parallel evaluieren wir eine leichtgewichtigere Lösung aus dem Open-Source-Bereich, um die Latenz im Sinne von SLA-ORI-02 noch besser zu kontrollieren."}
{"ts": "160:14", "speaker": "I", "text": "Welche Kriterien setzen Sie dabei an?"}
{"ts": "160:19", "speaker": "E", "text": "Zum einen natürlich die Messwerte aus unserem synthetischen Load-Test-Framework, zum anderen die Integrationsfähigkeit mit der mTLS-Schicht, die wir aus GW-4821 gelernt haben kritisch ist. Außerdem spielt der Pflegeaufwand für Runbooks wie RB-GW-014 eine Rolle, damit wir im Incident-Fall nicht von proprietären Eigenheiten überrascht werden."}
{"ts": "160:32", "speaker": "I", "text": "Sie sprachen die Lessons Learned aus Ticket GW-4821 an – wie fließen die nun in die Architekturentscheidungen ein?"}
{"ts": "160:38", "speaker": "E", "text": "Das Ticket hat uns gezeigt, dass Zertifikatsrotationen nicht isoliert betrachtet werden dürfen. Bei RLX-4 gab es z. B. Abbrüche bei parallelen Handshakes. Deshalb betrachten wir jetzt in jedem Architektur-Review die komplette Kette: Auth-API, Rate Limiter, Gateway-Core. Diese cross-subsystem Analyse ist jetzt fester Bestandteil des RFC-Prozesses."}
{"ts": "160:54", "speaker": "I", "text": "Und wie sieht es mit den Betriebsprozessen aus – gab es Anpassungen nach den letzten Simulationen?"}
{"ts": "161:00", "speaker": "E", "text": "Ja, wir haben das Blue/Green Deployment Runbook RB-GW-011 um einen zusätzlichen Pre-Switch Check erweitert, der explizit die Auth-API-Responsezeiten prüft. Das kam nach einer Simulation, bei der wir sahen, dass Blue zwar fehlerfrei war, Green aber wegen eines Auth-Timeouts das SLA verletzt hätte."}
{"ts": "161:15", "speaker": "I", "text": "Gibt es Risiken, die Sie für die Go-Live-Phase noch als kritisch einstufen?"}
{"ts": "161:20", "speaker": "E", "text": "Ja, zwei Hauptpunkte: Erstens die Abhängigkeit von unserem externen Zertifikatsprovider – das ist im Risikoregister unter R-ORI-07 vermerkt. Zweitens die noch nicht abschließend getestete Lastverteilung auf drei Rechenzentren. Beide Punkte haben klare Mitigationspläne, aber wir müssen das Monitoring dafür noch in Prometheus-Dashboards abbilden."}
{"ts": "161:35", "speaker": "I", "text": "Welche Trade-offs mussten Sie in der Architektur bewusst eingehen?"}
{"ts": "161:40", "speaker": "E", "text": "Wir haben uns entschieden, vorerst auf eine zentrale Policy Engine zu verzichten, um Komplexität zu reduzieren. Das bedeutet, dass Rate Limiting und Auth Policies lokal pro Gateway-Node konfiguriert werden. Vorteil: geringere Latenz. Nachteil: höhere Pflegekosten bei Policy-Änderungen."}
{"ts": "161:54", "speaker": "I", "text": "Wie dokumentieren Sie solche Entscheidungen?"}
{"ts": "162:00", "speaker": "E", "text": "Über unser internes Decision Record Repository (DR-ORI-*). Jede wichtige Architekturentscheidung bekommt eine ID, z. B. DR-ORI-12 für diesen Policy-Trade-off, und enthält Messwerte, Tickets, betroffene Runbooks und einen klaren Rückrollplan."}
{"ts": "162:12", "speaker": "I", "text": "Wann werden die nächsten evidenzbasierten Entscheidungen fallen?"}
{"ts": "162:17", "speaker": "E", "text": "Spätestens in Woche 3 des nächsten Quartals, wenn wir die finalen p95-Latenzwerte aus dem Staging-Cluster unter Produktionslastbedingungen haben. Diese Tests werden auch die neuen Checks aus RB-GW-011 einbeziehen, damit wir eine realistische Go-Live-Entscheidung treffen können."}
{"ts": "161:35", "speaker": "I", "text": "Lassen Sie uns noch einmal auf das Runbook RB-GW-011 zurückkommen – wie oft haben Sie persönlich diese Blue/Green Deployments im Orion Edge Gateway durchgeführt?"}
{"ts": "161:40", "speaker": "E", "text": "Also, in den letzten sechs Monaten, ähm, haben wir es viermal produktiv genutzt. Meistens bei größeren API-Version-Switches, vor allem wenn Auth-Module betroffen waren, um Downtime zu vermeiden."}
{"ts": "161:48", "speaker": "I", "text": "Gab es dabei Abweichungen zwischen Runbook und tatsächlicher Ausführung?"}
{"ts": "161:52", "speaker": "E", "text": "Ja, in einem Fall mussten wir die Health-Check-Intervalle verkürzen, weil die mTLS-Handshake-Phase länger dauerte als in RB-GW-011 vorgesehen. Wir haben das als Änderungsvorschlag im Confluence hinterlegt, aber noch nicht in der offiziellen Version eingepflegt."}
{"ts": "162:01", "speaker": "I", "text": "Und wie wirkt sich diese Änderung auf unser SLA-ORI-02 aus, speziell p95 Latenz unter 120 ms?"}
{"ts": "162:07", "speaker": "E", "text": "Die verkürzten Intervalle helfen, schneller zu erkennen, ob eine Green-Instanz tatsächlich die Latenz einhält. In zwei Deployments konnten wir so Unterschreitungen der SLA-Grenze proaktiv abfangen, bevor es beim Endnutzer auffiel."}
{"ts": "162:16", "speaker": "I", "text": "Das Monitoring für SLA-ORI-02 – ist das vollständig im Gateway selbst implementiert oder teilweise ausgelagert?"}
{"ts": "162:21", "speaker": "E", "text": "Teilweise ausgelagert. Wir haben ein internes Modul, das Latenzen misst, aber die korrelierte Analyse läuft in der zentralen Observability-Plattform, wo auch die Auth-API-Responsezeiten und Rate-Limiting-Verzögerungen einfließen."}
{"ts": "162:30", "speaker": "I", "text": "Das heißt, Abhängigkeiten zwischen Auth und Rate Limiting werden schon aggregiert?"}
{"ts": "162:34", "speaker": "E", "text": "Genau, das war eine der Lessons Learned aus Ticket GW-4821 – damals haben wir den Zusammenhang zu spät erkannt, weil die Metriken separiert waren. Jetzt laufen sie im gemeinsamen Dashboard mit Korrelationstriggern."}
{"ts": "162:43", "speaker": "I", "text": "Wie stabil ist aktuell die mTLS-Integration, gerade bei hoher Last?"}
{"ts": "162:47", "speaker": "E", "text": "Seit wir das Session-Caching für Zertifikate aktiviert haben, ist die Stabilität deutlich besser. Vorher kam es bei >5k TPS zu sporadischen Handshake-Timeouts, was das Gateway verlangsamt hat."}
{"ts": "162:56", "speaker": "I", "text": "Kommen wir zu Risiken: Was sehen Sie als größte Gefahr für die Inbetriebnahme?"}
{"ts": "163:00", "speaker": "E", "text": "Das größte Risiko ist aus meiner Sicht die gleichzeitige Einführung des neuen Rate-Limiting-Algorithmus und der geänderten Auth-API. Beides berührt die Pipeline, die für SLA-ORI-02 kritisch ist."}
{"ts": "163:08", "speaker": "I", "text": "Gab es bewusste Trade-offs bei der Architektur?"}
{"ts": "163:12", "speaker": "E", "text": "Ja, wir haben uns gegen eine vollständige Entkopplung von Auth und Gateway entschieden, um die Latenz zu minimieren. Das spart im Schnitt 15 ms, erhöht aber die Komplexität bei Änderungen – Ticket GW-4821 war genau so ein Fall."}
{"ts": "162:07", "speaker": "I", "text": "Können Sie bitte schildern, wie RB-GW-011 in den letzten beiden Releases angewendet wurde?"}
{"ts": "162:12", "speaker": "E", "text": "Ja, beim Release 1.4 und 1.4.1 haben wir das Blue/Green Deployment exakt nach RB-GW-011 gefahren. Wir haben die Green-Umgebung initial mit mTLS-Auth gegen die Staging-Auth-API getestet, bevor wir den Traffic umgeschaltet haben."}
{"ts": "162:23", "speaker": "I", "text": "Gab es Abweichungen vom Runbook während des Rollouts?"}
{"ts": "162:27", "speaker": "E", "text": "Minimal. Beim Pre-Switch Check mussten wir einen zusätzlichen Rate-Limit-Test einbauen, da wir in GW-4821 gelernt haben, dass Lastspitzen beim Auth-Handshake die p95 Latenz sofort über 120 ms treiben können."}
{"ts": "162:39", "speaker": "I", "text": "Wie genau wird die mTLS-Integration im Gateway umgesetzt?"}
{"ts": "162:44", "speaker": "E", "text": "Wir haben einen dedicated TLS-Termination-Container im Gateway-Pod, der Client-Zertifikate gegen unsere interne CA validiert. Danach leitet ein Sidecar-Proxy die Requests an die Auth-API weiter. Das Mapping der Subject-DNs zu internen User-IDs erfolgt inline."}
{"ts": "162:57", "speaker": "I", "text": "Und das Monitoring für SLA-ORI-02, wie greifen da Auth-API und Rate Limiter zusammen?"}
{"ts": "163:03", "speaker": "E", "text": "Wir messen die End-to-End-Latenz ab TLS-Handshake bis zur Antwort des Upstream-Services. Peaks kommen oft aus Auth-API-Timeouts, die wiederum den Rate Limiter blockieren. Darum haben wir Alerts gekoppelt, sodass bei >110 ms p95 sofort beides geprüft wird."}
